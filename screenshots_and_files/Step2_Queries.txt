Index: Courses Index
Query: search=github&%24top=1
URL: https://projectthreecognitivesearch.search.windows.net/indexes/courses-index/docs?api-version=2021-04-30-Preview&search=github&%24top=1

JSON:

{
  "@odata.context": "https://projectthreecognitivesearch.search.windows.net/indexes('courses-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 6.255562,
      "RowKey": "NDZlOTIyN2QtNjdjNC00NmI3LTkxNDUtNTFiODRjNDZhZGY10",
      "source": "MS Learn",
      "title": "Create and host web sites by using GitHub Pages",
      "description": "Learn how to host your personal, organization, and project sites for free with GitHub Pages.",
      "level": "beginner",
      "role": "developer",
      "product": "github",
      "duration": "72",
      "rating_count": "118",
      "rating_average": "2022-04-08T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-host-web-sites-github-pages/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "https://enrichedstorageaccount.blob.core.windows.net/courses/courses.csv;408",
      "skills": [
        "personal, organization",
        "project sites",
        "GitHub Pages"
      ],
      "instructor": null
    }
  ]
}



Index: library Index
Query: Sandhya Narayanan
URL: https://projectthreecognitivesearch.search.windows.net/indexes/library-index/docs?api-version=2021-04-30-Preview&search=Sandhya%20Narayanan

{
  "@odata.context": "https://projectthreecognitivesearch.search.windows.net/indexes('library-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 1.0775142,
      "content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nR E S E A R C H\n\nNarayanan et al. J Big Data            (2020) 7:13  \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence:   \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et  al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et  al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n\n\nPage 5 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to  ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table  2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I) −\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n• Let x significant features are identified from feature set (F ) represented as Fx ⊂ F\n• An active customer consists of significant feature having information Gain value \n\ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′x are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the  m\n\nth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using  L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1 + b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1, −1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2  G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure  3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9  GB dataset. But for DMRDF model time taken for 18  GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n9  GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nT\nim\n\ne \nT\n\nak\nen\n\n in\n s\n\nec\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24  months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope with the limitations of deep learn-\ning matrix factorization integrated with DMRDF can be adapted.\n\nAbbreviations\nDMRDF: Distributed Memory-based Resilient Dataset Filter; FIG: Feature information gain; RDD: Resilient distributed \ndataset; SVM: Support vector machine; LR: Logistic regression; LSA: Latent semantic analysis; PA: Prediction accuracy; \nP@R: Precision; R@R: Recall; MF: Matrix factorization.\n\nAcknowledgements\nNot applicable.\n\nAuthors’ contributions\nSN designed and implemented the model for Pre-launch product prediction. SN analysed and interpreted the customer \nreviews and ratings dataset regarding the pre-launch product prediction. PS supervised the design, implementation \nand analysis of the model for pre-launch product prediction. MC was a major contributor in writing the manuscript. All \nauthors read and approved the final manuscript.\n\nFunding\nNot applicable.\n\nAvailability of data and materials\nThe datasets generated and/or analysed during the current study are available in the Kaggle repository. [snap.stanford.\nedu/data/web-Amazon.html] [40] and [http://www.kaggl e.com/Promp tClou dHQ/flipk art-produ cts] [39].\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAuthor details\n1 Information Technology, School of Engineering, Cochin University of Science & Technology, Kochi 682022, India. \n2 Department of Computer Science, Cochin University of Science & Technology, Kochi 682022, India. 3 Department \nof Ship Technology, Cochin University of Science & Technology, Kochi 682022, India. \n\nReceived: 25 October 2019   Accepted: 17 February 2020\n\nhttp://www.kaggle.com/PromptCloudHQ/flipkart-products\n\n\nPage 14 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nReferences\n 1. Lau RY, Liao SY, Kwok RC, Xu K, Xia Y, Li Y. Text mining and probabilistic modeling for online review spam detection. \n\nACM Trans Manag Inform Syst. 2011;2(4):25.\n 2. Lin X, Li Y, Wang X. Social commerce research: definition, research themes and the trends. Int J Inform Manag. \n\n2017;37:190–201.\n 3. Matos CAD, Rossi CAV. Word-of-mouth communications in marketing: a meta-analytic review of the antecedents \n\nand moderators. J Acad Market Sci. 2008;36(4):578–96.\n 4. Jeon S, et al. Redundant data removal technique for efficient big data search processing. Int J Softw Eng Appl. \n\n2013;7.4:427–36.\n 5. Dave K, Lawrence S, and Pennock D. Mining the peanut gallery: opinion extraction and semantic classification of \n\nproduct reviews. WWW’2003.\n 6. Zhou Y, Wilkinson D, Schreiber R, Pan R. Large-scale parallel collaborative filtering for the netflix prize. 2008. p. \n\n337–48. https ://doi.org/10.1007/978-3-540-68880 -8_32.\n 7. Zhang KZK, Benyoucef M. Consumer behavior in social commerce: a literature review. Dec Support Syst. \n\n2016;86:95–108.\n 8. Cui Geng, Lui Hon-Kwong, Guo Xiaoning. The effect of online consumer reviews on new product sales. Int J Electron \n\nComm. 2012;17(1):39–58.\n 9. Manek AS, Shenoy PD, Mohan MC, et al. Detection of fraudulent and malicious websites by analysing user reviews \n\nfor online shopping websites. Int J Knowl Web Intell. 2016;5(3):171–89. https ://doi.org/10.1007/s1128 0-015-0381-x.\n 10. Singh S, and Singh N. Big data analytics. In: Proceedings of the 2012 international conference on communication, \n\ninformation & computing technology (ICCICT ), institute of electrical and electronics engineers (IEEE). 2012. p. 1–4. \nhttp://dx.doi.org/10.1109/iccic t.2012.63981 80.\n\n 11. Demchenko Yuri et al. Addressing big data challenges for scientific data infrastructure. In: IEEE 4th Int. conference \ncloud computing technology and science (CloudCom). 2012.\n\n 12. Sihong Xie, Guan Wang, Shuyang Lin and Yu Philip S. Review spam detection via time-series pattern discovery. In: \nACM Proceedings of the 21st international conference companion on World Wide Web. 2012. p. 635–6.\n\n 13. Koren Y, Bell R, Volinsky C. matrix factorization technique for recommender systems. Computer. 2009;8:30–7.\n 14. Salakhutdinov R, Mnih A, & Hinton G. Restricted boltzmann machines for collaborative filtering. In: Proc. of the 24th \n\nInt. conference on machine learning. 2007. p. 791–8.\n 15. Hao MA, King I, Lyu MR. Learning to recommend with explicit and implicit social relations. ACM Trans Intell Syst \n\nTechnol. 2011;2(3):29.\n 16. Bandakkanavar V, Ramesh M, Geeta V. A survey on detection of reviews using sentiment classification of methods. \n\nIJRITCC. 2014;2(2):310–4.\n 17. Gu V, and Li H. Memory or time—performance evaluation for iterative operation on hadoop and spark. In: Proc. of \n\nthe 2013 IEEE 10th Int. Con. on high-performance computing and communications. 2013. https ://doi.org/10.1109/\nhpcc.and.euc.2013.106.\n\n 18. Zhang Hanpeng, Wang Zhaohua, Chen Shengjun, Guo Chengqi. Product recommendation in online social net-\nworking communities—an empirical study of antecedents and a mediator. J Inform Manag. 2019;56(2):185–95.\n\n 19. Ghose A, Ipeirotis PG. Designing novel review ranking systems: predicting the usefulness and impact of reviews. In: \nInt Conference Electron Comm ACM. 2007. p. 303–10.\n\n 20. Chong AY, Ch’ng E, Liu MJ, Li B. Predicting consumer product demands via Big Data: the roles of online promotional \nmarketing and online reviews. Int J Prod Res. 2015;55:1–15. https ://doi.org/10.1080/00207 543.2015.10665 19.\n\n 21. Yang H, Fujimaki R, Kusumura Y, & Liu J. Online Feature Selection. In: Proceedings of the 22nd ACM SIGKDD Int. \nConference on KDD ‘16, 2016. https ://doi.org/10.1145/29396 72.29398 81.\n\n 22. Breese JS, Heckerman D, and Kadie C. Empirical analysis of predictive algorithms for collaborative filtering. In: Proc. \nof the 14th Conf. on Uncertainty in Artifical Intelligence, 1998.\n\n 23. Mukherjee A, Kumar A, Liu B, Wang J, Hsu M, Castellanos M, Ghosh R. Spotting opinion spammers using behavioral \nfootprints. In: Proc. of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining \nChicago, ACM. 2013. p. 632–40.\n\n 24. Makridakis S, Spiliotis E, Assimakopoulos V. Statistical and Machine Learning forecasting methods: concerns and \nways forward. PLoS ONE. 2018;13(3):e0194889. https ://doi.org/10.1371/journ al.pone.01948 89.\n\n 25. Imon A, Roy C, Manos C, Bhattacharjee S. Prediction of rainfall using logistic regression. Pak J Stat Oper Res. 2012. \nhttps ://doi.org/10.18187 /pjsor .v8i3.535.\n\n 26. Chen T, Zhang W, Lu Q, Chen K, Zheng Z, Yu Y. SVD Feature: a toolkit for feature-based collaborative filtering. J Mach \nLearn Res. 2012;13(1):3619–22.\n\n 27. Shi Y, Larson M, Hanjalic A. Collaborative filtering beyond the user-item matrix—a survey of the state of art and \nfuture challenges. ACM Comput Surv. 2014;47(1):3.\n\n 28. Shan H, & Banerjee A. Generalized probabilistic matrix factorizations for collaborative filtering, In Data mining \n(ICDM), IEEE 10th international conference. 2010. p. 1025–30.\n\n 29. Salakhutdinov R, & Mnih A. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. In: Proc. of \nthe 25th int. conference on machine learning. 2008. p. 880–7.\n\n 30. Crawford M, Khoshgoftaar TM, Prusa JD, Richter AN, Al Najada H. Survey of review spam detection using machine \nlearning techniques. J Big Data. 2015;2(1):23.\n\n 31. Wietsma TA, Ricci F. Product reviews in mobile decision aid systems. Francesco: PERMID; 2005. p. 15–8.\n 32. Jianguo C, et al. A disease diagnosis and treatment recommendation system based on big data mining and cloud \n\ncomputing. Inform Sci. 2018;435:124–49.\n 33. Manek AS, Shenoy PD, Mohan MC, Venugopal KR. Aspect term extraction for sentiment analysis in large movie \n\nreviews using Gini-index feature selection method and SVM classifier. World Wide Web. 2017;20:135–54. https ://doi.\norg/10.1007/s1128 0-015-0381-x.\n\n 34. Fan RE, Chang K-W, Hsieh C-J, Wang X-R, Lin C-J. LIBLINEAR: A library for large linear classification. J Mach Learn Res. \n2008;9:1871–4.\n\nhttps://doi.org/10.1007/978-3-540-68880-8_32\nhttps://doi.org/10.1007/s11280-015-0381-x\nhttp://dx.doi.org/10.1109/iccict.2012.6398180\nhttps://doi.org/10.1109/hpcc.and.euc.2013.106\nhttps://doi.org/10.1109/hpcc.and.euc.2013.106\nhttps://doi.org/10.1080/00207543.2015.1066519\nhttps://doi.org/10.1145/2939672.2939881\nhttps://doi.org/10.1371/journal.pone.0194889\nhttps://doi.org/10.18187/pjsor.v8i3.535\nhttps://doi.org/10.1007/s11280-015-0381-x\nhttps://doi.org/10.1007/s11280-015-0381-x\n\n\nPage 15 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n 35. Ribeiro MT, Singh S, and Guestrin C. Why should I trust you?: Explaining the predictions of any classifier. In: Proc. \nACMSIGKDD Int. Conf. Knowl. Discov. Data Mining. 2016. p. 1135–44.\n\n 36. Luo X, et al. An effective scheme for QoS estimation via alternating direction method-based matrix factorization. \nIEEE Trans Serv Comput. 2019;12(4):503–18.\n\n 37. Liu CL, Hsaio WH, Lee CH, Lu GC and Jou E. Movie rating and review summarization in mobile environment. In: IEEE \ntrans. systems, man and cybernetics, Part C: applications and reviews. 2012. p. 397–407.\n\n 38. Vapnik, VN. The nature of statistical learning theory, Springer, 2nd ed, 1999. Translated by Xu Jianghua, Zhang Xue-\ngong. Beijing: China Machine Press; 2000.\n\n 39. [Dataset] Flipkart-products. http://www.kaggl e.com/Promp tClou dHQ/flipk art-produ cts.\n 40. [Dataset] https ://snap.stanf ord.edu/data/web-Amazo n.html.\n 41. [Dataset] He R, McAuley J. Ups and downs: modeling the visual evolution of fashion trends with one-class collabora-\n\ntive filtering. WWW; 2016.\n 42. Popescu AM, Etzioni O. Extracting product features and opinions from reviews. 2005; EMNLP.\n 43. Zaharia M, Chowdhury M, Das T, Dave A, Ma J, McCauley M, Franklin M, Shenker S, Stoica I. Resilient distributed \n\ndatasets: A fault-tolerant abstraction for in-memory cluster computing Technical Report UCB/EECS-2011-82. UC \nBerkeley: EECS Department; 2011.\n\n 44. Davis J, Goadrich M. The relationship between precision-recall and ROC curves, In ICML. 2006. p. 233–40.\n 45. Lee JS, Lee ES. Exploring the usefulness of predicting people’s locations. Procedia Soc Beh Sci. 2014. https ://doi.\n\norg/10.1016/j.sbspr o.2014.04.451.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nhttp://www.kaggle.com/PromptCloudHQ/flipkart-products\nhttps://snap.stanford.edu/data/web-Amazon.html\nhttps://doi.org/10.1016/j.sbspro.2014.04.451\nhttps://doi.org/10.1016/j.sbspro.2014.04.451\n\n\tImproving prediction with enhanced Distributed Memory-based Resilient Dataset Filter\n\tAbstract \n\tIntroduction\n\tRelated work\n\tMethodology\n\tData collection phase\n\tDataset pre-processing\n\tResilient Distributed Dataset\n\n\tPrediction classifiers\n\tLogistic regression (LR)\n\tSupport Vector Machine (SVM)\n\n\tExperimental setup\n\n\tResults and discussions\n\tConclusion and future work\n\tAcknowledgements\n\tReferences\n\n\n\n\n",
      "metadata_storage_path": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9saWJyYXJ5L3M0MDUzNy0wMjAtMDAyOTIteS5wZGY1",
      "metadata_author": "Sandhya Narayanan ",
      "metadata_title": "Improving prediction with enhanced Distributed Memory-based Resilient Dataset Filter"
    },
    {
      "@search.score": 0.39407653,
      "content": "\nSentiment analysis and the complex \nnatural language\nMuhammad Taimoor Khan1*, Mehr Durrani2, Armughan Ali2, Irum Inayat3, Shehzad Khalid1 and Kamran \nHabib Khan4\n\nIntroduction\nSentiment analysis (Pang and Lillian 2008) is a type of text classification that deals with \nsubjective statements. It is also known as opinion mining, since it processes opinions in \norder to learn about public perception. Sentiment analysis and opinion mining are the \nsame, and are used interchangeably throughout the document. It uses natural language \nprocessing (NLP) to collect and examine opinion or sentiment words. SA is explained \nas identifying the sentiments of people about a topic and its features (Pang and Lillian \n2008). The reason for the popularity of opinion mining is because people prefer to take \nadvice from others in order to invest sensibly. Determining subjective attitudes in big \nsocial data is a hotspot in the field of data mining and NLP (Hai et al. 2014).\n\nAbstract \nThere is huge amount of content produced online by amateur authors, covering a \nlarge variety of topics. Sentiment analysis (SA) extracts and aggregates users’ senti-\nments towards a target entity. Machine learning (ML) techniques are frequently used \nas the natural language data is in abundance and has definite patterns. ML techniques \nadapt to domain specific solution at high accuracy depending upon the feature set \nused. The lexicon-based techniques, using external dictionary, are independent of data \nto prevent overfitting but they miss context too in specialized domains. Corpus-based \nstatistical techniques require large data to stabilize. Complex network based tech-\nniques are highly resourceful, preserving order, proximity, context and relationships. \nRecent applications developed incorporate the platform specific structural information \ni.e. meta-data. New sub-domains are introduced as influence analysis, bias analysis, and \ndata leakage analysis. The nature of data is also evolving where transcribed customer-\nagent phone conversation are also used for sentiment analysis. This paper reviews \nsentiment analysis techniques and highlight the need to address natural language \nprocessing (NLP) specific open challenges. Without resolving the complex NLP chal-\nlenges, ML techniques cannot make considerable advancements. The open issues and \nchallenges in the area are discussed, stressing on the need of standard datasets and \nevaluation methodology. It also emphasized on the need of better language models \nthat could capture context and proximity.\n\nKeywords: Sentiment analysis, Machine learning, Sentiment orientation, Complex \nnetworks\n\nOpen Access\n\n© 2016 Khan et al. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate \nif changes were made.\n\nR E V I E W\n\nKhan et al. Complex Adapt Syst Model  (2016) 4:2 \nDOI 10.1186/s40294-016-0016-9\n\n*Correspondence:   \ntaimoor.muhammad@gmail.\ncom \n1 Bahria University, Shangrilla \nRoad, Sector E-8, Islamabad, \nPakistan\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40294-016-0016-9&domain=pdf\n\n\nPage 2 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nManufacturers are also interested to know which features of their products are more \npopular in public, in order to make profitable business decisions. There is a huge reposi-\ntory of opinion content available at various online sources in the form of blogs, forums, \nsocial media, review websites etc. They are growing, with more opinionated content \npoured in continuously. It is, therefore, beyond the control of manual techniques to \nanalyze millions of reviews and to aggregate them towards a rapid and efficient deci-\nsion. Sentiment analysis techniques perform this task through automated processes with \nminimal or no user support. The online datasets may also contain objective statements, \nwhich do not contribute effectively in sentiment analysis. Such statements are filtered at \npre-processing.\n\nOpinion mining deals with identifying opinion patterns and presenting them in a \nway that is easy to understand. The outcome of sentiment analysis can be in the form \nof binary classification, such as categorizing opinions as recommended or not recom-\nmended. It can be considered as a multi-class classification problem on a given scale of \nlikeness. Cambria et al. (2013) used common-sense knowledge to improve the results of \nsentiment analysis. The results can be presented in the form of a short summary gen-\nerated from the overall analysis. Sentiment analysis has various sub streams including \nemotion analysis, trend analysis, and bias analysis etc. Its applications has outgrown \nfrom business to social, political and geographical domains. Sentiment analysis is \napplied to emails for gender identification through emotion analysis (Mohammad and \nYang 2011). Emotion is applied to fairy tales to draw interesting patterns (Mohammad \n2011). Considering text a complex network of words that are associated to each other \nwith sentiments, graph based analysis techniques are used for NLP tasks.\n\nNatural language processing\n\nOpinion mining requires NLP, to extract semantics of opinion words and sentences. \nHowever, NLP has open challenges that are too complex to be handled accurately till \ndate. Since sentiment analysis makes extensive use of NLP, it has this complex behav-\nior reflected. The assumptions in NLP for text categorization do not work with opinion \nmining, as they are different in nature. Documents having high frequency of matching \nwords may not necessarily possess same sentiment polarity. It is because, a fact in text \ncategorization could be either correct or incorrect, and is well known to all. Unlike facts, \na variety of opinions can be correct about the same product, due to its subjective nature. \nAnother difference is that, opinion mining is sensitive to individual words, where a sin-\ngle word like NOT may change the whole context. The open challenges are negations \nwithout using NOT word, sarcastic and comparative sentences etc. The later section has \na detailed discussion on NLP issues that affect sentiment analysis.\n\nThe subjective content from the online sources have simple, compound or complex \nsentences. Simple sentences possess single opinion about a product, while compound \nsentences have multiple opinions expressed together. Complex sentences have implicit \nmeaning and are hard to evaluate. Regular opinions pertain to a single entity only, while \ncomparative opinions have an object or some of its aspects discussed in comparison to \nanother object. Comparative opinions can either be objective or subjective. An example \nof a subjective sentence having comparison is “The sound effects of game X are much \nbetter than that of game Y” whereas an example of objective sentence with comparison is \n\n\n\nPage 3 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\n“Game X has twice as many control options as that of Game Y”. Opinion mining expects \na variety of sentence types, since people follow different writing styles in order to express \nthemselves in a better way.\n\nSentiment analysis\n\nThe machine learning (ML) based techniques are supervised, semi supervised or unsu-\npervised. The supervised techniques require labeled data, while the semi supervised \ntechniques need manual tuning from domain experts. The unsupervised techniques \nmake use of statistical analysis on large volume of data. ML techniques has a large fea-\nture set using Bag-of-words (BOW). Results are improved by pruning repetitive and \nlow quality features. The opinion words are extracted to identify the polarity of opinion \nexpressed for a feature. The performance of a classifier is measured through its effective-\nness at the cost of efficiency. Effectiveness is calculated as precision/recall and F-meas-\nure, which are measurements of relevance.\n\nSentiment analysis can also be considered as a complex network. It consists of nodes \nand edges joining them. Many complex systems from a variety of domains are repre-\nsented as network including environmental modeling (Niazi et  al.  2010), business sys-\ntems (Aoyama 2002), wireless sensors, and ad-hoc networks (Niazi and Hussain 2009). \nNetworks are rich in information, having a range of local and global properties. Text cor-\npora can be used with words as nodes and edges representing the structural or seman-\ntic association between them. The adjacent nodes sharing a link are closely associated \nand directly affect each other through the weight of the link they share. Representing \ntext as complex network, various properties like centrality, degree distribution, com-\nponents, communities, paths etc. can be used to explore the data thoroughly. Through \nmulti-partite graphs, nodes can be distributed among various clusters with inter-cluster \nedges only. It separates different types of entities discussed in comparison. Entities are \nlinked to their respective aspects/features and then to the sentiments associated. The \nsentiments can be linked with the reasons shared in support of those sentiments.\n\nData sources\n\nOpinion mining has diverse subjective data sources that are available online. They cover \na large number of topics and are up-to-date with current issues. Introduction of Web2.0 \nin the last decade has enabled people to post their thoughts and opinions on a range of \ntopics. The data produced online is growing all the time produced by people from differ-\nent backgrounds (Katz et al. 2015). Opinion mining makes use of this data generated by \nmillions of users all over the world. According to Business Week survey in 2009, 70 % of \nthe people consult online reviews and ratings to make a purchase. Comscore/The Kelsey \ngroup in 2007 reported that 97  % of the people who made purchases based on online \nreviews, found them to be honest.\n\nThe user generated subjective content is of value to be assessed and summarized for \nprospective customers. These online data sources are in the form of blogs, reviews and \nsocial media websites. The popularity of blogging is on the rise, where people from dif-\nferent walks of life express their opinions about various entities and events and get com-\nments on them. At times, it leads to a form of discussion among the author and various \nusers commenting on them. A detailed analysis on blogging styles of authors, as they \n\n\n\nPage 4 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nfollow their own unique approaches for expressing their feelings is provided in (Chau \nand Xu 2007). Blogs contain opinions about various products, services, their features, \npackages and promotions. Most of the online studies on opinion extraction use blogs as \ndatasets (Qiang and Rob 2009) to perform detailed analysis.\n\nThere are professional review websites providing customers’ feedbacks, used for sen-\ntiment analysis. E-commerce websites allow customers to comment on their products. \nSocial media is another popular medium of sharing information among like-minded \npeople. Here, a variety of subjects are discussed where people express their opinions, \nbased on their own experience. Social media websites have a very complex struc-\nture for extracting information having user opinions. They allow users to express their \nviews through sharing articles and other media sources as an external link. Twitter, also \nreferred to as microblogging, has the problem of reviews being too short and at times \nmiss the context.\n\nThis review article is organized into the following divisions. Section 2 reviews the Sen-\ntiment analysis techniques and the NLP issues. Section  3 provides a discussion on the \nreview studied and Sect. 4 list the application areas for sentiment analysis. Section 5 has \nconcluded the study to important issues drawn from the study. Section  6 has distribu-\ntion of the work carried out by the authors.\n\nReview\nThe sentiment analysis techniques categorize reviews into positive and negative bins or \nmultiple degrees of it. The social data can be analyzed at three different levels i.e. user \ndata, relationship data and content (Tang et al.  2014). In survey (Guellil and Boukhalfa \n2015) these categories are further elaborated. Recommender systems are extended to \nsupport textual content using knowledge (Tang et al. 2013). In our previous work (Khan \nand Khalid 2015) sentiment analysis is highlighted to address health care problems from \nthe view point of a user. The issues faced in SA also depend on the data sources and \nnature of analysis required. An important aspect of social data analysis is the identifi-\ncation of sentiments and sentiment targets (Tuveri and Angioni 2014; Zhang and Liu \n2014). Opinion mining also consider the additional features of opinion holder and time. \nSentiment analysis techniques can be separated into three groups: supervised, semi-\nsupervised and unsupervised techniques.\n\nThe supervised techniques are the machine learning classifiers. They are more accu-\nrate, however, need to be trained on a relevant domain. The unsupervised statistical \ntechniques do not require training. They are efficient in dynamic environment but at the \ncost of accuracy. Sentiment analysis techniques analyze opinion datasets to generate a \ngeneral perception that people have about a product. The classification of sentiments in \na review document is performed through identifying and separating all the positive and \nnegative opinion words. Considering the strength of these words, along with their polar-\nity, helps in multi-class classification. Machine learning classifiers such as Naive-Bayes, \nk-nearest neighbor and centroid based classifier etc., are successfully used for this pur-\npose. Semantic orientation based techniques used for opinion mining are Lexicon based \nand statistical analysis. Lexicon based technique works with individual words while sta-\ntistical analysis incorporates words co-occurrence using point wise mutual information \n(PMI) and latent semantic analysis (LSA). Semi-supervised techniques start with a small \n\n\n\nPage 5 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nset of opinion words from the given domain, and expand on it. More opinion words are \nexplored by querying the starting seeds. The newly found words are queried again to find \nmore words until no new words are returned. Orientation of the opinion word form the \nbasis for classification. Other attributes used are frequency of occurrence, location and \nco-occurrence with other words. The taxonomy of these approaches is shown in Fig. 1.\n\nSentiment classification\n\nThese are the machine learning classifiers used for sentiment analysis. They can be \napplied to text documents at three levels for analysis. A document level approach, \nwhich studies the whole document as a single entity is appropriate for text categoriza-\ntion. However, document level approach is not viable for sentiment analysis with docu-\nments having multiple opinions. Therefore, sentiment analysis is performed extensively \nat sentence or word level. Word level analysis is also known as sentiment level analysis. \nML techniques suits sentiment analysis as the data is in abundance and there is obvious \npresence of patterns (Schouten and Frasincar 2015). The classifiers are trained on label \ndataset having samples representing all classes. A test dataset is used to evaluate the per-\nformance of the classifiers for the given task. Let the set of documents as {D = d1,…,dn}, \nand set of classes labeled as {C = c1,…,cn}, then the task is to classify document di in D \nwith a label ci in C. This task can be performed using supervised classifiers. The more \nfrequently used classifiers for sentiment analysis are discussed below.\n\nNaïve Bayes\n\nNaive Bayes (NB) classifier is extensively used for text classification. It learns from a \ntraining dataset of annotated feature vectors, with labels as positive and negative (in case \nof binary classification). The probability of a feature vector is calculated with each label \nusing the annotated training dataset. The feature vector is assigned a label that has high-\nest probability for it. If this information is preserved, it can be used to show confidence \nin a label for a feature vector. In further modifications of NB a fuzzy region is defined \nin which feature vectors hold both labels with a certain level of confidence. Text data \nnormally have high dimensional feature vectors. Therefore, the process of calculating \nprobability is repeated for each feature vector, and then all the probabilities contribute \ntowards the final decision. The feature set is represented as F = f1, f2…fm}, where prob-\nability of a document belonging to a class shown as:\n\nFig. 1 Taxonomy of expository literature on sentiment analysis\n\n\n\nPage 6 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nShows the probability of a document dj represented by its vector dj\n* belonging to a class \n\nci. It is the product of probabilities for all the features in the feature set. The document \nvector dj\n\n* is assigned to a class ci in order to maximize P\n(\n\nci\n\n∣\n\n∣\n\n∣\nd∗j\n\n)\n\n. The logarithm of prob-\nabilities are summed up to classify an opinion document. It is preferred over product of \nprobabilities to avoid underflow. It addresses the missing value problem as well. Slack \nvariables add smoothing effect against noisy data. Weights can also be assigned to fea-\ntures which define their contribution towards the classification. It is a biased approach, \nwhere prominent features are given high weights to play a major role in choose a senti-\nment label.\n\nNaive Bayes works on the assumption that all the sentences of a review document are \nopinion sentences. It also assumes that features of a document are independent of each \nother. Despite of this unrealistic assumption, Naïve Bayes is very successful and is used \nin various practical applications. The assumption of treating features as independent of \neach other makes Naive Bayes highly efficient (Dai et  al. 2007). Although, Naive Bayes \nclassifier is simple, yet it is effective because of its robustness to irrelevant features. It \nperforms well in domains with many equally important features. It is considered to be \nmore reliable for text classification and sentiment analysis. The accuracy of the classifier \nimproves with pre-processing noise. It also used as transfer learning when trained on a \ndataset similar to the target dataset.\n\nNearest neighbor\n\nk-nearest neighbor classifier has been frequently used in literature for text classifica-\ntion. It considers the labels of k nearest neighbors to classify a test document. A special \ncase of the k-NN problem is typically referred to as classimbalance problem identified \nin (Yang and Liu 1999). Classes with more training data have higher influence to predict \nsame label for the new document. There are fewer chances of acquiring a class label if \nthat class has fewer training examples. (Li et al. 2003) catered this problem by using vari-\nable value of k for each class. Thus, the class having more training data will have higher \nvalue of k as compared to the one having few samples. This solution is helpful in online \nclassification, where there is time constraint on trying different values of k.\n\nA study on performance of k-NN using pre-processed dataset is conducted in (Shin \net al. 2006) claiming 10 % improvement when noise and outliers are filtered out. An opti-\nmum value is chosen as threshold to separate regular data from noise. Sentiment analy-\nsis is performed with a reduced set of feature vector in (Sreemathy and Balamurugan \n2012) to avoid the curse of dimensionality. Accuracy of the model improves as irrelevant \nfeatures were removed. Features are assigned weights to vary their contribution towards \ndecision making. Weights are extracted from probability of information in documents \nacross different categories. Tree-fast k-NN is introduced as fast kNN model (Soucy and \nMineau 2001). This tree based indexing of retrieval system improves the accuracy of \nk-NN in distance calculation. Its effective against large feature sets. The order of features \nand their thresholds are identified from within the training data. k-NN has promising \n\n(1)P(ci\n∣\n\n∣dj\n∗\n) =\n\np(ci)(\n∏m\n\ni=1 p(fi|ci))\n\np(d∗j )\n\n\n\nPage 7 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nresults in sentiment analysis; however, it is more susceptible to noise and high dimen-\nsional feature set. Therefore, more of the work in k-NN for text classification has focused \non feature selection and reduction techniques as they are the driving factors of k-NN’s \nperformance.\n\nCentroid based\n\nCentroid based (CB) classifier calculates centroid vector or prototype vector for each \nclass in the training dataset. Centroid vector is the central point of the class and may not \nrepresent an actual training data. The distance of each test document is calculated with \nthe prototype vector of the class and is classified based on similarity with it. Its perfor-\nmance depends on the chosen centroid vectors. It is efficient since time and space com-\nplexities are proportional to the number of classes rather than training documents. To \ndouble the training data reverse of reviews are generated in (Xia et  al. 2015) by invert-\ning the sentiment terms and their labels. Using both sets of training data with Mutual \nInformation (MI) the results were improved when only selected reviews were inverted. \nExternal dictionary WordNet is used to generate inverse for sentiment terms, however, \npseudo-antonyms can be generated internally using the corpus.\n\nms, however, pseudo-antonyms can be generated internally using the corpus. A variety \nof approaches have been used for CB classifier. Rocchio algorithm calculates centroid \nto represent feature space of documents (Ana and Arlindo 2007; Tan 2007a, b). Cen-\ntroid is computed through average of positive examples in (Han and Karypis 2000) and \nsum of positive cases i.e. the related training examples (Chuang et al. 2000). Normalized \nsum of positive vectors used in (Lertnattee and Theeramunkong2004), cosine similar-\nity between the test document and the Centroid of a class (Hidayet and Tunga 2012). \nCentroid is used with inverse of class similarity as well improving the accuracy close to \n100 % on the given dataset when characters are chosen as features instead of n-grams.\n\nCentroid evaluation is sensitive to noise in the training dataset which affects the over-\nall performance of the classifier. This shortfall is exposed when Centroid classifier is \napplied to a slightly different domain. The reason for this drawback is that some opinion \nwords are domain dependent. They have different polarity or strength of polarity when \nused in a different domain. Smoothing techniques have being proposed in (Tan 2007a, b;  \nLertnattee and Theeramunkong 2006; Guan 2009) that minimizes the effect of noise \nin the dataset. (Chizi et  al. 2009) defined a weighting scheme giving higher weights to \nexplicit opinion words. Characters and special characters for feature selection are used \nin (Ozgur and Gungor 2009). The work in (Shankar and Karypis 2000; Tan et  al. 2005) \nis focused on adjusting the value of centroid based with feedback looping, hypothesis \nmargin and weight-adjustment respectively. They try to rectify class Centroid, if it is not \ncalculated accurately. Centroid based classifier performs efficiently as it doesn’t consider \ntraining data each time to decide a test document.\n\nSupport vector machine\n\nSupport vector machine classifier is used for text classification in various studies. It finds \na separation among the data using the annotated training dataset. The margin of sep-\naration between classes, which is known as hyperplane, is used to classify the incom-\ning data. The hyperplane should give maximum separation between the classes. It is \n\n\n\nPage 8 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\napplicable even in the presence of high dimensional feature set representation. It classify \nbased o hyperplane among classes. Like centroid-based, SVM also consider the hyper-\nplane to classify a test document. (Brown et  al. 1997) has compared SVM with artifi-\ncial neural networks for text classification and has found it better. Since it has promising \nresults in text classification, it also performs well for opinion mining. They have also \nclaimed in (Brown et  al. 1997) that SVM is better than Naive Bayes and decision trees \nclassification algorithms. However, SVM consumes more resources at the training \nstage. Although, it is efficient with large feature set, Feldman et al.(2011) has shown that \ndimensionality reduction in feature set further improves the performance of SVM. It \nexhibits linear complexity and can scale up to a large dataset.\n\nSVM has a limitation of over-reliance on selection of suitable kernel function. Kernel \nis calculated through Linear, Polynomial, Gaussian or sigmoid methods but they tend to \nbe domain specific. Kernel functions that perform well for one domain may not repeat it \nfor next. Its accuracy is also sensitive to number of training samples close to hyperplane. \nSlack variables are introduced to limit the impact of boundary samples by generalizing \nthe classifier, known as soft margin classification. They also help to avoid over-fitting the \ntraining data.\n\nUnsupervised techniques\n\nThe unsupervised sentiment analysis techniques do not require training data and rather \nrely on semantic orientation. They make use of lexicons to identify the positive or neg-\native semantics of opinion words. The meaning of the word, expressed by its use in a \ncontext is called lexicon. An online or off-line dictionary is consulted for this purpose. \nStatistical analysis techniques are also unsupervised, identifying the orientation of senti-\nment words through statistical evaluations. They require large volume of data for high \naccuracy.\n\nLanguages consists of lexicons that are the words used for a particular sense, and a \ngrammar that connect these lexicons. Part-of-speech rules are used to extract senti-\nment phrases from text document. Search engines are used to identify the orientation \nof sentiment words that are missing in the dictionary. Its polarity is identified through \nthe nearby words brought by search engines. They purely rely on external sources and \ntherefore cannot address the context. Lexicon based techniques perform well for general \ndomains while statistical techniques addresses the context and are useful in specialized \ndomains. The two types of approaches are discussed in detail.\n\nDictionary (Lexicon) based techniques\n\nLexicon based techniques extract opinion lexicons from the document and analyzes \nits orientation without the support of any training data. These techniques process the \nopinion words separately, ignoring the relationship between them. Lexicons refer to the \nsemantic orientation. Lexicons are independent of the source data and therefore it does \nnot fall for over-fitting. But context not addressed either in this approach (Katz et  al. \n2015; Cambria 2013). Search engines are used to find the meaning of unknown opinion \nlexicons. They are searched and the top N results are accepted to identify its orientation. \nThe semantics of lexicons can be categorized as positive or negative with weights rep-\nresenting their strength. This approach struggles with lexicons having domain specific \n\n\n\nPage 9 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\npolarity. For example, good has positive polarity in any type of domain but “heavy \nweight” has positive polarity for bike domain but negative for the domain of electronic \ndevices.\n\nIn its simplest form, sentiment words are split into positive and negative as binary \ndistribution. A more sophisticated approach has fuzzy lexicons, introducing a grey area \nbetween the two categories. These fuzzy lexicons exist in both the classes with a score \nassociated to it, representing the strength of each label. Various manual and semi-auto-\nmatic techniques can be used for building lexicons. Princeton University’s WordNet is \na popular lexicon source available for sentiment analysis. Dictionaries like WordNet, \nextracts synonyms and antonyms for the provided opinion words. Manual cleansing is \nemployed to rectify the lists generated for the unknown sentiment words. These opinion \nwords are used to classify a review as positive or negative.\n\nFixed syntactic patterns are also used for expressing opinions which are composed of \npart-of-speech (POS) tags. The basic idea of this technique is to identify the patterns in \nwhich words co-occur with each other and to exploit those patterns for understanding \nits semantic orientation. One example of such pattern is an adverb followed by an adjec-\ntive. A more sophisticated approach was proposed by (Mohammad and Yang 2011), \nwhich used a WordNet distance based method to determine the sentiment orientation. \nThe distance d(t1, t2) between terms t1 and t2 is the length of the shortest path that con-\nnects them in WordNet, as shown in Eq. 2. The semantic orientation (SO) of an adjective \nterm t is determined by its relative distance from two reference (or seed) terms good and \nbad. The polarity of opinion term t is resolved through eq.\n\nStatistics (Corpus) based techniques\n\nStatistical analysis of large corpus of text can also be used to determine the sentiment \norientation of words. Co-occurrence of words is evaluated without consulting any exter-\nnal support. Two methods are used for this purpose which are point wise mutual infor-\nmation (PMI) and latent semantic analysis (LSA). PMI method for co-occurrence is \ngiven as:\n\nwhere w1 and w2 refers to two words in a given sentence. The main concept behind PMI \nbased techniques is that the semantic orientation of a word has a tendency of being \nclosely related to that of its neighbors. Equation 3 gives the probability of words w1 and \nw2 to co-exist, based on the measure of degree of statistical dependence between the \ntwo. This approach is, however, implemented differently in LSA based techniques. In \nLSA, matrix factorization technique is used with singular value decomposition to dem-\nonstrate the statistical co-occurrence of words. More formally, this process can be speci-\nfied as:\n\n(2)SO(t) =\nd(t, bad) − d(t, good)\n\nd(bad, good)\n\n(3)p(w1, w2) =\np(w1, w2)\n\np(w1) p(w2)\n\n(4)LSA(w) = LSA(w, {+paradigms}) − LSA(w, {−paradigms})\n\n\n\nPage 10 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nwhere a word w is passed to LSA with positive and negative paradigms. LSA based tech-\nniques develop a matrix having rows as words and columns as sentences or paragraphs. \nEach cell possesses a weight corresponding to the relation of the word in row with the \nsentence or paragraph in columns. This matrix is decomposed into three matrices using \nsingular value decomposition (SVD).\n\nComplex challenges\n\nOpinion mining is a relatively new area of research and there are open challenges that \nneed to be answered. Some of the challenges are common to opinion mining in general \nwhile others are related to their own sources and context depending upon the domain of \nthe dataset. These issues affect the performance of machine learning techniques, but it \nhas little control on them. Figure 2 gives NLP challenges faced in sentiment analysis, dis-\ntributing them into their logical groups. The groupings are based on the parsing level, at \nwhich these issues occur. The following sub section has detailed discussion on the NLP \nissues.\n\nDocument level\n\nDocument level NLP challenges are the ones that are faced at the document or review \nlevel. They deal in general with the review document or the reviewer style. It is common \nto find reviews that have the information about an object, given in an informal manner. \nCapitalization is over or under used. Spelling mistakes are ignored or words being short-\nened. It makes the analysis very difficult for the automatic techniques to identify features \nand associate them. The unknown words (shortened/miss spelled) are matched with \nsimilar words to identify the aspect or opinion words. Slang specific to a certain region \nare also occasionally used in reviews and discussions. Reviews having sarcastic expres-\nsions are the hardest to deal with. Even though they have the opinion words explicitly \nmentioned, they do not serve the purpose for which they are normally used. For exam-\nple, “What an awesome phone! It stopped responding in few hours”. These types of \nexpressions are quite frequent in political reviews.\n\nThere are some document level challenges that are specific to certain domains only. \nThe opinion words can also be domain dependent, where they have different orientation \ndepending upon the domain in which they are used. This problem arise in specialized \ndomains e.g. medical or astronomy etc. The opinion words in this case cannot be evalu-\nated correctly, without domain knowledge. The general opinion words, however, have \nsame orientation irrespective of the domain in which they are used e.g. good, bad etc. \nOpinion data attained from platforms like blogs and forums face the problem of dealing \nwith discussions. They allow their users to comment on reviews, which at times gets into \nthe shape of a debate. In discussions, users may agree with each other on some points \nwhile disagree on others. They need to be tackled differently, as they require the flow of \ncontext to be maintained from comment to comment in a sequence. Although, they are \ntough to process, discussions are very informative in which authors not only show their \nliking or disliking, but also support it through reasoning.\n\nSpamming has been an issue faced at multiple frontiers of online data and sentiment \nanalysis is no different. In fact it is the most highlighted area of sentiment analysis, con-\nsidering its popularity and impact for industries. Spammers post false reviews about \n\n\n\nPage 11 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nproducts either to promote or demote it and if there are more numbers of such reviews \nit will affect the performance of all opinion mining techniques adversely. (Mukherjee \net  al. 2011; Mukherjee and Liu 2012) has stressed on the identification and filtering of \nspam reviews, prior to applying any mining techniques. Spam reviews can be written by \nindividuals or commercial companies, dealing in such business. Spam opinions include \na fake review where the author does not write his own feelings about a product. Other \ntypes of spamming include irrelevant, non-review content and advertising text etc. Psy-\nchological studies are used to identify spamming, that helps to find patterns when peo-\nple lie. Meta information can also provide insight into it, as spammers normally tend to \npost more content in shorter time. This information can be very helpful if thoroughly \nstudied to search for outliers. In (Lim et  al. 2010; Kamps et  al. 2004; Mohammad and \nTony 2011) different machine learning techniques are used to detect spammers, who are \nthen assigned a spamming behavior number to keep track of them.\n\nSentence level\n\nThese challenges are faced at the sentence level while parsing a review. They arise when \nthe sentence expressing a review is not a simple sentence, that is expressing a single \n\nFig. 2 Complex NLP challenges in sentiment analysis\n\n\n\nPage 12 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nproduct or feature with a single opinion word. Complex sentences can be comparative, \nconditional or having grouped opinions etc. (Narayanan et  al. 2009) worked with sen-\ntences that are in the form of a question for the opinion audience. These sentences place \na condition on an entity and are hard to parse and evaluate for a certain opinion. For \nexample “if you are not happy with your notepad ++ code editor, try this new version of \ndreamweaver”. In this sentence author is positive about dreamweaver” whereas he/she \ndoes not say anything about the “notepad ++”. Without “if” it would be clearly a nega-\ntive opinion of the “notepad ++” but now its inconclusive towards it.\n\nComparative sentences express a situation in which the opinion about one feature is \ndiscussed in comparison to another. In this situation, identifying the target of opinion \nwords is very important, as there are more than one targets discussed. Secondly, aspects \nare to be associated to their respective products discussed in comparison to each other. \nFor example “HP Laptops are stylish as compared to Dell and Sony”. In order to resolve \nthis opinion, the information about the style of HP, Dell and Sony is crucial. In this case, \nwithout identifying the target features and their related opinions, the situation cannot be \nresolved (Jindal and Liu 2006). Must-link refer to the situation in which a single opinion \nword is shared by more than one features or entities. For example “My new office has \nattractive furniture, coloring and decoration”. In this sentence, the opinion attractive is \nbeing shared between three features of the entity office. Reviews are more opinion cen-\ntered as compared to blogs and forums where the focus may deviate from the topic. In \nsuch discussions, not all the sentences can be evaluated for extracting opinions. There \nare certain sentences that do not bear any opinion which needs to be filtered. For exam-\nple “Our team has strong batting line-up” is evaluative whereas “I am excited for our \nteam’s batting” is non-evaluative (Zhai et al. 2011). Mihalcea and Carlo 2009 has consid-\nered the problem of identifying words that make sense subjectively.\n\nThe opinion source and target identification is very important to classify opinions \naccurately. A target is the receiving entity of the opinion to which the opinion is enti-\ntled whereas; source is the person holding the opinion. Source identification is a concern \nwhen authors present the opinion of a third person. In the example “I bought a pen 2 days \nago. It was such a nice pen. Its feel in hand is really cool. Its tip is soft and is very fluent. \nHowever, my mother was mad with me as I did not tell her before I bought it. She also \nthought the pen was too expensive, and wanted me to return it to the shop”. The author \nhimself/herself is the source of the first four lines whereas the source of the last two lines \nis author’s mother, (Patella and Ciaccia 2009). Dealing with negation is also of high impor-\ntance, since it overturns the orientation of the opinion words. For example, the sentence \n“I am not interested in this car” is negative. Negation words need to be dealt with a lot of \ncare as not all occurrences of such words mean negation. For example, not in “not only \nbut also” is not used for negation. Similarly negation can also be used without explicitly \nusing any negation words like “Theoretically it takes care of the screen resolution”.\n\nFeature level\n\nThese are the open issues faced at the features level in sentiment analysis. Natural lan-\nguages are highly rich allowing a variety of words and phrases that could be used to \nexpress one’s feelings. They consist of words that are used interchangeably for the same \nfeature. If these synonyms are not identified, it will result in redundant features, which \n\n\n\nPage 13 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nat worse may have different opinion classification (Zhai et al. 2010). The features that are \nreferred using different words, are grouped together and all their opinions are merged to \nget an aggregate opinion. For example “picture” and “photo” refers to the same feature \nof camera. Such features needs to be grouped based on synonyms otherwise they might \nbe missed out or classified incorrectly. Feature stemming and pruning are also essential \nto identify similar opinion words and group them together. It reduces the set of opin-\nion words that are used for classification. For example words like attraction, attractive, \nattracted, attracting are stemmed to the word attract and are considered as a single opin-\nion word. Since all of the above words have same opinion with same orientation there-\nfore, stemming them will enable the classifier to treat them as the same word attract. \nReducing the feature set improves the performance of the classifiers. The opinion tar-\nget may also be implicit, in which case the opinion is mentioned without explicitly giv-\ning the product or its feature. It normally happens when the target product or feature is \nalready in discussion in previous sentence.\n\nComplex products like phones, laptops cannot be recommended or not recommended \nas a whole. They have many features or aspects which need more in-depth study. If the \nopinion words are associated directly to the target domain, while by passing its feature, \na lot of valuable information is missed. Aspect-based sentiment analysis (ABSA) con-\nsider aspect or features as the opinion targets. The features are associated to products \nto aggregate features opinions for products also. For example “Dell laptops have power-\nful batteries”. Such opinions need to be associated to the battery, which is a feature of \n“Dell laptop” and should not be referenced to it as a whole. This is also important for the \nreason that potential buyers are after certain strong features in the product considering \ntheir own situation and liking (Somprasertsri and Latitrojwong 2010).\n\nLexicon level\n\nThe problems faced at lexicon level are related to identifying the semantics of the word \nused. Dual meaning words and expressions depends on context of use. These words can-\nnot be considered as positive or negative without having context knowledge. For exam-\nple “the battery of this phone works for longer duration but the start-up takes longer \ntoo”. Here “longer” is a positive opinion for battery backup but a negative opinion for \nstart-up time (Ding and Liu 2007). The general opinion lexicon refers to opinion words \nlike good, excellent, bad and poor etc. Most of the sentence and document based opin-\nion mining use them as core of their techniques. There is only a small set of opinion \nlexicons publicly available. A universal opinion lexicon is required that would provide \ninformation on all such words (Qiu et  al. 2011). A semi-automatic technique of deal-\ning with this problem is to find synonyms and antonyms of initially given lexicon seeds \npassed to search engine. The process is repeated several times to explore as many opin-\nion words as possible.\n\nDing and Liu (2010) refers to the problem of product-aspect co-reference. It is \nrequired in scenarios where products and their aspects along with the associated opin-\nions, are not expressed in the same sentence. This is called opinion passage on aspect, \nexpressing opinion as a group of consecutive sentences. For example, “I bought a Honda \nbike yesterday. It looks beautiful. I took it out for a ride yesterday. That was a great feel-\ning”. In this example, It refers to bike whereas, that refers to ride which is a feature of \n\n\n\nPage 14 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nbike. This co-relation among products and their aspects need to be identified for associ-\nating products to their aspects in multiple sentences. It is called co-reference resolution \nproblem was identified by Ott et al. (2011). Partially supervised clustering techniques are \nfavored for this problem. Linguistic rules are very important to get sense of the opinions, \nrather than trusting the sentiment word only. Although, they are hard to apply, but are \nhelpful in exploring the implicit meaning of words, for the sense in which they are used. \nFor example “This car has good interior and not only that, the price is affordable too”. In \nspite of having the word not, the meaning of the sentence is not inverted as is normally \nthe case with the negation word.\n\nDiscussion\nSentiment analysis is an area of diversified research fields including machine learning, \nnatural language processing, language identification and text summarization. Most of \nits issues are related to NLP which are quite complex and under research focus. The text \nobtained from reviews need to be classified into different languages, while working with a \nmulti-lingual system. For each language, evaluative and subjective sentences are identified \nwhile others are discarded. Trimming is applied on the subject data for reducing the fea-\nture set which are further classified into either positive and negative (binary classification) \nor greater number of classes. Regression techniques are preferred for using multi-class \nproblems.  Table  1 provides a comparative analysis of the techniques used for sentiment \nanlaysis. Opinion mining has certain common grounds with text classification using tech-\nniques from Information retrieval. The NLP issues discussed affect all Sentiment analysis \ntechniques, however, supervised techniques are more vulnerable to it. Opinion orienta-\ntion has a context inclined towards psychology and linguistics. Complex networks can \n\nTable 1 Comparison of the techniques and approaches included in the study\n\n1 require training, 2 use training data to classify, 3 probabilistic approach, 4 driving factor, 5 similarity metric, 6 strength, 7 \nweakness, 8 support for streaming data\n\nSNo. Naïve Bayes k-Nearest neighbor Centroid\n\n1 Yes Yes Yes\n\n2 Yes Yes No\n\n3 Yes No No\n\n4 Word probability Value of k Centroid vector\n\n5 Probability weights Distance similarity Vector distance\n\n6 Simple and fast Handle co-related features Classify on vector distance\n\n7 Assume feature independence Sensitive to irrelevant features Sensitive to noise\n\n8 Yes Too expensive No\n\nSNo. Support vector machine Lexicon (dictionary) based Statistical (corpus) based\n\n1 Yes No No\n\n2 No NA NA\n\n3 No No Yes\n\n4 Kernel function Word polarity Feature matrix\n\n5 Hyperplane Word polarity Word distance\n\n6 Classify on hyperplane Can identify new lexicons Handle online data\n\n7 Require more resources Struggle with domain context Conceptual document size\n\n8 No Yes Yes\n\n\n\nPage 15 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nhelp in resolving context through preserving sequence and by associating words in a sen-\ntence, sentences in a paragraph and even paragraphs in an article. Sentiment analysis has \nits roads crossed with many different research areas and therefore, its problems are to be \naddressed with solutions coming from areas other than machine learning.\n\nSentiment analysis and opinion mining is out of its earlier stages and there is a strong \nneed to standardize the datasets and evaluation methodology (Schouten and Frasincar \n2015). Accuracy, area under the curve, precision/recall and F-measure are frequently \nused for evaluation. The bagof-words approach do not contain information about con-\ntext and proximity and therefore, needs to be replaced with concept-centric approach. \nIn survey (Cambria and White 2014) the need for bag-of-concept is emphasized and \neven bag-of-narratives was suggested. Sentiments also need to be contextualized and \nconceptualized (Gangemi et al. 2014). The work of Weichselbraun et al. (2014) is a step-\nping stone towards contextualizing sentiment analysis by integrating different semantic \nrepositories i.e. WordNet, SentiWordNet, WordNetAffect etc. It also help to distinguish \namong specific aspects that were previously studied in isolation. SentiWordNet has low \nstrength sentiments that are not contributing positively (Tsai et al. 2013).\n\nThe techniques developed for sentiment analysis needs to focus on the type of sup-\nporting application as they have different content style. Microblogging (twitter) and \ntranscribed text is unstructured having more noise and therefore, lexicon-based tech-\nniques do not perform well (Katz et  al. 2015). Similarly depending upon the nature of \nplatform structural information can also be incorporated e.g. likes, share, retweets, \nhashtags etc. Ofek (2014) showed drop in accuracy when twitter data was used instead \nof Wall street journal content, even after including emoticons and hashtags (Ofek 2014). \nMachine learning techniques are more supportive to accommodate structural informa-\ntion e.g. meta-data as non-textual features (Katz et al. 2015). ML techniques depend on \nthe feature set to which proximity and context based features can also be added. Tran-\nscribed text is used in Takeuchi and Yamaguchi (2014) and Cailliau and Cavet (2013) \nintroducing new type of textual content. It also contain terms like “Emm” and “Aah” etc. \nthat doesn’t have any meaning but are used while speaking. Similarly sentences are left \nincomplete and grammar is ignored. This opens new avenues to these techniques to deal \nwith this type of content.\n\nApplication areas\nPreviously if customers want to know about something, they would ask their friends and \nfamily while businesses would conduct surveys and polls. Sentiment analysis applica-\ntions have spread to almost every possible domain, from consumer products, services, \nhealth care, and financial services to social events and political elections etc. Customers \nmay analyze the feedback of various features of the product given by other customers in \na way that would help in decision making. The sentiment analysis outcome of products \nand its features can be compared for competing products. Jaafar et  al. (2015) consider \nbig social data analysis as a concern for search engines and industries.\n\nAn application having opinion reason mining could be more helpful for both custom-\ners and companies towards making a sound decision. In opinion reason mining, not only \nthe opinions about aspects are extracted but reason of the opinion is also extracted. It \nfurther helps manufacturers as their problems are highlighted. In (Zhang and Skiena \n\n\n\nPage 16 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\n2010; Sakunkoo and Nathan 2009) expert investors use twitter moods to predict stock \nmarket. Blog and news sentiment were used to study trading strategies (Groh and Jan \n2011). In (Stoyanov and Claire 2006) social influences in online book reviews were \nstudied. Sentiment analysis is used to characterize social relations (Akkaya et al. 2009). \n(Mohammad and Tony 2011; Mohammad 2011) worked with emotion analysis on vari-\nous sources. This is a very interesting study that can help to find what male and female \ncustomers look for, in a certain product that can be focused on.\n\nOpinion mining on social media can have applications to rank celebrities, sportsmen \nand championships based on their popularity in public. It can be used to find the popu-\nlarity of politicians prior to election etc. Influence analysis is performed in Nguyen et al. \n(2015) while Rabade et al. (2014) has discussed different influence indicators. There are \nvery useful applications of opinion mining available that may be used online for finding \nthe orientation of a text. Some of the notable ones are online message sentiment filter-\ning, e-mail sentiment classification, web blog author’s attitude analysis etc. Data leakage \nanalysis is an emerging area that can be focused on in security systems (Katz et al. 2014).\n\nConclusion\nOpinion mining has its boundaries extended from computer science to management sci-\nences. Sentiment analysis, though recently introduced as in research focus for commer-\ncial and social content. A detailed analysis of the problem through ML based techniques \nhas made it clear that SA and NLP has many open issues that are beyond the control of \nthe methods in practice. Having close relevance to NLP, sentiment analysis faces NLP \nissues like co-reference resolution, negation handling, and word sense disambiguation \netc., which add more difficulties due to their variation. However, it is also useful to real-\nize that sentiment analysis is a highly restricted NLP problem because the system does \nnot need to fully understand the semantics of each and every word. Complex network \nanalysis has been popularly used for various problems and can produce useful patterns \nin subjective text. Knowledge-bases systems incorporate domain specific guidance from \na knowledge source to improve results in specialized domains. More ML based solutions \nproposed, however, there is a strong need for considering solutions coming from dif-\nferent research domains. Machines generated data needs to be considered as meta-data \nalong the content dimension for many useful purposes.\nAuthors’ contributions\nMTK performed critical analysis on the data, drafted the manuscript and concluded the concepts presented. II helped \nwith data acquisition about machine learning sentiment analysis techniques and drawing important interpretations \nfrom it. MD identified the key challenges in Natural Language processing and categorized them based on the level \nwhere they tend to occur. AA carried out a study to investigate applications of Sentiment analysis and how they can \naffect the future of business intelligence. SK supervised the whole activity, helped in drafting and revised the whole \ndocument critically for proof of concepts. KHK helped in improving the quality of content with the revised version of the \nmanuscript. All authors read and approved the final manuscript.\n\nAuthor details\n1 Bahria University, Shangrilla Road, Sector E-8, Islamabad, Pakistan. 2 COMSATS Institute of Information Technology, \nKamra Road, Attock, Pakistan. 3 University of Malaya, Kuala Lumpur, Malaysia. 4 University of Haripur, Hattar Road, Haripur, \nPakistan. \n\nAcknowledgments\nWe would like to thank Bahria University, Islambad for providing the necessary environment and support to carry out this \nwork.\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\n\n\nPage 17 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nReceived: 1 November 2015   Accepted: 18 January 2016\n\nReferences\nAkkaya Cem, Janyce Wiebe, Rada Mihalcea (2009) Subjectivity word sense disambiguation. In: Proceedings of the 2009 \n\nConference on Empirical Methods in Natural Language Processing EMNLP\nAna C, Arlindo LO (2007) Semi-supervised single-label text categorization using centroid-based classifiers. ACM 844–851\nAoyama M (2002) A business-driven web service creation methodology, saint-w. IEEE\nBar-Haim R, Dinur E, Feldman R, Fresko M, Goldstein G (2011) Identifying and following expert investors in stock micro-\n\nblogs, In: Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2011)\nBasu A et al (2003) Support vector machines for text categorization. In: Proceedings of the IEEE Hawaii International \n\nconference on system sciences\nBrown MPS et al (1997) Support vector machine classification of microarray gene expression data. In: Proceedings of the \n\nNational academy of science, p 262–267\nCailliau F, Cavet A (2013) Mining automatic speech transcripts for the retrieval of problematic calls. Comput Linguist Intell \n\nText Process 83–95\nCambria E, Schuller B, Xia Y, Havasi C (2013) New avenues in opinion mining and sentiment analysis. IEEE Intell Syst \n\n2:15–21\nCambria E, White B (2014) Jumping NLP curves: a review of natural language processing research [review article]. Com-\n\nput Intell Mag IEEE 9(2):48–57\nChau M, Xu J (2007) Mining communities and their relationships in blogs: a study of online hate groups. Int J Hum \n\nComput Stud 65(1):57–70\nChizi B, Rokach L, Maimon O (2009) A survey of feature selection techniques. Encyclopedia of data warehousing and \n\nmining. 1888–1895\nChuang W, Tiyyagura A, Yang J, Giuffrida G (2000) A fast algorithm for hierarchical text classification. In: 2nd International \n\nConference on Data Warehousing and Knowledge Discovery, p 409–418\nDai Qingliang Miao Qiudan Li Ruwei (2009) AMAZING: a sentiment mining and retrieval system. Expert Syst Appl \n\n36:7192–7198\nDai W et al (2007) Transferring Naive Bayes Classifiers for Text Classification. In: Proceedings of the Twenty-Second AAAI \n\nConference on Artificial Intelligence 2007\nDing X, Liu B (2007) The utility of linguistic rules in opinion mining. In: Proceedings of the 30th Annual International ACM \n\nSIGIR Conference on Research and Development in Information Retrieval, Amsterdam, The Netherlands\nDing X, Liu B (2010) Resolving object and attribute conference in opinion mining, COLING 2010. In: 23rd International \n\nConference on Computational Linguistics, Proceedings of the Conference 2010\nFeldman R et al (2011) The stock sonar-sentiment analysis of stocks based on a hybrid approach. In: Twenty-Third IAAI \n\nConference\nGangemi A, Presutti V, Recupero RD (2014) Frame-based detection of opinion holders and topics: a model and a tool. \n\nComput Intell Magaz IEEE 9(1):20–30\nGroh G, Hauffa J (2011) Characterizing social relations via NLP based sentiment analysis. In: Proceedings of the 5th Inter-\n\nnational AAAI Conference on Weblogs and Social Media ICWSM, 2011\nGuan H, Zhou J, Guo M (2009) A class-feature-centroid classifier for text categorization, In Proceedings of the 18th inter-\n\nnational conference on World wide web Madrid. 2009\nGuellil I, Boukhalfa K (2015) Social big data mining: a survey focused on opinion mining and sentiments analysis. In: \n\nProgramming and Systems (ISPS), 2015 12th International Symposium IEEE, p 1–10\nHai Z, Chang K, Kim JJ, Yang CC (2014) Identifying features in opinion mining via intrinsic and extrinsic domain relevance. \n\nIEEE Trans Knowl Data Eng 26(3):623–634\nHan EH and Karypis G (2000) Analysis and experimental results. In: Principles of Data Mining and Knowledge Discovery, \n\nproceedings of the 4th European conference on centroid based document classification, p 424–431\nHidayet T, Tunga G (2012) A high performance centroid-based classification approach for language identification. Pattern \n\nRecognit Lett 33:2077–2084\nHu G, Jingyu Z, Minyi G (2009) A class-feature-centroid classifier for text categorization. ACM 201–210\nHull D (1994) Improving text retrieval for the routing problem using latent semantic indexing. In: Proceedings of SIGIR-94, \n\np 282–289\nJaafar N, Al-Jadaan M, Alnutaifi R (2015) Framework for social media big data quality analysis. In New Trends in Database \n\nand Information Systems II. Springer International Publishing, p 301–314\nJindal N, Liu B (2006) Mining comparative sentences and relations. AAAI Press, Menlo Park, pp 1331–1336\nJoachims T (1998) Text categorization with support vector machines: learning with many relevant features. In: Proceed-\n\nings of the European conference of machine learning\nKamps J, Marx M, Mokken RJ, De Rijke M (2004) Using WordNet to measure semantic orientation of adjectives. In: Pro-\n\nceedings of 4th International Conference on Language Resources and Evaluation, p 1115-1118\nKatz G, Elovici Y, Shapira B (2014) CoBAn: a context based model for data leakage prevention. Inform Sci 262:137–158\nKatz G, Ofek N, Shapira B (2015) ConSent: context-based sentiment analysis. Knowl Syst 84:162–178\nKhan MT, Khalid S (2015) Sentiment Analysis for Health Care. Int J Priv Health Inform Manag 3(2):78–91. doi:10.4018/\n\nIJPHIM.2015070105\nLertnattee V, Theeramunkong T (2004) Effect of term distributions on centroid-based text categorization. Inf Sci \n\n158(1):89–115\nLertnattee V, Theeramunkong T (2006) Class normalization in centroid-based text categorization. Inf Sci \n\n176(12):1712–1738\n\nhttp://dx.doi.org/10.4018/IJPHIM.2015070105\nhttp://dx.doi.org/10.4018/IJPHIM.2015070105\n\n\nPage 18 of 19Khan et al. Complex Adapt Syst Model  (2016) 4:2 \n\nLi B, Yu S, Lu Q (2003) An improved k-nearest neighbor algorithm for text categorization. CoRR 0306099\nLim EP, Nguyen VA, Jindal N, Liu B, Lauw HW (2010) Detecting product review spammers using rating behaviors. In: \n\nProceedings of ACM International Conference on Information and Knowledge Management CIKM\nMachova K, Marhefka L (2014) Opinion classification in conversational content using N-grams. In: Recent Developments \n\nin Computational Collective Intelligence, Springer International Publishing, p 177–186\nMedhat W, Hassan A, Korashy H (2014) Sentiment analysis algorithms and applications: a survey. Ain Shams Eng J \n\n5(4):1093–1113\nMihalcea R, Strapparava C (2009) The lie detector: Explorations in the automatic recognition of deceptive language, In: \n\nProceedings of the ACL-IJCNLP 2009\nMohammad S (2011) Once upon a time to happily ever after: tracking emotions in novels and fairy tales. In: Proceedings \n\nof the ACL 2011 Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) \n2011\n\nMohammad SM, Yang TW (2011) Tracking sentiment in mail: how genders differ on emotional axes. In: Proceedings of \nthe ACL Workshop on ACL 2011, Workshop on Computational Approaches to Subjectivity and Sentiment Analysis \nWASSA 2011\n\nMukherjee A et al (2011) Detecting group review spam. ACM 93-94\nMukherjee A, Liu B (2012) Spotting fake reviewer groups in consumer reviews. ACM\nMukherjee A et al (2011) Detecting group review spam. ACM, p 93–94\nNarayanan R et al (2009) Sentiment analysis of conditional sentences. ACL, p180–189\nNguyen DT, Hwang D, Jung JJ (2015) Time-frequency social data analytics for understanding social big data. In Intelligent \n\nDistributed Computing VIII. Springer International Publishing 223–228\nNiazi M, Hussain A (2009) Agent-based tools for modeling and simulation of selforganization in peer-to-peer, ad hoc, and \n\nother complex networks. IEEE Commun Magaz 47(3):166–173\nNiazi M and Hussain A (2011) Social network analysis of trends in the consumer electronics domain, consumer electron-\n\nics (ICCE) 2011. IEEE international conference, p 219–220\nNiazi MA, Siddique Q, Hussain A, Kolberg M (2010) Verification and validation of an agent-based forest fire simulation \n\nmodel. In: Proceedings of the 2010 Spring Simulation Multiconference\nOfek N, Rokach L, Mitra P (2014) Methodology for connecting nouns to their modifying adjectives. In Comput Linguist \n\nIntell Text Process 271–284\nOtt M, Choi Y, Cardie C, Hancock JT (2011) Finding deceptive opinion spam by any stretch of the imagination. In: Pro-\n\nceedings of the 49th Annual Meeting of the Association for Computational Linguistics ACL\nOzgur L, Gungor T (2009) Text classification with the support of pruned dependency patterns. Pattern Recognit Lett. \n\n31:1598–1607\nPang B, Lee L (2008) Opinion mining and sentiment analysis. Fund Trends Inf Ret 2:1–2\nPatella M, Ciaccia P (2009) Approximate similarity search: a multi-faceted problem. J Discrete Algorithms 7:36–48\nPoria S, Gelbukh A, Hussain A, Howard N, Das D, Bandyopadhyay S (2013) Enhanced SenticNet with affective labels for \n\nconcept-based opinion mining. IEEE Intell Syst 2:31–38\nQiu G et al (2011) Opinion word expansion and target extraction through double propagation. Comput Linguist l37:9–27\nRabade R, Mishra N, Sharma S (2014) Survey of influential user identification techniques in online social networks. In: \n\nRecent Advances in Intelligent Informatics. Springer International Publishing, p 359–370\nSakunkoo P, Sakunkoo N (2009) Analysis of social influence in online book reviews, Proceedings of 3rd International AAAI \n\nConference on Weblogs and Social Media ICWSM, 2009\nSchouten K, Frasincar F (2015) Survey on aspect-level sentiment analysis. 99\nShankar S, Karypis G (2000) Weight adjustment schemes for a centroid based classifier, Army High Performance Comput-\n\ning Research Center\nShin K, Abraham A, Han S (2006) Improving kNN text categorization by removing outliers from training set. Comput \n\nLinguis Intell Text Process 3878:563–566\nSomprasertsri G, Latitrojwong P (2010) Mining feature-opinion in online customer reviews for opinion summarization. \n\n16:938–955\nSoucy P, Mineau GW (2001) A simple K-NN algorithm for text categorization. In: Proceedings of ICDM-01, IEEE Interna-\n\ntional Conference on Data Mining, p 647–648\nSreemathy J, Balamurugan PS (2012) An efficient text classification using K-NN and Naive Bayesian. Engg Journals Publi-\n\ncations, London\nStoyanov V, Cardie C (2006) Partially supervised conference resolution for opinion summarization through structured rule \n\nlearning. In: Proceedings of Conference on Empirical Methods in Natural Language Processing EMNLP 2006\nTakeuchi H, Yamaguchi T (2014) Text mining of business-oriented conversations at a call center. In Data Mining for \n\nService, p 111–129\nTan S (2007a) Large margin dragpushing strategy for centroid text categorization. Expert Syst Appl 33(1):215–220\nTan S (2007b) An improved centroid classifier for text categorization. Expert Syst Appl 35(1–2):279–285\nTan S, Cheng X, Ghanem M, Wang B, Xu H (2005) A novel refinement approach for text categorization. CIKM 469-476\nTang J, Chang Y, Liu H (2014) Mining social media with social theories: a survey. ACM SIGKDD Explor Newslett 15(2):20–29\nTang J, Hu X, Liu H (2013) Social recommendation: a review. Soc Netw Anal Min 3(4):1113–1133\nTsai ACR, Wu CE, Tsai RTH, Hsu JYJ (2013) Building a concept-level sentiment dictionary based on commonsense. Knowl \n\nIEEE Intell Syst 2:22–30\nTuveri F, Angioni M (2014) An opinion mining model for generic domains distributed systems and applications of infor-\n\nmation filterin",
      "metadata_storage_path": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9saWJyYXJ5L3M0MDI5NC0wMTYtMDAxNi05LnBkZg2",
      "metadata_author": "Muhammad Taimoor Khan",
      "metadata_title": "Sentiment analysis and the complex natural language"
    }
  ]
}