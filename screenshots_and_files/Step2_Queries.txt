Index: Courses Index
Query: Github
URL: https://projectthreecognitivesearch.search.windows.net/indexes/courses-index/docs?api-version=2021-04-30-Preview&search=github

JSON:
{
  "@odata.context": "https://projectthreecognitivesearch.search.windows.net/indexes('courses-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 7.3471584,
      "source": "MS Learn",
      "title": "Create and host web sites by using GitHub Pages",
      "description": "Learn how to host your personal, organization, and project sites for free with GitHub Pages.",
      "level": "beginner",
      "role": "developer",
      "product": "github",
      "duration": "72",
      "rating_count": "118",
      "rating_average": "2022-04-08T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-host-web-sites-github-pages/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wcm9qZWN0dGhyZWVkYXRhL2NvdXJzZXMuY3N2OzQwOA2",
      "skills": [
        "personal, organization",
        "project sites",
        "GitHub Pages"
      ]
    },
    {
      "@search.score": 4.073926,
      "source": "MS Learn",
      "title": "Create and host web sites by using GitHub Pages",
      "description": "Learn how to host your personal, organization, and project sites for free with GitHub Pages.",
      "level": "beginner",
      "role": "student",
      "product": "github",
      "duration": "72",
      "rating_count": "118",
      "rating_average": "2022-04-08T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-host-web-sites-github-pages/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wcm9qZWN0dGhyZWVkYXRhL2NvdXJzZXMuY3N2OzQxMg2",
      "skills": [
        "personal, organization",
        "project sites",
        "GitHub Pages"
      ]
    },
    {
      "@search.score": 3.6714084,
      "source": "MS Learn",
      "title": "Create and host web sites by using GitHub Pages",
      "description": "Learn how to host your personal, organization, and project sites for free with GitHub Pages.",
      "level": "beginner",
      "role": "developer",
      "product": "azure",
      "duration": "72",
      "rating_count": "118",
      "rating_average": "2022-04-08T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-host-web-sites-github-pages/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wcm9qZWN0dGhyZWVkYXRhL2NvdXJzZXMuY3N2OzQwOQ2",
      "skills": [
        "personal, organization",
        "project sites",
        "GitHub Pages"
      ]
    },
    {
      "@search.score": 3.5723422,
      "source": "MS Learn",
      "title": "Create and host web sites by using GitHub Pages",
      "description": "Learn how to host your personal, organization, and project sites for free with GitHub Pages.",
      "level": "beginner",
      "role": "devops-engineer",
      "product": "github",
      "duration": "72",
      "rating_count": "118",
      "rating_average": "2022-04-08T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-host-web-sites-github-pages/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wcm9qZWN0dGhyZWVkYXRhL2NvdXJzZXMuY3N2OzQwNA2",
      "skills": [
        "personal, organization",
        "project sites",
        "GitHub Pages"
      ]
    },
    {
      "@search.score": 3.5723422,
      "source": "MS Learn",
      "title": "Create and host web sites by using GitHub Pages",
      "description": "Learn how to host your personal, organization, and project sites for free with GitHub Pages.",
      "level": "beginner",
      "role": "solution-architect",
      "product": "github",
      "duration": "72",
      "rating_count": "118",
      "rating_average": "2022-04-08T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-host-web-sites-github-pages/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wcm9qZWN0dGhyZWVkYXRhL2NvdXJzZXMuY3N2OzQxMA2",
      "skills": [
        "personal, organization",
        "project sites",
        "GitHub Pages"
      ]
    },
    {
      "@search.score": 3.267975,
      "source": "MS Learn",
      "title": "Create and host web sites by using GitHub Pages",
      "description": "Learn how to host your personal, organization, and project sites for free with GitHub Pages.",
      "level": "beginner",
      "role": "administrator",
      "product": "github",
      "duration": "72",
      "rating_count": "118",
      "rating_average": "2022-04-08T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-host-web-sites-github-pages/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wcm9qZWN0dGhyZWVkYXRhL2NvdXJzZXMuY3N2OzQwNg2",
      "skills": [
        "personal, organization",
        "project sites",
        "GitHub Pages"
      ]
    },
    {
      "@search.score": 3.0481017,
      "source": "MS Learn",
      "title": "Create and host web sites by using GitHub Pages",
      "description": "Learn how to host your personal, organization, and project sites for free with GitHub Pages.",
      "level": "beginner",
      "role": "student",
      "product": "azure",
      "duration": "72",
      "rating_count": "118",
      "rating_average": "2022-04-08T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-host-web-sites-github-pages/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wcm9qZWN0dGhyZWVkYXRhL2NvdXJzZXMuY3N2OzQxMw2",
      "skills": [
        "personal, organization",
        "project sites",
        "GitHub Pages"
      ]
    },
    {
      "@search.score": 2.5890563,
      "source": "MS Learn",
      "title": "Automate the deployment of ARM templates by using GitHub Actions",
      "description": "Learn how to deploy Azure Resource Manager (ARM) templates by using Azure PowerShell, the Azure CLI, and GitHub Actions.",
      "level": "intermediate",
      "role": "administrator",
      "product": "azure",
      "duration": "66",
      "rating_count": "27",
      "rating_average": "2022-04-07T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/deploy-templates-command-line-github-actions/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wcm9qZWN0dGhyZWVkYXRhL2NvdXJzZXMuY3N2OzIx0",
      "skills": [
        "Azure Resource Manager",
        "Azure PowerShell",
        "Azure CLI",
        "ARM) templates",
        "GitHub Actions"
      ]
    },
    {
      "@search.score": 2.5861955,
      "source": "MS Learn",
      "title": "Create and host web sites by using GitHub Pages",
      "description": "Learn how to host your personal, organization, and project sites for free with GitHub Pages.",
      "level": "beginner",
      "role": "solution-architect",
      "product": "azure",
      "duration": "72",
      "rating_count": "118",
      "rating_average": "2022-04-08T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-host-web-sites-github-pages/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wcm9qZWN0dGhyZWVkYXRhL2NvdXJzZXMuY3N2OzQxMQ2",
      "skills": [
        "personal, organization",
        "project sites",
        "GitHub Pages"
      ]
    },
    {
      "@search.score": 2.2928762,
      "source": "MS Learn",
      "title": "Create and host web sites by using GitHub Pages",
      "description": "Learn how to host your personal, organization, and project sites for free with GitHub Pages.",
      "level": "beginner",
      "role": "administrator",
      "product": "azure",
      "duration": "72",
      "rating_count": "118",
      "rating_average": "2022-04-08T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-host-web-sites-github-pages/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wcm9qZWN0dGhyZWVkYXRhL2NvdXJzZXMuY3N2OzQwNw2",
      "skills": [
        "personal, organization",
        "project sites",
        "GitHub Pages"
      ]
    },
    {
      "@search.score": 2.2673016,
      "source": "MS Learn",
      "title": "Automate the deployment of ARM templates by using GitHub Actions",
      "description": "Learn how to deploy Azure Resource Manager (ARM) templates by using Azure PowerShell, the Azure CLI, and GitHub Actions.",
      "level": "intermediate",
      "role": "administrator",
      "product": "vs-code",
      "duration": "66",
      "rating_count": "27",
      "rating_average": "2022-04-07T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/deploy-templates-command-line-github-actions/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wcm9qZWN0dGhyZWVkYXRhL2NvdXJzZXMuY3N2OzIy0",
      "skills": [
        "Azure Resource Manager",
        "Azure PowerShell",
        "Azure CLI",
        "ARM) templates",
        "GitHub Actions"
      ]
    },
    {
      "@search.score": 2.2673016,
      "source": "MS Learn",
      "title": "Automate the deployment of ARM templates by using GitHub Actions",
      "description": "Learn how to deploy Azure Resource Manager (ARM) templates by using Azure PowerShell, the Azure CLI, and GitHub Actions.",
      "level": "intermediate",
      "role": "solution-architect",
      "product": "azure",
      "duration": "66",
      "rating_count": "27",
      "rating_average": "2022-04-07T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/deploy-templates-command-line-github-actions/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wcm9qZWN0dGhyZWVkYXRhL2NvdXJzZXMuY3N2OzI10",
      "skills": [
        "Azure Resource Manager",
        "Azure PowerShell",
        "Azure CLI",
        "ARM) templates",
        "GitHub Actions"
      ]
    },
    {
      "@search.score": 1.3574673,
      "source": "MS Learn",
      "title": "Create and host web sites by using GitHub Pages",
      "description": "Learn how to host your personal, organization, and project sites for free with GitHub Pages.",
      "level": "beginner",
      "role": "devops-engineer",
      "product": "azure",
      "duration": "72",
      "rating_count": "118",
      "rating_average": "2022-04-08T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/create-host-web-sites-github-pages/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wcm9qZWN0dGhyZWVkYXRhL2NvdXJzZXMuY3N2OzQwNQ2",
      "skills": [
        "personal, organization",
        "project sites",
        "GitHub Pages"
      ]
    },
    {
      "@search.score": 1.3306602,
      "source": "MS Learn",
      "title": "Automate the deployment of ARM templates by using GitHub Actions",
      "description": "Learn how to deploy Azure Resource Manager (ARM) templates by using Azure PowerShell, the Azure CLI, and GitHub Actions.",
      "level": "intermediate",
      "role": "developer",
      "product": "vs-code",
      "duration": "66",
      "rating_count": "27",
      "rating_average": "2022-04-07T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/deploy-templates-command-line-github-actions/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wcm9qZWN0dGhyZWVkYXRhL2NvdXJzZXMuY3N2OzI00",
      "skills": [
        "Azure Resource Manager",
        "Azure PowerShell",
        "Azure CLI",
        "ARM) templates",
        "GitHub Actions"
      ]
    },
    {
      "@search.score": 1.3306602,
      "source": "MS Learn",
      "title": "Automate the deployment of ARM templates by using GitHub Actions",
      "description": "Learn how to deploy Azure Resource Manager (ARM) templates by using Azure PowerShell, the Azure CLI, and GitHub Actions.",
      "level": "intermediate",
      "role": "solution-architect",
      "product": "vs-code",
      "duration": "66",
      "rating_count": "27",
      "rating_average": "2022-04-07T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/deploy-templates-command-line-github-actions/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wcm9qZWN0dGhyZWVkYXRhL2NvdXJzZXMuY3N2OzI20",
      "skills": [
        "Azure Resource Manager",
        "Azure PowerShell",
        "Azure CLI",
        "ARM) templates",
        "GitHub Actions"
      ]
    },
    {
      "@search.score": 1.1454985,
      "source": "MS Learn",
      "title": "Automate the deployment of ARM templates by using GitHub Actions",
      "description": "Learn how to deploy Azure Resource Manager (ARM) templates by using Azure PowerShell, the Azure CLI, and GitHub Actions.",
      "level": "intermediate",
      "role": "developer",
      "product": "azure",
      "duration": "66",
      "rating_count": "27",
      "rating_average": "2022-04-07T00:00:00Z",
      "url": "https://docs.microsoft.com/en-us/learn/modules/deploy-templates-command-line-github-actions/?WT.mc_id=api_CatalogApi",
      "AzureSearch_DocumentKey": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9wcm9qZWN0dGhyZWVkYXRhL2NvdXJzZXMuY3N2OzIz0",
      "skills": [
        "Azure Resource Manager",
        "Azure PowerShell",
        "Azure CLI",
        "ARM) templates",
        "GitHub Actions"
      ]
    }
  ]
}



Index: library Index
Query: Ling Chen
URL: https://projectthreecognitivesearch.search.windows.net/indexes/azureblob-index/docs?api-version=2021-04-30-Preview&search=Ling%20Chen

{
  "@odata.context": "https://projectthreecognitivesearch.search.windows.net/indexes('azureblob-index')/$metadata#docs(*)",
  "value": [
    {
      "@search.score": 6.6314616,
      "content": "\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 \nDOI 10.1186/s40467-015-0033-9\n\nRESEARCH Open Access\n\nExtraction methods for uncertain inference\nrules by ant colony optimization\nLing Chen, Yun Sun* and Yuanguo Zhu\n\n*Correspondence:\nchinalsy_881220@163.com\nSchool of Science, Nanjing\nUniversity of Science and\nTechnology, Nanjing 210094, China\n\nAbstract\n\nIn recent years, the research on data mining methods has received increasing\nattention. In this paper, we design an uncertain system with the extracted uncertain\ninference rules to solve the classification problems in data mining. And then, two\nextraction methods integrated with ant colony optimization are proposed for the\ngeneration of the uncertain inference rules. Finally, two applications are given to verify\nthe effectiveness and superiority of the proposed methods.\n\nKeywords: Uncertain inference rule; Uncertain system; Ant colony optimization\nalgorithm; Rules extraction; Data classification\n\nIntroduction\nNowadays, databases and computer networks, coupled with the use of advanced auto-\nmated data generation and collection tools, are widely used in many different fields such\nas finance, E-commerce, logistics, etc. As a result, the amount of data that people have\nto deal with is dramatically increasing. People hope to carry out scientific research, busi-\nness decision, or business management on the basis of the analysis of the existing data.\nHowever, the current data analysis tools have difficulty in processing the data in depth.\nTo compensate for this deficiency, there come the data mining techniques. Data mining is\nthe computational process of discovering some interesting, potentially useful patterns in\nlarge data sets. Those patterns can be concepts, rules, laws, and modes. The overall goal\nof data mining is to extract information from a data set and transform it into an under-\nstandable structure for further use. Data mining helps us to discover valuable information\nand knowledge. Data mining is applied to many fields in reality. There are many successful\nexamples [1] of data mining in business and science research. For instance, data mining is\nwidely used in financial data analysis, telecommunication, retail, and biomedical research.\nTherefore, the study of data mining technology has an important practical significance.\nThe main jobs of data mining are data description, data classification, data dependency,\n\ndata compartment analysis, data regression, data aggregate, and data prediction. What\ndata classification does is to find a couple of models or functions that can accurately\ndescribe the characteristics of the data sets. Then, we can identify the categories of the\npreviously unknown data. After obtaining the models or functions from the set of training\ndata with data mining algorithms, we use many methods to describe the output such as\nclassification rules (if-then), decision trees, mathematical formula, and neutral network.\n\n© 2015 Chen et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: chinalsy_881220@163.com\nhttp://creativecommons.org/licenses/by/4.0\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 2 of 19\n\nThere are a variety of approaches in data mining. For mining objects in different fields,\nmany different specified methods are invented. The approaches we usually used are statis-\ntical methods, machine learning methods, and modern intelligent optimization methods.\nThe statistical methods are very effective methods from the start. In addition, many other\ndata mining methods are invented based on the statistical methods. When dealing with\nclassification problems, Bayesian classification and Bayesian belief network are important\nclassification methods that based on the statistical principle. Machine learning methods\nare mainly used to solve the conceptual learning, pattern classification, and pattern clus-\ntering problems. The core content of machine learning is inductive learning. And there\nalready exist a number of mature technology methods, such as decision tree method for\nclassification problems. Decision trees method is one of the most popular classification\nmethods. The early decision trees algorithm is ID3 method. Later, based on ID3, many\nalgorithms such as C4.5 method [2] are proposed. Besides, there are some variants of the\ndecision trees algorithm including incremental tree structure ID4, ID5, and expandable\ntree structure SLIQ for massive data set.\nIn recent years, intelligent optimization algorithms are widely applied into data min-\n\ning. Neutral network is a simulation model for complex system with nonlinear relations.\nIt is very suitable to deal with complex nonlinear relations in spatial data. Researchers\nhave already proposed different network models to realize the clustering, classification,\nregression, and pattern recognition of the data. Furthermore, many evolution algorithms\nsuch as simulated annealing algorithm are introduced into neutral network algorithm\nas the optimization strategies. Genetic algorithm is a global search algorithm that sim-\nulates the biological evolution and genetic mechanism. It plays an important role in\noptimization and classification machine learning. Mixed algorithms of genetic algorithm\nand other algorithms, such as decision trees, neutral network, have been applied to the\ndata mining technology. Ant colony optimization algorithm is a bionic optimization algo-\nrithm that simulates the behavior of the ants. Based on that, a data mining technique\nant-miner [3] was invented. And Herrera [4] applied it to fuzzy rules learning. How-\never, ant colony optimization algorithm has some weakness such as slow convergence,\nrandom initial solutions. For this reason, some improved ant colony optimization algo-\nrithms are proposed. Zhu proposed an improved ant colony optimization algorithm\n(ACOA) [5] and a mutation ant colony optimization algorithm (MACO) [6] to speed up\nthe algorithms and avoid the solutions getting stuck in local optimums. Hybrid genetic\nant colony optimization [7] and hybrid particle swarm ant colony optimization algo-\nrithm [8] significantly improve the performance of the original ant colony optimization\nalgorithm.\nThe real world is so complex that human being may face different types of indetermi-\n\nnacy everyday. To get a better understanding of the real world, many mathematical tools\nare created. One of them is probability theory which is used to model indeterminacy from\nsamples. However, in many cases, no samples are available to estimate a probability distri-\nbution. In this situation, we have no choice but to invite some domain experts to evaluate\nthe belief degree that each event may occur. We cannot use probability theory to deal\nwith belief degree since human beings usually overweight unlikely events which makes\nthe belief degrees deviate far from the frequency. In view of this, Liu [9] founded uncer-\ntainty theory based on normality axiom, duality axiom, subadditivity axiom, and product\nmeasure axiom. It has become a powerful mathematical tool dealing with indeterminacy.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 3 of 19\n\nMany researchers have done a lot of theoretical work related to uncertainty theory. In\n2008, Liu [10] presented the uncertain differential equation. Later, the existence and\nuniqueness theorem was given [11]. And the stability of uncertain differential equation\nwas discussed [12,13]. Also, some analysis and numerical methods for solving uncertain\ndifferential equation were proposed. With uncertain differential equation describing the\nevolution of the system, we may solve some practical problems. Peng and Yao [14] stud-\nied an option pricing models for stocks. Zhu [15] proposed an uncertain optimal control\nmodel in 2010.\nIn [16,17], Liu proposed and studied the uncertain systems based on the concepts of\n\nuncertain sets, membership functions, and uncertain inference rules. An uncertain sys-\ntem is a function from its inputs to outputs based on the uncertain inference rule. Usually,\nan uncertain system consists of five parts: inputs, rule-base, uncertain inference rules,\nexpected value operator, and outputs. Following that, Gao et al. [18] generalized uncertain\ninference rules and described uncertain systems with them. Peng and Chen [19] proved\nthat uncertain systems are universal approximator and then demonstrated that the uncer-\ntain controller is a reasonable tool. Gao [20] designed an uncertain inference controller\nthat successfully balanced an inverted pendulum with 5 × 5 if-then rules. What is more\nimportant is that this uncertain inference controller has a good ability of robustness.\nOn the basis of uncertainty theory, we consider two extraction methods for uncertain\n\ninference rules by ant colony optimization algorithm. In the next section, we review the\nant colony optimization algorithm and give some basic concepts about uncertain sets.\nThen, we formulate a model to extract inference rules based on data set. And then, we\npropose an extraction method for uncertain inference rules by ant colony optimization\nalgorithm with a mutation operation. Finally, we combine the ant colony optimiza-\ntion algorithm with simulated annealing algorithm to speed up the extraction method.\nIn the last section, we discuss two typical classification problems in data mining with\nour results.\n\nPreliminary\nIn this section, we review the ant colony optimization algorithm. And then, we give some\nbasic concepts on uncertainty sets.\n\nAnt colony optimization algorithm\n\nAnt colony optimization algorithm, initiated by Dorigo, is a heuristic optimization\napproach. It simulates the behavior of real ants when they forage for food which relies on\nthe pheromone communication. In ant colony optimization algorithm, each path of artifi-\ncial ants walking from the food sources to the nest is a candidate solution to the problem.\nWhen walking on the path, the ants will release pheromone which evaporates over time.\nAnd the artificial ants will lay down more pheromone on the path corresponding to the\nbetter solution. While one ant has many paths to go, it will make a choice according to\nthe amount of the pheromone on the paths. The more pheromone there is on the path,\nthe better the solution is. As a result, bad paths will disappear since the pheromone evap-\norates over time. And good paths will be reserved since ants walking on it increases the\npheromone levels. Finally, one path which is used by most of the ants is left. Then, the\noptimal solution to the problem is obtained.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 4 of 19\n\nConsider the following optimization problem:\n\n⎧⎪⎪⎪⎨\n⎪⎪⎪⎩\nmin f (x)\ns.t.\n\ng(x) ≥ 0\nx ∈ D\n\n(1)\n\nwhere x is the decision variable in the domain D. And f (x) is the objective function while\ng(x) is the constraint function.\nWe can use ant colony optimization algorithm to obtain the optimal solution to the\n\nproblem (1). The parameters in the algorithm are initial pheromone τ0, ant transfer prob-\nability p, number of ants M, pheromone evaporation rate ρ, and number of iterations T.\nThe procedures are as follows.\n\nStep 1 Randomly generate a feasible solution x0 and set optimal solution s = x0. Initialize\nall pheromone trails with the same pheromone level τ0. Set k ← 0.\nStep 2 The artificial ant generates a walking path x in some probability p according to\n\nthe pheromone trails. If x ∈ D, then go to Step 3; otherwise, repeat Step 2 until x ∈ D.\nStep 3 Repeat Step 2 until for each ant and generate M feasible solutions. Let sk be the\n\nbest solution in this iteration.\nStep 4 If f (sk) < f (s), then s ← sk and update the pheromone trails according to the\n\noptimal solution in the current iteration.\nStep 5 If k < T, then k ← k + 1 and go to Step 2; otherwise, terminate.\nStep 6 Report the optimal solution.\n\nUncertain set\n\nLet � be a nonempty set and L be σ -algebra over �. Each � ∈ L is called an event. For\nany �, M{�} ∈ [0, 1]. The set function M defined on L is called an uncertain measure\nif it satisfies the following three axiom: M{�} = 1; M{�} + M{�c} = 1 for any � ∈ L;\nM\n\n{⋃∞\ni=1 �i\n\n} ≤ ∑∞i=1 M{�i} for all �1, �2, · · · ∈ L. Then, the triplet (�, L, M) is called\nan uncertainty space [9]. The product uncertain measure M is an uncertain measure sat-\nisfying M\n\n{∏∞\ni=1 �k\n\n} = ∞∧\ni=1\n\nMk{�k}, where �k are arbitrarily chosen events from Lk for\nk = 1, 2, · · · , respectively.\n\nDefinition 1. [16] An uncertain set is a function ξ from an uncertainty space (�, L, M)\nto a collection of sets of real numbers such that both {B ⊂ ξ } and {ξ ⊂ B} are events for\nany Borel set B.\n\nExample 1. Take (�, L, M) to be {γ1, γ2, γ3} with power set L. Then, the set-valued\nfunction\n\nξ(γ ) =\n\n⎧⎪⎪⎨\n⎪⎪⎩\n[ 1, 3] , if γ = γ1\n[ 2, 4] , if γ = γ2\n[ 3, 5] , if γ = γ3\n\nis an uncertain set on (�, L, M).\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 5 of 19\n\nDefinition 2. [16] The uncertain sets ξ1, ξ2, ξ3, · · · , ξn are said to be independent if for\nany Borel sets B1, B2, B3, · · · , Bn, we have\n\nM\n\n{ n⋂\ni=1\n\n(\nξ\n\n∗\ni ⊂ Bi\n\n)} = n∧\ni=1\n\nM\n{\nξ\n\n∗\ni ⊂ Bi\n\n}\nand\n\nM\n\n{ n⋃\ni=1\n\n(\nξ\n\n∗\ni ⊂ Bi\n\n)} = n∨\ni=1\n\nM\n{\nξ\n\n∗\ni ⊂ Bi\n\n}\n\nwhere ξ ∗i are arbitrarily chosen from\n{\nξi, ξ ci\n\n}\n, i = 1, 2, · · · , n, respectively.\n\nDefinition 3. [21] An uncertain set ξ is said to have a membership function μ if for any\nBorel set B of real numbers, we have\n\nM{B ⊂ ξ } = inf\nx∈B\n\nμ(x), M{ξ ⊂ B} = 1 − sup\nx∈Bc\n\nμ(x).\n\nThe above equations will be called measures inversion formulas.\n\nRemark 1. When an uncertain set ξ does have a membership function μ, it follows\nfrom the first measure inversion formula that\n\nμ(x) = M{x ∈ ξ }.\n\nExample 2. An uncertain set ξ is called triangular if it has a membership function\n\nμ(x) =\n⎧⎨\n⎩\n\nx−a\nb−a , a ≤ x ≤ b\nx−c\nb−c , b ≤ x ≤ c\n\n(2)\n\ndenoted by (a, b, c) where a, b, c are real numbers with a < b < c.\n\nDefinition 4. [21] A membership function μ is said to be regular if there exists a point x0\nsuch that μ(x0) = 1, and μ(x) is unimodal about the mode x0. That is, μ(x) is increasing\non (−∞, x0] and decreasing on [ x0, +∞).\n\nDefinition 5. [16] Let ξ be an uncertain set. Then, the expected value of ξ is defined by\n\nE[ ξ] =\n∫ +∞\n0\n\nM{ξ \n r}dr −\n∫ 0\n\n−∞\nM{ξ � r}dr\n\nprovided that at least one of the two integrals is finite and\n\nM{ξ \n r} = 1\n2\n(M{ξ ≥ r} + 1 − M{ξ < r}),\n\nM{ξ � r} = 1\n2\n(M{ξ ≤ r} + 1 − M{ξ > r}).\n\nTheorem 1. [13] Let ξ be an uncertain set with regular membership function μ. Then\n\nE[ ξ] = x0 +\n1\n2\n\n∫ +∞\nx0\n\nμ(x)dx − 1\n2\n\n∫ x0\n−∞\n\nμ(x)dx, (3)\n\nwhere x0 is a point such that μ(x0) = 1.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 6 of 19\n\nExample 3. Let ξ be a triangular uncertain set denoted by (a, b, c). Then, according to\nTheorem 1, we have\n\nE[ ξ] = a + 2b + c\n4\n\n.\n\nIn fact, it follows from Equations 2 and 3 that\n\nE[ ξ] = b + 1\n2\n\n∫ c\nb\n\nx − c\nb − cdx −\n\n1\n2\n\n∫ b\na\n\nx − a\nb − adx\n\n= b − 1\n4\n(b − c) − 1\n\n4\n(b − a)\n\n= a + 2b + c\n4\n\n.\n\nUncertain inference rule\n\nHere, we introduce concepts of the uncertain inference and uncertain system. Inference\nrules are the key points of the inference systems. In fuzzy systems, CRI approach [22],\nMamdani inference rules [23] and Takagi-Sugeno inference rules [24] are the most com-\nmon used inference rules. Fuzzy if-then inference rules use fuzzy sets to describe the\nantecedents and the consequents. Unlike fuzzy inference, both antecedents and conse-\nquents in uncertain inference are characterized by uncertain sets. Uncertain inference\n[16] is a process of deriving consequences from human knowledge via uncertain set\ntheory. First, we introduce the following inference rule.\n\nInference Rule 1. [16] Let X and Y be two concepts. Assume a rule ‘if X is an uncertain\nset ξ , then Y is an uncertain set η’. From X is a constant a, we infer that Y is an uncertain\nset\n\nη\n∗ = η|a∈ξ\n\nwhich is the conditional uncertain set of η given a ∈ ξ . The inference rule is represented by\n\nRule: If X is ξ, then Y is η\n\nFrom: X is a constant a\n\nInfer: Y is η∗ = η|a∈ξ\n\nTheorem 2. [16] Let ξ and η be independent uncertain sets with membership functions\nμ and ν, respectively. If ξ ∗ is a constant a, then the Inference Rule 1 yields that η∗ has a\nmembership function\n\nν\n∗\n(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nν(y)\nμ(a) , if ν(y) <\n\nμ(a)\n2\n\nν(y)+μ(a)−1\nμ(a) , if ν(y) > 1 − μ(a)2\n0.5, otherwise.\n\nBased on Inference Rule 1, Gao et al. [18] proposed the multi-input, multi-if-then-rule\ninference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 7 of 19\n\nInference Rule 2. [13] Let X1, X2, · · · , Xm, Y be concepts. Assume rules ‘if X1 is ξi1\nand · · · and Xm is ξim, then Y is ηi’ for i = 1, 2, · · · , k. From X1 is a constant a1 and · · ·\nand Xm is a constant am, we infer that\n\nη\n∗ =\n\nk∑\ni=1\n\nci · ηi|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\nc1 + c2 + · · · + ck\n\n, (4)\n\nwhere the coefficients are determined by\n\nci = M{(a1 ∈ ξi1) ∩ (a2 ∈ ξi2) ∩ · · · ∩ (am ∈ ξim)}\nfor i = 1, 2, · · · , k. The inference rule is represented by\n\nRule 1: If X1 is ξ11 and · · · and Xm is ξ1m, then Y is η1\nRule 2: If X1 is ξ21 and · · · and Xm is ξ2m, then Y is η2\n\n· · ·\nRule k: If X1 is ξk1 and · · · and Xm is ξkm, then Y is ηk\nFrom: X1 is a1 and · · · and Xm is am\nInfer: Y is determined by Eq. (4)\n\nTheorem 3. [13] Assume ξi1, ξi2, · · · , ξim, ηi are independent uncertain sets with mem-\nbership functions μi1, μi2, · · · , μim, νi, i = 1, 2, · · · , k, respectively. If ξ ∗1 , ξ ∗2 , · · · , ξ ∗m are\nconstants a1, a2, · · · , am, respectively, then the Inference Rule 2 yields\n\nη\n∗ =\n\nk∑\ni=1\n\nci · η∗i\nc1 + c2 + · · · + ck\n\nwhere η∗i are uncertain sets whose membership functions are given by\n\nν\n∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci\n\n, if νi(y) < ci2\nνi(y)+ci−1\n\nμ(a) , if νi(y) > 1 − ci2\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nUncertain system\n\nUncertain system, proposed by Liu [16], is a function from its inputs to outputs based\non the uncertain inference rule. Usually, an uncertain system consists of five parts: inputs\nthat are crisp data to be fed into the uncertain system; a rule-base that contains a set of\nif-then rules provided by the experts; an uncertain inference rule that infers uncertain\nconsequents from the uncertain antecedents; an expected value operator that converts\nthe uncertain consequents to crisp values; and outputs that are crisp data yielded from\nthe expected value operator.\nNow, we consider an uncertain system with m crisp inputs α1, α2, · · · , αm, and n crisp\n\noutputs β1, β2, · · · , βn. We have the following if-then rules:\nIf X1 is ξ11 and · · · and Xm is ξ1m, then Y1 is η11 and Y2 is η12 and · · · and Yn is η1n\nIf X1 is ξ21 and · · · and Xm is ξ2m, then Y1 is η21 and Y2 is η22 and · · · and Yn is η2n\n\n· · ·\nIf X1 is ξk1 and · · · and Xm is ξkm, then Y1 is ηk1 and Y2 is ηk2 and · · · and Yn is ηkn\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 8 of 19\n\nThus, according to Inference Rule 1 and 2, we can infer that Yj(j = 1, 2, · · · , n) are\n\nη\n∗\nj =\n\nk∑\ni=1\n\nci · ηij|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\nc1 + c2 + · · · + ck\n\n,\n\nwhere ci = M{(a1 ∈ ξi1) ∩ (a2 ∈ ξi2) ∩ · · · ∩ (am ∈ ξim)} for i = 1, 2, · · · , k. Then, by using\nthe expected value operator, we obtain\n\nβj = E\n[\nη\n\n∗\nj\n\n]\nfor j = 1, 2, · · · , n. Now, we construct a function from crisp inputs α1, α2, · · · , αm to crisp\noutputs β1, β2, · · · , βn, i.e.,\n\n(β1, β2, · · · , βn) = f (α1, α2, · · · , αm).\nThen, we get an uncertain system f. For the uncertain system we proposed, we have the\n\nfollowing theorem.\n\nTheorem 4. [13] Assume that ξi1, ξi2, · · · , ξim and ηi1, ηi2, · · · , ηin are indepen-\ndent uncertain sets with membership functions μi1, μi2, · · · , μim, νi1, νi2, · · · , νin, i =\n1, 2, · · · , k, respectively. Then, the uncertain system from α1, α2, · · · , αm to β1, β2, · · · , βn is\n\nbj =\nk∑\n\ni=1\n\nci · E[ η∗ij]\nc1 + c2 + · · · + ck\n\n,\n\nwhere j = 1, 2, · · · , n and η∗ij are uncertain sets whose membership functions are given by\n\nν\n∗\nij(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνij(y)\nci\n\n, if νij(y) < ci2\nνij(y)+ci−1\n\nμ(a) , if νij(y) > 1 − ci2\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nNext, we discuss the expected value of a special triangular uncertain set. Without loss of\ngenerality, we assume n = 1. Then the uncertain system proposed in the above becomes:\n\nb =\nk∑\n\ni=1\n\nci · E[ η∗i ]\nc1 + c2 + · · · + ck\n\n, (5)\n\nν\n∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci\n\n, if νi(y) < ci2\nνi(y)+ci−1\n\nμ(a) , if νi(y) > 1 − ci2\n0.5, otherwise,\n\n(6)\n\nci = min\n1≤l≤m\n\nμil(al). (7)\n\nTheorem 5. Assume we have an uncertain system with m inputs and 1 output consist-\ning of k inference rules. The antecedents of the rules are represented by the uncertain sets ξi\nwith membership functions μi1, μi2, · · · , μim, i = 1, 2, · · · , k. And the consequent is repre-\nsented by an triangular uncertain set ηi = (αi, βi, γi) with a membership function νi, where\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 9 of 19\n\nthe coefficients satisfy\n\nαi + γi = 2βi, i = 1, 2, · · · , k. (8)\nWe have\n\nE\n[\nη\n\n∗\ni\n] = βi, i = 1, 2, · · · , k.\n\nProof. Given the m input data a1, a2, · · · , am, we can calculate ci from Equation 7.\nThen, we can get the membership functions ν∗i of the consequence uncertain sets η\n\n∗\ni\n\naccording to Equation 6. Next, the computation of the expected value of uncertain\nconsequence breaks into three cases.\nCase 1: Assume ci/2 = 0.5. We can immediately have ν∗i (y) = νi(y), thus\n\nE[ η∗i ] =\nαi + 2βi + γi\n\n4\n= βi.\n\nCase 2: Assume ci/2 < 0.5. Let yi11 and y\ni\n12\n\n(\nyi11 < y\n\ni\n12\n\n)\nbe the two points that satisfy\n\nthe equation νi(y) = ci/2. Similarly, yi21 and yi22\n(\nyi21 < y\n\ni\n22\n\n)\nsatisfy the equation νi(y) =\n\n1 − ci/2. Since the membership function of a triangular uncertain set has a symmetry\nproperty, we have\n\nyi11 + yi12 = 2βi, yi21 + yi22 = 2βi. (9)\nThen, we can rewrite the membership function of ηi as follows:\n\nν\n∗\ni =\n\n⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nνi(y)\nci\n\n, if αi ≤ y < yi11\nνi(y)+ci−1\n\nci\n, if yi21 ≤ y < yi22\n\nνi(y)\nci\n\n, if yi12 ≤ y < γi\n0.5, otherwise.\n\n(10)\n\nAnd ν∗i (βi) = 1. Together with Equations 3, 8, and 9, we have\n\nE[ η∗i ] = βi +\n1\n2\n\n(∫ yi22\nβi\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi12\nyi22\n\n0.5dy +\n∫ γi\nyi12\n\nνi(y)\nci\n\ndy\n)\n\n−1\n2\n\n(∫ βi\nyi21\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi21\nyi11\n\n0.5dy +\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n= βi +\n1\n2\n\n(∫ yi22\nβi\n\nνi(y) + ci − 1\nci\n\ndy −\n∫ βi\nyi21\n\nνi(y) + ci − 1\nci\n\ndy\n)\n\n+1\n2\n\n(∫ γi\nyi12\n\nνi(y)\nci\n\ndy −\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n+1\n2\n\n(∫ yi12\nyi22\n\n0.5dy −\n∫ yi21\nyi11\n\n0.5dy\n)\n\n= βi.\nCase 3: Assume ci > 0.5. Similarly, we have E[ η∗i ] = βi. Thus, we have proved the\n\ntheorem.\n\nProblem formulation\nIn this section, we propose an extraction model to obtain uncertain inference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 10 of 19\n\nLet X = (x1, x2, · · · , xn) be the decision vector, which represents a rule base consisting\nof n rules. Each rule has m antecedents which are described by Q uncertain sets and one\nconsequent which is described by R uncertain sets. Each variable xi represents a sequence\nxi1xi2 · · · ximxim+1, where xij ∈ {0, 1, 2, · · · , Q}(i = 1, 2, · · · , n; j = 1, 2, · · · , m) represent\nthe antecedents of the inference rule. And xim+1 ∈ {0, 1, 2, · · · , R}(i = 1, 2, · · · , n) repre-\nsent the consequent. Thus, each variable of decision vector represents one inference rule.\nSome xij = 0 means this antecedent is not included. And some xim+1 = 0 means this\ninference rule will not be included in the rule base. For example, assume that we have one\ninference rule consists of 4 antecedents and 1 consequent. They are described by 5 uncer-\ntain sets which refer to five descriptions: very low, low, medium, high, and very high. We\nuse 1, 2, 3, 4, 5 to denote them. Thus, sequence “23045”, for example, represents the rule:\n“if input 1 is low, input 2 is medium, and input 4 is high, then the output is very high”.\nUncertain systems can be used for classification. But which uncertain system is better\n\ndepends on the rule base. Here, we try to find best rule base by comparing the mean\nabsolute errors of the origin output and the system output. That is,\n\nMAE = 1\nP\n\nP∑\ni=1\n\n|oi − ti|, (11)\n\nwhere P is the number of training data, oi, ti(i = 1, 2, · · · , P) are the system outputs and\norigin outputs, respectively. If we find the rule base with the least mean absolute error, we\nextract the uncertain inference rules successfully. We can obtain the system outputs by\nEquation 5. However, they may not be integers. To avoid this nonsense, for a classification\nproblem with C classes, we can divide interval that covers all the system outputs into C\nsubintervals. Then, if the output from Equation 5 is in the ith subinterval, we have oi = i.\nThus, we transfer the classification problem to the following optimization model:⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨\n\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nmin\nX\n\nF(X) = MAE\ns.t.\n\nX = (x1, x2, · · · , xn)\nxi = xi1 · · · ximxim+1\nxij ∈ {0, 1, · · · , Q}\nxim+1 ∈ {0, 1, · · · , R}\ni = 1, 2, · · · , n\nj = 1, 2, · · · , m\n\nExtraction method for uncertain inference rules with mutations\nIn this section, we propose the extraction method for uncertain inference rules with\nmutations by ant colony optimization algorithm.\nAs stated before, each xi is a sequence of m values in {0, 1, 2, · · · , Q} and 1 value in\n\n{0, 1, 2, · · · , R}. Without loss of generality, we set Q = R. Each number in {0, 1, 2, · · · , Q}\nis a node. Let ants walking across these nodes. Ants choose the next node in probability\nbased on the pheromone levels in the Q + 1 choices at every step. Once ants move m + 1\nsteps, a candidate decision variable is generated. After repeat this process n times, we get\na candidate solution. After all ants finish their walk, update the pheromone trails. Denote\nthe pheromone trail by τi;k,j(t) associated to the node j at step k of xi in iteration t. The\nprocedures are described as follows.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 11 of 19\n\n(1) Initialization: Randomly generate a feasible solution X0, and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · , m + 1, j = 0, 1, 2, · · · , Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik, select the\n\nnext node in probability following\n\npk;k+1 =\nτi;k+1,j(t)\n\nQ∑\nq=0\n\nτi;k+1,q(t)\n. (12)\n\nIn this way, we could get a sequence xi1xi2 · · · xim+1. To speed up the algorithm, we mutate\nthis sequence to get a new candidate sequence. The mutation is made as follows: ran-\ndomly add 1 or subtract 1 to each element xij in the sequence; if the element is 0, the\nmutated element is 1; if the element is Q, the mutated element is Q − 1. Assume X′ is\nthe mutated solution, if \rF = F(X′ ) − F(X) ≤ 0, then X ← X′; otherwise, keep the\ncurrent solution. If Q is very large, we could repeat this mutation until some termination\ncondition is satisfied.\n(3) Pheromone Update: At each iteration t, let X̂ be the optimal solution found so far\n\nand Xt be the best feasible solution in the current iteration. Assume F(X̂) and F(Xt) are\nthe corresponding objective function values.\n\nIf F(Xt) < F(X̂), then X̂ ← Xt.\nReinforce the pheromone trails on nodes of X̂ and evaporate the pheromone trails on\n\nthe left nodes:\n\nτi;j,k(t) =\n{\n\n(1 − ρ)τi;j,k(t − 1) + ρg(X̂), if (k, j) ∈ X̂\n(1 − ρ)τi;j,k(t − 1), otherwise\n\n(13)\n\nwhere ρ (0 < ρ < 1) is the evaporation rate, g(x)(0 < g(x) < +∞) is a function with that\ng(x) ≥ g(y) if F(x) < F(y), for example, g(x) = L/(|F(x)| + 1) is a function satisfying the\ncondition where L > 0.\n\nLet τ0 be the initial value of pheromone trails, n be the number of decision variables,\nM be the number of ants, ρ be evaporation rate and T be the number of iterations. Now,\nwe summarize the algorithm as follows.\n\nStep 1 Initialize all pheromone trails with the same pheromone level τ0. Randomly\ngenerate a feasible solution X0, and set optimal solution X̂ = X0. Set l ← 0.\nStep 2 Ant movement in probability following Equation 12. Generate a decision variable\n\nxi after m + 1 steps.\nStep 3 Repeat Step 2 until X = (x1, x2, · · · , xn) is generated; mutate every xi: thus, gen-\n\nerate a new decision vector X\n′ = (x′1, x\n\n′\n2, · · · , x\n\n′\nn); if \rF = F(X\n\n′\n) − F(X) ≤ 0, then\n\nX ← X′.\nStep 4 Repeat Step 2 and Step 3 for all M ants.\nStep 5 Calculate the system outputs by Equation 5. Then, calculate the objective function\n\nvalues for the M candidate solutions by Equation 11. Denote the best solution in this\niteration by Xl.\nStep 6 If F(Xl) < F(X̂), then X̂ ← Xl; update the pheromone trails according to\n\nEquation 13.\nStep 7 l ← l + 1; if l = T, terminate; otherwise, go to Step 2.\nStep 8 Report the optimal solution X̂.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 12 of 19\n\nWith this algorithm above, we obtain an uncertain rule base. Then, we successfully\ndesign an uncertain system and can use it for classification.\n\nExtraction method for uncertain inference rules with SA\nIn the previous section, to speed up the algorithm, we introduce a mutation operation.\nHere, we introduce the simulated annealing algorithm as the local search operation.\nSimulated annealing algorithm was initiated by Metropolis in 1953, applied to portfolio\n\noptimization by Kirkpatrick [25] in 1983. The name and inspiration come from anneal-\ning in metallurgy, a technique involving heating and controlled cooling of a material to\nincrease the size of its crystals and reduce their defects. Simulated annealing algorithm is\nexcellent at avoiding getting stuck in local optimums. It has a good robust property and is\nuniversal and easy to implement.\nFor optimization problem (1), we can use simulated annealing algorithm to search for\n\nthe optimal solution. The algorithm is as follows.\n\nStep 1 Randomly generate a initial solution x0; x ← x0; k ← 0; t0 ← tmax(initial\ntemperature);\nStep 2 If the temperature satisfies the inner cycle termination criterion, go to Step 3;\n\notherwise, randomly choose a point x\n′\nin the neighborhood N(x), calculate \rf = f (x′ ) −\n\nf (x). If \rf ≤ 0, then x ← x′; otherwise, according to Metropolis acceptance criterion, if\nexp(−\rf /tk) > random(0, 1), then x ← x′. Repeat Step 2.\nStep 3 tk+1 = d(tk) (temperature decrease); k ← k + 1; if the termination criterion is\n\nsatisfied, stop and report the optimal solution; otherwise, go to Step 2.\n\nIn this section, we combine ant colony optimization algorithm and simulated annealing\nalgorithm. In each iteration of ant colony optimization algorithm, we get a feasible solu-\ntion. Then, we use it as the initial solution of the simulated annealing algorithm to get a\nneighbor solution. This neighbor solution will be accepted in probability. And for each\ndecision vector X = (x1, x2, · · · , xn), xi = xi1xi2 · · · xim+1, we build the neighbor solution\nas follows: for each xi, for some randomly generated p and q (1 ≤ p < q ≤ m), reverse the\norder of the sequence xip · · · xiq, i.e., x′i = xi1 · · · xip−1xiqxiq−1 · · · xip+1xipxiq+1 · · · xim+1.\nFor example, assume xi is 0123456, p = 2, q = 6, and the neighbor solution x′i is 0543216.\nIn this way, we obtain a neighbor solution X\n\n′\n. If \rF = F(X′ ) − F(X) ≤ 0, X ← X′;\n\notherwise, if exp(−\rF/tk) > random(0, 1), then X ← X′; otherwise, abandon this neigh-\nbor solution. Still denote the pheromone trail by τi;k,j(t). The procedure are described as\nfollows.\n\n(1) Initialization: Generate a feasible solution X0 randomly and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · , m + 1, j = 0, 1, 2, · · · , Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik, select the\n\nnext node in probability following Equation 12. In this way, we could get a sequence\nxi1xi2 · · · xim+1. In order to expand the search range, we use simulated annealing algo-\nrithm to search locally around the solution at this step. Assume the neighbor solution is\nX\n\n′\n. If \rF = F(X′ ) − F(X) ≤ 0, X ← X′; otherwise, if exp(−\rF/tk) > random(0, 1)\n\nwhere tk is the current temperature and tk → 0 when k → ∞, then X ← X′; otherwise,\nabandon this neighbor solution and still choose the original feasible solution.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 13 of 19\n\n(3) Pheromone Update: Let X̂ be the optimal solution found so far and Xt be the best\nfeasible solution in the current iteration t. Assume F(X̂) and F(Xt) are the correspond-\ning objective function values. To avoid the optimal solution X̂ getting stuck in local\noptimums, we also use acceptance function here.\n\nIf F(Xt) < F(X̂), then X̂ ← Xt.\nBuild a neighbor solution X̂\n\n′\n.\n\nIf F(X̂\n′\n) ≤ F(X̂), then X̂ ← X̂′;\n\nIf F(X̂\n′\n) > F(X̂), check the Metropolis acceptance criterion, i.e., if\n\nexp(−\rF̂/Tt) > random(0, 1), Tt → 0, t → ∞, then X∗ ← X̂′.\nReinforce the pheromone trails on the nodes of X̂ and X∗ and evaporate the pheromone\n\ntrails on the left nodes:\n\nτi;j,k(t) =\n\n⎧⎪⎨\n⎪⎩\n\n(1 − ρ)τi;j,k(t − 1) + ρg(X̂), if (k, j) ∈ X̂\n(1 − ρ)τi;j,k(t − 1) + ρ2 g(X̂), if (k, j) ∈ X∗\n\n(1 − ρ)τi;j,k(t − 1), otherwise\n(14)\n\nwhere, ρ (0 < ρ < 1) is the evaporate rate, and g(x) (0 < g(x) < +∞) is a function with\nthat g(x) ≥ g(y) if F(x) < F(y). For example, g(x) = L/(|F(x)| +1) is an available function\nif L > 0.\n\nNow, we summarize the algorithm as follows.\n\nStep 1 Initialize all pheromone trails with the same pheromone level τ0. Randomly\ngenerate a feasible solution X0, and set optimal solution X̂ = X0. Set t ← 0.\nStep 2 Ant movement in probability following Equation 12. Generate a decision variable\n\nxi after m + 1 steps.\nStep 3 Repeat Step 2 until decision vector X = (x1, x2, · · · , xn) is generated. Build the\n\nneighbor solution X\n′\n. If \rF = F(X′ ) − F(X) ≤ 0, X ← X′; otherwise, if exp(−\rF/tk) >\n\nrandom(0, 1) where tk is the current temperature and tk → 0 when k → ∞, then X ← X′.\nStep 4 Repeat Step 2 and Step 3 until all ants finish their walk, and generate M candidate\n\nsolutions.\nStep 5 Calculate the system outputs by Equation 5. Then, calculate the objective function\n\nvalues for the M candidate solutions by Equation 11. Denote the best solution in this\niteration by Xt.\nStep 6 If F(Xt) < F(X̂), then X̂ ← Xt. Build the neighbor solution of X̂, which is denoted\n\nby X̂\n′\n. If \rF̂ = F(X̂′ ) − F(X̂) ≤ 0, then X̂ ← X̂′; otherwise, if Metropolis acceptance\n\ncriterion is satisfied, i.e., if exp(−\rF̂/Tt) > random(0, 1), Tt → 0, t → ∞, then X∗ ← X̂′.\nStep 7 Update the pheromone trails according to Equation 14.\nStep 8 t ← t + 1; if t = T, terminate; otherwise, go to Step 2.\nStep 9 Report the optimal solution X̂.\n\nTable 1 Parameters\n\nap bp cp\n\np = 1 0.5 1.01 1.52\np = 2 1.7 2.74 4.48\np = 3 5 6.07 7.14\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 14 of 19\n\nFigure 1 Results of method A.\n\nExperiments\nIn this section, we use our two extraction methods to extract uncertain inference rules.\nAnd then use the uncertain systems to solve some classification problems. We applied our\nmethods to the IRIS [26] classification problem and the Wisconsin Breast Cancer (WBC)\n[27] classification problem.\n\nIRIS classification\n\nIRIS data set is the typical date set in data classification. It contains 150 instances of 3\nclasses, which are Setosa, Versicolor, and Virginica. Each class has 50 instances. Each\ninstance has 4 attributes which are sepal length (SL), sepal width (SW), petal length (PL),\n\nFigure 2 Results of method B.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 15 of 19\n\nTable 2 Accuracy comparison\n\nMethod Paper Accuracy rate (%)\n\nMethod A This paper 97.33\n\nMethod B This paper 97.5\n\nC4.5 [2] 94.0\n\nACOA [5] 96.6\n\nMACO [6] 95.53\n\nHNFQ [28] 98.67\n\nTable 3 IRIS classification rules extracted by method A\n\nIF THEN\n\nSL SW PL PW Class\n\n1 3 1 3 1\n\n1 0 1 1 1\n\n1 2 3 2 1\n\n1 1 2 1 2\n\n2 1 0 3 2\n\n3 2 0 2 3\n\n1 1 3 3 3\n\nTable 4 IRIS classification rules extracted by method B\n\nIF THEN\n\nSL SW PL PW Class\n\n3 2 3 1 2\n\n1 1 0 0 2\n\n0 2 1 1 3\n\n0 1 1 3 1\n\n1 1 3 3 2\n\n1 1 3 1 1\n\n2 1 1 2 1\n\nTable 5 Parameters\n\nap bp cp\n\np = 1 0.3 1.01 1.72\np = 2 2 6.07 10.14\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 16 of 19\n\nFigure 3 Results of method A.\n\nand petal width (PW). They are described by 3 uncertain sets: low (1), medium (2), and\nhigh (3). The membership functions are\n\nμq(x) = exp\n(\n\n− (x − Vq)\n2\n\n2β2\n\n)\n,\n\nwhere x is the input, β = 0.618 and Vq = q−12 , q = 1, 2, 3. Based on these 4 attributes,\nwe try to infer which class does the instance belong to. We use 3 triangular uncertain\nsets ηp = (ap, bp, cp) (p = 1, 2, 3) to describe the possible classes (class 1: Setosa; class 2:\nVersicolor; class 3: Virginica). And the parameters ap, bp, cp ∈ R are listed in Table 1.\nFirst, we normalize the data to [0, 1] to simplify the computation. IRIS data set is our\n\ntraining set while it is also used for testing. Then, we set maximum number of rules n =\n10, number of ants M = 10, evaporate rate ρ = 0.3, and number of iterations T = 300.\nEach algorithm runs ten times. The results are in Figures 1 and 2. Denote the extraction\n\nFigure 4 Results of method B.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 17 of 19\n\nTable 6 Accuracy rate comparison\n\nMethod Paper Accuracy rate (%)\n\nMethod A This paper 98.3\n\nMethod B This paper 98.33\n\nC4.5 [2] 94.25\n\nACOA [5] 97.91\n\nMACO [6] 97.07\n\nFMM [29] 97.86\n\nmethod with mutation by A and the method with SA by B. It can be seen that the method\nA converges fast at about 120th iteration. And method B converges a little slower at about\n150th iteration.\nThen, we can classify the IRIS data with the uncertain systems we introduced. We\n\nfind the average accuracy rates of the two methods are 97.33% and 97.5%, respectively.\nComparison with other methods are listed in Table 2.\nList the rule bases we get with the highest accuracy rates (98.0% and 98.67%, respec-\n\ntively) in Tables 3 and 4. Note that although the maximum number of rules is 10, the final\nrule bases we obtain has only 7 rules.\n\nWisconsin Breast Cancer classification\n\nWisconsin Breast Cancer data set is a common medical date set. It contains 699 instances\nof 2 classes, which are sick and healthy. Two hundred forty-one instances are sick and 458\ninstances are healthy. Each instances has 9 attributes, which are clump thickness (CT),\nuniformity of cell size (UCS), uniformity of cell shape (UCCS), marginal adhesion (MA),\nsingle epithelial cell size (SPCS), bare nuclei (BN), bland chromatin (BC), normal nucleoli\n(NN), and mitoses (MT). They are described by 5 uncertain sets: very low (1), low (2),\nmedium (3), high (4), and very high (5). The membership functions are\n\nμq(x) = exp\n(\n\n− (x − Vq)\n2\n\n2β2\n\n)\n,\n\nwhere x is the input, β = 0.4247, and Vq = q−12 , q = 1, 2, 3, 4, 5. Based on these attributes,\nwe diagnose whether one instance is sick or not. We use 2 triangular uncertain sets\nηp = (ap, bp, cp) (p = 1, 2) to describe the possible classes (sick and healthy). And the\nparameters ap, bp, cp ∈ R are listed in Table 5.\n\nTable 7 WBC classification rules extracted by method A\n\nIF THEN\n\nCT UCS UCCS MA SPCS BN BC NN MT Class\n\n1 5 0 3 2 0 4 2 1 2\n\n1 2 1 1 4 4 2 1 0 1\n\n1 3 5 2 3 2 1 1 4 2\n\n1 3 4 2 3 1 2 2 1 1\n\n2 4 4 1 1 2 4 5 1 2\n\n3 3 4 5 4 3 2 4 4 2\n\n5 2 4 0 3 0 0 2 1 1\n\n2 4 4 1 1 2 4 5 1 1\n\n2 4 2 3 5 3 2 5 5 2\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 18 of 19\n\nTable 8 WBC classification rules extracted by method B\n\nIF THEN\n\nCT UCS UCCS MA SPCS BN BC NN MT Class\n\n5 3 3 3 3 2 3 2 4 2\n\n0 0 0 0 0 0 0 4 0 1\n\n4 4 4 4 0 1 1 1 4 2\n\n1 4 4 1 1 2 1 5 1 2\n\n1 1 1 2 0 3 5 5 5 2\n\n0 3 0 0 0 0 0 1 2 1\n\nFirst, we normalize the data to [0, 1] to simplify the computation. The first 460 instances\nare used for training while the left 239 instances are used for testing. Then, we set max-\nimum number of rules n = 10, number of ants M = 20, evaporate rate ρ = 0.3, and\nnumber of iterations T = 200. Each algorithm runs ten times. The results are in Figures 3\nand 4. We still find that method A converges faster than method B. Method A stabilizes\nat about 50th iteration while method B stabilizes until about 80th iteration.\nThen, we test the uncertain systems we get with the later 239 instances. We find the\n\naverage accuracy rates of the two methods on the training set are 96.0% and 96.26%,\nrespectively. Using the uncertain system with the highest accuracy rate of each method\non the test set, we find the accuracy rates are 98.37% and 98.33%. Comparison with other\nmethods are listed in Table 6.\nThe rule base with the highest accuracy rates (98.37% and 98.33%, respectively) on the\n\ntest set are listed in Tables 7 and 8. Method A gives us a rule base of 9 rules, and method\nB provides a rule base of 6 rules.\nWe apply our two extraction methods to the classification problems of IRIS data set\n\nand WBC data set. Compare our results with other researchers’ work, we can find that\nboth methods have higher accuracy rate than ACOA and MACO in two classification\nproblems. And for IRIS data set, accuracy rates of method A and B are lower than HNFQ\nbut higher than C4.5. For WBC data set, their accuracy rates are higher than C4.5 and\nFMM.\n\nConclusions\nIn this paper, we designed an uncertain system for data classification. And we proposed\ntwo extraction methods for uncertain inference rules by using ant colony optimization\nalgorithm. Then, we applied our methods to IRIS classification problem and WBC clas-\nsification problem. Our methods are shown to be superior in accuracy to some existing\nmethods.\n\nAcknowledgements\nThis work is supported by the National Natural Science Foundation of China (No.61273009).\n\nReceived: 14 December 2014 Accepted: 20 April 2015\n\nReferences\n1. Kantardzic, M: Data Mining: Concepts, Models, Methods, and Algorithms. 2nd ed. Wiley, Hoboken (2011)\n2. Quinlan, JR: Improved use of continuous attributes in C4.5. J. Artif. Intell. Res. 4(1), 77–90 (1996)\n3. Parpinelli, RS, Lopes, HS, Freitas, AA: Data mining with an ant colony optimization algorithm. IEEE Trans. Evolut.\n\nComput. 6(4), 321–332 (2002)\n4. Casillas, J, Cordón, O, Herrera, F: Learning fuzzy rules using ant colony optimization algorithms. In: Proceedings of\n\nthe 2nd International Workshop on Ant Algorithms: From Ant Colonies to Artificial Ants, pp. 13–21, Brussels, (2000)\n\n\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 19 of 19\n\n5. Zhu, Y: Ant colony optimization-based hybrid intelligent algorithms. World J. Modell. Simul. 2(5), 283–289 (2006)\n6. Zhu, Y: An intelligent algorithm: MACO for continuous optimization models. J. Intell. Fuzzy Syst. 24, 31–36 (2013)\n7. Lee, Z, Su, S, Chuang, C, Liu, K: Genetic algorithm with ant colony optimization (GA-ACO) for multiple sequence\n\nalignment. Appl. Soft Comput. 8(1), 55–78 (2008)\n8. Shelokar, PS, Siarry, P, Jayaraman, VK, Kulkarni, BD: Particle swarm and ant colony algorithms hybridized for improved\n\ncontinuous optimization. Appl. Math. Comput. 188(1), 129–142 (2007)\n9. Liu, B: Uncertainty Theory. 2nd ed. Springer, Berlin (2007)\n10. Liu, B: Fuzzy process, hybrid process and uncertain process. J. Uncertain Syst. 2(1), 3–16 (2008)\n11. Chen, X, Liu, B: Existence and uniqueness theorem for uncertain differential equations. Fuzzy Optimization Decis.\n\nMak. 9(1), 69–81 (2010)\n12. Liu, B: Some research problems in uncertainty theory. J. Uncertain Syst. 3(1), 3–10 (2009)\n13. Liu, B: Uncertainty Theory: A Branch of Mathematics for Modeling Human Uncertainty. Springer, Berlin (2010)\n14. Peng, J, Yao, K: A new option pricing model for stocks in uncertainty markets. Int. J. Oper. Res. 8(2), 18–26 (2011)\n15. Zhu, Y: Uncertain optimal control with application to a portfolio selection model. Cybern. Syst. 41(7), 535–547 (2010)\n16. Liu, B: Uncertain set theory and uncertain inference rule with application to uncertain control. J. Uncertain Syst. 4(2),\n\n83–98 (2010)\n17. Liu, B: Uncertain logic for modeling human language. J. Uncertain Syst. 5(1), 3–20 (2011)\n18. Gao, X, Gao, Y, Ralescu, DA: On Liu’s inference rule for uncertain systems. Int. J. Uncertain. Fuzz. Knowledged-Based\n\nSyst. 18(1), 1–11 (2010)\n19. Peng, Z, Chen, X: Uncertain systems are universal approximators. J. Uncertainty Anal. Appl. 2, Article, 13 (2014)\n20. Gao, Y: Uncertain inference control for balancing inverted pendulum. Fuzzy Optimization Decis. Mak. 11(4), 481–492\n\n(2012)\n21. Liu, B: Membership functions and operational law of uncertain sets. Fuzzy Optimization Decis. Mak. 11(4), 387–410\n\n(2012)\n22. Zadeh, LA: Outline of a new approach to the analysis of complex systems and decision processes. IEEE Trans. Syst.\n\nMan Cybern. 3(1), 28–44 (1973)\n23. Mamdani, EH: Applications of fuzzy algorithms for control of a simple dynamic plant. Proc. Institution Electr. Eng.\n\nControl Sci. 121(12), 1585–1588 (1974)\n24. Takagi, K, Sugeno, M: Fuzzy identification of system and its applications to modeling and control. IEEE Trans. Syst.\n\nMan Cybern. 15(1), 116–132 (1985)\n25. Kirkpatrick, S, Gelatt, CD, Vecchi, MP: Optimization by simmulated annealing. Science. 220(4598), 671–680 (1983)\n26. Iris dataset (1936). https://archive.ics.uci.edu/ml/datasets/Iris\n27. Wisconsin Breast Cancer Dataset (1992). https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)\n28. de Souza, FJ, Vellasco, M, Pacheco MA: Hierarchical neuro-fuzzy quadtree models. Fuzzy Sets Syst. 130(2), 189–205\n\n(2002)\n29. Gabrys, B, Bargiela, A: General fuzzy min-max neural network for clustering and classification. IEEE Trans. Neural\n\nNetwor. 11(3), 769–783 (2000)\n\nSubmit your manuscript to a \njournal and benefi t from:\n\n7 Convenient online submission\n7 Rigorous peer review\n7 Immediate publication on acceptance\n7 Open access: articles freely available online\n7 High visibility within the fi eld\n7 Retaining the copyright to your article\n\n    Submit your next manuscript at 7 springeropen.com\n\nhttps://archive.ics.uci.edu/ml/datasets/Iris\nhttps://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)\n\n\tAbstract\n\tKeywords\n\n\tIntroduction\n\tPreliminary\n\tAnt colony optimization algorithm\n\tUncertain set\n\tUncertain inference rule\n\tUncertain system\n\n\tProblem formulation\n\tExtraction method for uncertain inference rules with mutations\n\tExtraction method for uncertain inference rules with SA\n\tExperiments\n\tIRIS classification\n\tWisconsin Breast Cancer classification\n\n\tConclusions\n\tAcknowledgements\n\tReferences\n\n",
      "metadata_storage_path": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9saWJyYXJ5L3M0MDQ2Ny0wMTUtMDAzMy05LnBkZg2",
      "authors": [
        "Chen",
        "Ling Chen",
        "Yun Sun",
        "Yuanguo Zhu",
        "Herrera",
        "Zhu",
        "nacy",
        "Liu",
        "Peng",
        "Yao",
        "Gao",
        "Dorigo",
        "M",
        "Borel",
        "Mamdani",
        "Takagi",
        "Sugeno",
        "yi22",
        "yi21",
        "yi11",
        "Kirkpatrick",
        "anneal",
        "Virginica",
        "Kantardzic, M",
        "Quinlan",
        "JR",
        "J. Artif",
        "Parpinelli",
        "Lopes",
        "Freitas",
        "Casillas",
        "J",
        "Cordón",
        "O",
        "F",
        "Y",
        "World J. Modell",
        "J. Intell",
        "Lee",
        "Z",
        "Su",
        "S",
        "Chuang",
        "C",
        "Liu, K",
        "Shelokar",
        "PS",
        "Siarry",
        "Jayaraman",
        "VK",
        "Kulkarni",
        "BD",
        "Liu, B",
        "X",
        "Peng, J",
        "Yao, K",
        "Ralescu",
        "Gao, Y",
        "Man Cybern",
        "Takagi, K",
        "Sugeno, M",
        "Gelatt",
        "Vecchi",
        "de Souza",
        "FJ",
        "Vellasco",
        "Pacheco",
        "Gabrys",
        "B",
        "Bargiela",
        "A"
      ],
      "institutions": [
        "School of Science",
        "University of Science",
        "Technology",
        "Springer",
        "ACOA",
        "Borel",
        "MAE",
        "ximxim+1",
        "Metropolis",
        "Wisconsin Breast Cancer",
        "WBC",
        "IRIS",
        "MACO",
        "HNFQ",
        "Breast Cancer",
        "FMM",
        "National Natural Science Foundation",
        "Wiley",
        "IEEE",
        "Fuzzy Optimization Decis",
        "Electr",
        "Networ",
        "springeropen.com"
      ],
      "key_phrases": [
        "Creative Commons Attribution License",
        "interesting, potentially useful patterns",
        "Ant colony optimization algorithm",
        "modern intelligent optimization methods",
        "many different specified methods",
        "current data analysis tools",
        "important practical significance",
        "Open Access article",
        "many successful examples",
        "machine learning methods",
        "Uncertain inference rule",
        "RESEARCH Open Access",
        "many different fields",
        "Bayesian belief network",
        "large data sets",
        "financial data analysis",
        "data compartment analysis",
        "data mining techniques",
        "data mining algorithms",
        "mated data generation",
        "data mining methods",
        "data mining technology",
        "many methods",
        "collection tools",
        "many fields",
        "inference rules",
        "many other",
        "Uncertainty Analysis",
        "neutral network",
        "Bayesian classification",
        "Extraction methods",
        "tical methods",
        "effective methods",
        "uncertain system",
        "mining objects",
        "Data classification",
        "existing data",
        "data description",
        "data dependency",
        "data regression",
        "data aggregate",
        "data prediction",
        "Rules extraction",
        "classification rules",
        "Yun Sun",
        "Yuanguo Zhu",
        "recent years",
        "classification problems",
        "computer networks",
        "scientific research",
        "ness decision",
        "computational process",
        "overall goal",
        "standable structure",
        "biomedical research",
        "main jobs",
        "decision trees",
        "mathematical formula",
        "original work",
        "Nanjing University",
        "business management",
        "valuable information",
        "two applications",
        "science research",
        "unrestricted use",
        "Ling Chen",
        "Journal",
        "DOI",
        "Correspondence",
        "chinalsy",
        "School",
        "Abstract",
        "increasing",
        "attention",
        "paper",
        "effectiveness",
        "superiority",
        "Keywords",
        "Introduction",
        "databases",
        "finance",
        "E-commerce",
        "logistics",
        "result",
        "amount",
        "people",
        "basis",
        "difficulty",
        "depth",
        "deficiency",
        "concepts",
        "laws",
        "modes",
        "knowledge",
        "reality",
        "instance",
        "telecommunication",
        "retail",
        "study",
        "couple",
        "models",
        "functions",
        "characteristics",
        "categories",
        "training",
        "output",
        "licensee",
        "Springer",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "Page",
        "variety",
        "approaches",
        "start",
        "addition",
        "mutation ant colony optimization algorithm",
        "original ant colony optimization algorithm",
        "early decision trees algorithm",
        "simulated annealing algorithm",
        "global search algorithm",
        "powerful mathematical tool",
        "uncertain differential equation",
        "mature technology methods",
        "Decision trees method",
        "fuzzy rules learning",
        "many mathematical tools",
        "massive data set",
        "data mining technique",
        "neutral network algorithm",
        "probability distri- bution",
        "product measure axiom",
        "decision tree method",
        "incremental tree structure",
        "intelligent optimization algorithms",
        "different network models",
        "random initial solutions",
        "Machine learning methods",
        "popular classification methods",
        "complex nonlinear relations",
        "classification machine learning",
        "many evolution algorithms",
        "Genetic algorithm",
        "optimization strategies",
        "bionic optimization",
        "numerical methods",
        "conceptual learning",
        "inductive learning",
        "ID3 method",
        "C4.5 method",
        "different types",
        "many cases",
        "spatial data",
        "probability theory",
        "normality axiom",
        "duality axiom",
        "subadditivity axiom",
        "pattern classification",
        "statistical principle",
        "tering problems",
        "core content",
        "simulation model",
        "complex system",
        "pattern recognition",
        "biological evolution",
        "genetic mechanism",
        "important role",
        "Mixed algorithms",
        "other algorithms",
        "slow convergence",
        "local optimums",
        "Hybrid genetic",
        "hybrid particle",
        "real world",
        "human being",
        "indetermi- nacy",
        "domain experts",
        "belief degree",
        "tainty theory",
        "Many researchers",
        "theoretical work",
        "uniqueness theorem",
        "practical problems",
        "number",
        "variants",
        "SLIQ",
        "clustering",
        "regression",
        "behavior",
        "ant-miner",
        "Herrera",
        "weakness",
        "reason",
        "Zhu",
        "ACOA",
        "MACO",
        "performance",
        "understanding",
        "indeterminacy",
        "samples",
        "situation",
        "choice",
        "event",
        "unlikely",
        "frequency",
        "view",
        "Liu",
        "Chen",
        "Applications",
        "lot",
        "existence",
        "stability",
        "Peng",
        "Yao",
        "two typical classification problems",
        "ant colony optimization algorithm",
        "uncertain optimal control model",
        "heuristic optimization approach",
        "ant transfer prob",
        "option pricing models",
        "expected value operator",
        "two extraction methods",
        "M feasible solutions",
        "following optimization problem",
        "uncertain inference rule",
        "pheromone evaporation rate",
        "same pheromone level",
        "uncertain inference controller",
        "optimal solution s",
        "one ant",
        "artificial ant",
        "tain controller",
        "uncertain systems",
        "uncertain sets",
        "membership functions",
        "five parts",
        "universal approximator",
        "reasonable tool",
        "inverted pendulum",
        "uncertainty theory",
        "mutation operation",
        "data mining",
        "uncertainty sets",
        "decision variable",
        "domain D",
        "candidate solution",
        "best solution",
        "pheromone communication",
        "pheromone levels",
        "initial pheromone",
        "pheromone trails",
        "many paths",
        "bad paths",
        "good paths",
        "next section",
        "basic concepts",
        "last section",
        "objective function",
        "constraint function",
        "good ability",
        "data set",
        "food sources",
        "real ants",
        "cial ants",
        "one path",
        "walking path",
        "stocks",
        "inputs",
        "outputs",
        "rule-base",
        "Gao",
        "robustness",
        "results",
        "Preliminary",
        "Dorigo",
        "nest",
        "time",
        "parameters",
        "iterations",
        "T.",
        "procedures",
        "Step",
        "probability",
        "sk",
        "5",
        "⎪⎪⎪",
        "first measure inversion formula",
        "following three axiom",
        "product uncertain measure",
        "Mamdani inference rules",
        "Takagi-Sugeno inference rules",
        "triangular uncertain set",
        "regular membership function μ",
        "inversion formulas",
        "inference systems",
        "fuzzy inference",
        "nonempty set",
        "optimal solution",
        "current iteration",
        "uncertainty space",
        "real numbers",
        "expected value",
        "two integrals",
        "key points",
        "fuzzy systems",
        "CRI approach",
        "set function",
        "fuzzy sets",
        "Mk{�k",
        "isfying M",
        "M{ξ � r",
        "M{B ⊂",
        "B.",
        "algebra",
        "L.",
        "triplet",
        "Lk",
        "Definition",
        "collection",
        "Borel",
        "Example",
        "power",
        "ξn",
        "Bi",
        "sup",
        "Bc",
        "equations",
        "Remark",
        "mode",
        "μ(x",
        "Theorem",
        "x0",
        "2b",
        "fact",
        "antecedents",
        "consequents",
        "σ",
        "∈",
        "∞",
        "⎪",
        "γ",
        "1",
        "∫",
        "independent uncertain sets",
        "uncertain set theory",
        "conditional uncertain set",
        "m crisp inputs",
        "n crisp outputs",
        "crisp data",
        "crisp values",
        "Uncertain system",
        "uncertain antecedents",
        "human knowledge",
        "uncertain consequents",
        "constant a",
        "two concepts",
        "process",
        "consequences",
        "following",
        "Xm",
        "X1",
        "im",
        "ck",
        "coefficients",
        "Eq.",
        "ηi",
        "constants",
        "k∑",
        "min",
        "μil",
        "experts",
        "Y1",
        "Y2",
        "Yn",
        "Yj",
        "ξi",
        "βj",
        "ν",
        "η∗",
        "⎪⎪⎪⎪",
        "α",
        "special triangular uncertain set",
        "m input data",
        "dent uncertain sets",
        "Q uncertain sets",
        "R uncertain sets",
        "uncertain inference rules",
        "one inference rule",
        "k inference rules",
        "m inputs",
        "uncertain consequence",
        "three cases",
        "two points",
        "symmetry property",
        "Problem formulation",
        "extraction model",
        "decision vector",
        "rule base",
        "n rules",
        "m antecedents",
        "variable xi",
        "k.",
        "βn",
        "theorem",
        "ξim",
        "μim",
        "bj",
        "ij",
        "loss",
        "generality",
        "above",
        "1 output",
        "ing",
        "αi",
        "γi",
        "2βi",
        "Proof",
        "Equation",
        "computation",
        "dy",
        "section",
        "corresponding objective function values",
        "following optimization model",
        "candidate decision variable",
        "feasible solution X0",
        "best feasible solution",
        "Q + 1 choices",
        "mean absolute error",
        "best rule base",
        "new candidate sequence",
        "m values",
        "absolute errors",
        "Uncertain systems",
        "mutated solution",
        "current solution",
        "tain sets",
        "five descriptions",
        "training data",
        "system outputs",
        "origin outputs",
        "C classes",
        "ith subinterval",
        "Extraction method",
        "The procedures",
        "fixed parameter",
        "classification problem",
        "next node",
        "sequence xi1xi2",
        "mutated element",
        "step k",
        "left nodes",
        "Once ants",
        "Q∑",
        "example",
        "4 antecedents",
        "input",
        "MAE",
        "P∑",
        "integers",
        "nonsense",
        "subintervals",
        "xij",
        "mutations",
        "1 value",
        "R.",
        "steps",
        "walk",
        "Denote",
        "τi",
        "xik",
        "way",
        "termination",
        "condition",
        "⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪",
        "ρ",
        "inner cycle termination criterion",
        "new decision vector X",
        "good robust property",
        "feasible solu- tion",
        "uncertain rule base",
        "M candidate solutions",
        "local search operation",
        "Metropolis acceptance criterion",
        "Step 2 Ant movement",
        "Simulated annealing algorithm",
        "F(x",
        "portfolio optimization",
        "optimization problem",
        "decision variables",
        "initial solution",
        "neighbor solution",
        "evaporation rate",
        "initial value",
        "M ants",
        "anneal- ing",
        "controlled cooling",
        "previous section",
        "Repeat Step",
        "1 steps",
        "values",
        "Xl.",
        "classification",
        "Kirkpatrick",
        "name",
        "inspiration",
        "metallurgy",
        "technique",
        "heating",
        "material",
        "size",
        "crystals",
        "defects",
        "temperature",
        "point",
        "neighborhood",
        "xi",
        "order",
        "sequence",
        "Table 1 Parameters ap bp cp",
        "method A. Experiments",
        "Wisconsin Breast Cancer",
        "objective function values",
        "decision vector X",
        "typical date set",
        "IRIS [26] classification problem",
        "original feasible solution",
        "IRIS data set",
        "neighbor solution X",
        "IRIS classification",
        "acceptance function",
        "data classification",
        "F(X̂",
        "available function",
        "search range",
        "current temperature",
        "evaporate rate",
        "Figure 1 Results",
        "F(Xt",
        "Xt.",
        "procedure",
        "rithm",
        "tk",
        "Tt",
        "ρ2 g",
        "ants",
        "WBC",
        "150 instances",
        "3 classes",
        "Setosa",
        "Versicolor",
        "Virginica",
        "τ",
        "CT UCS UCCS MA SPCS BN BC NN MT Class",
        "Accuracy rate comparison Method Paper Accuracy rate",
        "Wisconsin Breast Cancer data set",
        "SL SW PL PW Class",
        "Wisconsin Breast Cancer classification",
        "common medical date set",
        "single epithelial cell size",
        "Two hundred forty-one instances",
        "Table 7 WBC classification rules",
        "average accuracy rates",
        "highest accuracy rates",
        "Table 5 Parameters ap bp cp",
        "IRIS classification rules",
        "3 triangular uncertain sets",
        "2 triangular uncertain sets",
        "Accuracy comparison",
        "method B. Chen",
        "two methods",
        "3 uncertain sets",
        "5 uncertain sets",
        "cell shape",
        "method A",
        "sepal length",
        "sepal width",
        "petal length",
        "petal width",
        "120th iteration",
        "150th iteration",
        "other methods",
        "rule bases",
        "clump thickness",
        "marginal adhesion",
        "bare nuclei",
        "bland chromatin",
        "normal nucleoli",
        "Figure 2 Results",
        "Figure 3 Results",
        "Figure 4 Results",
        "possible classes",
        "maximum number",
        "7 rules",
        "Table 2",
        "Table 3",
        "Table 4",
        "699 instances",
        "458 instances",
        "2 classes",
        "4 attributes",
        "HNFQ",
        "μq",
        "exp",
        "Vq",
        "testing",
        "algorithm",
        "Figures",
        "extraction",
        "FMM",
        "mutation",
        "Tables",
        "final",
        "9 attributes",
        "uniformity",
        "mitoses",
        "ηp",
        "2β2",
        "Ant colony optimization-based hybrid intelligent algorithms",
        "National Natural Science Foundation",
        "ant colony optimization algorithms",
        "ant colony algorithms",
        "Table 8 WBC classification rules",
        "multiple sequence alignment",
        "2nd International Workshop",
        "Appl. Soft Comput",
        "Appl. Math. Comput",
        "IRIS classification problem",
        "WBC data set",
        "two classification problems",
        "highest accuracy rate",
        "higher accuracy rate",
        "World J. Modell.",
        "continuous optimization models",
        "other researchers’ work",
        "Ant Algorithms",
        "hybrid process",
        "Ant Colonies",
        "test set",
        "Data Mining",
        "2nd ed",
        "continuous attributes",
        "uncertain process",
        "Uncertain Syst.",
        "training set",
        "first 460 instances",
        "left 239 instances",
        "50th iteration",
        "80th iteration",
        "later 239 instances",
        "J. Artif.",
        "IEEE Trans.",
        "Cordón",
        "fuzzy rules",
        "Fuzzy Syst",
        "Particle swarm",
        "Uncertainty Theory",
        "Fuzzy process",
        "method B",
        "existing methods",
        "Artificial Ants",
        "J. Intell.",
        "imum number",
        "9 rules",
        "6 rules",
        "Comparison",
        "Conclusions",
        "Acknowledgements",
        "China",
        "20 April",
        "References",
        "Kantardzic",
        "Concepts",
        "Wiley",
        "Hoboken",
        "Quinlan",
        "JR",
        "use",
        "Res.",
        "Parpinelli",
        "Lopes",
        "HS",
        "Freitas",
        "AA",
        "Evolut",
        "Casillas",
        "Proceedings",
        "Brussels",
        "Simul",
        "Lee",
        "Chuang",
        "GA-ACO",
        "Shelokar",
        "PS",
        "Siarry",
        "Jayaraman",
        "VK",
        "Kulkarni",
        "BD",
        "Berlin",
        "96",
        "98.",
        "14",
        "Preliminary Ant colony optimization algorithm",
        "General fuzzy min-max neural network",
        "IEEE Trans. Syst. Man Cybern",
        "Hierarchical neuro-fuzzy quadtree models",
        "new option pricing model",
        "27. Wisconsin Breast Cancer Dataset",
        "portfolio selection model",
        "Fuzzy Optimization Decis",
        "simple dynamic plant",
        "Proc. Institution Electr",
        "7 Convenient online submission",
        "7 Rigorous peer review",
        "Int. J. Oper",
        "uncertain differential equations",
        "Fuzzy Sets Syst.",
        "Int. J. Uncertain",
        "Eng. Control Sci",
        "J. Uncertain Syst.",
        "J. Uncertainty Anal",
        "Uncertain optimal control",
        "Uncertain inference control",
        "Uncertain set theory",
        "new approach",
        "fuzzy algorithms",
        "Fuzzy identification",
        "uncertain control",
        "Uncertain logic",
        "26. Iris dataset",
        "Human Uncertainty",
        "uncertainty markets",
        "research problems",
        "human language",
        "universal approximators",
        "Membership functions",
        "operational law",
        "complex systems",
        "decision processes",
        "simmulated annealing",
        "archive.ics",
        "de Souza",
        "Pacheco MA",
        "7 Immediate publication",
        "Open access",
        "7 High visibility",
        "fi eld",
        "Abstract Keywords",
        "SA Experiments",
        "Acknowledgements References",
        "next manuscript",
        "Existence",
        "Mak.",
        "Branch",
        "Mathematics",
        "application",
        "Ralescu",
        "Knowledged-Based",
        "Appl.",
        "Article",
        "inverted",
        "pendulum",
        "Zadeh",
        "Outline",
        "analysis",
        "Mamdani",
        "Takagi",
        "Sugeno",
        "modeling",
        "Gelatt",
        "CD",
        "Vecchi",
        "Science",
        "uci",
        "ml",
        "datasets",
        "Breast+Cancer",
        "FJ",
        "Vellasco",
        "Gabrys",
        "Bargiela",
        "journal",
        "benefi",
        "acceptance",
        "copyright"
      ],
      "merged_content": "\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 \nDOI 10.1186/s40467-015-0033-9\n\nRESEARCH Open Access\n\nExtraction methods for uncertain inference\nrules by ant colony optimization\nLing Chen, Yun Sun* and Yuanguo Zhu\n\n*Correspondence:\nchinalsy_881220@163.com\nSchool of Science, Nanjing\nUniversity of Science and\nTechnology, Nanjing 210094, China\n\nAbstract\n\nIn recent years, the research on data mining methods has received increasing\nattention. In this paper, we design an uncertain system with the extracted uncertain\ninference rules to solve the classification problems in data mining. And then, two\nextraction methods integrated with ant colony optimization are proposed for the\ngeneration of the uncertain inference rules. Finally, two applications are given to verify\nthe effectiveness and superiority of the proposed methods.\n\nKeywords: Uncertain inference rule; Uncertain system; Ant colony optimization\nalgorithm; Rules extraction; Data classification\n\nIntroduction\nNowadays, databases and computer networks, coupled with the use of advanced auto-\nmated data generation and collection tools, are widely used in many different fields such\nas finance, E-commerce, logistics, etc. As a result, the amount of data that people have\nto deal with is dramatically increasing. People hope to carry out scientific research, busi-\nness decision, or business management on the basis of the analysis of the existing data.\nHowever, the current data analysis tools have difficulty in processing the data in depth.\nTo compensate for this deficiency, there come the data mining techniques. Data mining is\nthe computational process of discovering some interesting, potentially useful patterns in\nlarge data sets. Those patterns can be concepts, rules, laws, and modes. The overall goal\nof data mining is to extract information from a data set and transform it into an under-\nstandable structure for further use. Data mining helps us to discover valuable information\nand knowledge. Data mining is applied to many fields in reality. There are many successful\nexamples [1] of data mining in business and science research. For instance, data mining is\nwidely used in financial data analysis, telecommunication, retail, and biomedical research.\nTherefore, the study of data mining technology has an important practical significance.\nThe main jobs of data mining are data description, data classification, data dependency,\n\ndata compartment analysis, data regression, data aggregate, and data prediction. What\ndata classification does is to find a couple of models or functions that can accurately\ndescribe the characteristics of the data sets. Then, we can identify the categories of the\npreviously unknown data. After obtaining the models or functions from the set of training\ndata with data mining algorithms, we use many methods to describe the output such as\nclassification rules (if-then), decision trees, mathematical formula, and neutral network.\n\n© 2015 Chen et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: chinalsy_881220@163.com\nhttp://creativecommons.org/licenses/by/4.0\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 2 of 19\n\nThere are a variety of approaches in data mining. For mining objects in different fields,\nmany different specified methods are invented. The approaches we usually used are statis-\ntical methods, machine learning methods, and modern intelligent optimization methods.\nThe statistical methods are very effective methods from the start. In addition, many other\ndata mining methods are invented based on the statistical methods. When dealing with\nclassification problems, Bayesian classification and Bayesian belief network are important\nclassification methods that based on the statistical principle. Machine learning methods\nare mainly used to solve the conceptual learning, pattern classification, and pattern clus-\ntering problems. The core content of machine learning is inductive learning. And there\nalready exist a number of mature technology methods, such as decision tree method for\nclassification problems. Decision trees method is one of the most popular classification\nmethods. The early decision trees algorithm is ID3 method. Later, based on ID3, many\nalgorithms such as C4.5 method [2] are proposed. Besides, there are some variants of the\ndecision trees algorithm including incremental tree structure ID4, ID5, and expandable\ntree structure SLIQ for massive data set.\nIn recent years, intelligent optimization algorithms are widely applied into data min-\n\ning. Neutral network is a simulation model for complex system with nonlinear relations.\nIt is very suitable to deal with complex nonlinear relations in spatial data. Researchers\nhave already proposed different network models to realize the clustering, classification,\nregression, and pattern recognition of the data. Furthermore, many evolution algorithms\nsuch as simulated annealing algorithm are introduced into neutral network algorithm\nas the optimization strategies. Genetic algorithm is a global search algorithm that sim-\nulates the biological evolution and genetic mechanism. It plays an important role in\noptimization and classification machine learning. Mixed algorithms of genetic algorithm\nand other algorithms, such as decision trees, neutral network, have been applied to the\ndata mining technology. Ant colony optimization algorithm is a bionic optimization algo-\nrithm that simulates the behavior of the ants. Based on that, a data mining technique\nant-miner [3] was invented. And Herrera [4] applied it to fuzzy rules learning. How-\never, ant colony optimization algorithm has some weakness such as slow convergence,\nrandom initial solutions. For this reason, some improved ant colony optimization algo-\nrithms are proposed. Zhu proposed an improved ant colony optimization algorithm\n(ACOA) [5] and a mutation ant colony optimization algorithm (MACO) [6] to speed up\nthe algorithms and avoid the solutions getting stuck in local optimums. Hybrid genetic\nant colony optimization [7] and hybrid particle swarm ant colony optimization algo-\nrithm [8] significantly improve the performance of the original ant colony optimization\nalgorithm.\nThe real world is so complex that human being may face different types of indetermi-\n\nnacy everyday. To get a better understanding of the real world, many mathematical tools\nare created. One of them is probability theory which is used to model indeterminacy from\nsamples. However, in many cases, no samples are available to estimate a probability distri-\nbution. In this situation, we have no choice but to invite some domain experts to evaluate\nthe belief degree that each event may occur. We cannot use probability theory to deal\nwith belief degree since human beings usually overweight unlikely events which makes\nthe belief degrees deviate far from the frequency. In view of this, Liu [9] founded uncer-\ntainty theory based on normality axiom, duality axiom, subadditivity axiom, and product\nmeasure axiom. It has become a powerful mathematical tool dealing with indeterminacy.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 3 of 19\n\nMany researchers have done a lot of theoretical work related to uncertainty theory. In\n2008, Liu [10] presented the uncertain differential equation. Later, the existence and\nuniqueness theorem was given [11]. And the stability of uncertain differential equation\nwas discussed [12,13]. Also, some analysis and numerical methods for solving uncertain\ndifferential equation were proposed. With uncertain differential equation describing the\nevolution of the system, we may solve some practical problems. Peng and Yao [14] stud-\nied an option pricing models for stocks. Zhu [15] proposed an uncertain optimal control\nmodel in 2010.\nIn [16,17], Liu proposed and studied the uncertain systems based on the concepts of\n\nuncertain sets, membership functions, and uncertain inference rules. An uncertain sys-\ntem is a function from its inputs to outputs based on the uncertain inference rule. Usually,\nan uncertain system consists of five parts: inputs, rule-base, uncertain inference rules,\nexpected value operator, and outputs. Following that, Gao et al. [18] generalized uncertain\ninference rules and described uncertain systems with them. Peng and Chen [19] proved\nthat uncertain systems are universal approximator and then demonstrated that the uncer-\ntain controller is a reasonable tool. Gao [20] designed an uncertain inference controller\nthat successfully balanced an inverted pendulum with 5 × 5 if-then rules. What is more\nimportant is that this uncertain inference controller has a good ability of robustness.\nOn the basis of uncertainty theory, we consider two extraction methods for uncertain\n\ninference rules by ant colony optimization algorithm. In the next section, we review the\nant colony optimization algorithm and give some basic concepts about uncertain sets.\nThen, we formulate a model to extract inference rules based on data set. And then, we\npropose an extraction method for uncertain inference rules by ant colony optimization\nalgorithm with a mutation operation. Finally, we combine the ant colony optimiza-\ntion algorithm with simulated annealing algorithm to speed up the extraction method.\nIn the last section, we discuss two typical classification problems in data mining with\nour results.\n\nPreliminary\nIn this section, we review the ant colony optimization algorithm. And then, we give some\nbasic concepts on uncertainty sets.\n\nAnt colony optimization algorithm\n\nAnt colony optimization algorithm, initiated by Dorigo, is a heuristic optimization\napproach. It simulates the behavior of real ants when they forage for food which relies on\nthe pheromone communication. In ant colony optimization algorithm, each path of artifi-\ncial ants walking from the food sources to the nest is a candidate solution to the problem.\nWhen walking on the path, the ants will release pheromone which evaporates over time.\nAnd the artificial ants will lay down more pheromone on the path corresponding to the\nbetter solution. While one ant has many paths to go, it will make a choice according to\nthe amount of the pheromone on the paths. The more pheromone there is on the path,\nthe better the solution is. As a result, bad paths will disappear since the pheromone evap-\norates over time. And good paths will be reserved since ants walking on it increases the\npheromone levels. Finally, one path which is used by most of the ants is left. Then, the\noptimal solution to the problem is obtained.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 4 of 19\n\nConsider the following optimization problem:\n\n⎧⎪⎪⎪⎨\n⎪⎪⎪⎩\nmin f (x)\ns.t.\n\ng(x) ≥ 0\nx ∈ D\n\n(1)\n\nwhere x is the decision variable in the domain D. And f (x) is the objective function while\ng(x) is the constraint function.\nWe can use ant colony optimization algorithm to obtain the optimal solution to the\n\nproblem (1). The parameters in the algorithm are initial pheromone τ0, ant transfer prob-\nability p, number of ants M, pheromone evaporation rate ρ, and number of iterations T.\nThe procedures are as follows.\n\nStep 1 Randomly generate a feasible solution x0 and set optimal solution s = x0. Initialize\nall pheromone trails with the same pheromone level τ0. Set k ← 0.\nStep 2 The artificial ant generates a walking path x in some probability p according to\n\nthe pheromone trails. If x ∈ D, then go to Step 3; otherwise, repeat Step 2 until x ∈ D.\nStep 3 Repeat Step 2 until for each ant and generate M feasible solutions. Let sk be the\n\nbest solution in this iteration.\nStep 4 If f (sk) < f (s), then s ← sk and update the pheromone trails according to the\n\noptimal solution in the current iteration.\nStep 5 If k < T, then k ← k + 1 and go to Step 2; otherwise, terminate.\nStep 6 Report the optimal solution.\n\nUncertain set\n\nLet � be a nonempty set and L be σ -algebra over �. Each � ∈ L is called an event. For\nany �, M{�} ∈ [0, 1]. The set function M defined on L is called an uncertain measure\nif it satisfies the following three axiom: M{�} = 1; M{�} + M{�c} = 1 for any � ∈ L;\nM\n\n{⋃∞\ni=1 �i\n\n} ≤ ∑∞i=1 M{�i} for all �1, �2, · · · ∈ L. Then, the triplet (�, L, M) is called\nan uncertainty space [9]. The product uncertain measure M is an uncertain measure sat-\nisfying M\n\n{∏∞\ni=1 �k\n\n} = ∞∧\ni=1\n\nMk{�k}, where �k are arbitrarily chosen events from Lk for\nk = 1, 2, · · · , respectively.\n\nDefinition 1. [16] An uncertain set is a function ξ from an uncertainty space (�, L, M)\nto a collection of sets of real numbers such that both {B ⊂ ξ } and {ξ ⊂ B} are events for\nany Borel set B.\n\nExample 1. Take (�, L, M) to be {γ1, γ2, γ3} with power set L. Then, the set-valued\nfunction\n\nξ(γ ) =\n\n⎧⎪⎪⎨\n⎪⎪⎩\n[ 1, 3] , if γ = γ1\n[ 2, 4] , if γ = γ2\n[ 3, 5] , if γ = γ3\n\nis an uncertain set on (�, L, M).\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 5 of 19\n\nDefinition 2. [16] The uncertain sets ξ1, ξ2, ξ3, · · · , ξn are said to be independent if for\nany Borel sets B1, B2, B3, · · · , Bn, we have\n\nM\n\n{ n⋂\ni=1\n\n(\nξ\n\n∗\ni ⊂ Bi\n\n)} = n∧\ni=1\n\nM\n{\nξ\n\n∗\ni ⊂ Bi\n\n}\nand\n\nM\n\n{ n⋃\ni=1\n\n(\nξ\n\n∗\ni ⊂ Bi\n\n)} = n∨\ni=1\n\nM\n{\nξ\n\n∗\ni ⊂ Bi\n\n}\n\nwhere ξ ∗i are arbitrarily chosen from\n{\nξi, ξ ci\n\n}\n, i = 1, 2, · · · , n, respectively.\n\nDefinition 3. [21] An uncertain set ξ is said to have a membership function μ if for any\nBorel set B of real numbers, we have\n\nM{B ⊂ ξ } = inf\nx∈B\n\nμ(x), M{ξ ⊂ B} = 1 − sup\nx∈Bc\n\nμ(x).\n\nThe above equations will be called measures inversion formulas.\n\nRemark 1. When an uncertain set ξ does have a membership function μ, it follows\nfrom the first measure inversion formula that\n\nμ(x) = M{x ∈ ξ }.\n\nExample 2. An uncertain set ξ is called triangular if it has a membership function\n\nμ(x) =\n⎧⎨\n⎩\n\nx−a\nb−a , a ≤ x ≤ b\nx−c\nb−c , b ≤ x ≤ c\n\n(2)\n\ndenoted by (a, b, c) where a, b, c are real numbers with a < b < c.\n\nDefinition 4. [21] A membership function μ is said to be regular if there exists a point x0\nsuch that μ(x0) = 1, and μ(x) is unimodal about the mode x0. That is, μ(x) is increasing\non (−∞, x0] and decreasing on [ x0, +∞).\n\nDefinition 5. [16] Let ξ be an uncertain set. Then, the expected value of ξ is defined by\n\nE[ ξ] =\n∫ +∞\n0\n\nM{ξ \n r}dr −\n∫ 0\n\n−∞\nM{ξ � r}dr\n\nprovided that at least one of the two integrals is finite and\n\nM{ξ \n r} = 1\n2\n(M{ξ ≥ r} + 1 − M{ξ < r}),\n\nM{ξ � r} = 1\n2\n(M{ξ ≤ r} + 1 − M{ξ > r}).\n\nTheorem 1. [13] Let ξ be an uncertain set with regular membership function μ. Then\n\nE[ ξ] = x0 +\n1\n2\n\n∫ +∞\nx0\n\nμ(x)dx − 1\n2\n\n∫ x0\n−∞\n\nμ(x)dx, (3)\n\nwhere x0 is a point such that μ(x0) = 1.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 6 of 19\n\nExample 3. Let ξ be a triangular uncertain set denoted by (a, b, c). Then, according to\nTheorem 1, we have\n\nE[ ξ] = a + 2b + c\n4\n\n.\n\nIn fact, it follows from Equations 2 and 3 that\n\nE[ ξ] = b + 1\n2\n\n∫ c\nb\n\nx − c\nb − cdx −\n\n1\n2\n\n∫ b\na\n\nx − a\nb − adx\n\n= b − 1\n4\n(b − c) − 1\n\n4\n(b − a)\n\n= a + 2b + c\n4\n\n.\n\nUncertain inference rule\n\nHere, we introduce concepts of the uncertain inference and uncertain system. Inference\nrules are the key points of the inference systems. In fuzzy systems, CRI approach [22],\nMamdani inference rules [23] and Takagi-Sugeno inference rules [24] are the most com-\nmon used inference rules. Fuzzy if-then inference rules use fuzzy sets to describe the\nantecedents and the consequents. Unlike fuzzy inference, both antecedents and conse-\nquents in uncertain inference are characterized by uncertain sets. Uncertain inference\n[16] is a process of deriving consequences from human knowledge via uncertain set\ntheory. First, we introduce the following inference rule.\n\nInference Rule 1. [16] Let X and Y be two concepts. Assume a rule ‘if X is an uncertain\nset ξ , then Y is an uncertain set η’. From X is a constant a, we infer that Y is an uncertain\nset\n\nη\n∗ = η|a∈ξ\n\nwhich is the conditional uncertain set of η given a ∈ ξ . The inference rule is represented by\n\nRule: If X is ξ, then Y is η\n\nFrom: X is a constant a\n\nInfer: Y is η∗ = η|a∈ξ\n\nTheorem 2. [16] Let ξ and η be independent uncertain sets with membership functions\nμ and ν, respectively. If ξ ∗ is a constant a, then the Inference Rule 1 yields that η∗ has a\nmembership function\n\nν\n∗\n(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nν(y)\nμ(a) , if ν(y) <\n\nμ(a)\n2\n\nν(y)+μ(a)−1\nμ(a) , if ν(y) > 1 − μ(a)2\n0.5, otherwise.\n\nBased on Inference Rule 1, Gao et al. [18] proposed the multi-input, multi-if-then-rule\ninference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 7 of 19\n\nInference Rule 2. [13] Let X1, X2, · · · , Xm, Y be concepts. Assume rules ‘if X1 is ξi1\nand · · · and Xm is ξim, then Y is ηi’ for i = 1, 2, · · · , k. From X1 is a constant a1 and · · ·\nand Xm is a constant am, we infer that\n\nη\n∗ =\n\nk∑\ni=1\n\nci · ηi|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\nc1 + c2 + · · · + ck\n\n, (4)\n\nwhere the coefficients are determined by\n\nci = M{(a1 ∈ ξi1) ∩ (a2 ∈ ξi2) ∩ · · · ∩ (am ∈ ξim)}\nfor i = 1, 2, · · · , k. The inference rule is represented by\n\nRule 1: If X1 is ξ11 and · · · and Xm is ξ1m, then Y is η1\nRule 2: If X1 is ξ21 and · · · and Xm is ξ2m, then Y is η2\n\n· · ·\nRule k: If X1 is ξk1 and · · · and Xm is ξkm, then Y is ηk\nFrom: X1 is a1 and · · · and Xm is am\nInfer: Y is determined by Eq. (4)\n\nTheorem 3. [13] Assume ξi1, ξi2, · · · , ξim, ηi are independent uncertain sets with mem-\nbership functions μi1, μi2, · · · , μim, νi, i = 1, 2, · · · , k, respectively. If ξ ∗1 , ξ ∗2 , · · · , ξ ∗m are\nconstants a1, a2, · · · , am, respectively, then the Inference Rule 2 yields\n\nη\n∗ =\n\nk∑\ni=1\n\nci · η∗i\nc1 + c2 + · · · + ck\n\nwhere η∗i are uncertain sets whose membership functions are given by\n\nν\n∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci\n\n, if νi(y) < ci2\nνi(y)+ci−1\n\nμ(a) , if νi(y) > 1 − ci2\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nUncertain system\n\nUncertain system, proposed by Liu [16], is a function from its inputs to outputs based\non the uncertain inference rule. Usually, an uncertain system consists of five parts: inputs\nthat are crisp data to be fed into the uncertain system; a rule-base that contains a set of\nif-then rules provided by the experts; an uncertain inference rule that infers uncertain\nconsequents from the uncertain antecedents; an expected value operator that converts\nthe uncertain consequents to crisp values; and outputs that are crisp data yielded from\nthe expected value operator.\nNow, we consider an uncertain system with m crisp inputs α1, α2, · · · , αm, and n crisp\n\noutputs β1, β2, · · · , βn. We have the following if-then rules:\nIf X1 is ξ11 and · · · and Xm is ξ1m, then Y1 is η11 and Y2 is η12 and · · · and Yn is η1n\nIf X1 is ξ21 and · · · and Xm is ξ2m, then Y1 is η21 and Y2 is η22 and · · · and Yn is η2n\n\n· · ·\nIf X1 is ξk1 and · · · and Xm is ξkm, then Y1 is ηk1 and Y2 is ηk2 and · · · and Yn is ηkn\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 8 of 19\n\nThus, according to Inference Rule 1 and 2, we can infer that Yj(j = 1, 2, · · · , n) are\n\nη\n∗\nj =\n\nk∑\ni=1\n\nci · ηij|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\nc1 + c2 + · · · + ck\n\n,\n\nwhere ci = M{(a1 ∈ ξi1) ∩ (a2 ∈ ξi2) ∩ · · · ∩ (am ∈ ξim)} for i = 1, 2, · · · , k. Then, by using\nthe expected value operator, we obtain\n\nβj = E\n[\nη\n\n∗\nj\n\n]\nfor j = 1, 2, · · · , n. Now, we construct a function from crisp inputs α1, α2, · · · , αm to crisp\noutputs β1, β2, · · · , βn, i.e.,\n\n(β1, β2, · · · , βn) = f (α1, α2, · · · , αm).\nThen, we get an uncertain system f. For the uncertain system we proposed, we have the\n\nfollowing theorem.\n\nTheorem 4. [13] Assume that ξi1, ξi2, · · · , ξim and ηi1, ηi2, · · · , ηin are indepen-\ndent uncertain sets with membership functions μi1, μi2, · · · , μim, νi1, νi2, · · · , νin, i =\n1, 2, · · · , k, respectively. Then, the uncertain system from α1, α2, · · · , αm to β1, β2, · · · , βn is\n\nbj =\nk∑\n\ni=1\n\nci · E[ η∗ij]\nc1 + c2 + · · · + ck\n\n,\n\nwhere j = 1, 2, · · · , n and η∗ij are uncertain sets whose membership functions are given by\n\nν\n∗\nij(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνij(y)\nci\n\n, if νij(y) < ci2\nνij(y)+ci−1\n\nμ(a) , if νij(y) > 1 − ci2\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nNext, we discuss the expected value of a special triangular uncertain set. Without loss of\ngenerality, we assume n = 1. Then the uncertain system proposed in the above becomes:\n\nb =\nk∑\n\ni=1\n\nci · E[ η∗i ]\nc1 + c2 + · · · + ck\n\n, (5)\n\nν\n∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci\n\n, if νi(y) < ci2\nνi(y)+ci−1\n\nμ(a) , if νi(y) > 1 − ci2\n0.5, otherwise,\n\n(6)\n\nci = min\n1≤l≤m\n\nμil(al). (7)\n\nTheorem 5. Assume we have an uncertain system with m inputs and 1 output consist-\ning of k inference rules. The antecedents of the rules are represented by the uncertain sets ξi\nwith membership functions μi1, μi2, · · · , μim, i = 1, 2, · · · , k. And the consequent is repre-\nsented by an triangular uncertain set ηi = (αi, βi, γi) with a membership function νi, where\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 9 of 19\n\nthe coefficients satisfy\n\nαi + γi = 2βi, i = 1, 2, · · · , k. (8)\nWe have\n\nE\n[\nη\n\n∗\ni\n] = βi, i = 1, 2, · · · , k.\n\nProof. Given the m input data a1, a2, · · · , am, we can calculate ci from Equation 7.\nThen, we can get the membership functions ν∗i of the consequence uncertain sets η\n\n∗\ni\n\naccording to Equation 6. Next, the computation of the expected value of uncertain\nconsequence breaks into three cases.\nCase 1: Assume ci/2 = 0.5. We can immediately have ν∗i (y) = νi(y), thus\n\nE[ η∗i ] =\nαi + 2βi + γi\n\n4\n= βi.\n\nCase 2: Assume ci/2 < 0.5. Let yi11 and y\ni\n12\n\n(\nyi11 < y\n\ni\n12\n\n)\nbe the two points that satisfy\n\nthe equation νi(y) = ci/2. Similarly, yi21 and yi22\n(\nyi21 < y\n\ni\n22\n\n)\nsatisfy the equation νi(y) =\n\n1 − ci/2. Since the membership function of a triangular uncertain set has a symmetry\nproperty, we have\n\nyi11 + yi12 = 2βi, yi21 + yi22 = 2βi. (9)\nThen, we can rewrite the membership function of ηi as follows:\n\nν\n∗\ni =\n\n⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nνi(y)\nci\n\n, if αi ≤ y < yi11\nνi(y)+ci−1\n\nci\n, if yi21 ≤ y < yi22\n\nνi(y)\nci\n\n, if yi12 ≤ y < γi\n0.5, otherwise.\n\n(10)\n\nAnd ν∗i (βi) = 1. Together with Equations 3, 8, and 9, we have\n\nE[ η∗i ] = βi +\n1\n2\n\n(∫ yi22\nβi\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi12\nyi22\n\n0.5dy +\n∫ γi\nyi12\n\nνi(y)\nci\n\ndy\n)\n\n−1\n2\n\n(∫ βi\nyi21\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi21\nyi11\n\n0.5dy +\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n= βi +\n1\n2\n\n(∫ yi22\nβi\n\nνi(y) + ci − 1\nci\n\ndy −\n∫ βi\nyi21\n\nνi(y) + ci − 1\nci\n\ndy\n)\n\n+1\n2\n\n(∫ γi\nyi12\n\nνi(y)\nci\n\ndy −\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n+1\n2\n\n(∫ yi12\nyi22\n\n0.5dy −\n∫ yi21\nyi11\n\n0.5dy\n)\n\n= βi.\nCase 3: Assume ci > 0.5. Similarly, we have E[ η∗i ] = βi. Thus, we have proved the\n\ntheorem.\n\nProblem formulation\nIn this section, we propose an extraction model to obtain uncertain inference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 10 of 19\n\nLet X = (x1, x2, · · · , xn) be the decision vector, which represents a rule base consisting\nof n rules. Each rule has m antecedents which are described by Q uncertain sets and one\nconsequent which is described by R uncertain sets. Each variable xi represents a sequence\nxi1xi2 · · · ximxim+1, where xij ∈ {0, 1, 2, · · · , Q}(i = 1, 2, · · · , n; j = 1, 2, · · · , m) represent\nthe antecedents of the inference rule. And xim+1 ∈ {0, 1, 2, · · · , R}(i = 1, 2, · · · , n) repre-\nsent the consequent. Thus, each variable of decision vector represents one inference rule.\nSome xij = 0 means this antecedent is not included. And some xim+1 = 0 means this\ninference rule will not be included in the rule base. For example, assume that we have one\ninference rule consists of 4 antecedents and 1 consequent. They are described by 5 uncer-\ntain sets which refer to five descriptions: very low, low, medium, high, and very high. We\nuse 1, 2, 3, 4, 5 to denote them. Thus, sequence “23045”, for example, represents the rule:\n“if input 1 is low, input 2 is medium, and input 4 is high, then the output is very high”.\nUncertain systems can be used for classification. But which uncertain system is better\n\ndepends on the rule base. Here, we try to find best rule base by comparing the mean\nabsolute errors of the origin output and the system output. That is,\n\nMAE = 1\nP\n\nP∑\ni=1\n\n|oi − ti|, (11)\n\nwhere P is the number of training data, oi, ti(i = 1, 2, · · · , P) are the system outputs and\norigin outputs, respectively. If we find the rule base with the least mean absolute error, we\nextract the uncertain inference rules successfully. We can obtain the system outputs by\nEquation 5. However, they may not be integers. To avoid this nonsense, for a classification\nproblem with C classes, we can divide interval that covers all the system outputs into C\nsubintervals. Then, if the output from Equation 5 is in the ith subinterval, we have oi = i.\nThus, we transfer the classification problem to the following optimization model:⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨\n\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nmin\nX\n\nF(X) = MAE\ns.t.\n\nX = (x1, x2, · · · , xn)\nxi = xi1 · · · ximxim+1\nxij ∈ {0, 1, · · · , Q}\nxim+1 ∈ {0, 1, · · · , R}\ni = 1, 2, · · · , n\nj = 1, 2, · · · , m\n\nExtraction method for uncertain inference rules with mutations\nIn this section, we propose the extraction method for uncertain inference rules with\nmutations by ant colony optimization algorithm.\nAs stated before, each xi is a sequence of m values in {0, 1, 2, · · · , Q} and 1 value in\n\n{0, 1, 2, · · · , R}. Without loss of generality, we set Q = R. Each number in {0, 1, 2, · · · , Q}\nis a node. Let ants walking across these nodes. Ants choose the next node in probability\nbased on the pheromone levels in the Q + 1 choices at every step. Once ants move m + 1\nsteps, a candidate decision variable is generated. After repeat this process n times, we get\na candidate solution. After all ants finish their walk, update the pheromone trails. Denote\nthe pheromone trail by τi;k,j(t) associated to the node j at step k of xi in iteration t. The\nprocedures are described as follows.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 11 of 19\n\n(1) Initialization: Randomly generate a feasible solution X0, and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · , m + 1, j = 0, 1, 2, · · · , Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik, select the\n\nnext node in probability following\n\npk;k+1 =\nτi;k+1,j(t)\n\nQ∑\nq=0\n\nτi;k+1,q(t)\n. (12)\n\nIn this way, we could get a sequence xi1xi2 · · · xim+1. To speed up the algorithm, we mutate\nthis sequence to get a new candidate sequence. The mutation is made as follows: ran-\ndomly add 1 or subtract 1 to each element xij in the sequence; if the element is 0, the\nmutated element is 1; if the element is Q, the mutated element is Q − 1. Assume X′ is\nthe mutated solution, if \rF = F(X′ ) − F(X) ≤ 0, then X ← X′; otherwise, keep the\ncurrent solution. If Q is very large, we could repeat this mutation until some termination\ncondition is satisfied.\n(3) Pheromone Update: At each iteration t, let X̂ be the optimal solution found so far\n\nand Xt be the best feasible solution in the current iteration. Assume F(X̂) and F(Xt) are\nthe corresponding objective function values.\n\nIf F(Xt) < F(X̂), then X̂ ← Xt.\nReinforce the pheromone trails on nodes of X̂ and evaporate the pheromone trails on\n\nthe left nodes:\n\nτi;j,k(t) =\n{\n\n(1 − ρ)τi;j,k(t − 1) + ρg(X̂), if (k, j) ∈ X̂\n(1 − ρ)τi;j,k(t − 1), otherwise\n\n(13)\n\nwhere ρ (0 < ρ < 1) is the evaporation rate, g(x)(0 < g(x) < +∞) is a function with that\ng(x) ≥ g(y) if F(x) < F(y), for example, g(x) = L/(|F(x)| + 1) is a function satisfying the\ncondition where L > 0.\n\nLet τ0 be the initial value of pheromone trails, n be the number of decision variables,\nM be the number of ants, ρ be evaporation rate and T be the number of iterations. Now,\nwe summarize the algorithm as follows.\n\nStep 1 Initialize all pheromone trails with the same pheromone level τ0. Randomly\ngenerate a feasible solution X0, and set optimal solution X̂ = X0. Set l ← 0.\nStep 2 Ant movement in probability following Equation 12. Generate a decision variable\n\nxi after m + 1 steps.\nStep 3 Repeat Step 2 until X = (x1, x2, · · · , xn) is generated; mutate every xi: thus, gen-\n\nerate a new decision vector X\n′ = (x′1, x\n\n′\n2, · · · , x\n\n′\nn); if \rF = F(X\n\n′\n) − F(X) ≤ 0, then\n\nX ← X′.\nStep 4 Repeat Step 2 and Step 3 for all M ants.\nStep 5 Calculate the system outputs by Equation 5. Then, calculate the objective function\n\nvalues for the M candidate solutions by Equation 11. Denote the best solution in this\niteration by Xl.\nStep 6 If F(Xl) < F(X̂), then X̂ ← Xl; update the pheromone trails according to\n\nEquation 13.\nStep 7 l ← l + 1; if l = T, terminate; otherwise, go to Step 2.\nStep 8 Report the optimal solution X̂.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 12 of 19\n\nWith this algorithm above, we obtain an uncertain rule base. Then, we successfully\ndesign an uncertain system and can use it for classification.\n\nExtraction method for uncertain inference rules with SA\nIn the previous section, to speed up the algorithm, we introduce a mutation operation.\nHere, we introduce the simulated annealing algorithm as the local search operation.\nSimulated annealing algorithm was initiated by Metropolis in 1953, applied to portfolio\n\noptimization by Kirkpatrick [25] in 1983. The name and inspiration come from anneal-\ning in metallurgy, a technique involving heating and controlled cooling of a material to\nincrease the size of its crystals and reduce their defects. Simulated annealing algorithm is\nexcellent at avoiding getting stuck in local optimums. It has a good robust property and is\nuniversal and easy to implement.\nFor optimization problem (1), we can use simulated annealing algorithm to search for\n\nthe optimal solution. The algorithm is as follows.\n\nStep 1 Randomly generate a initial solution x0; x ← x0; k ← 0; t0 ← tmax(initial\ntemperature);\nStep 2 If the temperature satisfies the inner cycle termination criterion, go to Step 3;\n\notherwise, randomly choose a point x\n′\nin the neighborhood N(x), calculate \rf = f (x′ ) −\n\nf (x). If \rf ≤ 0, then x ← x′; otherwise, according to Metropolis acceptance criterion, if\nexp(−\rf /tk) > random(0, 1), then x ← x′. Repeat Step 2.\nStep 3 tk+1 = d(tk) (temperature decrease); k ← k + 1; if the termination criterion is\n\nsatisfied, stop and report the optimal solution; otherwise, go to Step 2.\n\nIn this section, we combine ant colony optimization algorithm and simulated annealing\nalgorithm. In each iteration of ant colony optimization algorithm, we get a feasible solu-\ntion. Then, we use it as the initial solution of the simulated annealing algorithm to get a\nneighbor solution. This neighbor solution will be accepted in probability. And for each\ndecision vector X = (x1, x2, · · · , xn), xi = xi1xi2 · · · xim+1, we build the neighbor solution\nas follows: for each xi, for some randomly generated p and q (1 ≤ p < q ≤ m), reverse the\norder of the sequence xip · · · xiq, i.e., x′i = xi1 · · · xip−1xiqxiq−1 · · · xip+1xipxiq+1 · · · xim+1.\nFor example, assume xi is 0123456, p = 2, q = 6, and the neighbor solution x′i is 0543216.\nIn this way, we obtain a neighbor solution X\n\n′\n. If \rF = F(X′ ) − F(X) ≤ 0, X ← X′;\n\notherwise, if exp(−\rF/tk) > random(0, 1), then X ← X′; otherwise, abandon this neigh-\nbor solution. Still denote the pheromone trail by τi;k,j(t). The procedure are described as\nfollows.\n\n(1) Initialization: Generate a feasible solution X0 randomly and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · , m + 1, j = 0, 1, 2, · · · , Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik, select the\n\nnext node in probability following Equation 12. In this way, we could get a sequence\nxi1xi2 · · · xim+1. In order to expand the search range, we use simulated annealing algo-\nrithm to search locally around the solution at this step. Assume the neighbor solution is\nX\n\n′\n. If \rF = F(X′ ) − F(X) ≤ 0, X ← X′; otherwise, if exp(−\rF/tk) > random(0, 1)\n\nwhere tk is the current temperature and tk → 0 when k → ∞, then X ← X′; otherwise,\nabandon this neighbor solution and still choose the original feasible solution.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 13 of 19\n\n(3) Pheromone Update: Let X̂ be the optimal solution found so far and Xt be the best\nfeasible solution in the current iteration t. Assume F(X̂) and F(Xt) are the correspond-\ning objective function values. To avoid the optimal solution X̂ getting stuck in local\noptimums, we also use acceptance function here.\n\nIf F(Xt) < F(X̂), then X̂ ← Xt.\nBuild a neighbor solution X̂\n\n′\n.\n\nIf F(X̂\n′\n) ≤ F(X̂), then X̂ ← X̂′;\n\nIf F(X̂\n′\n) > F(X̂), check the Metropolis acceptance criterion, i.e., if\n\nexp(−\rF̂/Tt) > random(0, 1), Tt → 0, t → ∞, then X∗ ← X̂′.\nReinforce the pheromone trails on the nodes of X̂ and X∗ and evaporate the pheromone\n\ntrails on the left nodes:\n\nτi;j,k(t) =\n\n⎧⎪⎨\n⎪⎩\n\n(1 − ρ)τi;j,k(t − 1) + ρg(X̂), if (k, j) ∈ X̂\n(1 − ρ)τi;j,k(t − 1) + ρ2 g(X̂), if (k, j) ∈ X∗\n\n(1 − ρ)τi;j,k(t − 1), otherwise\n(14)\n\nwhere, ρ (0 < ρ < 1) is the evaporate rate, and g(x) (0 < g(x) < +∞) is a function with\nthat g(x) ≥ g(y) if F(x) < F(y). For example, g(x) = L/(|F(x)| +1) is an available function\nif L > 0.\n\nNow, we summarize the algorithm as follows.\n\nStep 1 Initialize all pheromone trails with the same pheromone level τ0. Randomly\ngenerate a feasible solution X0, and set optimal solution X̂ = X0. Set t ← 0.\nStep 2 Ant movement in probability following Equation 12. Generate a decision variable\n\nxi after m + 1 steps.\nStep 3 Repeat Step 2 until decision vector X = (x1, x2, · · · , xn) is generated. Build the\n\nneighbor solution X\n′\n. If \rF = F(X′ ) − F(X) ≤ 0, X ← X′; otherwise, if exp(−\rF/tk) >\n\nrandom(0, 1) where tk is the current temperature and tk → 0 when k → ∞, then X ← X′.\nStep 4 Repeat Step 2 and Step 3 until all ants finish their walk, and generate M candidate\n\nsolutions.\nStep 5 Calculate the system outputs by Equation 5. Then, calculate the objective function\n\nvalues for the M candidate solutions by Equation 11. Denote the best solution in this\niteration by Xt.\nStep 6 If F(Xt) < F(X̂), then X̂ ← Xt. Build the neighbor solution of X̂, which is denoted\n\nby X̂\n′\n. If \rF̂ = F(X̂′ ) − F(X̂) ≤ 0, then X̂ ← X̂′; otherwise, if Metropolis acceptance\n\ncriterion is satisfied, i.e., if exp(−\rF̂/Tt) > random(0, 1), Tt → 0, t → ∞, then X∗ ← X̂′.\nStep 7 Update the pheromone trails according to Equation 14.\nStep 8 t ← t + 1; if t = T, terminate; otherwise, go to Step 2.\nStep 9 Report the optimal solution X̂.\n\nTable 1 Parameters\n\nap bp cp\n\np = 1 0.5 1.01 1.52\np = 2 1.7 2.74 4.48\np = 3 5 6.07 7.14\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 14 of 19\n\nFigure 1 Results of method A.\n\nExperiments\nIn this section, we use our two extraction methods to extract uncertain inference rules.\nAnd then use the uncertain systems to solve some classification problems. We applied our\nmethods to the IRIS [26] classification problem and the Wisconsin Breast Cancer (WBC)\n[27] classification problem.\n\nIRIS classification\n\nIRIS data set is the typical date set in data classification. It contains 150 instances of 3\nclasses, which are Setosa, Versicolor, and Virginica. Each class has 50 instances. Each\ninstance has 4 attributes which are sepal length (SL), sepal width (SW), petal length (PL),\n\nFigure 2 Results of method B.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 15 of 19\n\nTable 2 Accuracy comparison\n\nMethod Paper Accuracy rate (%)\n\nMethod A This paper 97.33\n\nMethod B This paper 97.5\n\nC4.5 [2] 94.0\n\nACOA [5] 96.6\n\nMACO [6] 95.53\n\nHNFQ [28] 98.67\n\nTable 3 IRIS classification rules extracted by method A\n\nIF THEN\n\nSL SW PL PW Class\n\n1 3 1 3 1\n\n1 0 1 1 1\n\n1 2 3 2 1\n\n1 1 2 1 2\n\n2 1 0 3 2\n\n3 2 0 2 3\n\n1 1 3 3 3\n\nTable 4 IRIS classification rules extracted by method B\n\nIF THEN\n\nSL SW PL PW Class\n\n3 2 3 1 2\n\n1 1 0 0 2\n\n0 2 1 1 3\n\n0 1 1 3 1\n\n1 1 3 3 2\n\n1 1 3 1 1\n\n2 1 1 2 1\n\nTable 5 Parameters\n\nap bp cp\n\np = 1 0.3 1.01 1.72\np = 2 2 6.07 10.14\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 16 of 19\n\nFigure 3 Results of method A.\n\nand petal width (PW). They are described by 3 uncertain sets: low (1), medium (2), and\nhigh (3). The membership functions are\n\nμq(x) = exp\n(\n\n− (x − Vq)\n2\n\n2β2\n\n)\n,\n\nwhere x is the input, β = 0.618 and Vq = q−12 , q = 1, 2, 3. Based on these 4 attributes,\nwe try to infer which class does the instance belong to. We use 3 triangular uncertain\nsets ηp = (ap, bp, cp) (p = 1, 2, 3) to describe the possible classes (class 1: Setosa; class 2:\nVersicolor; class 3: Virginica). And the parameters ap, bp, cp ∈ R are listed in Table 1.\nFirst, we normalize the data to [0, 1] to simplify the computation. IRIS data set is our\n\ntraining set while it is also used for testing. Then, we set maximum number of rules n =\n10, number of ants M = 10, evaporate rate ρ = 0.3, and number of iterations T = 300.\nEach algorithm runs ten times. The results are in Figures 1 and 2. Denote the extraction\n\nFigure 4 Results of method B.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 17 of 19\n\nTable 6 Accuracy rate comparison\n\nMethod Paper Accuracy rate (%)\n\nMethod A This paper 98.3\n\nMethod B This paper 98.33\n\nC4.5 [2] 94.25\n\nACOA [5] 97.91\n\nMACO [6] 97.07\n\nFMM [29] 97.86\n\nmethod with mutation by A and the method with SA by B. It can be seen that the method\nA converges fast at about 120th iteration. And method B converges a little slower at about\n150th iteration.\nThen, we can classify the IRIS data with the uncertain systems we introduced. We\n\nfind the average accuracy rates of the two methods are 97.33% and 97.5%, respectively.\nComparison with other methods are listed in Table 2.\nList the rule bases we get with the highest accuracy rates (98.0% and 98.67%, respec-\n\ntively) in Tables 3 and 4. Note that although the maximum number of rules is 10, the final\nrule bases we obtain has only 7 rules.\n\nWisconsin Breast Cancer classification\n\nWisconsin Breast Cancer data set is a common medical date set. It contains 699 instances\nof 2 classes, which are sick and healthy. Two hundred forty-one instances are sick and 458\ninstances are healthy. Each instances has 9 attributes, which are clump thickness (CT),\nuniformity of cell size (UCS), uniformity of cell shape (UCCS), marginal adhesion (MA),\nsingle epithelial cell size (SPCS), bare nuclei (BN), bland chromatin (BC), normal nucleoli\n(NN), and mitoses (MT). They are described by 5 uncertain sets: very low (1), low (2),\nmedium (3), high (4), and very high (5). The membership functions are\n\nμq(x) = exp\n(\n\n− (x − Vq)\n2\n\n2β2\n\n)\n,\n\nwhere x is the input, β = 0.4247, and Vq = q−12 , q = 1, 2, 3, 4, 5. Based on these attributes,\nwe diagnose whether one instance is sick or not. We use 2 triangular uncertain sets\nηp = (ap, bp, cp) (p = 1, 2) to describe the possible classes (sick and healthy). And the\nparameters ap, bp, cp ∈ R are listed in Table 5.\n\nTable 7 WBC classification rules extracted by method A\n\nIF THEN\n\nCT UCS UCCS MA SPCS BN BC NN MT Class\n\n1 5 0 3 2 0 4 2 1 2\n\n1 2 1 1 4 4 2 1 0 1\n\n1 3 5 2 3 2 1 1 4 2\n\n1 3 4 2 3 1 2 2 1 1\n\n2 4 4 1 1 2 4 5 1 2\n\n3 3 4 5 4 3 2 4 4 2\n\n5 2 4 0 3 0 0 2 1 1\n\n2 4 4 1 1 2 4 5 1 1\n\n2 4 2 3 5 3 2 5 5 2\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 18 of 19\n\nTable 8 WBC classification rules extracted by method B\n\nIF THEN\n\nCT UCS UCCS MA SPCS BN BC NN MT Class\n\n5 3 3 3 3 2 3 2 4 2\n\n0 0 0 0 0 0 0 4 0 1\n\n4 4 4 4 0 1 1 1 4 2\n\n1 4 4 1 1 2 1 5 1 2\n\n1 1 1 2 0 3 5 5 5 2\n\n0 3 0 0 0 0 0 1 2 1\n\nFirst, we normalize the data to [0, 1] to simplify the computation. The first 460 instances\nare used for training while the left 239 instances are used for testing. Then, we set max-\nimum number of rules n = 10, number of ants M = 20, evaporate rate ρ = 0.3, and\nnumber of iterations T = 200. Each algorithm runs ten times. The results are in Figures 3\nand 4. We still find that method A converges faster than method B. Method A stabilizes\nat about 50th iteration while method B stabilizes until about 80th iteration.\nThen, we test the uncertain systems we get with the later 239 instances. We find the\n\naverage accuracy rates of the two methods on the training set are 96.0% and 96.26%,\nrespectively. Using the uncertain system with the highest accuracy rate of each method\non the test set, we find the accuracy rates are 98.37% and 98.33%. Comparison with other\nmethods are listed in Table 6.\nThe rule base with the highest accuracy rates (98.37% and 98.33%, respectively) on the\n\ntest set are listed in Tables 7 and 8. Method A gives us a rule base of 9 rules, and method\nB provides a rule base of 6 rules.\nWe apply our two extraction methods to the classification problems of IRIS data set\n\nand WBC data set. Compare our results with other researchers’ work, we can find that\nboth methods have higher accuracy rate than ACOA and MACO in two classification\nproblems. And for IRIS data set, accuracy rates of method A and B are lower than HNFQ\nbut higher than C4.5. For WBC data set, their accuracy rates are higher than C4.5 and\nFMM.\n\nConclusions\nIn this paper, we designed an uncertain system for data classification. And we proposed\ntwo extraction methods for uncertain inference rules by using ant colony optimization\nalgorithm. Then, we applied our methods to IRIS classification problem and WBC clas-\nsification problem. Our methods are shown to be superior in accuracy to some existing\nmethods.\n\nAcknowledgements\nThis work is supported by the National Natural Science Foundation of China (No.61273009).\n\nReceived: 14 December 2014 Accepted: 20 April 2015\n\nReferences\n1. Kantardzic, M: Data Mining: Concepts, Models, Methods, and Algorithms. 2nd ed. Wiley, Hoboken (2011)\n2. Quinlan, JR: Improved use of continuous attributes in C4.5. J. Artif. Intell. Res. 4(1), 77–90 (1996)\n3. Parpinelli, RS, Lopes, HS, Freitas, AA: Data mining with an ant colony optimization algorithm. IEEE Trans. Evolut.\n\nComput. 6(4), 321–332 (2002)\n4. Casillas, J, Cordón, O, Herrera, F: Learning fuzzy rules using ant colony optimization algorithms. In: Proceedings of\n\nthe 2nd International Workshop on Ant Algorithms: From Ant Colonies to Artificial Ants, pp. 13–21, Brussels, (2000)\n\n Published online: 14 May 2015 \n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications  (2015) 3:9 Page 19 of 19\n\n5. Zhu, Y: Ant colony optimization-based hybrid intelligent algorithms. World J. Modell. Simul. 2(5), 283–289 (2006)\n6. Zhu, Y: An intelligent algorithm: MACO for continuous optimization models. J. Intell. Fuzzy Syst. 24, 31–36 (2013)\n7. Lee, Z, Su, S, Chuang, C, Liu, K: Genetic algorithm with ant colony optimization (GA-ACO) for multiple sequence\n\nalignment. Appl. Soft Comput. 8(1), 55–78 (2008)\n8. Shelokar, PS, Siarry, P, Jayaraman, VK, Kulkarni, BD: Particle swarm and ant colony algorithms hybridized for improved\n\ncontinuous optimization. Appl. Math. Comput. 188(1), 129–142 (2007)\n9. Liu, B: Uncertainty Theory. 2nd ed. Springer, Berlin (2007)\n10. Liu, B: Fuzzy process, hybrid process and uncertain process. J. Uncertain Syst. 2(1), 3–16 (2008)\n11. Chen, X, Liu, B: Existence and uniqueness theorem for uncertain differential equations. Fuzzy Optimization Decis.\n\nMak. 9(1), 69–81 (2010)\n12. Liu, B: Some research problems in uncertainty theory. J. Uncertain Syst. 3(1), 3–10 (2009)\n13. Liu, B: Uncertainty Theory: A Branch of Mathematics for Modeling Human Uncertainty. Springer, Berlin (2010)\n14. Peng, J, Yao, K: A new option pricing model for stocks in uncertainty markets. Int. J. Oper. Res. 8(2), 18–26 (2011)\n15. Zhu, Y: Uncertain optimal control with application to a portfolio selection model. Cybern. Syst. 41(7), 535–547 (2010)\n16. Liu, B: Uncertain set theory and uncertain inference rule with application to uncertain control. J. Uncertain Syst. 4(2),\n\n83–98 (2010)\n17. Liu, B: Uncertain logic for modeling human language. J. Uncertain Syst. 5(1), 3–20 (2011)\n18. Gao, X, Gao, Y, Ralescu, DA: On Liu’s inference rule for uncertain systems. Int. J. Uncertain. Fuzz. Knowledged-Based\n\nSyst. 18(1), 1–11 (2010)\n19. Peng, Z, Chen, X: Uncertain systems are universal approximators. J. Uncertainty Anal. Appl. 2, Article, 13 (2014)\n20. Gao, Y: Uncertain inference control for balancing inverted pendulum. Fuzzy Optimization Decis. Mak. 11(4), 481–492\n\n(2012)\n21. Liu, B: Membership functions and operational law of uncertain sets. Fuzzy Optimization Decis. Mak. 11(4), 387–410\n\n(2012)\n22. Zadeh, LA: Outline of a new approach to the analysis of complex systems and decision processes. IEEE Trans. Syst.\n\nMan Cybern. 3(1), 28–44 (1973)\n23. Mamdani, EH: Applications of fuzzy algorithms for control of a simple dynamic plant. Proc. Institution Electr. Eng.\n\nControl Sci. 121(12), 1585–1588 (1974)\n24. Takagi, K, Sugeno, M: Fuzzy identification of system and its applications to modeling and control. IEEE Trans. Syst.\n\nMan Cybern. 15(1), 116–132 (1985)\n25. Kirkpatrick, S, Gelatt, CD, Vecchi, MP: Optimization by simmulated annealing. Science. 220(4598), 671–680 (1983)\n26. Iris dataset (1936). https://archive.ics.uci.edu/ml/datasets/Iris\n27. Wisconsin Breast Cancer Dataset (1992). https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)\n28. de Souza, FJ, Vellasco, M, Pacheco MA: Hierarchical neuro-fuzzy quadtree models. Fuzzy Sets Syst. 130(2), 189–205\n\n(2002)\n29. Gabrys, B, Bargiela, A: General fuzzy min-max neural network for clustering and classification. IEEE Trans. Neural\n\nNetwor. 11(3), 769–783 (2000)\n\nSubmit your manuscript to a \njournal and benefi t from:\n\n7 Convenient online submission\n7 Rigorous peer review\n7 Immediate publication on acceptance\n7 Open access: articles freely available online\n7 High visibility within the fi eld\n7 Retaining the copyright to your article\n\n    Submit your next manuscript at 7 springeropen.com\n\nhttps://archive.ics.uci.edu/ml/datasets/Iris\nhttps://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)\n\n\tAbstract\n\tKeywords\n\n\tIntroduction\n\tPreliminary\n\tAnt colony optimization algorithm\n\tUncertain set\n\tUncertain inference rule\n\tUncertain system\n\n\tProblem formulation\n\tExtraction method for uncertain inference rules with mutations\n\tExtraction method for uncertain inference rules with SA\n\tExperiments\n\tIRIS classification\n\tWisconsin Breast Cancer classification\n\n\tConclusions\n\tAcknowledgements\n\tReferences\n\n",
      "text": [
        "Published online: 14 May 2015"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"Published online: 14 May 2015\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":16},{\"x\":897,\"y\":17},{\"x\":896,\"y\":72},{\"x\":0,\"y\":70}],\"text\":\"Published online: 14 May 2015\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":18},{\"x\":284,\"y\":17},{\"x\":284,\"y\":71},{\"x\":0,\"y\":68}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":294,\"y\":17},{\"x\":504,\"y\":16},{\"x\":505,\"y\":72},{\"x\":294,\"y\":71}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":514,\"y\":16},{\"x\":587,\"y\":16},{\"x\":588,\"y\":73},{\"x\":515,\"y\":72}],\"text\":\"14\"},{\"boundingBox\":[{\"x\":597,\"y\":16},{\"x\":735,\"y\":17},{\"x\":736,\"y\":73},{\"x\":597,\"y\":73}],\"text\":\"May\"},{\"boundingBox\":[{\"x\":745,\"y\":17},{\"x\":893,\"y\":17},{\"x\":895,\"y\":73},{\"x\":746,\"y\":73}],\"text\":\"2015\"}]}"
      ]
    },
    {
      "@search.score": 4.189812,
      "content": "\nMobile marketing recommendation method \nbased on user location feedback\nChunyong Yin1 , Shilei Ding1 and Jin Wang2*\n\nIntroduction\nIn recent years, the e-commerce industry has developed rapidly with the popularization \nof the Internet. At this time, famous e-commerce platforms such as Alibaba and Ama-\nzon were born. E-commerce moved physical store products to a virtual network plat-\nform. On the one hand, it is convenient for users to buy various products without leaving \nthe home. On the other hand, it is also convenient for sellers to sell their own goods \nand reduce costs. However, the various products have made it more difficult for users \nto select products. E-commerce platform can generate a large amount of user location \nfeedback data which contains a wealth of user preference information [1]. It is significant \nto predict the location of the next consumer’s consumption from these behavioral data. \nAt present, most of the recommended methods focus on the user-product binary matrix \nand directly model their binary relationships [2]. The users’ location information and \nshopping location information are considered as the third factor. In this case, you can \nonly use the limited check-in data. The users’ location feedback behavior and the timeli-\nness of behavior are often overlooked.\n\nThe mobile recommendation system takes advantage of the mobile network environ-\nment in terms of information recommendation and overcomes the disadvantages. Filter-\ning irrelevant information by predicting potential mobile user preferences and providing \n\nAbstract \nLocation-based mobile marketing recommendation has become one of the hot spots \nin e-commerce. The current mobile marketing recommendation system only treats \nlocation information as a recommended attribute, which weakens the role of users and \nshopping location information in the recommendation. This paper focuses on location \nfeedback data of user and proposes a location-based mobile marketing recommenda-\ntion model by convolutional neural network (LBCNN). First, the users’ location-based \nbehaviors are divided into different time windows. For each window, the extractor \nachieves users’ timing preference characteristics from different dimensions. Next, we \nuse the convolutional model in the convolutional neural network model to train a \nclassifier. The experimental results show that the model proposed in this paper is better \nthan the traditional recommendation models in the terms of accuracy rate and recall \nrate, both of which increase nearly 10%.\n\nKeywords: Location feedback, Mobile marketing, Convolutional neural network, \nSequential behavior\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nR E S E A R C H\n\nYin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14  \nhttps://doi.org/10.1186/s13673-019-0177-6\n\n*Correspondence:   \njinwang@csust.edu.cn \n2 School of Computer & \nCommunication Engineering, \nChangsha University \nof Science & Technology, \nChangsha 410004, China\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-5764-2432\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-019-0177-6&domain=pdf\n\n\nPage 2 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nmobile users with results that meet users’ individual needs gradually become an effec-\ntive means to alleviate “mobile information overload” [3]. Mobile users have different \npreferences in different geographical locations. For this problem, how to use location \ninformation to obtain mobile users’ preferences and provide accurate personalized \nrecommendations has become a hot topic in mobile recommendation research [4]. \nAlthough there are many researches based on location recommendation, they mainly \nfocus on service resources without positional relevance. To solve the shortcomings of \nresearch on location relevance of service resources is few [5], Zhu et  al. [6] proposed \nthe method which is based on the user’s context information to analyze the user’s pref-\nerences and retrograde. Their approach is to derive user preferences by proposing two \ndifferent assumptions and then recommending user models based on preference analy-\nsis. Yin et  al. [7] proposed LA-LDA. The method is a location-aware based generation \nprobability model, which uses scoring based on location to model user information and \nrecommend to users. However, these methods only treat location information as an \nattribute without considering the spatial information of users or items and weaken loca-\ntion information’s role in the recommendation. There are some studies determine user \npreferences by the distance between the mobile user and the merchant [8], but only set \nthe area based on the proximity of the distance and ignore the spatial activities of the \nmobile user [9]. However, these methods were limited to the analysis of user informa-\ntion and product information, and did not carefully consider the importance of user and \nbusiness location information. Therefore, the user preference model based on location \nrecommendation they created has some gap.\n\nConsidering the core of mobile marketing recommendation is location movement, \nLian et al. [10] proposed an implied feature-based cognitive feature collaborative filter-\ning (ICCF) framework, which avoids the impact of negative samples by combining con-\nventional methods and semantic content. In terms of algorithms, the author proposed \nan improved algorithm that can expand according to data size and feature size. To deter-\nmine the relevance of the project to user needs, Lee et al. [11] developed context infor-\nmation analysis and collaborative filtering methods for multimedia recommendations in \nmobile environments. Nevertheless, these methods only used small-scale training data \nand could not achieve accurate prediction of long-term interest for users. In this paper, \ndeep learning and time stamps are used to compensate for these shortcomings.\n\nWith great achievements in visual and speech tasks, the Deep Learning (DL) model \nhas become a novel field of study [12]. Because of the interventional optimization of \ndeep learning algorithms, artificial intelligence has made great breakthroughs in many \naspects. It is well known that models obtained through deep learning and machine learn-\ning models have very similar effects, which learns advanced abstract features from the \noriginal input features by simulating the network structure of the human nervous sys-\ntem. Experiments show that the deep model can express the characteristics of the data \nbetter than the shallow model [13]. Weight sharing by convolution makes CNN similar \nto biological neural networks, which reduces the difficulty of network structure and the \nnumber of weights. The structure of CNN is roughly divided into two layers. It is well \nknown that the first layer is a convolutional layer. Each neuron’s input is connected to the \nprevious layer through a convolution kernel and the local features are extracted. Next \nlayer is a pooling layer. In this layer, the neurons in the network are connected through \n\n\n\nPage 3 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\na convolution kernel to extract the overall features. Convolutional neural networks have \ngreat advantages in processing two-dimensional features [14], such as images.\n\nBased on our detailed comparative analysis, this paper proposes a location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN). \nFirstly, we use user-product information as a training sample, and treat this problem as \na two-class problem. The category of the problem is divided into the purchase behav-\nior and the purchase behavior of the product at the next moment. In order to capture \nthe user’s timing preference characteristics, we divide the behavior of the merchandise \naccording to a certain length of time window and dig deeper into the behavior charac-\nteristics of each time window. Secondly, we consider the users’ timing preferences and \noverall preferences for the product. Then, the features of time window are used to train \nconvolutional neural network models. Finally, we input the sample features of the test \nset into the model and generate the Top-K sample as the location-based purchase fore-\ncast results [15].\n\nRemain of the paper is divided into four sections. Related work is shown in “Related \nwork” section. Necessary definitions and specific implementation of the location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN) \nare shown in “Location-based mobile marketing recommendation model by CNN” sec-\ntion. In “Experimental analysis” section, experimental analysis is introduced. “Conclu-\nsion” section summarizes the strengths and weaknesses of the paper and proposes plans \nfor future progress.\n\nRelated work\nIn the current chapter, we will review existing methods for recommending systems \nthat can be broadly divided into three parts: content filtering, collaborative filtering \nand hybrid methods. We also discuss the establishment of feature models based on \ntime series to clearly represent the differences between our research and other existing \nmethods.\n\nTraditional recommendation method\n\nIn the general products recommendation system, the similarity between users is calcu-\nlated by the user’s interest feature vector. Then, the system recommends some products \nwith similarity greater than a certain threshold or the similar Top-N products to the tar-\nget user. This is a traditional recommendation algorithm based on content and the rec-\nommendation is based on comparing users.\n\na. Content‑based recommendation method\n\nContent-based information filtering has proven to be an effective application for \nlocating text documents related to topics. In particular, we need to focus on the \napplication of content-based information filtering in the recommendation system. \nContent-based methods allow for accurate comparisons between different texts \nor projects, so the recommended results are similar to the historical content of the \nuser’s consumption. The content-based recommendation algorithm involves the fol-\nlowing aspects. User description file describes the user’s preferences, which can be \nfilled by the user and dynamically updated based on the user’s feedback information \n\n\n\nPage 4 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\n(purchasing, reading, clicking, etc.) during the operation of the system. The project \nprofile describes the content characteristics of each project, which constitutes the \nfeature vector of the project. In addition, the similarity calculation is the similarity \nbetween the user’s description file and the item feature vector.\n\nThe similarity calculation of the content-based recommendation algorithm usually \nadopts the cosine similarity algorithm. The algorithm needs to calculate the similarity \nbetween the feature vector of user u and the feature vector of item i. The calculation \nformula is as shown in Formula (1).\n\nwhere ⇀u denotes the user feature vector, \n⇀\n\ni  denotes the project feature vector, \n⇀\n\n|u| is the \nmodulus of the user feature vector and \n\n⇀\n\n|i| is the model of the project feature vector.\nRepresentative content-based recommendation systems mainly include Lops, \n\nGemmis, and Semeraro [16]. Compared to other methods, content-based recom-\nmendations have no cold-start issues and recommendations are easy to understand. \nHowever, the content filtering based recommendation method has various draw-\nbacks, such as strongly relying on the availability of content and ignoring the context \ninformation of the recommended party. The content-based recommendation method \nalso has certain requirements for the format of the project. Besides, it is difficult to \ndistinguish the merits of the project. The same type of project may have the same type \nof features, which are difficult to reflect the quality of the project.\n\nb. Collaborative filtering method\n\nThe recommendation based on collaborative filtering solves the recommendation \nproblem by using the information of similar users in the same partition to analyze and \nrecommend new content that has not been scored or seen by the target user.\n\nRegarding the traditional collaborative filtering method based on memory, we \nunderstand that this method is based on the different relationships between users and \nprojects. According to expert research, the traditional collaborative filtering method \nbased on memory should be divided into the following three steps.\n\nStep 1: collection of user behavior data, this step represents the user’s past behav-\nior with a m * n matrix R. The matrix  Umn represents the feedback that the user m \nhas on the recommended object n. Rating is a range of values and different values \nrepresent how much the user likes the recommended object.\n\nStep 2: establishment of a user neighbor: establish mutual user relationships by \nanalyzing all user historical behavior data.\n\n(1)sim(u, i) =\n⇀\nu ·\n\n⇀\n\ni\n\n⇀\n\n|u|\n⇀\n\n|i|\n\nU =\n\n\n\n\n\n\n\nU11 U12 . . . U1n\nU21 U22 . . . U2n\n. . . . . . . . . . . .\n\nUm1 Um2 . . . Umn\n\n\n\n\n\n\n.\n\n\n\nPage 5 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nStep 3: generate recommendation results: find the most likely N objects from the rec-\nommended items selected by similar user sets.\n\nTherefore, recommendations are made by mining common features in similar users’ pref-\nerence information [17]. The normal methods in this classification include k-nearest neigh-\nbor (k-NN), matrix decomposition, and semi-supervised learning. According to the survey, \nAmazon uses an item-by-item collaborative filtering method to recommend personalized \nonline stores for each customer.\n\nCompared to other method, collaborative filtering has the ability to filter out informa-\ntion that can be automatically recognized by the machine and effectively use feedback from \nother similar users. However, collaborative filtering requires more ratings for the project, \nso it is affected by the issue of rating sparsity. In addition, this method does not provide a \nstandard recommendation for new users and new projects, which is called a cold start issue.\n\nc. Hybrid recommendation method\n\nThe hybrid recommendation method combines the above techniques in different ways to \nimprove the recommended performance and optimize the shortcomings of the conven-\ntional method. Projects that cannot be recommended for collaborative filtering are gener-\nally addressed by combining them with content-based filtering [18].\n\nThe core of this method is to independently calculate the recommendation results of the \ntwo types of recommendation algorithms, and then mix the results. There are two specific \nhybrid methods. One method is to mix the predicted scores of the two algorithms linearly. \nAnother hybrid method is to set up an evaluation standard, compare the recommended \nresults of the two algorithms, and take the recommendation results of the higher evaluation \nalgorithms. In general, the hybrid recommendation achieves a certain degree of compensa-\ntion between different recommendation algorithms. However, the hybrid recommendation \nalgorithm still needs improvement in complexity.\n\nd. Recommendation based on association rules\n\nThe association rule algorithm is a traditional data mining method that has been widely \nused in business for many years. The core idea is to analyze the rules of user historical \nbehavior data to recommend more similar behavioral items [19]. Rules can be either user-\ndefined or dynamically generated by using rule algorithms. The effect of the algorithm \ndepends mainly on the quantity and quality of the rules so the focus of the algorithm is on \nhow to develop high quality rules.\n\nDefine N as the total number of transactions, R is the total project and U and V are two \ndisjoint sets of items (U∩V ≠ ∅, U∈R, V∈R). The association rule is essentially an IF–Then \nstatement, here is expressed by U → V. The strength of the association rule U → V can be \nmeasured by two criteria: support and confidence. S is the ratio containing U and V data \nwhich both represent the number of transactions, which is shown in Formula (2).\n\nC is the ratio of U, V data to the only U data which represents the number of transac-\ntions, as shown in Formula (3)\n\n(2)S(U → V ) =\nN(U ∪ V )\n\nN\n.\n\n\n\nPage 6 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nThe recommendation process of the algorithm is shown in below.\nFirstly, according to the items of interest to the user, the user’s interest in other \n\nunknown items is predicted by rules. Secondly, compare the support of the rules. Finally, \nthe recommended items of TOP-N are obtained to the user.\n\nThe recommendation system based on association rules includes three parts: the key-\nword, the presentation and the user interface. The keyword layer is a set of keyword \nattributes and dependencies between keywords. The description layer connects the \nkeyword layer and the user layer and the main function is to describe the user and the \nresource. The user interface layer is the layer that interacts directly with the user. How-\never, the system becomes more and more difficult to manage as the rules increasing. In \naddition, there is a strong dependence on the quality of the rules and a cold start prob-\nlem is existed.\n\nMost of the recommendation systems use collaborative filtering algorithm to recom-\nmend for users. However, the traditional algorithm can only analyze ready-made data \nsimply, and most systems simply preprocess the data. In our method, we preprocess the \ndataset by extending the time information of the data to a time label. The next section is \nan explanation of the specific implementation.\n\nConstruction of time series behavior’s preference features\n\nThe timing recommendation model is based primarily on the Markov chain. This model \nmakes full use of timing behavior data to predict the next purchase behavior based on \nthe user’s last behavior. The advantage of this model is that it can generate good recom-\nmendations by timing behavior.\n\nAs shown in Fig.  1, the prediction problem of product purchase can be expressed as \npredicts the user’s purchase behavior at time T by a user behavior record set D before \ntime T [20]. Different actions occur at different times. For example, user1 visit location \na and b when user1 purchasing b and c at T − 3. We need to predict T-time consumer \nbehavior based on different timing behavior characteristics.\n\nAccording to relevant professional research, we divide the data sets of user behav-\nior into three groups in a pre-processing manner. By the feature statistics method, the \n\n(3)C(U → V ) =\nN(U ∪ V )\n\nN\n.\n\nFig. 1 The time series of user position feedback\n\n\n\n\n\nPage 7 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nfeatures are divided into two types, as shown in Table 1. “True” indicates that the feature \ngroup has corresponding features. Conversely, “False” means no such feature. Next we \nexplain these features.\n\na. Counting feature\n\nFor each feature statistics window, we use the behavioral counting feature and the de-\nduplication counting feature. The behavior count is a cumulative measure of the num-\nber of behaviors that occurred in and before the current window. For the location visit \nbehavior, it represents the number of visits to the product location by the user, the total \nnumber of visits by the user and the total number of visits to the merchandise. The de-\nduplication count feature is similar to the behavioral count, but only the number of non-\nrepetitive behavioral data is counted.\n\nb. Mean feature\n\nIn order to describe the activity of the user and the popularity of the product better, \nthis article derives a series of mean-type features based on the counting features. Take \nthe location visit behavior as an example, the user characteristics group includes the \nuser’s average number of visiting to the product. The average number of visiting to \nthe product by user i is calculated as shown in Formula (4).\n\nc. Ratio feature\n\nThe ratio of user-product behavior to the total behavior of the user and the product \nis also an aspect affecting the user’s degree of preference for the product. In the time \nwindow t, the method to calculate the ratio of the user’s visit to the products’ total \nvisit is shown in Formula (5).\n\nOur work presents a mobile marketing recommendation model is trained by adding \nthe time axis to the user position features. Contrary to current research, it is highly \nusable and low difficulty of achievement for real-world work applications. Consider-\ning the speed of calculation, we study the method of directly embedding time series \ninformation into the collaborative filtering calculation process to improve the recom-\nmendation quality. Specific information will be covered in the following sections.\n\n(4)avgui(t, i, visit) =\naction_count(t, U, Ui, visit)\n\nuser_unique_item(t, U, Ui, visit)\n.\n\n(5)rate_ui_in_u(t, i, j, visit) =\naction_count(t, UI, Ui, Ij, visit)\n\naction_count(t, U, Ui, visit)\n.\n\nTable 1 Characteristic system diagram (True/False)\n\nFeature group Counting feature Mean feature Ratio feature\n\nUser-product True False True\n\nUser feature True True False\n\nProduct feature True True False\n\n\n\nPage 8 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nLocation‑based mobile marketing recommendation model by CNN\nCreating the model is one of the most important aspects, which is an evaluation crite-\nrion to make sure correctness of the next step. This section mainly describes the rel-\nevant definitions of LBCNN that are shown in “Relevant definitions of the LBCNN” \nsection, and specific implementation of the model is shown in “Specific implementa-\ntion of the model” section.\n\nRelevant definitions of the LBCNN\n\nIn order to get better feature expression, we consider the user’s timing sensitivity of the \nproduct preferences and the user’s overall preferences comprehensively. This paper uses a \nconvolutional neural network as the basis to build location-based mobile marketing recom-\nmendation model. In the next step, we give the relevant definition.\n\na. Definition 1 (Model framework): based on the above analysis and user’s timing behav-\nior preference feature. We use the convolutional neural network model shown in Fig. 2. The \nmodel is divided into four layers that are input layer, multi-window convolution layer, pool-\ning layer and output layer. The input layer is a well-constructed input feature which trans-\nforms the input features into a two-dimensional plane by time series. Each time window is \nexpressed as an eigenvector. The multi-window convolutional layer convolves the input fea-\nture plane through different lengths of time windows to obtain different feature maps. The \npooling layer reduces the dimension of the feature map to obtain a pooled feature vector. \nThe output layer and the pooling layer are fully connected network structures.\n\nb. Definition 2 (Convolution layer): assume that there are N time windows of the feature \nand each time window has K user preference feature for the commodity. Then input sam-\nple × can be expressed as a matrix of T × K. The feature map in the convolutional layer is \ncalculated by the input layer and the convolution kernel. The window length of the convolu-\ntion kernel is h. xi,i+j represents the eigenvector added by time window i and time window \ni + j. The convolution kernel w can be expressed as a vector of h × K. Feature map f = [f1, f2, \n…, fT−h+1]. The i-th feature fi is calculated according to Formula (6):\n\n(6)fi = σ(w · xi,i+h−1 + b)\n\nFig. 2 The framework of the LBCNN\n\n\n\n\n\nPage 9 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nwhere b is an offset term and a real number. σ(x) is a nonlinear activation function. This \npaper uses ReLu and Tanh as an activation function. Relu is shown in Formula (7):\n\nc. Definition 3 (Max-pooling): the pooling layer is to scale the feature map while reduc-\ning the complexity of the network. The maximum features of the convolution kernel can \nbe obtained according to the maximum pooling operation. The feature map obtained \nat the kth product of the convolutional kernel is fk = [fk,1, fk,2, …, fk,T−h +1]. The pooling \noperation can be expressed as Formula (8):\n\nd. Definition 4 (Probability distribution): there are M convolution kernels and the output \nlayer has C categories [19]. The weight parameter θ of the output layer is a C × M matrix. \nThe pooled feature f̂  of x is an M-dimensional vector. The probability that x belongs to \nthe i-th category can be expressed as Formula (9):\n\nwhere  bk represents the k-th offset of the fully connected layer. The loss function of the \nmodel can be obtained by the likelihood probability value, as shown in Formula (10):\n\nwhere T is the training data set,  yi is the real category of the i-th sample, xi is the charac-\nteristic of the i-th sample and θ is the model’s parameters. We learn model parameters \nby minimizing the loss function. The training method adopts the improved gradient \ndescent method proposed by Zeiler. In addition, we have adopted Dropout process-\ning on the convolutional layer to prevent over-fitting of the trained model [21]. The \nDropout method randomizes the neurons in the convolutional layer to 0 with a certain \nprobability.\n\ne. Definition 5 (Latent factor): the value of the latent factor vector is true [22]. Whether \nan item belongs to a class is determined entirely by the user’s behavior. We assume that \ntwo items are liked by many users at the same time, then these two items have a high \nprobability of belonging to the same class. The weight of an item in a class can also be \ncalculated by itself. The implicit semantic model calculates the user’s (u) interest in the \nitem (i) are shown in Formula (11):\n\n(7)\nReLu = max(0, x).\n\nTanh(x) =\nex − e−x\n\nex + e−x\n.\n\n(8)Pool_feature(j) = down(fi).\n\n(9)p(i|x, θ) =\ne(θi·\n\n⌢\nf +bi)\n\n∑C\nk−1 e\n\n(θk·\n⌢\nf +bk)\n\n(10)J(θ) = −\nk\n\n∑\n\ni=1\n\nlog(p(yi|x, θ))\n\n(11)R(u, i) = rui = p\nT\nu qi =\n\nF\n∑\n\nf =1\n\npu,kqi,k\n\n\n\nPage 10 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nwhere p is the relationship between the user interest and the kth implicit class. q is the \nrelationship between the kth implicit class and the item i. F is the number of hidden \nclasses, and r is the user’s interest in the item.\n\nSpecific implementation of the model\n\nWe can draw from Fig. 3 that the proposed model is divided into two processes. The first \nprocess is the training process and includes two parts. The top module shows how to gener-\nate CNN inputs and outputs from historical data. The other module in the training process \nshows that the traditional CNN parameters are trained by provided data. The second pro-\ncess finished a new location-based marketing resources recommendation. The recommen-\ndation process can work through the CNN parameters provided by the training process.\n\nTo achieve the features of users and location-based mobile marketing resources, the \nlatent factor model (LFM) is used. In traditional LFM, L2-norm regularization is often used \nto optimize training results. However, using L2-norm regularization often leads to excessive \nsmoothing problems. In our model, LFM results are used to represent the characteristics of \nthe training data. In this kind of thinking, we can learn from the training method of regres-\nsion coefficient in regression analysis, and construct a loss function. Therefore, it is more \nreasonable to use sparseness before the specification results. Based on these analyses, we \npropose an improved matrix decomposition method and try to normalize the solution by \n\nFig. 3 Location-based mobile marketing recommendation model by convolutional neural network\n\n\n\n\n\nPage 11 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nusing the premise of verifying the sparseness of the matrix. The model is presented as For-\nmula (12):\n\nThe next question is how to calculate these two parameters p and q. For the calculation \nof this linear model, this paper uses the gradient descent method. In the Formula (12), \n puk is a user bias item that represents the average of a user’s rating.  qik is an item offset \nitem that represents the average of an item being scored. The offset term is an intrinsic \nproperty that indicates whether the item is popular with the public or a user is harsh \non the item. For positive samples, we specify  ru,i = 1 based on experience and negative \nsample  ru,i = 0, which is shown in Formula (11). The latter λ is a regularization term to \nprevent overfitting.\n\na. Description of the training section\n\nIn Fig. 3, If you want to train CNN, the first thing you need to solve is its input and out-\nput problems. For input, a language model is usually used.\n\nIn terms of output, we propose an improvement in model training by LFM, which is \nconstrained by the regularization of the L1-norm [23]. LFM training data is a historical \nscore between the user and the location-based marketing resources. The rating score can \nbe explicit because it is based on a user tag or an implied tag and it is predicted from the \nuser’s behavior. In this model, in order to ensure that the trained model is representative, \nthe training data we input is to select the existing authoritative standard training set.\n\nb. Description of the recommended part\n\nOnce the LBCNN model structure is established and the model parameters are trained \nusing the training data set, the recommended real-time performance can be achieved. \nThe real-time performance is based on the update of network model parameters in the \nbackground, and it uses some past behavior data and information of the recommended \npeople and products.\n\nUser information and product information can be obtained in advance and digitized. \nIn the offline training model phase, digitized user information, product information, and \nbehavior information are utilized [24]. The same model is trained for the same type of \nusers, and the parameters of the model are periodically updated within a certain period \nof time. In the real-time recommendation stage, real-time recommendation can be real-\nized only by integrating the collected behavior data with the previous data and inputting \nit into the model.\n\nExperimental analysis\nIn order to verify the advantages of convolutional neural network in capturing user’s \ntiming preferences for product and mining users’ temporal behavior characteristics, \nwe compare several commonly used classification models under the same conditions of \ntraining features. They are Linear Logistic Regression Classification Model (LR), Support \n\n(12)J(U, V ) =\n∑\n\nu,i∈K\n\n(\n\nru,i −\n\nk\n∑\n\nk=1\n\npu,kqi,k\n\n)2\n\n+ ��puk�\n2\n+ ��qik�\n\n2.\n\n\n\nPage 12 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nVector Machine (SVM), Random Forest Model (RF) and Gradient Boosting Regression \nTree Model (GBDT) [25]. We also compare the products that have been visited for the \nlast 8  h. Experimental tool is sklearn kit. The hyper parameter settings for each model \nduring the experiment are:\n\na. LR: select L2 regular and the regularization coefficient is 0.1.\nb. SVM: choose radial basis kernel function (RBF) and gamma of kernel function is \n\n0.005.\nc. RF: the number of trees is 200, the entropy is selected as the feature segmentation \n\nstandard and the random feature ratio is 0.5.\nd. GBDT: the number of trees is 100, the learning rate is 0.1 and the maximum depth of \n\nthe tree is 3.\n\nDescription of the data set\n\nThe experiment in our paper uses the dataset disclosed according to the Alibaba Group’s \nmobile recommendation algorithm contest held in 2015. This data set contains 1 month \nof user behavior data and product information. The user’s behavior data includes 10 mil-\nlion users’ various behaviors on 2,876,947 items. Behavior types include clicks, shopping \ncarts and purchases. In addition, each behavior record identifies behavior time that is \naccurate to the hour. The product information includes product category information, \nand identifies whether the product is an online to offline type. In a real business sce-\nnario, we often need to build a personalized recommendation model for a subset of all \nproducts. In the process of completing this task, we not only need to take advantage of \nthe user’s behavior data on this subset of goods, but also need to use more abundant user \nbehavior data. We need to define the following symbols: U (User collection), I (Product \ncollection), P (Product subset, P ⊆ I), D (User behavior data collection for the complete \nset of products). Our goal is to use D to construct a recommendation model for users in \nU to products in P.\n\nThe data mainly consists of two parts. The first part is the mobile behavior data (D) of \n10 million users on the product collection, including the following fields, as shown in \nTable 2.\n\nFor example, “141278390, 282725298, 1, 95jnuqm, 5027, 2014-11-18 08” is one of \nthe data. The Behavior_type and the Time in these fields contain the largest amount \n\nTable 2 The mobile behavior data of the Ali mobile recommendation data set\n\nField Field description Extraction instruction\n\nUser_id User differentiation Sampling and data masking\n\nItem_id Product differentiation Data masking\n\nBehavior_type The type of behavior of the user on the \nproduct\n\nIncluding browsing, collecting, adding shop-\nping carts, and purchasing, the values are 1, 2, \n3, 4 respectively\n\nUser_geoinfo The spatial reference identifier of the user’s \nlocation\n\nFormed by latitude and longitude data through \na secret algorithm\n\nItem category Product classification identifier Field masking\n\nTime Action time Accurate to hour level\n\n\n\nPage 13 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nof information. The User_geohash field is basically unusable due to too many missing \nvalues.\n\nThe second part is the product subset (P), which contains the following fields, as \nshown in Table 3.\n\nSimilar to the above, “117151719, 96ulbnj, 7350” is one of the product information. \nThe training data contains the mobile behavior data (D) of a sample of a certain user \nwithin 1 month (11.18–12.18). The scoring data is the purchase data of the product sub-\nset (P) by these users 1  day (12.19) after this 1  month. We should be training the data \nmodel to output the predicted results of the user’s purchase behavior on the next day.\n\nData preprocessing\n\nWe found that there are some users have a lot of page views (maximum of 2 million), which \nis beyond reasonable levels. We analyze that these users may be crawler users, so the behav-\nior of these users on the goods is not the basis for predicting the user’s purchase. At the \nsame time, we predict the user product pairs that have appeared in all historical records. \nThe existence of these users will undoubtedly increase our forecasting amount and interfere \nwith our normal model training. Therefore, we choose to filter out these users, the filtering \nrules are shown as Fig. 4.\n\nTable 3 The product subset of the Ali mobile recommendation data set\n\nField Field description Extraction instruction\n\nItem_id Product differentiation Sampling and data masking\n\nItem_geohash Spatial information of the product location, \nwhich can be empty\n\nFormed by latitude and longi-\ntude data through a secret \nalgorithm\n\nItem_category Product classification identifier Data masking\n\nFig. 4 Data filtering rules\n\n\n\n\n\nPage 14 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nEvaluation index\n\nThe purpose of the proposed method is to predict the user’s purchased business in the next \nposition based on the user’s historical behavior record. Therefore, we evaluate the model \nwith the data of the last day. The sample construction of time series method is shown in \nFig.  1. F1-score can be viewed as a harmonic mean of accuracy and recall. At present, \nF1-score has been widely used in the evaluation of the recommendation system.\n\nwhere Formula (13) is the calculation method of the accuracy rate, Formula (14) is the \ncalculation method of the recall rate, and Formula (15) is the calculation method of \nF1-score. Prediction_set is the predicted purchase of the user-item. Answer_set is a real-\npurchased user-item collection.\n\nThe distribution of positive and negative samples used in this experiment is extremely \nunbalanced, and negative samples contain more noise. In order to make the model more \nsuitable for learning under unbalanced data, we perform under sampling on negative \nsamples. The model training process adopts AdaDelta Update Rule to adjust the param-\neters by using the stochastic gradient descent method. Hyper Parameters of the model \nare described in Table  4. The value in the table is the final hyper parameter when the \nerror of the validation set is minimal. Convolution time window in convolution kernel \nis 2 and 3. The number of convolution kernels for two different length windows is 200. \nIn this experiment, the training process needs to iterate ten times. To achieve the con-\nvergence of the model, we observe the accuracy of the training set every iteration in the \nmodel training process.\n\nIn Fig. 5, the abscissa indicates the number of iterations, and the ordinate indicates the \naccuracy of the sample. As we can see from the figure, the accuracy of the training set \nhas been increasing and the verification set accuracy has declined after the fifth iteration \nof the model [26]. This situation shows that the model training has been overfitting after \n\n(13)precision =\n∣\n\n∣prediction_set ∩ answer_set\n∣\n\n∣\n\n∣\n\n∣prediction_set\n∣\n\n∣\n\n(14)Recall =\n∣\n\n∣prediction_set ∩ answer_set\n∣\n\n∣\n\n|answer_set|\n\n(15)F1 − score =\n2 × precision × recall\n\nprecision + recall\n\nTable 4 Parameter settings of convolutional neural networks\n\nParameter name Parameter value\n\nActivation function of convolution kernel Tanh\n\nSize of convolution kernel window [2, 3]\n\nNumber of convolution kernel 400\n\nDropout ratio 0.5\n\nBatch size 64\n\nEpoch 5\n\n\n\nPage 15 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nthe 5th iteration. In addition, we found that the test set accuracy is higher than the train-\ning set and verification set.\n\nExperimental results and comparison\n\nThe experimental results obtained using the above parameters are shown in Table 5. As \ncan be seen from Table  5, the machine learning model using the features designed in \nthis paper is superior to the traditional method. Our model achieves an 80% accuracy \nin predicting the accuracy of user behavior, which is significantly better than traditional \nmodels at least 10%. In terms of recall rate, LBCNN reached 8.14%, which is at least 2% \nhigher than the traditional method. Similarly, our model is up to 8.07% in F1-score.\n\nThis result shows that the user’s time-series behavior preference model is reasonable. \nThis solution works well for improving the accuracy and quality of recommendations. In \na single model, the LBCNN model works best. Since the linear model assumes that each \nfeature is independent, it is impossible to excavate the intrinsic relationship between \nfeatures. The proposed method can mine the intrinsic link between user timing prefer-\nence features better. The experimental results show that the user preferences we build \nare more accurate and convolutional neural networks have strong capabilities of feature \nextraction and model generalization.\n\nFig. 5 Training process of the LBCNN\n\nTable 5 Comparing the experimental results of each model (%)\n\nModel Accuracy Recall rate F1-score\n\nTraditional method 31.4 5.60 4.02\n\nLR 75.0 7.63 7.57\n\nSVM 70.0 7.12 7.06\n\nRF 57.5 5.85 5.80\n\nGBDT 62.5 6.36 6.13\n\nLBCNN 80.0 8.14 8.07\n\n\n\n\n\nPage 16 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nConclusion\nThe current mobile marketing recommendation system only treats location informa-\ntion as a recommended attribute, which weakens the role of the location information in \nthe recommendation. For the implicit feedback behavior of users, this paper proposes \na location-based mobile marketing method by convolutional neural network. First, we \ndivide the user location-based behaviors into several time windows according to the \ntimestamp of these behaviors, and model the user preference in different dimensions \nfor each window. Then we utilize the convolutional neural network to train a classifier. \nFinally, the experimental process of this paper is introduced, and a good prediction effect \nis obtained on effective data sets. The final experimental results express that the pro-\nposed method has different feature extraction perspectives from other models. Because \nof using convolutional neural networks, the proposed method has stronger capability \nof feature extraction and generalization. This method helps to change the accuracy and \nquality of the recommendation system and user satisfaction.\n\nThe work introduced here is to show the prospects for further research. The method \nproposed in this paper has a certain dependence on the user’s geographical location \ninformation during the training process of the user preference model. In addition, the \nrecommendation system will encounter a cold-start problem with sparse user infor-\nmation. For dealing with these discovered issues, we plan to use the hot start case to \nimprove the recommended cold start problem. Meanwhile, we are investigating new \nmethod which uses a better big data framework (such as Hadoop MapReduce) to ensure \nthe efficiency of training large data sets. In the future, we will show recommended meth-\nods to improve performance in other applications.\nAuthors’ contributions\nCY conceptualized the study and analyzed all the data. SD performed all experiments and wrote the manuscript. JW \nadvised on the manuscript preparation and technical knowledge. All authors read and approved the final manuscript.\n\nAuthor details\n1 School of Computer and Software, Jiangsu Engineering Center of Network Monitoring, Nanjing University of Informa-\ntion Science & Technology, Nanjing 210044, China. 2 School of Computer & Communication Engineering, Changsha \nUniversity of Science & Technology, Changsha 410004, China. \n\nAcknowledgements\nIt was supported by the Priority Academic Program Development of Jiangsu Higher Education Institutions (PAPD), Post-\ngraduate Research & Practice Innovation Program of Jiangsu Province (KYCX18_1032).\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAvailability of data and materials\nWe declared that materials described in the manuscript will be freely available to any scientist wishing to use them for \nnon-commercial purposes.\n\nFunding\nThis work was supported by the National Natural Science Foundation of China (61772282, 61772454, 61811530332, \n61811540410).\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 25 September 2018   Accepted: 4 April 2019\n\nReferences\n 1. Fernández-Tobías I, Braunhofer M, Elahi M, Ricci F, Cantador I (2016) Alleviating the new user problem in collabora-\n\ntive filtering by exploiting personality information. User Model User Adap Inter 26(2–3):221–255\n\n\n\n\n\nPage 17 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\n 2. Koohi H, Kiani K (2016) User based collaborative filtering using fuzzy C-means. Measurement 91:134–139\n 3. Xu X, Fu S, Qi L, Zhang X, Liu Q, He Q, Li S (2018) An IoT-oriented data placement method with privacy preservation \n\nin cloud environment. J Netw Comput Appl 124:148–157\n 4. Yingyuan X, Pengqiang A, Ching-Hsien H, Hongya W, Xu J (2015) Time-ordered collaborative filtering for news \n\nrecommendation. China Commun 12(12):53–62\n 5. Kaminskas M, Ricci F (2011) Location-adapted music recommendation using tags. In: International conference on \n\nuser modeling, adaptation, and personalization. pp 183–194\n 6. Zhu H, Chen E, Xiong H, Yu K, Cao H, Tian J (2015) Mining mobile user preferences for personalized context-aware \n\nrecommendation. ACM Trans Intell Syst Technol TIST 5(4):58\n 7. Yin H, Cui B, Chen L, Hu Z, Zhang C (2015) Modeling location-based user rating profiles for personalized recommen-\n\ndation. ACM Trans Knowl Discov Data TKDD 9(3):19\n 8. Li X, Xu G, Chen E, Li L (2015) Learning user preferences across multiple aspects for merchant recommendation. In: \n\n2015 IEEE international conference on data mining (ICDM). pp 865–870\n 9. Yin C, Xi J, Sun R, Wang J (2018) Location privacy protection based on differential privacy strategy for big data in \n\nindustrial internet-of-things. IEEE Trans Industr Inf 14(8):3628–3636\n 10. Lian D, Ge Y, Zhang F, Yuan NJ, Xie X, Zhou T, Rui Y (2015) Content-aware collaborative filtering for location recom-\n\nmendation based on human mobility data. In: 2015 IEEE international conference on data mining. pp 261–270\n 11. Lee WP, Tseng GY (2016) Incorporating contextual information and collaborative filtering methods for multimedia \n\nrecommendation in a mobile environment. Multimedia Tools Appl 75(24):16719–16739\n 12. Bengio Y (2009) Learning deep architectures for AI. Found Trends Mach Learn 2(1):1–127\n 13. LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521(7553):436–444\n 14. Yuan C, Li X, Wu QJ, Li J, Sun X (2017) Fingerprint liveness detection from different fingerprint materials using convo-\n\nlutional neural network and principal component analysis. Comput Mater Continua 53(4):357–372\n 15. Yin C, Wang J, Park JH (2017) An improved recommendation algorithm for big data cloud service based on the trust \n\nin sociology. Neurocomputing 256:49–55\n 16. Längkvist M, Karlsson L, Loutfi A (2014) A review of unsupervised feature learning and deep learning for time-series \n\nmodeling. Pattern Recogn Lett 42:11–24\n 17. Tu Y, Lin Y, Wang J, Kim JU (2018) Semi-supervised learning with generative adversarial networks on digital signal \n\nmodulation classification. Comput Mater Continua 55(2):243–254\n 18. Nilashi M, Bin Ibrahim O, Ithnin N (2014) Hybrid recommendation approaches for multi-criteria collaborative filter-\n\ning. Expert Syst Appl 41(8):3879–3900\n 19. Zeng D, Dai Y, Li F, Sherratt RS, Wang J (2018) Adversarial learning for distant supervised relation extraction. Comput \n\nMater Continua 55(1):121–136\n 20. Yin C, Xia L, Zhang S, Sun R, Wang J (2018) Improved clustering algorithm based on high-speed network data \n\nstream. Soft Comput 22(13):4185–4195\n 21. Krizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classification with deep convolutional neural networks. In: \n\nAdvances in neural information processing systems, pp 1097–1105\n 22. Li X, Yao C, Fan F, Yu X (2017) A text similarity measurement method based on singular value decomposition and \n\nsemantic relevance. J Inf Process Syst 13(4):863–875\n 23. Li CN, Shao YH, Deng NY (2015) Robust L1-norm two-dimensional linear discriminant analysis. Neural Networks \n\n65:92–104\n 24. Fattah MA (2017) A novel statistical feature selection approach for text categorization. J Inf Process Syst \n\n13(5):1397–1409\n 25. Wang Y, Feng D, Li D, Chen X, Zhao Y, Niu X (2016) A mobile recommendation system based on logistic regression \n\nand Gradient Boosting Decision Trees. In: International joint conference on neural networks. pp 1896–1902\n 26. Yin C, Zhang S, Xi J, Wang J (2017) An improved anonymity model for big data security based on clustering algo-\n\nrithm. Concurr Comput Pract Exp 29(7):e3902\n\n\n\tMobile marketing recommendation method based on user location feedback\n\tAbstract \n\tIntroduction\n\tRelated work\n\tTraditional recommendation method\n\ta. Content-based recommendation method\n\tb. Collaborative filtering method\n\tc. Hybrid recommendation method\n\td. Recommendation based on association rules\n\n\tConstruction of time series behavior’s preference features\n\ta. Counting feature\n\tb. Mean feature\n\tc. Ratio feature\n\n\n\tLocation-based mobile marketing recommendation model by CNN\n\tRelevant definitions of the LBCNN\n\tSpecific implementation of the model\n\ta. Description of the training section\n\tb. Description of the recommended part\n\n\n\tExperimental analysis\n\tDescription of the data set\n\tData preprocessing\n\tEvaluation index\n\tExperimental results and comparison\n\n\tConclusion\n\tAuthors’ contributions\n\tReferences\n\n\n\n\n",
      "metadata_storage_path": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9saWJyYXJ5L3MxMzY3My0wMTktMDE3Ny02LnBkZg2",
      "authors": [
        "Chunyong Yin1",
        "Shilei Ding1",
        "Jin Wang2",
        "R E S",
        "A R C H",
        "jinwang",
        "Zhu",
        "Yin",
        "Lian",
        "Lee",
        "Gemmis",
        "Semeraro",
        "lem",
        "Markov",
        "Relu",
        "Zeiler",
        "nario",
        "Fernández-Tobías I",
        "Braunhofer M",
        "Elahi M",
        "Ricci F",
        "Cantador I",
        "Koohi H",
        "Kiani K",
        "Xu X",
        "Fu S",
        "Qi L",
        "Zhang X",
        "Liu Q",
        "He Q",
        "Li S",
        "Yingyuan X",
        "Pengqiang A",
        "Ching-Hsien H",
        "Hongya W",
        "Xu J",
        "Kaminskas M",
        "Zhu H",
        "Chen E",
        "Xiong H",
        "Yu K",
        "Cao H",
        "Tian J",
        "Yin H",
        "Cui B",
        "Chen L",
        "Hu Z",
        "Zhang C",
        "Li X",
        "Xu G",
        "Li L",
        "Yin C",
        "Xi J",
        "Sun R",
        "Wang J",
        "Lian D",
        "Ge Y",
        "Zhang F",
        "Yuan NJ",
        "Xie X",
        "Zhou T",
        "Rui Y",
        "Lee WP",
        "Tseng GY",
        "Bengio Y",
        "LeCun Y",
        "Hinton G",
        "Yuan C",
        "Wu QJ",
        "Li J",
        "Sun X",
        "Park JH",
        "Längkvist M",
        "Karlsson L",
        "Loutfi A",
        "Tu Y",
        "Lin Y",
        "Kim JU",
        "Nilashi M",
        "Bin Ibrahim O",
        "Ithnin N",
        "Zeng D",
        "Dai Y",
        "Li F",
        "Sherratt RS",
        "Xia L",
        "Zhang S",
        "Krizhevsky A",
        "Sutskever I",
        "Hinton GE",
        "Yao C",
        "Fan F",
        "Yu X",
        "Li CN",
        "Shao YH",
        "Deng NY",
        "Fattah",
        "Wang Y",
        "Feng D",
        "Li D",
        "Chen X",
        "Zhao Y",
        "Niu X"
      ],
      "institutions": [
        "Alibaba",
        "Ama",
        "LBCNN",
        "Commons",
        "Changsha University",
        "Technology",
        "CNN",
        "U11 U12",
        "U21 U22",
        "U2n",
        "Amazon",
        "LFM",
        "Gradient Boosting Regression",
        "Alibaba Group",
        "Ali",
        "Hadoop MapReduce",
        "JW",
        "1 School of Computer and Software",
        "Jiangsu Engineering Center of Network Monitoring",
        "Nanjing University of Informa",
        "tion Science & Technology",
        "School of",
        "& Communication",
        "University of Science & Technology",
        "Priority Academic Program Development of Jiangsu Higher Education Institutions",
        "PAPD",
        "National Natural Science Foundation of",
        "Springer Nature",
        "ACM",
        "IEEE"
      ],
      "key_phrases": [
        "R E S E A R C H Yin",
        "Creative Commons Attribution 4.0 International License",
        "current mobile marketing recommendation system",
        "Sequential behavior Open Access",
        "Mobile marketing recommendation method",
        "users’ timing preference characteristics",
        "Location-based mobile marketing recommendation",
        "potential mobile user preferences",
        "Creative Commons license",
        "convolutional neural network model",
        "original author(s",
        "users’ location feedback behavior",
        "user location feedback data",
        "mobile recommendation system",
        "traditional recommendation models",
        "user preference information",
        "creat iveco mmons",
        "users’ location-based behaviors",
        "mobile information overload",
        "user-product binary matrix",
        "different geographical locations",
        "shopping location information",
        "famous e-commerce platforms",
        "physical store products",
        "different time windows",
        "users’ location information",
        "Hum. Cent. Comput",
        "mobile network",
        "information recommendation",
        "different preferences",
        "mobile users",
        "convolutional model",
        "author information",
        "irrelevant information",
        "binary relationships",
        "different dimensions",
        "behavioral data",
        "Chunyong Yin1",
        "Shilei Ding1",
        "Jin Wang2",
        "recent years",
        "e-commerce industry",
        "one hand",
        "various products",
        "other hand",
        "large amount",
        "next consumer",
        "recommended methods",
        "third factor",
        "limited check",
        "timeli- ness",
        "hot spots",
        "accuracy rate",
        "recall rate",
        "unrestricted use",
        "appropriate credit",
        "Inf. Sci.",
        "Communication Engineering",
        "Full list",
        "individual needs",
        "doi.org",
        "orcid.org",
        "experimental results",
        "Changsha University",
        "Introduction",
        "popularization",
        "Internet",
        "Alibaba",
        "zon",
        "home",
        "sellers",
        "goods",
        "costs",
        "wealth",
        "consumption",
        "case",
        "advantage",
        "terms",
        "Abstract",
        "attribute",
        "role",
        "paper",
        "LBCNN",
        "extractor",
        "classifier",
        "Keywords",
        "article",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "Correspondence",
        "jinwang",
        "csust",
        "2 School",
        "Computer",
        "Science",
        "Technology",
        "China",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "dialog",
        "Page",
        "17Yin",
        "means",
        "problem",
        "implied feature-based cognitive feature collaborative filter- ing",
        "location-aware based generation probability model",
        "collaborative filtering methods",
        "biological neural networks",
        "Convolutional neural networks",
        "advanced abstract features",
        "loca- tion information",
        "accurate personalized recommendations",
        "small-scale training data",
        "detailed comparative analysis",
        "user informa- tion",
        "mobile marketing recommendation",
        "original input features",
        "business location information",
        "user preference model",
        "mobile recommendation research",
        "feature size",
        "deep learning algorithms",
        "mobile users’ preferences",
        "ing models",
        "deep model",
        "DL) model",
        "shallow model",
        "multimedia recommendations",
        "mobile environments",
        "accurate prediction",
        "local features",
        "overall features",
        "two-dimensional features",
        "user preferences",
        "location recommendation",
        "convolutional layer",
        "spatial information",
        "product information",
        "user information",
        "hot topic",
        "service resources",
        "different assumptions",
        "spatial activities",
        "location movement",
        "negative samples",
        "semantic content",
        "data size",
        "mation analysis",
        "long-term interest",
        "time stamps",
        "great achievements",
        "speech tasks",
        "novel field",
        "interventional optimization",
        "artificial intelligence",
        "great breakthroughs",
        "similar effects",
        "Weight sharing",
        "great advantages",
        "user needs",
        "positional relevance",
        "location relevance",
        "first layer",
        "previous layer",
        "convolution kernel",
        "pooling layer",
        "context information",
        "ventional methods",
        "user models",
        "many researches",
        "two layers",
        "network structure",
        "shortcomings",
        "Zhu",
        "retrograde",
        "approach",
        "Yin",
        "LA-LDA",
        "scoring",
        "items",
        "studies",
        "distance",
        "merchant",
        "area",
        "proximity",
        "importance",
        "gap",
        "core",
        "Lian",
        "impact",
        "author",
        "project",
        "Lee",
        "visual",
        "study",
        "aspects",
        "machine",
        "Experiments",
        "characteristics",
        "CNN",
        "difficulty",
        "number",
        "weights",
        "neuron",
        "Hum",
        "Cent",
        "Comput",
        "images",
        "location-based",
        "Location-based mobile marketing recommendation model",
        "Content‑based recommendation method",
        "convolutional neural network models",
        "Representative content-based recommendation systems",
        "general products recommendation system",
        "Traditional recommendation method",
        "traditional recommendation algorithm",
        "content-based recommendation algorithm",
        "similar Top-N products",
        "timing preference characteristics",
        "interest feature vector",
        "Content-based information filtering",
        "Experimental analysis” section",
        "item feature vector",
        "project feature vector",
        "other existing methods",
        "cosine similarity algorithm",
        "Related work” section",
        "user feature vector",
        "users’ timing preferences",
        "User description file",
        "feature models",
        "Content-based methods",
        "content filtering",
        "content characteristics",
        "other methods",
        "user-product information",
        "collaborative filtering",
        "feedback information",
        "hybrid methods",
        "historical content",
        "training sample",
        "next moment",
        "time window",
        "overall preferences",
        "Top-K sample",
        "four sections",
        "Necessary definitions",
        "specific implementation",
        "future progress",
        "current chapter",
        "three parts",
        "time series",
        "text documents",
        "accurate comparisons",
        "different texts",
        "lowing aspects",
        "Hum. Cent",
        "project profile",
        "user u",
        "similarity calculation",
        "sample features",
        "cast results",
        "effective application",
        "calculation formula",
        "two-class problem",
        "purchase behavior",
        "category",
        "order",
        "merchandise",
        "length",
        "test",
        "Remain",
        "sion",
        "strengths",
        "weaknesses",
        "plans",
        "establishment",
        "differences",
        "research",
        "threshold",
        "a.",
        "topics",
        "projects",
        "purchasing",
        "operation",
        "addition",
        "modulus",
        "Lops",
        "Gemmis",
        "Semeraro",
        "content filtering based recommendation method",
        "m * n matrix R",
        "traditional data mining method",
        "user historical behavior data",
        "traditional collaborative filtering method",
        "likely N objects",
        "user behavior data",
        "various draw- backs",
        "nearest neigh- bor",
        "conven- tional method",
        "association rule algorithm",
        "higher evaluation algorithms",
        "hybrid recommendation algorithm",
        "similar user sets",
        "mutual user relationships",
        "content-based recommendation method",
        "Hybrid recommendation method",
        "other similar users",
        "different recommendation algorithms",
        "The matrix  Umn",
        "content-based filtering",
        "other method",
        "hybrid method",
        "matrix decomposition",
        "different relationships",
        "One method",
        "new content",
        "evaluation standard",
        "association rules",
        "recommendation problem",
        "standard recommendation",
        "two algorithms",
        "different ways",
        "target user",
        "user neighbor",
        "cold-start issues",
        "same type",
        "same partition",
        "expert research",
        "three steps",
        "recommendation results",
        "ommended items",
        "normal methods",
        "supervised learning",
        "online stores",
        "informa- tion",
        "new users",
        "two types",
        "two specific",
        "compensa- tion",
        "different values",
        "common features",
        "rating sparsity",
        "erence information",
        "new projects",
        "mendations",
        "availability",
        "context",
        "party",
        "requirements",
        "merits",
        "quality",
        "memory",
        "following",
        "collection",
        "feedback",
        "range",
        "U11",
        "U1n",
        "U2n",
        "Um1",
        "classification",
        "survey",
        "Amazon",
        "personalized",
        "customer",
        "ratings",
        "techniques",
        "performance",
        "ally",
        "degree",
        "improvement",
        "complexity",
        "different timing behavior characteristics",
        "good recom- mendations",
        "relevant professional research",
        "T-time consumer behavior",
        "historical behavior data",
        "timing behavior data",
        "next purchase behavior",
        "user position feedback",
        "similar behavioral items",
        "collaborative filtering algorithm",
        "user behavior record",
        "User2 Visit location",
        "Forecast User3 Shopping",
        "time series behavior",
        "feature statistics method",
        "timing recommendation model",
        "U, V data",
        "user interface layer",
        "high quality rules",
        "last behavior",
        "Different actions",
        "different times",
        "U data",
        "next section",
        "product purchase",
        "Forecast Shopping",
        "recommendation process",
        "recommendation systems",
        "ready-made data",
        "data sets",
        "many years",
        "core idea",
        "rule algorithms",
        "total project",
        "disjoint sets",
        "association rule",
        "two criteria",
        "transac- tions",
        "key- word",
        "description layer",
        "main function",
        "strong dependence",
        "most systems",
        "time information",
        "time label",
        "Markov chain",
        "full use",
        "prediction problem",
        "time T",
        "three groups",
        "processing manner",
        "feature group",
        "Counting feature",
        "keyword layer",
        "user layer",
        "unknown items",
        "traditional algorithm",
        "preference features",
        "corresponding features",
        "total number",
        "c Result",
        "∩V",
        "V.",
        "business",
        "effect",
        "quantity",
        "focus",
        "transactions",
        "statement",
        "strength",
        "support",
        "confidence",
        "ratio",
        "Formula",
        "interest",
        "other",
        "TOP-N",
        "presentation",
        "attributes",
        "dependencies",
        "keywords",
        "resource",
        "users",
        "dataset",
        "explanation",
        "Construction",
        "Fig.",
        "example",
        "user1",
        "Table",
        "False",
        "∪",
        "Location‑based mobile marketing recommendation model",
        "Counting feature Mean feature Ratio feature",
        "Table 1 Characteristic system diagram",
        "collaborative filtering calculation process",
        "True False Product feature",
        "False True User feature",
        "location-based mobile marketing",
        "duplication counting feature",
        "behavioral counting feature",
        "pooled feature vector",
        "different feature maps",
        "multi-window convolutional layer",
        "repetitive behavioral data",
        "duplication count feature",
        "feature statistics window",
        "Specific implementa- tion",
        "multi-window convolution layer",
        "pool- ing layer",
        "ior preference feature",
        "real-world work applications",
        "location visit behavior",
        "user characteristics group",
        "products’ total visit",
        "user position features",
        "Feature group",
        "counting features",
        "feature expression",
        "input feature",
        "product location",
        "network structures",
        "behavior count",
        "Model framework",
        "different lengths",
        "total behavior",
        "input layer",
        "output layer",
        "current window",
        "mean-type features",
        "cumulative measure",
        "time axis",
        "current research",
        "low difficulty",
        "mendation quality",
        "following sections",
        "important aspects",
        "crite- rion",
        "next step",
        "evant definitions",
        "timing sensitivity",
        "timing behav",
        "four layers",
        "user-product behavior",
        "The model",
        "Specific information",
        "product preferences",
        "average number",
        "relevant definition",
        "two-dimensional plane",
        "behaviors",
        "visits",
        "activity",
        "popularity",
        "method",
        "usable",
        "achievement",
        "speed",
        "avgui",
        "Ij",
        "True/False",
        "evaluation",
        "correctness",
        "basis",
        "above",
        "analysis",
        "eigenvector",
        "Input feature convolutional layer Multi-window Max-pooling Output layer layer",
        "K user preference feature",
        "convolu- tion kernel",
        "M convolution kernels",
        "gradient descent method",
        "N time windows",
        "convolution kernel w",
        "nonlinear activation function",
        "kth implicit class",
        "traditional CNN parameters",
        "implicit semantic model",
        "maximum pooling operation",
        "i-th feature fi",
        "training data set",
        "C × M matrix",
        "latent factor vector",
        "likelihood probability value",
        "Convolution layer",
        "convolutional kernel",
        "feature map",
        "pooled feature",
        "kth product",
        "training method",
        "maximum features",
        "CNN inputs",
        "Dropout method",
        "i-th category",
        "loss function",
        "i-th sample",
        "Time series",
        "same time",
        "window length",
        "offset term",
        "C categories",
        "M-dimensional vector",
        "k-th offset",
        "real category",
        "two items",
        "many users",
        "same class",
        "hidden classes",
        "Specific implementation",
        "two processes",
        "two parts",
        "top module",
        "historical data",
        "other module",
        "training process",
        "Probability distribution",
        "real number",
        "weight parameter",
        "model parameters",
        "user interest",
        "K.",
        "∑C",
        "Definition 2",
        "commodity",
        "j.",
        "framework",
        "ReLu",
        "Tanh",
        "network",
        "fk",
        "bk",
        "teristic",
        "Zeiler",
        "fitting",
        "neurons",
        "behavior",
        "high",
        "Pool_feature",
        "J(θ",
        "log",
        "rui",
        "qi",
        "relationship",
        "outputs",
        "σ",
        "existing Users Latent location- historical factor based location data model marketing resources",
        "new resources process Language CNN Results Recommending model User preferences",
        "User feature resources Training process preferences model Input Output CNN",
        "existing authoritative standard training set",
        "new location-based marketing resources recommendation",
        "location-based mobile marketing resources",
        "latent factor model",
        "offline training model phase",
        "new location-based Features",
        "language model",
        "LBCNN model structure",
        "convolutional neural network",
        "historical score",
        "real-time recommendation stage",
        "LFM training data",
        "past behavior data",
        "network model parameters",
        "user bias item",
        "dation process",
        "CNN parameters",
        "matrix decomposition method",
        "Time series Features",
        "training results",
        "model training",
        "item offset item",
        "specification results",
        "LFM results",
        "previous data",
        "linear model",
        "same model",
        "training section",
        "user tag",
        "User information",
        "real-time performance",
        "sion coefficient",
        "regression analysis",
        "next question",
        "two parameters",
        "intrinsic property",
        "positive samples",
        "negative sample",
        "first thing",
        "implied tag",
        "Experimental analysis",
        "behavior information",
        "traditional LFM",
        "L2-norm regularization",
        "regularization term",
        "smoothing problems",
        "rating score",
        "excessive",
        "kind",
        "thinking",
        "sparseness",
        "analyses",
        "solution",
        "premise",
        "mula",
        "calculation",
        "puk",
        "average",
        "qik",
        "public",
        "experience",
        "latter",
        "overfitting",
        "Description",
        "L1-norm",
        "part",
        "update",
        "background",
        "people",
        "products",
        "advance",
        "period",
        "inputting",
        "advantages",
        "λ",
        "10 mil- lion users’ various behaviors",
        "Field Field description Extraction instruction",
        "Linear Logistic Regression Classification Model",
        "Gradient Boosting Regression Tree Model",
        "mining users’ temporal behavior characteristics",
        "Field masking Time Action time",
        "Ali mobile recommendation data set",
        "secret algorithm Item category",
        "mobile recommendation algorithm contest",
        "User_id User differentiation Sampling",
        "radial basis kernel function",
        "Product differentiation Data masking",
        "abundant user behavior data",
        "The User_geohash field",
        "User behavior data collection",
        "personalized recommendation model",
        "Random Forest Model",
        "hyper parameter settings",
        "feature segmentation standard",
        "random feature ratio",
        "real business sce",
        "spatial reference identifier",
        "mobile behavior data",
        "Product classification identifier",
        "shop- ping carts",
        "many missing values",
        "product category information",
        "behavior time",
        "classification models",
        "10 million users",
        "Behavior types",
        "behavior record",
        "complete set",
        "User collection",
        "Product collection",
        "longitude data",
        "training data",
        "scoring data",
        "purchase data",
        "timing preferences",
        "same conditions",
        "training features",
        "Vector Machine",
        "Experimental tool",
        "sklearn kit",
        "L2 regular",
        "regularization coefficient",
        "learning rate",
        "maximum depth",
        "Alibaba Group",
        "following symbols",
        "first part",
        "largest amount",
        "second part",
        "following fields",
        "Product subset",
        "c. RF",
        "offline type",
        "hour level",
        "several",
        "LR",
        "Support",
        "��puk",
        "��qik",
        "SVM",
        "GBDT",
        "8  h",
        "RBF",
        "gamma",
        "trees",
        "entropy",
        "1 month",
        "2,876,947 items",
        "clicks",
        "shopping",
        "purchases",
        "online",
        "nario",
        "process",
        "task",
        "goal",
        "P.",
        "Behavior_type",
        "Item_id",
        "browsing",
        "User_geoinfo",
        "location",
        "latitude",
        "96ulbnj",
        "sample",
        "1  month",
        "∑",
        "convolutional neural networks Parameter name Parameter value",
        "two different length windows",
        "stochastic gradient descent method",
        "Item_id Product differentiation Sampling",
        "User behavior data Pageviews",
        "final hyper parameter",
        "Table 4 Parameter settings",
        "Item_geohash Spatial information",
        "AdaDelta Update Rule",
        "train- ing set",
        "historical behavior record",
        "Convolution time window",
        "time series method",
        "convolution kernel Tanh",
        "convolution kernel window",
        "user product pairs",
        "test set accuracy",
        "Data filtering rules",
        "verification set accuracy",
        "normal model training",
        "model training process",
        "recommendation system",
        "training set",
        "validation set",
        "product subset",
        "historical records",
        "Hyper Parameters",
        "convolution kernels",
        "calculation method",
        "Data preprocessing",
        "data masking",
        "reasonable levels",
        "forecasting amount",
        "secret algorithm",
        "last day",
        "harmonic mean",
        "Activation function",
        "Dropout ratio",
        "fifth iteration",
        "5th iteration",
        "unbalanced data",
        "next day",
        "Evaluation index",
        "sample construction",
        "user-item collection",
        "Batch size",
        "page views",
        "crawler users",
        "results",
        "lot",
        "existence",
        "Item_category",
        "purpose",
        "position",
        "F1-score",
        "present",
        "Prediction_set",
        "Answer_set",
        "positive",
        "experiment",
        "noise",
        "error",
        "vergence",
        "abscissa",
        "iterations",
        "ordinate",
        "figure",
        "situation",
        "2 × precision",
        "Epoch",
        "location-based mobile marketing method",
        "time-series behavior preference model",
        "different feature extraction perspectives",
        "Model Accuracy Recall rate",
        "convolutional neural networks",
        "implicit feedback behavior",
        "several time windows",
        "good prediction effect",
        "hot start case",
        "Jiangsu Engineering Center",
        "effective data sets",
        "cold start problem",
        "big data framework",
        "large data sets",
        "machine learning model",
        "Fig. 5 Training process",
        "accuary Training accuracy",
        "user location-based behaviors",
        "final experimental results",
        "user behavior",
        "Network Monitoring",
        "experimental process",
        "cold-start problem",
        "single model",
        "final manuscript",
        "intrinsic relationship",
        "intrinsic link",
        "user timing",
        "strong capabilities",
        "location informa",
        "stronger capability",
        "user satisfaction",
        "geographical location",
        "sparse user",
        "Hadoop MapReduce",
        "other applications",
        "technical knowledge",
        "Author details",
        "Validation accuracy",
        "model generalization",
        "traditional method",
        "new method",
        "manuscript preparation",
        "LBCNN model",
        "other models",
        "Authors’ contributions",
        "Nanjing University",
        "Informa- tion",
        "80% accuracy",
        "information",
        "comparison",
        "parameters",
        "features",
        "recommendations",
        "proposed",
        "accurate",
        "RF",
        "Testing",
        "Number",
        "Conclusion",
        "timestamp",
        "prospects",
        "dependence",
        "issues",
        "efficiency",
        "future",
        "ods",
        "SD",
        "experiments",
        "JW",
        "1 School",
        "Software",
        "ACM Trans Intell Syst Technol TIST",
        "ACM Trans Knowl Discov Data TKDD",
        "User Model User Adap Inter",
        "digital signal modulation classification",
        "IEEE Trans Industr Inf",
        "Priority Academic Program Development",
        "location-based user rating profiles",
        "Jiangsu Higher Education Institutions",
        "Fernández-Tobías I",
        "IoT-oriented data placement method",
        "National Natural Science Foundation",
        "big data cloud service",
        "J Netw Comput Appl",
        "Practice Innovation Program",
        "new user problem",
        "Post- graduate Research",
        "human mobility data",
        "Multimedia Tools Appl",
        "Fingerprint liveness detection",
        "lutional neural network",
        "principal component analysis",
        "Pattern Recogn Lett",
        "generative adversarial networks",
        "Comput Mater Continua",
        "2015 IEEE international conference",
        "differential privacy strategy",
        "Location-adapted music recommendation",
        "personalized context-aware recommendation",
        "improved recommendation algorithm",
        "Time-ordered collaborative filtering",
        "Content-aware collaborative filtering",
        "mobile user preferences",
        "unsupervised feature learning",
        "different fingerprint materials",
        "Location privacy protection",
        "Längkvist M",
        "user modeling",
        "Jiangsu Province",
        "Cantador I",
        "cloud environment",
        "data mining",
        "multimedia recommendation",
        "privacy preservation",
        "news recommendation",
        "merchant recommendation",
        "mobile environment",
        "Braunhofer M",
        "Elahi M",
        "Xu J",
        "Kaminskas M",
        "Tian J",
        "Xi J",
        "Wang J",
        "Li J",
        "Qi L",
        "Chen L",
        "Li L",
        "Karlsson L",
        "Competing interests",
        "non-commercial purposes",
        "jurisdictional claims",
        "institutional affiliations",
        "Ricci F",
        "personality information",
        "Koohi H",
        "Kiani K",
        "fuzzy C-means",
        "Xu X",
        "Fu S",
        "Zhang X",
        "Liu Q",
        "He Q",
        "Li S",
        "Yingyuan X",
        "Pengqiang A",
        "Ching-Hsien H",
        "Hongya W",
        "Zhu H",
        "Chen E",
        "Xiong H",
        "Yu K",
        "Cao H",
        "Yin H",
        "Cui B",
        "Hu Z",
        "Zhang C",
        "Li X",
        "Xu G",
        "multiple aspects",
        "Yin C",
        "Sun R",
        "industrial internet",
        "Lian D",
        "Ge Y",
        "Zhang F",
        "Yuan NJ",
        "Xie X",
        "Zhou T",
        "Rui Y",
        "Lee WP",
        "Tseng GY",
        "contextual information",
        "Bengio Y",
        "deep architectures",
        "LeCun Y",
        "Hinton G",
        "Deep learning",
        "Yuan C",
        "Wu QJ",
        "Sun X",
        "Park JH",
        "Loutfi A",
        "time-series modeling",
        "Tu Y",
        "Lin Y",
        "Kim JU",
        "Springer Nature",
        "China Commun",
        "Changsha",
        "University",
        "Acknowledgements",
        "PAPD",
        "KYCX18",
        "authors",
        "Availability",
        "manuscript",
        "scientist",
        "Funding",
        "Publisher",
        "Note",
        "regard",
        "maps",
        "4 April",
        "01 May",
        "Measurement",
        "tags",
        "adaptation",
        "personalization",
        "ICDM",
        "things",
        "Trends",
        "Mach",
        "trust",
        "sociology",
        "Neurocomputing",
        "review",
        "25",
        "Robust L1-norm two-dimensional linear discriminant analysis",
        "novel statistical feature selection approach",
        "distant supervised relation extraction",
        "Gradient Boosting Decision Trees",
        "high-speed network data stream",
        "Data preprocessing Evaluation index",
        "neural information processing systems",
        "text similarity measurement method",
        "Concurr Comput Pract Exp",
        "J Inf Process Syst",
        "deep convolutional neural networks",
        "Traditional recommendation method a.",
        "Content-based recommendation method",
        "Expert Syst Appl",
        "Collaborative filtering method",
        "Hybrid recommendation approaches",
        "big data security",
        "Bin Ibrahim O",
        "singular value decomposition",
        "International joint conference",
        "user location feedback",
        "Authors’ contributions References",
        "improved anonymity model",
        "model a.",
        "data set",
        "text categorization",
        "Mean feature",
        "Ratio feature",
        "Krizhevsky A",
        "Soft Comput",
        "Nilashi M",
        "Ithnin N",
        "Zeng D",
        "Dai Y",
        "Li F",
        "Sherratt RS",
        "Adversarial learning",
        "Mater Continua",
        "Xia L",
        "Zhang S",
        "Sutskever I",
        "Hinton GE",
        "Imagenet classification",
        "Yao C",
        "Fan F",
        "Yu X",
        "semantic relevance",
        "Li CN",
        "Shao YH",
        "Fattah MA",
        "Wang Y",
        "Feng D",
        "Li D",
        "Chen X",
        "Zhao Y",
        "Niu X",
        "logistic regression",
        "Related work",
        "Relevant definitions",
        "Experimental results",
        "clustering algorithm",
        "Advances",
        "Deng"
      ],
      "merged_content": "\nMobile marketing recommendation method \nbased on user location feedback\nChunyong Yin1 , Shilei Ding1 and Jin Wang2*\n\nIntroduction\nIn recent years, the e-commerce industry has developed rapidly with the popularization \nof the Internet. At this time, famous e-commerce platforms such as Alibaba and Ama-\nzon were born. E-commerce moved physical store products to a virtual network plat-\nform. On the one hand, it is convenient for users to buy various products without leaving \nthe home. On the other hand, it is also convenient for sellers to sell their own goods \nand reduce costs. However, the various products have made it more difficult for users \nto select products. E-commerce platform can generate a large amount of user location \nfeedback data which contains a wealth of user preference information [1]. It is significant \nto predict the location of the next consumer’s consumption from these behavioral data. \nAt present, most of the recommended methods focus on the user-product binary matrix \nand directly model their binary relationships [2]. The users’ location information and \nshopping location information are considered as the third factor. In this case, you can \nonly use the limited check-in data. The users’ location feedback behavior and the timeli-\nness of behavior are often overlooked.\n\nThe mobile recommendation system takes advantage of the mobile network environ-\nment in terms of information recommendation and overcomes the disadvantages. Filter-\ning irrelevant information by predicting potential mobile user preferences and providing \n\nAbstract \nLocation-based mobile marketing recommendation has become one of the hot spots \nin e-commerce. The current mobile marketing recommendation system only treats \nlocation information as a recommended attribute, which weakens the role of users and \nshopping location information in the recommendation. This paper focuses on location \nfeedback data of user and proposes a location-based mobile marketing recommenda-\ntion model by convolutional neural network (LBCNN). First, the users’ location-based \nbehaviors are divided into different time windows. For each window, the extractor \nachieves users’ timing preference characteristics from different dimensions. Next, we \nuse the convolutional model in the convolutional neural network model to train a \nclassifier. The experimental results show that the model proposed in this paper is better \nthan the traditional recommendation models in the terms of accuracy rate and recall \nrate, both of which increase nearly 10%.\n\nKeywords: Location feedback, Mobile marketing, Convolutional neural network, \nSequential behavior\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nR E S E A R C H\n\nYin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14  \nhttps://doi.org/10.1186/s13673-019-0177-6\n\n*Correspondence:   \njinwang@csust.edu.cn \n2 School of Computer & \nCommunication Engineering, \nChangsha University \nof Science & Technology, \nChangsha 410004, China\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-5764-2432\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-019-0177-6&domain=pdf\n\n\nPage 2 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nmobile users with results that meet users’ individual needs gradually become an effec-\ntive means to alleviate “mobile information overload” [3]. Mobile users have different \npreferences in different geographical locations. For this problem, how to use location \ninformation to obtain mobile users’ preferences and provide accurate personalized \nrecommendations has become a hot topic in mobile recommendation research [4]. \nAlthough there are many researches based on location recommendation, they mainly \nfocus on service resources without positional relevance. To solve the shortcomings of \nresearch on location relevance of service resources is few [5], Zhu et  al. [6] proposed \nthe method which is based on the user’s context information to analyze the user’s pref-\nerences and retrograde. Their approach is to derive user preferences by proposing two \ndifferent assumptions and then recommending user models based on preference analy-\nsis. Yin et  al. [7] proposed LA-LDA. The method is a location-aware based generation \nprobability model, which uses scoring based on location to model user information and \nrecommend to users. However, these methods only treat location information as an \nattribute without considering the spatial information of users or items and weaken loca-\ntion information’s role in the recommendation. There are some studies determine user \npreferences by the distance between the mobile user and the merchant [8], but only set \nthe area based on the proximity of the distance and ignore the spatial activities of the \nmobile user [9]. However, these methods were limited to the analysis of user informa-\ntion and product information, and did not carefully consider the importance of user and \nbusiness location information. Therefore, the user preference model based on location \nrecommendation they created has some gap.\n\nConsidering the core of mobile marketing recommendation is location movement, \nLian et al. [10] proposed an implied feature-based cognitive feature collaborative filter-\ning (ICCF) framework, which avoids the impact of negative samples by combining con-\nventional methods and semantic content. In terms of algorithms, the author proposed \nan improved algorithm that can expand according to data size and feature size. To deter-\nmine the relevance of the project to user needs, Lee et al. [11] developed context infor-\nmation analysis and collaborative filtering methods for multimedia recommendations in \nmobile environments. Nevertheless, these methods only used small-scale training data \nand could not achieve accurate prediction of long-term interest for users. In this paper, \ndeep learning and time stamps are used to compensate for these shortcomings.\n\nWith great achievements in visual and speech tasks, the Deep Learning (DL) model \nhas become a novel field of study [12]. Because of the interventional optimization of \ndeep learning algorithms, artificial intelligence has made great breakthroughs in many \naspects. It is well known that models obtained through deep learning and machine learn-\ning models have very similar effects, which learns advanced abstract features from the \noriginal input features by simulating the network structure of the human nervous sys-\ntem. Experiments show that the deep model can express the characteristics of the data \nbetter than the shallow model [13]. Weight sharing by convolution makes CNN similar \nto biological neural networks, which reduces the difficulty of network structure and the \nnumber of weights. The structure of CNN is roughly divided into two layers. It is well \nknown that the first layer is a convolutional layer. Each neuron’s input is connected to the \nprevious layer through a convolution kernel and the local features are extracted. Next \nlayer is a pooling layer. In this layer, the neurons in the network are connected through \n\n\n\nPage 3 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\na convolution kernel to extract the overall features. Convolutional neural networks have \ngreat advantages in processing two-dimensional features [14], such as images.\n\nBased on our detailed comparative analysis, this paper proposes a location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN). \nFirstly, we use user-product information as a training sample, and treat this problem as \na two-class problem. The category of the problem is divided into the purchase behav-\nior and the purchase behavior of the product at the next moment. In order to capture \nthe user’s timing preference characteristics, we divide the behavior of the merchandise \naccording to a certain length of time window and dig deeper into the behavior charac-\nteristics of each time window. Secondly, we consider the users’ timing preferences and \noverall preferences for the product. Then, the features of time window are used to train \nconvolutional neural network models. Finally, we input the sample features of the test \nset into the model and generate the Top-K sample as the location-based purchase fore-\ncast results [15].\n\nRemain of the paper is divided into four sections. Related work is shown in “Related \nwork” section. Necessary definitions and specific implementation of the location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN) \nare shown in “Location-based mobile marketing recommendation model by CNN” sec-\ntion. In “Experimental analysis” section, experimental analysis is introduced. “Conclu-\nsion” section summarizes the strengths and weaknesses of the paper and proposes plans \nfor future progress.\n\nRelated work\nIn the current chapter, we will review existing methods for recommending systems \nthat can be broadly divided into three parts: content filtering, collaborative filtering \nand hybrid methods. We also discuss the establishment of feature models based on \ntime series to clearly represent the differences between our research and other existing \nmethods.\n\nTraditional recommendation method\n\nIn the general products recommendation system, the similarity between users is calcu-\nlated by the user’s interest feature vector. Then, the system recommends some products \nwith similarity greater than a certain threshold or the similar Top-N products to the tar-\nget user. This is a traditional recommendation algorithm based on content and the rec-\nommendation is based on comparing users.\n\na. Content‑based recommendation method\n\nContent-based information filtering has proven to be an effective application for \nlocating text documents related to topics. In particular, we need to focus on the \napplication of content-based information filtering in the recommendation system. \nContent-based methods allow for accurate comparisons between different texts \nor projects, so the recommended results are similar to the historical content of the \nuser’s consumption. The content-based recommendation algorithm involves the fol-\nlowing aspects. User description file describes the user’s preferences, which can be \nfilled by the user and dynamically updated based on the user’s feedback information \n\n\n\nPage 4 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\n(purchasing, reading, clicking, etc.) during the operation of the system. The project \nprofile describes the content characteristics of each project, which constitutes the \nfeature vector of the project. In addition, the similarity calculation is the similarity \nbetween the user’s description file and the item feature vector.\n\nThe similarity calculation of the content-based recommendation algorithm usually \nadopts the cosine similarity algorithm. The algorithm needs to calculate the similarity \nbetween the feature vector of user u and the feature vector of item i. The calculation \nformula is as shown in Formula (1).\n\nwhere ⇀u denotes the user feature vector, \n⇀\n\ni  denotes the project feature vector, \n⇀\n\n|u| is the \nmodulus of the user feature vector and \n\n⇀\n\n|i| is the model of the project feature vector.\nRepresentative content-based recommendation systems mainly include Lops, \n\nGemmis, and Semeraro [16]. Compared to other methods, content-based recom-\nmendations have no cold-start issues and recommendations are easy to understand. \nHowever, the content filtering based recommendation method has various draw-\nbacks, such as strongly relying on the availability of content and ignoring the context \ninformation of the recommended party. The content-based recommendation method \nalso has certain requirements for the format of the project. Besides, it is difficult to \ndistinguish the merits of the project. The same type of project may have the same type \nof features, which are difficult to reflect the quality of the project.\n\nb. Collaborative filtering method\n\nThe recommendation based on collaborative filtering solves the recommendation \nproblem by using the information of similar users in the same partition to analyze and \nrecommend new content that has not been scored or seen by the target user.\n\nRegarding the traditional collaborative filtering method based on memory, we \nunderstand that this method is based on the different relationships between users and \nprojects. According to expert research, the traditional collaborative filtering method \nbased on memory should be divided into the following three steps.\n\nStep 1: collection of user behavior data, this step represents the user’s past behav-\nior with a m * n matrix R. The matrix  Umn represents the feedback that the user m \nhas on the recommended object n. Rating is a range of values and different values \nrepresent how much the user likes the recommended object.\n\nStep 2: establishment of a user neighbor: establish mutual user relationships by \nanalyzing all user historical behavior data.\n\n(1)sim(u, i) =\n⇀\nu ·\n\n⇀\n\ni\n\n⇀\n\n|u|\n⇀\n\n|i|\n\nU =\n\n\n\n\n\n\n\nU11 U12 . . . U1n\nU21 U22 . . . U2n\n. . . . . . . . . . . .\n\nUm1 Um2 . . . Umn\n\n\n\n\n\n\n.\n\n\n\nPage 5 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nStep 3: generate recommendation results: find the most likely N objects from the rec-\nommended items selected by similar user sets.\n\nTherefore, recommendations are made by mining common features in similar users’ pref-\nerence information [17]. The normal methods in this classification include k-nearest neigh-\nbor (k-NN), matrix decomposition, and semi-supervised learning. According to the survey, \nAmazon uses an item-by-item collaborative filtering method to recommend personalized \nonline stores for each customer.\n\nCompared to other method, collaborative filtering has the ability to filter out informa-\ntion that can be automatically recognized by the machine and effectively use feedback from \nother similar users. However, collaborative filtering requires more ratings for the project, \nso it is affected by the issue of rating sparsity. In addition, this method does not provide a \nstandard recommendation for new users and new projects, which is called a cold start issue.\n\nc. Hybrid recommendation method\n\nThe hybrid recommendation method combines the above techniques in different ways to \nimprove the recommended performance and optimize the shortcomings of the conven-\ntional method. Projects that cannot be recommended for collaborative filtering are gener-\nally addressed by combining them with content-based filtering [18].\n\nThe core of this method is to independently calculate the recommendation results of the \ntwo types of recommendation algorithms, and then mix the results. There are two specific \nhybrid methods. One method is to mix the predicted scores of the two algorithms linearly. \nAnother hybrid method is to set up an evaluation standard, compare the recommended \nresults of the two algorithms, and take the recommendation results of the higher evaluation \nalgorithms. In general, the hybrid recommendation achieves a certain degree of compensa-\ntion between different recommendation algorithms. However, the hybrid recommendation \nalgorithm still needs improvement in complexity.\n\nd. Recommendation based on association rules\n\nThe association rule algorithm is a traditional data mining method that has been widely \nused in business for many years. The core idea is to analyze the rules of user historical \nbehavior data to recommend more similar behavioral items [19]. Rules can be either user-\ndefined or dynamically generated by using rule algorithms. The effect of the algorithm \ndepends mainly on the quantity and quality of the rules so the focus of the algorithm is on \nhow to develop high quality rules.\n\nDefine N as the total number of transactions, R is the total project and U and V are two \ndisjoint sets of items (U∩V ≠ ∅, U∈R, V∈R). The association rule is essentially an IF–Then \nstatement, here is expressed by U → V. The strength of the association rule U → V can be \nmeasured by two criteria: support and confidence. S is the ratio containing U and V data \nwhich both represent the number of transactions, which is shown in Formula (2).\n\nC is the ratio of U, V data to the only U data which represents the number of transac-\ntions, as shown in Formula (3)\n\n(2)S(U → V ) =\nN(U ∪ V )\n\nN\n.\n\n\n\nPage 6 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nThe recommendation process of the algorithm is shown in below.\nFirstly, according to the items of interest to the user, the user’s interest in other \n\nunknown items is predicted by rules. Secondly, compare the support of the rules. Finally, \nthe recommended items of TOP-N are obtained to the user.\n\nThe recommendation system based on association rules includes three parts: the key-\nword, the presentation and the user interface. The keyword layer is a set of keyword \nattributes and dependencies between keywords. The description layer connects the \nkeyword layer and the user layer and the main function is to describe the user and the \nresource. The user interface layer is the layer that interacts directly with the user. How-\never, the system becomes more and more difficult to manage as the rules increasing. In \naddition, there is a strong dependence on the quality of the rules and a cold start prob-\nlem is existed.\n\nMost of the recommendation systems use collaborative filtering algorithm to recom-\nmend for users. However, the traditional algorithm can only analyze ready-made data \nsimply, and most systems simply preprocess the data. In our method, we preprocess the \ndataset by extending the time information of the data to a time label. The next section is \nan explanation of the specific implementation.\n\nConstruction of time series behavior’s preference features\n\nThe timing recommendation model is based primarily on the Markov chain. This model \nmakes full use of timing behavior data to predict the next purchase behavior based on \nthe user’s last behavior. The advantage of this model is that it can generate good recom-\nmendations by timing behavior.\n\nAs shown in Fig.  1, the prediction problem of product purchase can be expressed as \npredicts the user’s purchase behavior at time T by a user behavior record set D before \ntime T [20]. Different actions occur at different times. For example, user1 visit location \na and b when user1 purchasing b and c at T − 3. We need to predict T-time consumer \nbehavior based on different timing behavior characteristics.\n\nAccording to relevant professional research, we divide the data sets of user behav-\nior into three groups in a pre-processing manner. By the feature statistics method, the \n\n(3)C(U → V ) =\nN(U ∪ V )\n\nN\n.\n\nFig. 1 The time series of user position feedback\n\n Time series T-3 T-2 7-1 T User1 Visit location: a, b Visit location: a, b Forecast Shopping: b, c Shopping: b, c Result ? User2 Visit location: a, b Forecast Shopping: b, c Result ? Visit location: a, b Forecast User3 Shopping: b, c Result ? \n\n\n\nPage 7 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nfeatures are divided into two types, as shown in Table 1. “True” indicates that the feature \ngroup has corresponding features. Conversely, “False” means no such feature. Next we \nexplain these features.\n\na. Counting feature\n\nFor each feature statistics window, we use the behavioral counting feature and the de-\nduplication counting feature. The behavior count is a cumulative measure of the num-\nber of behaviors that occurred in and before the current window. For the location visit \nbehavior, it represents the number of visits to the product location by the user, the total \nnumber of visits by the user and the total number of visits to the merchandise. The de-\nduplication count feature is similar to the behavioral count, but only the number of non-\nrepetitive behavioral data is counted.\n\nb. Mean feature\n\nIn order to describe the activity of the user and the popularity of the product better, \nthis article derives a series of mean-type features based on the counting features. Take \nthe location visit behavior as an example, the user characteristics group includes the \nuser’s average number of visiting to the product. The average number of visiting to \nthe product by user i is calculated as shown in Formula (4).\n\nc. Ratio feature\n\nThe ratio of user-product behavior to the total behavior of the user and the product \nis also an aspect affecting the user’s degree of preference for the product. In the time \nwindow t, the method to calculate the ratio of the user’s visit to the products’ total \nvisit is shown in Formula (5).\n\nOur work presents a mobile marketing recommendation model is trained by adding \nthe time axis to the user position features. Contrary to current research, it is highly \nusable and low difficulty of achievement for real-world work applications. Consider-\ning the speed of calculation, we study the method of directly embedding time series \ninformation into the collaborative filtering calculation process to improve the recom-\nmendation quality. Specific information will be covered in the following sections.\n\n(4)avgui(t, i, visit) =\naction_count(t, U, Ui, visit)\n\nuser_unique_item(t, U, Ui, visit)\n.\n\n(5)rate_ui_in_u(t, i, j, visit) =\naction_count(t, UI, Ui, Ij, visit)\n\naction_count(t, U, Ui, visit)\n.\n\nTable 1 Characteristic system diagram (True/False)\n\nFeature group Counting feature Mean feature Ratio feature\n\nUser-product True False True\n\nUser feature True True False\n\nProduct feature True True False\n\n\n\nPage 8 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nLocation‑based mobile marketing recommendation model by CNN\nCreating the model is one of the most important aspects, which is an evaluation crite-\nrion to make sure correctness of the next step. This section mainly describes the rel-\nevant definitions of LBCNN that are shown in “Relevant definitions of the LBCNN” \nsection, and specific implementation of the model is shown in “Specific implementa-\ntion of the model” section.\n\nRelevant definitions of the LBCNN\n\nIn order to get better feature expression, we consider the user’s timing sensitivity of the \nproduct preferences and the user’s overall preferences comprehensively. This paper uses a \nconvolutional neural network as the basis to build location-based mobile marketing recom-\nmendation model. In the next step, we give the relevant definition.\n\na. Definition 1 (Model framework): based on the above analysis and user’s timing behav-\nior preference feature. We use the convolutional neural network model shown in Fig. 2. The \nmodel is divided into four layers that are input layer, multi-window convolution layer, pool-\ning layer and output layer. The input layer is a well-constructed input feature which trans-\nforms the input features into a two-dimensional plane by time series. Each time window is \nexpressed as an eigenvector. The multi-window convolutional layer convolves the input fea-\nture plane through different lengths of time windows to obtain different feature maps. The \npooling layer reduces the dimension of the feature map to obtain a pooled feature vector. \nThe output layer and the pooling layer are fully connected network structures.\n\nb. Definition 2 (Convolution layer): assume that there are N time windows of the feature \nand each time window has K user preference feature for the commodity. Then input sam-\nple × can be expressed as a matrix of T × K. The feature map in the convolutional layer is \ncalculated by the input layer and the convolution kernel. The window length of the convolu-\ntion kernel is h. xi,i+j represents the eigenvector added by time window i and time window \ni + j. The convolution kernel w can be expressed as a vector of h × K. Feature map f = [f1, f2, \n…, fT−h+1]. The i-th feature fi is calculated according to Formula (6):\n\n(6)fi = σ(w · xi,i+h−1 + b)\n\nFig. 2 The framework of the LBCNN\n\n Time series- ------ -- - - - - - -- -- Input feature convolutional layer Multi-window Max-pooling Output layer layer \n\n\n\nPage 9 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nwhere b is an offset term and a real number. σ(x) is a nonlinear activation function. This \npaper uses ReLu and Tanh as an activation function. Relu is shown in Formula (7):\n\nc. Definition 3 (Max-pooling): the pooling layer is to scale the feature map while reduc-\ning the complexity of the network. The maximum features of the convolution kernel can \nbe obtained according to the maximum pooling operation. The feature map obtained \nat the kth product of the convolutional kernel is fk = [fk,1, fk,2, …, fk,T−h +1]. The pooling \noperation can be expressed as Formula (8):\n\nd. Definition 4 (Probability distribution): there are M convolution kernels and the output \nlayer has C categories [19]. The weight parameter θ of the output layer is a C × M matrix. \nThe pooled feature f̂  of x is an M-dimensional vector. The probability that x belongs to \nthe i-th category can be expressed as Formula (9):\n\nwhere  bk represents the k-th offset of the fully connected layer. The loss function of the \nmodel can be obtained by the likelihood probability value, as shown in Formula (10):\n\nwhere T is the training data set,  yi is the real category of the i-th sample, xi is the charac-\nteristic of the i-th sample and θ is the model’s parameters. We learn model parameters \nby minimizing the loss function. The training method adopts the improved gradient \ndescent method proposed by Zeiler. In addition, we have adopted Dropout process-\ning on the convolutional layer to prevent over-fitting of the trained model [21]. The \nDropout method randomizes the neurons in the convolutional layer to 0 with a certain \nprobability.\n\ne. Definition 5 (Latent factor): the value of the latent factor vector is true [22]. Whether \nan item belongs to a class is determined entirely by the user’s behavior. We assume that \ntwo items are liked by many users at the same time, then these two items have a high \nprobability of belonging to the same class. The weight of an item in a class can also be \ncalculated by itself. The implicit semantic model calculates the user’s (u) interest in the \nitem (i) are shown in Formula (11):\n\n(7)\nReLu = max(0, x).\n\nTanh(x) =\nex − e−x\n\nex + e−x\n.\n\n(8)Pool_feature(j) = down(fi).\n\n(9)p(i|x, θ) =\ne(θi·\n\n⌢\nf +bi)\n\n∑C\nk−1 e\n\n(θk·\n⌢\nf +bk)\n\n(10)J(θ) = −\nk\n\n∑\n\ni=1\n\nlog(p(yi|x, θ))\n\n(11)R(u, i) = rui = p\nT\nu qi =\n\nF\n∑\n\nf =1\n\npu,kqi,k\n\n\n\nPage 10 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nwhere p is the relationship between the user interest and the kth implicit class. q is the \nrelationship between the kth implicit class and the item i. F is the number of hidden \nclasses, and r is the user’s interest in the item.\n\nSpecific implementation of the model\n\nWe can draw from Fig. 3 that the proposed model is divided into two processes. The first \nprocess is the training process and includes two parts. The top module shows how to gener-\nate CNN inputs and outputs from historical data. The other module in the training process \nshows that the traditional CNN parameters are trained by provided data. The second pro-\ncess finished a new location-based marketing resources recommendation. The recommen-\ndation process can work through the CNN parameters provided by the training process.\n\nTo achieve the features of users and location-based mobile marketing resources, the \nlatent factor model (LFM) is used. In traditional LFM, L2-norm regularization is often used \nto optimize training results. However, using L2-norm regularization often leads to excessive \nsmoothing problems. In our model, LFM results are used to represent the characteristics of \nthe training data. In this kind of thinking, we can learn from the training method of regres-\nsion coefficient in regression analysis, and construct a loss function. Therefore, it is more \nreasonable to use sparseness before the specification results. Based on these analyses, we \npropose an improved matrix decomposition method and try to normalize the solution by \n\nFig. 3 Location-based mobile marketing recommendation model by convolutional neural network\n\n The existing Users Latent location- historical factor based location data model marketing resources Time series Features of User feature resources Training process preferences model Input Output CNN A new location-based Features of marketing resources new resources process Language CNN Results Recommending model User preferences \n\n\n\nPage 11 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nusing the premise of verifying the sparseness of the matrix. The model is presented as For-\nmula (12):\n\nThe next question is how to calculate these two parameters p and q. For the calculation \nof this linear model, this paper uses the gradient descent method. In the Formula (12), \n puk is a user bias item that represents the average of a user’s rating.  qik is an item offset \nitem that represents the average of an item being scored. The offset term is an intrinsic \nproperty that indicates whether the item is popular with the public or a user is harsh \non the item. For positive samples, we specify  ru,i = 1 based on experience and negative \nsample  ru,i = 0, which is shown in Formula (11). The latter λ is a regularization term to \nprevent overfitting.\n\na. Description of the training section\n\nIn Fig. 3, If you want to train CNN, the first thing you need to solve is its input and out-\nput problems. For input, a language model is usually used.\n\nIn terms of output, we propose an improvement in model training by LFM, which is \nconstrained by the regularization of the L1-norm [23]. LFM training data is a historical \nscore between the user and the location-based marketing resources. The rating score can \nbe explicit because it is based on a user tag or an implied tag and it is predicted from the \nuser’s behavior. In this model, in order to ensure that the trained model is representative, \nthe training data we input is to select the existing authoritative standard training set.\n\nb. Description of the recommended part\n\nOnce the LBCNN model structure is established and the model parameters are trained \nusing the training data set, the recommended real-time performance can be achieved. \nThe real-time performance is based on the update of network model parameters in the \nbackground, and it uses some past behavior data and information of the recommended \npeople and products.\n\nUser information and product information can be obtained in advance and digitized. \nIn the offline training model phase, digitized user information, product information, and \nbehavior information are utilized [24]. The same model is trained for the same type of \nusers, and the parameters of the model are periodically updated within a certain period \nof time. In the real-time recommendation stage, real-time recommendation can be real-\nized only by integrating the collected behavior data with the previous data and inputting \nit into the model.\n\nExperimental analysis\nIn order to verify the advantages of convolutional neural network in capturing user’s \ntiming preferences for product and mining users’ temporal behavior characteristics, \nwe compare several commonly used classification models under the same conditions of \ntraining features. They are Linear Logistic Regression Classification Model (LR), Support \n\n(12)J(U, V ) =\n∑\n\nu,i∈K\n\n(\n\nru,i −\n\nk\n∑\n\nk=1\n\npu,kqi,k\n\n)2\n\n+ ��puk�\n2\n+ ��qik�\n\n2.\n\n\n\nPage 12 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nVector Machine (SVM), Random Forest Model (RF) and Gradient Boosting Regression \nTree Model (GBDT) [25]. We also compare the products that have been visited for the \nlast 8  h. Experimental tool is sklearn kit. The hyper parameter settings for each model \nduring the experiment are:\n\na. LR: select L2 regular and the regularization coefficient is 0.1.\nb. SVM: choose radial basis kernel function (RBF) and gamma of kernel function is \n\n0.005.\nc. RF: the number of trees is 200, the entropy is selected as the feature segmentation \n\nstandard and the random feature ratio is 0.5.\nd. GBDT: the number of trees is 100, the learning rate is 0.1 and the maximum depth of \n\nthe tree is 3.\n\nDescription of the data set\n\nThe experiment in our paper uses the dataset disclosed according to the Alibaba Group’s \nmobile recommendation algorithm contest held in 2015. This data set contains 1 month \nof user behavior data and product information. The user’s behavior data includes 10 mil-\nlion users’ various behaviors on 2,876,947 items. Behavior types include clicks, shopping \ncarts and purchases. In addition, each behavior record identifies behavior time that is \naccurate to the hour. The product information includes product category information, \nand identifies whether the product is an online to offline type. In a real business sce-\nnario, we often need to build a personalized recommendation model for a subset of all \nproducts. In the process of completing this task, we not only need to take advantage of \nthe user’s behavior data on this subset of goods, but also need to use more abundant user \nbehavior data. We need to define the following symbols: U (User collection), I (Product \ncollection), P (Product subset, P ⊆ I), D (User behavior data collection for the complete \nset of products). Our goal is to use D to construct a recommendation model for users in \nU to products in P.\n\nThe data mainly consists of two parts. The first part is the mobile behavior data (D) of \n10 million users on the product collection, including the following fields, as shown in \nTable 2.\n\nFor example, “141278390, 282725298, 1, 95jnuqm, 5027, 2014-11-18 08” is one of \nthe data. The Behavior_type and the Time in these fields contain the largest amount \n\nTable 2 The mobile behavior data of the Ali mobile recommendation data set\n\nField Field description Extraction instruction\n\nUser_id User differentiation Sampling and data masking\n\nItem_id Product differentiation Data masking\n\nBehavior_type The type of behavior of the user on the \nproduct\n\nIncluding browsing, collecting, adding shop-\nping carts, and purchasing, the values are 1, 2, \n3, 4 respectively\n\nUser_geoinfo The spatial reference identifier of the user’s \nlocation\n\nFormed by latitude and longitude data through \na secret algorithm\n\nItem category Product classification identifier Field masking\n\nTime Action time Accurate to hour level\n\n\n\nPage 13 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nof information. The User_geohash field is basically unusable due to too many missing \nvalues.\n\nThe second part is the product subset (P), which contains the following fields, as \nshown in Table 3.\n\nSimilar to the above, “117151719, 96ulbnj, 7350” is one of the product information. \nThe training data contains the mobile behavior data (D) of a sample of a certain user \nwithin 1 month (11.18–12.18). The scoring data is the purchase data of the product sub-\nset (P) by these users 1  day (12.19) after this 1  month. We should be training the data \nmodel to output the predicted results of the user’s purchase behavior on the next day.\n\nData preprocessing\n\nWe found that there are some users have a lot of page views (maximum of 2 million), which \nis beyond reasonable levels. We analyze that these users may be crawler users, so the behav-\nior of these users on the goods is not the basis for predicting the user’s purchase. At the \nsame time, we predict the user product pairs that have appeared in all historical records. \nThe existence of these users will undoubtedly increase our forecasting amount and interfere \nwith our normal model training. Therefore, we choose to filter out these users, the filtering \nrules are shown as Fig. 4.\n\nTable 3 The product subset of the Ali mobile recommendation data set\n\nField Field description Extraction instruction\n\nItem_id Product differentiation Sampling and data masking\n\nItem_geohash Spatial information of the product location, \nwhich can be empty\n\nFormed by latitude and longi-\ntude data through a secret \nalgorithm\n\nItem_category Product classification identifier Data masking\n\nFig. 4 Data filtering rules\n\n User behavior data Pageviews > No Retain data 30,000 Yes Purchase >3 Yes- Retain data 0 No Eliminate data \n\n\n\nPage 14 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nEvaluation index\n\nThe purpose of the proposed method is to predict the user’s purchased business in the next \nposition based on the user’s historical behavior record. Therefore, we evaluate the model \nwith the data of the last day. The sample construction of time series method is shown in \nFig.  1. F1-score can be viewed as a harmonic mean of accuracy and recall. At present, \nF1-score has been widely used in the evaluation of the recommendation system.\n\nwhere Formula (13) is the calculation method of the accuracy rate, Formula (14) is the \ncalculation method of the recall rate, and Formula (15) is the calculation method of \nF1-score. Prediction_set is the predicted purchase of the user-item. Answer_set is a real-\npurchased user-item collection.\n\nThe distribution of positive and negative samples used in this experiment is extremely \nunbalanced, and negative samples contain more noise. In order to make the model more \nsuitable for learning under unbalanced data, we perform under sampling on negative \nsamples. The model training process adopts AdaDelta Update Rule to adjust the param-\neters by using the stochastic gradient descent method. Hyper Parameters of the model \nare described in Table  4. The value in the table is the final hyper parameter when the \nerror of the validation set is minimal. Convolution time window in convolution kernel \nis 2 and 3. The number of convolution kernels for two different length windows is 200. \nIn this experiment, the training process needs to iterate ten times. To achieve the con-\nvergence of the model, we observe the accuracy of the training set every iteration in the \nmodel training process.\n\nIn Fig. 5, the abscissa indicates the number of iterations, and the ordinate indicates the \naccuracy of the sample. As we can see from the figure, the accuracy of the training set \nhas been increasing and the verification set accuracy has declined after the fifth iteration \nof the model [26]. This situation shows that the model training has been overfitting after \n\n(13)precision =\n∣\n\n∣prediction_set ∩ answer_set\n∣\n\n∣\n\n∣\n\n∣prediction_set\n∣\n\n∣\n\n(14)Recall =\n∣\n\n∣prediction_set ∩ answer_set\n∣\n\n∣\n\n|answer_set|\n\n(15)F1 − score =\n2 × precision × recall\n\nprecision + recall\n\nTable 4 Parameter settings of convolutional neural networks\n\nParameter name Parameter value\n\nActivation function of convolution kernel Tanh\n\nSize of convolution kernel window [2, 3]\n\nNumber of convolution kernel 400\n\nDropout ratio 0.5\n\nBatch size 64\n\nEpoch 5\n\n\n\nPage 15 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nthe 5th iteration. In addition, we found that the test set accuracy is higher than the train-\ning set and verification set.\n\nExperimental results and comparison\n\nThe experimental results obtained using the above parameters are shown in Table 5. As \ncan be seen from Table  5, the machine learning model using the features designed in \nthis paper is superior to the traditional method. Our model achieves an 80% accuracy \nin predicting the accuracy of user behavior, which is significantly better than traditional \nmodels at least 10%. In terms of recall rate, LBCNN reached 8.14%, which is at least 2% \nhigher than the traditional method. Similarly, our model is up to 8.07% in F1-score.\n\nThis result shows that the user’s time-series behavior preference model is reasonable. \nThis solution works well for improving the accuracy and quality of recommendations. In \na single model, the LBCNN model works best. Since the linear model assumes that each \nfeature is independent, it is impossible to excavate the intrinsic relationship between \nfeatures. The proposed method can mine the intrinsic link between user timing prefer-\nence features better. The experimental results show that the user preferences we build \nare more accurate and convolutional neural networks have strong capabilities of feature \nextraction and model generalization.\n\nFig. 5 Training process of the LBCNN\n\nTable 5 Comparing the experimental results of each model (%)\n\nModel Accuracy Recall rate F1-score\n\nTraditional method 31.4 5.60 4.02\n\nLR 75.0 7.63 7.57\n\nSVM 70.0 7.12 7.06\n\nRF 57.5 5.85 5.80\n\nGBDT 62.5 6.36 6.13\n\nLBCNN 80.0 8.14 8.07\n\n Validation accuracy -Testing accuary Training accuracy 98 96 94 92 Accuracy rate/% 06 88 86 84 82 1 2 3 4 5 6 7 Number of iterations \n\n\n\nPage 16 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\nConclusion\nThe current mobile marketing recommendation system only treats location informa-\ntion as a recommended attribute, which weakens the role of the location information in \nthe recommendation. For the implicit feedback behavior of users, this paper proposes \na location-based mobile marketing method by convolutional neural network. First, we \ndivide the user location-based behaviors into several time windows according to the \ntimestamp of these behaviors, and model the user preference in different dimensions \nfor each window. Then we utilize the convolutional neural network to train a classifier. \nFinally, the experimental process of this paper is introduced, and a good prediction effect \nis obtained on effective data sets. The final experimental results express that the pro-\nposed method has different feature extraction perspectives from other models. Because \nof using convolutional neural networks, the proposed method has stronger capability \nof feature extraction and generalization. This method helps to change the accuracy and \nquality of the recommendation system and user satisfaction.\n\nThe work introduced here is to show the prospects for further research. The method \nproposed in this paper has a certain dependence on the user’s geographical location \ninformation during the training process of the user preference model. In addition, the \nrecommendation system will encounter a cold-start problem with sparse user infor-\nmation. For dealing with these discovered issues, we plan to use the hot start case to \nimprove the recommended cold start problem. Meanwhile, we are investigating new \nmethod which uses a better big data framework (such as Hadoop MapReduce) to ensure \nthe efficiency of training large data sets. In the future, we will show recommended meth-\nods to improve performance in other applications.\nAuthors’ contributions\nCY conceptualized the study and analyzed all the data. SD performed all experiments and wrote the manuscript. JW \nadvised on the manuscript preparation and technical knowledge. All authors read and approved the final manuscript.\n\nAuthor details\n1 School of Computer and Software, Jiangsu Engineering Center of Network Monitoring, Nanjing University of Informa-\ntion Science & Technology, Nanjing 210044, China. 2 School of Computer & Communication Engineering, Changsha \nUniversity of Science & Technology, Changsha 410004, China. \n\nAcknowledgements\nIt was supported by the Priority Academic Program Development of Jiangsu Higher Education Institutions (PAPD), Post-\ngraduate Research & Practice Innovation Program of Jiangsu Province (KYCX18_1032).\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAvailability of data and materials\nWe declared that materials described in the manuscript will be freely available to any scientist wishing to use them for \nnon-commercial purposes.\n\nFunding\nThis work was supported by the National Natural Science Foundation of China (61772282, 61772454, 61811530332, \n61811540410).\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 25 September 2018   Accepted: 4 April 2019\n\nReferences\n 1. Fernández-Tobías I, Braunhofer M, Elahi M, Ricci F, Cantador I (2016) Alleviating the new user problem in collabora-\n\ntive filtering by exploiting personality information. User Model User Adap Inter 26(2–3):221–255\n\n Published online: 01 May 2019 \n\n\n\nPage 17 of 17Yin et al. Hum. Cent. Comput. Inf. Sci.            (2019) 9:14 \n\n 2. Koohi H, Kiani K (2016) User based collaborative filtering using fuzzy C-means. Measurement 91:134–139\n 3. Xu X, Fu S, Qi L, Zhang X, Liu Q, He Q, Li S (2018) An IoT-oriented data placement method with privacy preservation \n\nin cloud environment. J Netw Comput Appl 124:148–157\n 4. Yingyuan X, Pengqiang A, Ching-Hsien H, Hongya W, Xu J (2015) Time-ordered collaborative filtering for news \n\nrecommendation. China Commun 12(12):53–62\n 5. Kaminskas M, Ricci F (2011) Location-adapted music recommendation using tags. In: International conference on \n\nuser modeling, adaptation, and personalization. pp 183–194\n 6. Zhu H, Chen E, Xiong H, Yu K, Cao H, Tian J (2015) Mining mobile user preferences for personalized context-aware \n\nrecommendation. ACM Trans Intell Syst Technol TIST 5(4):58\n 7. Yin H, Cui B, Chen L, Hu Z, Zhang C (2015) Modeling location-based user rating profiles for personalized recommen-\n\ndation. ACM Trans Knowl Discov Data TKDD 9(3):19\n 8. Li X, Xu G, Chen E, Li L (2015) Learning user preferences across multiple aspects for merchant recommendation. In: \n\n2015 IEEE international conference on data mining (ICDM). pp 865–870\n 9. Yin C, Xi J, Sun R, Wang J (2018) Location privacy protection based on differential privacy strategy for big data in \n\nindustrial internet-of-things. IEEE Trans Industr Inf 14(8):3628–3636\n 10. Lian D, Ge Y, Zhang F, Yuan NJ, Xie X, Zhou T, Rui Y (2015) Content-aware collaborative filtering for location recom-\n\nmendation based on human mobility data. In: 2015 IEEE international conference on data mining. pp 261–270\n 11. Lee WP, Tseng GY (2016) Incorporating contextual information and collaborative filtering methods for multimedia \n\nrecommendation in a mobile environment. Multimedia Tools Appl 75(24):16719–16739\n 12. Bengio Y (2009) Learning deep architectures for AI. Found Trends Mach Learn 2(1):1–127\n 13. LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521(7553):436–444\n 14. Yuan C, Li X, Wu QJ, Li J, Sun X (2017) Fingerprint liveness detection from different fingerprint materials using convo-\n\nlutional neural network and principal component analysis. Comput Mater Continua 53(4):357–372\n 15. Yin C, Wang J, Park JH (2017) An improved recommendation algorithm for big data cloud service based on the trust \n\nin sociology. Neurocomputing 256:49–55\n 16. Längkvist M, Karlsson L, Loutfi A (2014) A review of unsupervised feature learning and deep learning for time-series \n\nmodeling. Pattern Recogn Lett 42:11–24\n 17. Tu Y, Lin Y, Wang J, Kim JU (2018) Semi-supervised learning with generative adversarial networks on digital signal \n\nmodulation classification. Comput Mater Continua 55(2):243–254\n 18. Nilashi M, Bin Ibrahim O, Ithnin N (2014) Hybrid recommendation approaches for multi-criteria collaborative filter-\n\ning. Expert Syst Appl 41(8):3879–3900\n 19. Zeng D, Dai Y, Li F, Sherratt RS, Wang J (2018) Adversarial learning for distant supervised relation extraction. Comput \n\nMater Continua 55(1):121–136\n 20. Yin C, Xia L, Zhang S, Sun R, Wang J (2018) Improved clustering algorithm based on high-speed network data \n\nstream. Soft Comput 22(13):4185–4195\n 21. Krizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classification with deep convolutional neural networks. In: \n\nAdvances in neural information processing systems, pp 1097–1105\n 22. Li X, Yao C, Fan F, Yu X (2017) A text similarity measurement method based on singular value decomposition and \n\nsemantic relevance. J Inf Process Syst 13(4):863–875\n 23. Li CN, Shao YH, Deng NY (2015) Robust L1-norm two-dimensional linear discriminant analysis. Neural Networks \n\n65:92–104\n 24. Fattah MA (2017) A novel statistical feature selection approach for text categorization. J Inf Process Syst \n\n13(5):1397–1409\n 25. Wang Y, Feng D, Li D, Chen X, Zhao Y, Niu X (2016) A mobile recommendation system based on logistic regression \n\nand Gradient Boosting Decision Trees. In: International joint conference on neural networks. pp 1896–1902\n 26. Yin C, Zhang S, Xi J, Wang J (2017) An improved anonymity model for big data security based on clustering algo-\n\nrithm. Concurr Comput Pract Exp 29(7):e3902\n\n\n\tMobile marketing recommendation method based on user location feedback\n\tAbstract \n\tIntroduction\n\tRelated work\n\tTraditional recommendation method\n\ta. Content-based recommendation method\n\tb. Collaborative filtering method\n\tc. Hybrid recommendation method\n\td. Recommendation based on association rules\n\n\tConstruction of time series behavior’s preference features\n\ta. Counting feature\n\tb. Mean feature\n\tc. Ratio feature\n\n\n\tLocation-based mobile marketing recommendation model by CNN\n\tRelevant definitions of the LBCNN\n\tSpecific implementation of the model\n\ta. Description of the training section\n\tb. Description of the recommended part\n\n\n\tExperimental analysis\n\tDescription of the data set\n\tData preprocessing\n\tEvaluation index\n\tExperimental results and comparison\n\n\tConclusion\n\tAuthors’ contributions\n\tReferences\n\n\n\n\n",
      "text": [
        "Time series T-3 T-2 7-1 T User1 Visit location: a, b Visit location: a, b Forecast Shopping: b, c Shopping: b, c Result ? User2 Visit location: a, b Forecast Shopping: b, c Result ? Visit location: a, b Forecast User3 Shopping: b, c Result ?",
        "Time series- ------ -- - - - - - -- -- Input feature convolutional layer Multi-window Max-pooling Output layer layer",
        "The existing Users Latent location- historical factor based location data model marketing resources Time series Features of User feature resources Training process preferences model Input Output CNN A new location-based Features of marketing resources new resources process Language CNN Results Recommending model User preferences",
        "User behavior data Pageviews > No Retain data 30,000 Yes Purchase >3 Yes- Retain data 0 No Eliminate data",
        "Validation accuracy -Testing accuary Training accuracy 98 96 94 92 Accuracy rate/% 06 88 86 84 82 1 2 3 4 5 6 7 Number of iterations",
        "Published online: 01 May 2019"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"Time series T-3 T-2 7-1 T User1 Visit location: a, b Visit location: a, b Forecast Shopping: b, c Shopping: b, c Result ? User2 Visit location: a, b Forecast Shopping: b, c Result ? Visit location: a, b Forecast User3 Shopping: b, c Result ?\",\"lines\":[{\"boundingBox\":[{\"x\":890,\"y\":2},{\"x\":1149,\"y\":3},{\"x\":1149,\"y\":48},{\"x\":890,\"y\":48}],\"text\":\"Time series\"},{\"boundingBox\":[{\"x\":485,\"y\":101},{\"x\":549,\"y\":101},{\"x\":549,\"y\":145},{\"x\":484,\"y\":145}],\"text\":\"T-3\"},{\"boundingBox\":[{\"x\":904,\"y\":99},{\"x\":982,\"y\":100},{\"x\":979,\"y\":146},{\"x\":903,\"y\":144}],\"text\":\"T-2\"},{\"boundingBox\":[{\"x\":1324,\"y\":101},{\"x\":1400,\"y\":101},{\"x\":1399,\"y\":146},{\"x\":1322,\"y\":146}],\"text\":\"7-1\"},{\"boundingBox\":[{\"x\":1812,\"y\":104},{\"x\":1848,\"y\":105},{\"x\":1845,\"y\":145},{\"x\":1810,\"y\":144}],\"text\":\"T\"},{\"boundingBox\":[{\"x\":41,\"y\":316},{\"x\":166,\"y\":315},{\"x\":167,\"y\":359},{\"x\":41,\"y\":362}],\"text\":\"User1\"},{\"boundingBox\":[{\"x\":310,\"y\":283},{\"x\":715,\"y\":285},{\"x\":715,\"y\":333},{\"x\":310,\"y\":329}],\"text\":\"Visit location: a, b\"},{\"boundingBox\":[{\"x\":1161,\"y\":283},{\"x\":1566,\"y\":284},{\"x\":1565,\"y\":334},{\"x\":1161,\"y\":332}],\"text\":\"Visit location: a, b\"},{\"boundingBox\":[{\"x\":1729,\"y\":285},{\"x\":1921,\"y\":285},{\"x\":1921,\"y\":328},{\"x\":1729,\"y\":328}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":310,\"y\":348},{\"x\":634,\"y\":348},{\"x\":634,\"y\":402},{\"x\":310,\"y\":402}],\"text\":\"Shopping: b, c\"},{\"boundingBox\":[{\"x\":1164,\"y\":347},{\"x\":1480,\"y\":347},{\"x\":1480,\"y\":402},{\"x\":1164,\"y\":402}],\"text\":\"Shopping: b, c\"},{\"boundingBox\":[{\"x\":1738,\"y\":348},{\"x\":1916,\"y\":347},{\"x\":1917,\"y\":391},{\"x\":1738,\"y\":392}],\"text\":\"Result ?\"},{\"boundingBox\":[{\"x\":43,\"y\":518},{\"x\":176,\"y\":517},{\"x\":177,\"y\":562},{\"x\":44,\"y\":564}],\"text\":\"User2\"},{\"boundingBox\":[{\"x\":738,\"y\":484},{\"x\":1141,\"y\":484},{\"x\":1141,\"y\":535},{\"x\":738,\"y\":533}],\"text\":\"Visit location: a, b\"},{\"boundingBox\":[{\"x\":1729,\"y\":487},{\"x\":1923,\"y\":487},{\"x\":1923,\"y\":529},{\"x\":1729,\"y\":530}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":740,\"y\":550},{\"x\":1056,\"y\":551},{\"x\":1055,\"y\":606},{\"x\":739,\"y\":604}],\"text\":\"Shopping: b, c\"},{\"boundingBox\":[{\"x\":1737,\"y\":549},{\"x\":1915,\"y\":548},{\"x\":1915,\"y\":596},{\"x\":1737,\"y\":597}],\"text\":\"Result ?\"},{\"boundingBox\":[{\"x\":1159,\"y\":687},{\"x\":1566,\"y\":688},{\"x\":1565,\"y\":736},{\"x\":1159,\"y\":733}],\"text\":\"Visit location: a, b\"},{\"boundingBox\":[{\"x\":1728,\"y\":689},{\"x\":1921,\"y\":689},{\"x\":1922,\"y\":732},{\"x\":1728,\"y\":732}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":39,\"y\":720},{\"x\":172,\"y\":719},{\"x\":172,\"y\":763},{\"x\":40,\"y\":764}],\"text\":\"User3\"},{\"boundingBox\":[{\"x\":1162,\"y\":753},{\"x\":1481,\"y\":753},{\"x\":1481,\"y\":805},{\"x\":1162,\"y\":806}],\"text\":\"Shopping: b, c\"},{\"boundingBox\":[{\"x\":1737,\"y\":753},{\"x\":1915,\"y\":752},{\"x\":1915,\"y\":796},{\"x\":1737,\"y\":797}],\"text\":\"Result ?\"}],\"words\":[{\"boundingBox\":[{\"x\":896,\"y\":3},{\"x\":1009,\"y\":3},{\"x\":1010,\"y\":49},{\"x\":898,\"y\":49}],\"text\":\"Time\"},{\"boundingBox\":[{\"x\":1018,\"y\":3},{\"x\":1149,\"y\":3},{\"x\":1149,\"y\":49},{\"x\":1019,\"y\":49}],\"text\":\"series\"},{\"boundingBox\":[{\"x\":484,\"y\":101},{\"x\":549,\"y\":101},{\"x\":549,\"y\":145},{\"x\":484,\"y\":145}],\"text\":\"T-3\"},{\"boundingBox\":[{\"x\":903,\"y\":99},{\"x\":980,\"y\":100},{\"x\":979,\"y\":146},{\"x\":903,\"y\":144}],\"text\":\"T-2\"},{\"boundingBox\":[{\"x\":1325,\"y\":101},{\"x\":1400,\"y\":101},{\"x\":1400,\"y\":146},{\"x\":1325,\"y\":146}],\"text\":\"7-1\"},{\"boundingBox\":[{\"x\":1814,\"y\":104},{\"x\":1846,\"y\":105},{\"x\":1845,\"y\":145},{\"x\":1813,\"y\":144}],\"text\":\"T\"},{\"boundingBox\":[{\"x\":41,\"y\":316},{\"x\":166,\"y\":315},{\"x\":167,\"y\":360},{\"x\":41,\"y\":362}],\"text\":\"User1\"},{\"boundingBox\":[{\"x\":314,\"y\":283},{\"x\":416,\"y\":284},{\"x\":417,\"y\":331},{\"x\":315,\"y\":329}],\"text\":\"Visit\"},{\"boundingBox\":[{\"x\":425,\"y\":284},{\"x\":621,\"y\":285},{\"x\":622,\"y\":333},{\"x\":426,\"y\":331}],\"text\":\"location:\"},{\"boundingBox\":[{\"x\":630,\"y\":285},{\"x\":672,\"y\":285},{\"x\":673,\"y\":333},{\"x\":631,\"y\":333}],\"text\":\"a,\"},{\"boundingBox\":[{\"x\":681,\"y\":285},{\"x\":714,\"y\":285},{\"x\":715,\"y\":333},{\"x\":682,\"y\":333}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1162,\"y\":284},{\"x\":1267,\"y\":284},{\"x\":1266,\"y\":332},{\"x\":1162,\"y\":331}],\"text\":\"Visit\"},{\"boundingBox\":[{\"x\":1276,\"y\":284},{\"x\":1469,\"y\":284},{\"x\":1469,\"y\":334},{\"x\":1275,\"y\":332}],\"text\":\"location:\"},{\"boundingBox\":[{\"x\":1478,\"y\":284},{\"x\":1518,\"y\":284},{\"x\":1518,\"y\":334},{\"x\":1478,\"y\":334}],\"text\":\"a,\"},{\"boundingBox\":[{\"x\":1528,\"y\":284},{\"x\":1565,\"y\":285},{\"x\":1564,\"y\":335},{\"x\":1527,\"y\":334}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1731,\"y\":285},{\"x\":1922,\"y\":286},{\"x\":1921,\"y\":328},{\"x\":1729,\"y\":328}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":312,\"y\":348},{\"x\":532,\"y\":349},{\"x\":532,\"y\":403},{\"x\":311,\"y\":401}],\"text\":\"Shopping:\"},{\"boundingBox\":[{\"x\":542,\"y\":349},{\"x\":587,\"y\":349},{\"x\":588,\"y\":401},{\"x\":543,\"y\":402}],\"text\":\"b,\"},{\"boundingBox\":[{\"x\":597,\"y\":349},{\"x\":631,\"y\":349},{\"x\":633,\"y\":400},{\"x\":598,\"y\":401}],\"text\":\"c\"},{\"boundingBox\":[{\"x\":1165,\"y\":347},{\"x\":1381,\"y\":348},{\"x\":1380,\"y\":403},{\"x\":1164,\"y\":400}],\"text\":\"Shopping:\"},{\"boundingBox\":[{\"x\":1391,\"y\":348},{\"x\":1436,\"y\":348},{\"x\":1435,\"y\":402},{\"x\":1391,\"y\":403}],\"text\":\"b,\"},{\"boundingBox\":[{\"x\":1446,\"y\":348},{\"x\":1480,\"y\":349},{\"x\":1480,\"y\":400},{\"x\":1446,\"y\":401}],\"text\":\"c\"},{\"boundingBox\":[{\"x\":1739,\"y\":349},{\"x\":1879,\"y\":348},{\"x\":1880,\"y\":392},{\"x\":1739,\"y\":392}],\"text\":\"Result\"},{\"boundingBox\":[{\"x\":1887,\"y\":348},{\"x\":1916,\"y\":348},{\"x\":1917,\"y\":392},{\"x\":1888,\"y\":392}],\"text\":\"?\"},{\"boundingBox\":[{\"x\":43,\"y\":518},{\"x\":174,\"y\":517},{\"x\":175,\"y\":563},{\"x\":43,\"y\":564}],\"text\":\"User2\"},{\"boundingBox\":[{\"x\":739,\"y\":485},{\"x\":841,\"y\":485},{\"x\":841,\"y\":534},{\"x\":739,\"y\":533}],\"text\":\"Visit\"},{\"boundingBox\":[{\"x\":850,\"y\":485},{\"x\":1044,\"y\":485},{\"x\":1044,\"y\":535},{\"x\":850,\"y\":534}],\"text\":\"location:\"},{\"boundingBox\":[{\"x\":1053,\"y\":485},{\"x\":1095,\"y\":485},{\"x\":1095,\"y\":536},{\"x\":1054,\"y\":535}],\"text\":\"a,\"},{\"boundingBox\":[{\"x\":1104,\"y\":485},{\"x\":1139,\"y\":485},{\"x\":1139,\"y\":536},{\"x\":1105,\"y\":536}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1731,\"y\":488},{\"x\":1923,\"y\":487},{\"x\":1924,\"y\":530},{\"x\":1729,\"y\":531}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":741,\"y\":550},{\"x\":956,\"y\":551},{\"x\":956,\"y\":606},{\"x\":740,\"y\":602}],\"text\":\"Shopping:\"},{\"boundingBox\":[{\"x\":967,\"y\":551},{\"x\":1011,\"y\":551},{\"x\":1011,\"y\":605},{\"x\":966,\"y\":606}],\"text\":\"b,\"},{\"boundingBox\":[{\"x\":1021,\"y\":552},{\"x\":1056,\"y\":552},{\"x\":1056,\"y\":604},{\"x\":1021,\"y\":605}],\"text\":\"c\"},{\"boundingBox\":[{\"x\":1737,\"y\":551},{\"x\":1878,\"y\":549},{\"x\":1879,\"y\":597},{\"x\":1737,\"y\":596}],\"text\":\"Result\"},{\"boundingBox\":[{\"x\":1887,\"y\":549},{\"x\":1914,\"y\":549},{\"x\":1915,\"y\":596},{\"x\":1888,\"y\":596}],\"text\":\"?\"},{\"boundingBox\":[{\"x\":1163,\"y\":688},{\"x\":1266,\"y\":687},{\"x\":1266,\"y\":734},{\"x\":1163,\"y\":733}],\"text\":\"Visit\"},{\"boundingBox\":[{\"x\":1274,\"y\":687},{\"x\":1472,\"y\":688},{\"x\":1472,\"y\":736},{\"x\":1275,\"y\":734}],\"text\":\"location:\"},{\"boundingBox\":[{\"x\":1481,\"y\":688},{\"x\":1522,\"y\":689},{\"x\":1522,\"y\":737},{\"x\":1481,\"y\":736}],\"text\":\"a,\"},{\"boundingBox\":[{\"x\":1531,\"y\":689},{\"x\":1566,\"y\":689},{\"x\":1566,\"y\":737},{\"x\":1531,\"y\":737}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1731,\"y\":690},{\"x\":1922,\"y\":690},{\"x\":1921,\"y\":733},{\"x\":1729,\"y\":733}],\"text\":\"Forecast\"},{\"boundingBox\":[{\"x\":40,\"y\":722},{\"x\":172,\"y\":720},{\"x\":173,\"y\":764},{\"x\":42,\"y\":765}],\"text\":\"User3\"},{\"boundingBox\":[{\"x\":1163,\"y\":754},{\"x\":1381,\"y\":754},{\"x\":1382,\"y\":806},{\"x\":1162,\"y\":806}],\"text\":\"Shopping:\"},{\"boundingBox\":[{\"x\":1391,\"y\":754},{\"x\":1435,\"y\":754},{\"x\":1437,\"y\":806},{\"x\":1392,\"y\":806}],\"text\":\"b,\"},{\"boundingBox\":[{\"x\":1446,\"y\":754},{\"x\":1480,\"y\":754},{\"x\":1481,\"y\":805},{\"x\":1447,\"y\":805}],\"text\":\"c\"},{\"boundingBox\":[{\"x\":1738,\"y\":754},{\"x\":1879,\"y\":753},{\"x\":1879,\"y\":797},{\"x\":1738,\"y\":798}],\"text\":\"Result\"},{\"boundingBox\":[{\"x\":1887,\"y\":753},{\"x\":1914,\"y\":752},{\"x\":1915,\"y\":796},{\"x\":1888,\"y\":797}],\"text\":\"?\"}]}",
        "{\"language\":\"en\",\"text\":\"Time series- ------ -- - - - - - -- -- Input feature convolutional layer Multi-window Max-pooling Output layer layer\",\"lines\":[{\"boundingBox\":[{\"x\":46,\"y\":254},{\"x\":45,\"y\":535},{\"x\":0,\"y\":535},{\"x\":0,\"y\":254}],\"text\":\"Time series-\"},{\"boundingBox\":[{\"x\":828,\"y\":195},{\"x\":923,\"y\":148},{\"x\":926,\"y\":156},{\"x\":831,\"y\":202}],\"text\":\"------\"},{\"boundingBox\":[{\"x\":877,\"y\":416},{\"x\":907,\"y\":403},{\"x\":910,\"y\":408},{\"x\":879,\"y\":421}],\"text\":\"--\"},{\"boundingBox\":[{\"x\":908,\"y\":402},{\"x\":923,\"y\":396},{\"x\":925,\"y\":401},{\"x\":910,\"y\":407}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":827,\"y\":438},{\"x\":848,\"y\":429},{\"x\":850,\"y\":437},{\"x\":829,\"y\":446}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":847,\"y\":429},{\"x\":862,\"y\":424},{\"x\":864,\"y\":431},{\"x\":849,\"y\":436}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":861,\"y\":422},{\"x\":878,\"y\":417},{\"x\":880,\"y\":422},{\"x\":862,\"y\":428}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1437,\"y\":405},{\"x\":1456,\"y\":396},{\"x\":1458,\"y\":407},{\"x\":1439,\"y\":415}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1392,\"y\":433},{\"x\":1430,\"y\":413},{\"x\":1435,\"y\":423},{\"x\":1395,\"y\":443}],\"text\":\"-- --\"},{\"boundingBox\":[{\"x\":194,\"y\":780},{\"x\":498,\"y\":779},{\"x\":498,\"y\":830},{\"x\":194,\"y\":831}],\"text\":\"Input feature\"},{\"boundingBox\":[{\"x\":1004,\"y\":805},{\"x\":1328,\"y\":802},{\"x\":1328,\"y\":848},{\"x\":1005,\"y\":851}],\"text\":\"convolutional\"},{\"boundingBox\":[{\"x\":1106,\"y\":874},{\"x\":1231,\"y\":874},{\"x\":1231,\"y\":922},{\"x\":1106,\"y\":921}],\"text\":\"layer\"},{\"boundingBox\":[{\"x\":997,\"y\":732},{\"x\":1997,\"y\":752},{\"x\":1996,\"y\":812},{\"x\":996,\"y\":782}],\"text\":\"Multi-window Max-pooling Output layer\"},{\"boundingBox\":[{\"x\":1445,\"y\":808},{\"x\":1567,\"y\":808},{\"x\":1565,\"y\":855},{\"x\":1446,\"y\":854}],\"text\":\"layer\"}],\"words\":[{\"boundingBox\":[{\"x\":46,\"y\":263},{\"x\":47,\"y\":382},{\"x\":1,\"y\":382},{\"x\":1,\"y\":264}],\"text\":\"Time\"},{\"boundingBox\":[{\"x\":47,\"y\":391},{\"x\":43,\"y\":533},{\"x\":2,\"y\":534},{\"x\":1,\"y\":391}],\"text\":\"series-\"},{\"boundingBox\":[{\"x\":840,\"y\":190},{\"x\":923,\"y\":149},{\"x\":925,\"y\":157},{\"x\":842,\"y\":197}],\"text\":\"------\"},{\"boundingBox\":[{\"x\":887,\"y\":412},{\"x\":908,\"y\":404},{\"x\":910,\"y\":409},{\"x\":889,\"y\":418}],\"text\":\"--\"},{\"boundingBox\":[{\"x\":920,\"y\":398},{\"x\":924,\"y\":396},{\"x\":925,\"y\":401},{\"x\":922,\"y\":403}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":830,\"y\":436},{\"x\":838,\"y\":433},{\"x\":841,\"y\":440},{\"x\":833,\"y\":444}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":856,\"y\":425},{\"x\":861,\"y\":424},{\"x\":864,\"y\":431},{\"x\":858,\"y\":433}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":863,\"y\":422},{\"x\":868,\"y\":420},{\"x\":870,\"y\":426},{\"x\":864,\"y\":428}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1448,\"y\":399},{\"x\":1455,\"y\":396},{\"x\":1459,\"y\":406},{\"x\":1453,\"y\":409}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1392,\"y\":434},{\"x\":1413,\"y\":422},{\"x\":1416,\"y\":432},{\"x\":1395,\"y\":443}],\"text\":\"--\"},{\"boundingBox\":[{\"x\":1420,\"y\":419},{\"x\":1431,\"y\":414},{\"x\":1434,\"y\":424},{\"x\":1423,\"y\":429}],\"text\":\"--\"},{\"boundingBox\":[{\"x\":195,\"y\":781},{\"x\":320,\"y\":780},{\"x\":320,\"y\":832},{\"x\":195,\"y\":832}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":330,\"y\":780},{\"x\":498,\"y\":780},{\"x\":499,\"y\":831},{\"x\":330,\"y\":832}],\"text\":\"feature\"},{\"boundingBox\":[{\"x\":1005,\"y\":808},{\"x\":1327,\"y\":802},{\"x\":1326,\"y\":850},{\"x\":1006,\"y\":851}],\"text\":\"convolutional\"},{\"boundingBox\":[{\"x\":1106,\"y\":874},{\"x\":1231,\"y\":874},{\"x\":1231,\"y\":922},{\"x\":1106,\"y\":921}],\"text\":\"layer\"},{\"boundingBox\":[{\"x\":997,\"y\":733},{\"x\":1330,\"y\":734},{\"x\":1330,\"y\":787},{\"x\":997,\"y\":780}],\"text\":\"Multi-window\"},{\"boundingBox\":[{\"x\":1351,\"y\":734},{\"x\":1669,\"y\":743},{\"x\":1669,\"y\":798},{\"x\":1351,\"y\":787}],\"text\":\"Max-pooling\"},{\"boundingBox\":[{\"x\":1702,\"y\":744},{\"x\":1865,\"y\":752},{\"x\":1865,\"y\":807},{\"x\":1702,\"y\":800}],\"text\":\"Output\"},{\"boundingBox\":[{\"x\":1874,\"y\":752},{\"x\":1995,\"y\":759},{\"x\":1995,\"y\":813},{\"x\":1874,\"y\":807}],\"text\":\"layer\"},{\"boundingBox\":[{\"x\":1445,\"y\":808},{\"x\":1564,\"y\":808},{\"x\":1564,\"y\":855},{\"x\":1445,\"y\":854}],\"text\":\"layer\"}]}",
        "{\"language\":\"en\",\"text\":\"The existing Users Latent location- historical factor based location data model marketing resources Time series Features of User feature resources Training process preferences model Input Output CNN A new location-based Features of marketing resources new resources process Language CNN Results Recommending model User preferences\",\"lines\":[{\"boundingBox\":[{\"x\":314,\"y\":43},{\"x\":621,\"y\":46},{\"x\":620,\"y\":105},{\"x\":314,\"y\":100}],\"text\":\"The existing\"},{\"boundingBox\":[{\"x\":1160,\"y\":80},{\"x\":1305,\"y\":82},{\"x\":1305,\"y\":132},{\"x\":1162,\"y\":131}],\"text\":\"Users\"},{\"boundingBox\":[{\"x\":1617,\"y\":81},{\"x\":1779,\"y\":81},{\"x\":1779,\"y\":130},{\"x\":1616,\"y\":129}],\"text\":\"Latent\"},{\"boundingBox\":[{\"x\":357,\"y\":115},{\"x\":573,\"y\":117},{\"x\":572,\"y\":170},{\"x\":357,\"y\":168}],\"text\":\"location-\"},{\"boundingBox\":[{\"x\":1142,\"y\":141},{\"x\":1373,\"y\":142},{\"x\":1372,\"y\":194},{\"x\":1142,\"y\":194}],\"text\":\"historical\"},{\"boundingBox\":[{\"x\":1630,\"y\":139},{\"x\":1771,\"y\":142},{\"x\":1768,\"y\":191},{\"x\":1630,\"y\":191}],\"text\":\"factor\"},{\"boundingBox\":[{\"x\":394,\"y\":188},{\"x\":540,\"y\":187},{\"x\":539,\"y\":240},{\"x\":395,\"y\":242}],\"text\":\"based\"},{\"boundingBox\":[{\"x\":1098,\"y\":203},{\"x\":1420,\"y\":204},{\"x\":1420,\"y\":256},{\"x\":1097,\"y\":254}],\"text\":\"location data\"},{\"boundingBox\":[{\"x\":1620,\"y\":206},{\"x\":1777,\"y\":202},{\"x\":1778,\"y\":249},{\"x\":1621,\"y\":253}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":339,\"y\":263},{\"x\":595,\"y\":263},{\"x\":595,\"y\":322},{\"x\":339,\"y\":321}],\"text\":\"marketing\"},{\"boundingBox\":[{\"x\":348,\"y\":344},{\"x\":581,\"y\":342},{\"x\":581,\"y\":385},{\"x\":348,\"y\":387}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":571,\"y\":387},{\"x\":857,\"y\":387},{\"x\":857,\"y\":441},{\"x\":571,\"y\":441}],\"text\":\"Time series\"},{\"boundingBox\":[{\"x\":1228,\"y\":390},{\"x\":1513,\"y\":388},{\"x\":1513,\"y\":440},{\"x\":1228,\"y\":441}],\"text\":\"Features of\"},{\"boundingBox\":[{\"x\":1675,\"y\":392},{\"x\":1792,\"y\":391},{\"x\":1790,\"y\":440},{\"x\":1673,\"y\":440}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":631,\"y\":451},{\"x\":798,\"y\":451},{\"x\":799,\"y\":500},{\"x\":631,\"y\":500}],\"text\":\"feature\"},{\"boundingBox\":[{\"x\":1250,\"y\":470},{\"x\":1483,\"y\":469},{\"x\":1483,\"y\":511},{\"x\":1250,\"y\":512}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":52,\"y\":718},{\"x\":59,\"y\":304},{\"x\":116,\"y\":305},{\"x\":111,\"y\":719}],\"text\":\"Training process\"},{\"boundingBox\":[{\"x\":1587,\"y\":464},{\"x\":1877,\"y\":463},{\"x\":1878,\"y\":519},{\"x\":1587,\"y\":522}],\"text\":\"preferences\"},{\"boundingBox\":[{\"x\":636,\"y\":514},{\"x\":797,\"y\":511},{\"x\":798,\"y\":559},{\"x\":636,\"y\":561}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":528,\"y\":880},{\"x\":659,\"y\":880},{\"x\":659,\"y\":936},{\"x\":530,\"y\":937}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":1396,\"y\":879},{\"x\":1573,\"y\":880},{\"x\":1573,\"y\":938},{\"x\":1396,\"y\":937}],\"text\":\"Output\"},{\"boundingBox\":[{\"x\":950,\"y\":1116},{\"x\":1084,\"y\":1116},{\"x\":1085,\"y\":1162},{\"x\":950,\"y\":1162}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":208,\"y\":1277},{\"x\":738,\"y\":1276},{\"x\":738,\"y\":1330},{\"x\":208,\"y\":1331}],\"text\":\"A new location-based\"},{\"boundingBox\":[{\"x\":1351,\"y\":1271},{\"x\":1633,\"y\":1269},{\"x\":1634,\"y\":1316},{\"x\":1351,\"y\":1318}],\"text\":\"Features of\"},{\"boundingBox\":[{\"x\":221,\"y\":1353},{\"x\":715,\"y\":1354},{\"x\":715,\"y\":1407},{\"x\":221,\"y\":1406}],\"text\":\"marketing resources\"},{\"boundingBox\":[{\"x\":1436,\"y\":1346},{\"x\":1542,\"y\":1344},{\"x\":1542,\"y\":1380},{\"x\":1437,\"y\":1382}],\"text\":\"new\"},{\"boundingBox\":[{\"x\":1375,\"y\":1409},{\"x\":1606,\"y\":1408},{\"x\":1606,\"y\":1450},{\"x\":1375,\"y\":1450}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":82,\"y\":1544},{\"x\":80,\"y\":1350},{\"x\":131,\"y\":1350},{\"x\":133,\"y\":1543}],\"text\":\"process\"},{\"boundingBox\":[{\"x\":652,\"y\":1460},{\"x\":896,\"y\":1463},{\"x\":895,\"y\":1520},{\"x\":652,\"y\":1518}],\"text\":\"Language\"},{\"boundingBox\":[{\"x\":1034,\"y\":1497},{\"x\":1165,\"y\":1497},{\"x\":1164,\"y\":1546},{\"x\":1035,\"y\":1548}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":1764,\"y\":1496},{\"x\":1953,\"y\":1492},{\"x\":1954,\"y\":1543},{\"x\":1765,\"y\":1547}],\"text\":\"Results\"},{\"boundingBox\":[{\"x\":3,\"y\":1742},{\"x\":8,\"y\":1348},{\"x\":66,\"y\":1349},{\"x\":58,\"y\":1744}],\"text\":\"Recommending\"},{\"boundingBox\":[{\"x\":694,\"y\":1534},{\"x\":849,\"y\":1530},{\"x\":850,\"y\":1583},{\"x\":694,\"y\":1585}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":1431,\"y\":1621},{\"x\":1553,\"y\":1623},{\"x\":1552,\"y\":1673},{\"x\":1431,\"y\":1673}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":1347,\"y\":1690},{\"x\":1636,\"y\":1689},{\"x\":1636,\"y\":1745},{\"x\":1348,\"y\":1747}],\"text\":\"preferences\"}],\"words\":[{\"boundingBox\":[{\"x\":315,\"y\":44},{\"x\":407,\"y\":44},{\"x\":409,\"y\":102},{\"x\":318,\"y\":100}],\"text\":\"The\"},{\"boundingBox\":[{\"x\":418,\"y\":44},{\"x\":621,\"y\":46},{\"x\":621,\"y\":106},{\"x\":420,\"y\":102}],\"text\":\"existing\"},{\"boundingBox\":[{\"x\":1160,\"y\":80},{\"x\":1303,\"y\":81},{\"x\":1302,\"y\":132},{\"x\":1160,\"y\":130}],\"text\":\"Users\"},{\"boundingBox\":[{\"x\":1617,\"y\":82},{\"x\":1778,\"y\":83},{\"x\":1776,\"y\":131},{\"x\":1617,\"y\":130}],\"text\":\"Latent\"},{\"boundingBox\":[{\"x\":357,\"y\":115},{\"x\":571,\"y\":120},{\"x\":571,\"y\":169},{\"x\":358,\"y\":170}],\"text\":\"location-\"},{\"boundingBox\":[{\"x\":1143,\"y\":142},{\"x\":1371,\"y\":144},{\"x\":1371,\"y\":195},{\"x\":1142,\"y\":195}],\"text\":\"historical\"},{\"boundingBox\":[{\"x\":1630,\"y\":139},{\"x\":1769,\"y\":140},{\"x\":1768,\"y\":192},{\"x\":1630,\"y\":190}],\"text\":\"factor\"},{\"boundingBox\":[{\"x\":394,\"y\":188},{\"x\":537,\"y\":187},{\"x\":538,\"y\":241},{\"x\":394,\"y\":242}],\"text\":\"based\"},{\"boundingBox\":[{\"x\":1098,\"y\":204},{\"x\":1299,\"y\":206},{\"x\":1298,\"y\":256},{\"x\":1098,\"y\":254}],\"text\":\"location\"},{\"boundingBox\":[{\"x\":1309,\"y\":206},{\"x\":1420,\"y\":206},{\"x\":1419,\"y\":254},{\"x\":1308,\"y\":256}],\"text\":\"data\"},{\"boundingBox\":[{\"x\":1621,\"y\":208},{\"x\":1775,\"y\":202},{\"x\":1775,\"y\":250},{\"x\":1621,\"y\":253}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":339,\"y\":267},{\"x\":596,\"y\":263},{\"x\":594,\"y\":323},{\"x\":340,\"y\":317}],\"text\":\"marketing\"},{\"boundingBox\":[{\"x\":349,\"y\":345},{\"x\":582,\"y\":343},{\"x\":582,\"y\":384},{\"x\":350,\"y\":386}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":572,\"y\":388},{\"x\":694,\"y\":388},{\"x\":695,\"y\":442},{\"x\":575,\"y\":442}],\"text\":\"Time\"},{\"boundingBox\":[{\"x\":705,\"y\":388},{\"x\":856,\"y\":390},{\"x\":855,\"y\":440},{\"x\":706,\"y\":442}],\"text\":\"series\"},{\"boundingBox\":[{\"x\":1230,\"y\":392},{\"x\":1436,\"y\":390},{\"x\":1436,\"y\":441},{\"x\":1228,\"y\":441}],\"text\":\"Features\"},{\"boundingBox\":[{\"x\":1446,\"y\":390},{\"x\":1513,\"y\":389},{\"x\":1514,\"y\":441},{\"x\":1446,\"y\":441}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":1673,\"y\":391},{\"x\":1790,\"y\":391},{\"x\":1791,\"y\":439},{\"x\":1673,\"y\":440}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":632,\"y\":452},{\"x\":797,\"y\":452},{\"x\":795,\"y\":500},{\"x\":632,\"y\":501}],\"text\":\"feature\"},{\"boundingBox\":[{\"x\":1251,\"y\":472},{\"x\":1483,\"y\":470},{\"x\":1483,\"y\":512},{\"x\":1250,\"y\":512}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":52,\"y\":714},{\"x\":61,\"y\":511},{\"x\":116,\"y\":512},{\"x\":112,\"y\":715}],\"text\":\"Training\"},{\"boundingBox\":[{\"x\":61,\"y\":499},{\"x\":62,\"y\":306},{\"x\":116,\"y\":308},{\"x\":116,\"y\":500}],\"text\":\"process\"},{\"boundingBox\":[{\"x\":1587,\"y\":465},{\"x\":1877,\"y\":465},{\"x\":1876,\"y\":519},{\"x\":1588,\"y\":523}],\"text\":\"preferences\"},{\"boundingBox\":[{\"x\":636,\"y\":517},{\"x\":796,\"y\":512},{\"x\":797,\"y\":560},{\"x\":637,\"y\":561}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":528,\"y\":880},{\"x\":657,\"y\":880},{\"x\":657,\"y\":936},{\"x\":528,\"y\":937}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":1397,\"y\":880},{\"x\":1571,\"y\":882},{\"x\":1569,\"y\":939},{\"x\":1398,\"y\":938}],\"text\":\"Output\"},{\"boundingBox\":[{\"x\":950,\"y\":1116},{\"x\":1079,\"y\":1116},{\"x\":1079,\"y\":1162},{\"x\":950,\"y\":1162}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":208,\"y\":1278},{\"x\":243,\"y\":1278},{\"x\":244,\"y\":1332},{\"x\":209,\"y\":1332}],\"text\":\"A\"},{\"boundingBox\":[{\"x\":254,\"y\":1278},{\"x\":363,\"y\":1278},{\"x\":364,\"y\":1332},{\"x\":255,\"y\":1332}],\"text\":\"new\"},{\"boundingBox\":[{\"x\":374,\"y\":1278},{\"x\":737,\"y\":1277},{\"x\":737,\"y\":1332},{\"x\":374,\"y\":1332}],\"text\":\"location-based\"},{\"boundingBox\":[{\"x\":1353,\"y\":1271},{\"x\":1564,\"y\":1270},{\"x\":1562,\"y\":1317},{\"x\":1352,\"y\":1318}],\"text\":\"Features\"},{\"boundingBox\":[{\"x\":1573,\"y\":1270},{\"x\":1634,\"y\":1269},{\"x\":1632,\"y\":1317},{\"x\":1571,\"y\":1317}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":222,\"y\":1354},{\"x\":466,\"y\":1354},{\"x\":467,\"y\":1407},{\"x\":223,\"y\":1405}],\"text\":\"marketing\"},{\"boundingBox\":[{\"x\":476,\"y\":1354},{\"x\":714,\"y\":1357},{\"x\":714,\"y\":1405},{\"x\":477,\"y\":1407}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":1437,\"y\":1347},{\"x\":1534,\"y\":1345},{\"x\":1533,\"y\":1382},{\"x\":1438,\"y\":1382}],\"text\":\"new\"},{\"boundingBox\":[{\"x\":1376,\"y\":1411},{\"x\":1606,\"y\":1409},{\"x\":1605,\"y\":1451},{\"x\":1377,\"y\":1450}],\"text\":\"resources\"},{\"boundingBox\":[{\"x\":82,\"y\":1545},{\"x\":81,\"y\":1350},{\"x\":131,\"y\":1351},{\"x\":134,\"y\":1544}],\"text\":\"process\"},{\"boundingBox\":[{\"x\":652,\"y\":1461},{\"x\":895,\"y\":1465},{\"x\":894,\"y\":1518},{\"x\":652,\"y\":1516}],\"text\":\"Language\"},{\"boundingBox\":[{\"x\":1034,\"y\":1497},{\"x\":1163,\"y\":1497},{\"x\":1163,\"y\":1547},{\"x\":1034,\"y\":1548}],\"text\":\"CNN\"},{\"boundingBox\":[{\"x\":1765,\"y\":1497},{\"x\":1954,\"y\":1493},{\"x\":1954,\"y\":1545},{\"x\":1766,\"y\":1548}],\"text\":\"Results\"},{\"boundingBox\":[{\"x\":4,\"y\":1743},{\"x\":9,\"y\":1349},{\"x\":67,\"y\":1351},{\"x\":56,\"y\":1744}],\"text\":\"Recommending\"},{\"boundingBox\":[{\"x\":695,\"y\":1536},{\"x\":848,\"y\":1531},{\"x\":847,\"y\":1585},{\"x\":695,\"y\":1585}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":1431,\"y\":1621},{\"x\":1552,\"y\":1622},{\"x\":1552,\"y\":1674},{\"x\":1431,\"y\":1673}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":1348,\"y\":1691},{\"x\":1634,\"y\":1692},{\"x\":1631,\"y\":1744},{\"x\":1348,\"y\":1748}],\"text\":\"preferences\"}]}",
        "{\"language\":\"en\",\"text\":\"User behavior data Pageviews > No Retain data 30,000 Yes Purchase >3 Yes- Retain data 0 No Eliminate data\",\"lines\":[{\"boundingBox\":[{\"x\":74,\"y\":44},{\"x\":727,\"y\":44},{\"x\":727,\"y\":116},{\"x\":74,\"y\":116}],\"text\":\"User behavior data\"},{\"boundingBox\":[{\"x\":169,\"y\":328},{\"x\":620,\"y\":328},{\"x\":621,\"y\":400},{\"x\":169,\"y\":403}],\"text\":\"Pageviews >\"},{\"boundingBox\":[{\"x\":865,\"y\":378},{\"x\":980,\"y\":379},{\"x\":979,\"y\":444},{\"x\":865,\"y\":441}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":1151,\"y\":377},{\"x\":1548,\"y\":376},{\"x\":1549,\"y\":447},{\"x\":1151,\"y\":448}],\"text\":\"Retain data\"},{\"boundingBox\":[{\"x\":280,\"y\":431},{\"x\":521,\"y\":429},{\"x\":522,\"y\":503},{\"x\":280,\"y\":504}],\"text\":\"30,000\"},{\"boundingBox\":[{\"x\":337,\"y\":632},{\"x\":469,\"y\":637},{\"x\":467,\"y\":699},{\"x\":334,\"y\":698}],\"text\":\"Yes\"},{\"boundingBox\":[{\"x\":180,\"y\":885},{\"x\":622,\"y\":884},{\"x\":623,\"y\":956},{\"x\":180,\"y\":958}],\"text\":\"Purchase >3\"},{\"boundingBox\":[{\"x\":850,\"y\":909},{\"x\":997,\"y\":917},{\"x\":994,\"y\":979},{\"x\":847,\"y\":977}],\"text\":\"Yes-\"},{\"boundingBox\":[{\"x\":1151,\"y\":911},{\"x\":1548,\"y\":910},{\"x\":1549,\"y\":980},{\"x\":1151,\"y\":982}],\"text\":\"Retain data\"},{\"boundingBox\":[{\"x\":373,\"y\":1000},{\"x\":419,\"y\":1002},{\"x\":419,\"y\":1051},{\"x\":372,\"y\":1048}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":348,\"y\":1144},{\"x\":455,\"y\":1149},{\"x\":451,\"y\":1213},{\"x\":344,\"y\":1207}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":153,\"y\":1308},{\"x\":656,\"y\":1308},{\"x\":656,\"y\":1375},{\"x\":153,\"y\":1375}],\"text\":\"Eliminate data\"}],\"words\":[{\"boundingBox\":[{\"x\":75,\"y\":45},{\"x\":235,\"y\":44},{\"x\":235,\"y\":117},{\"x\":75,\"y\":117}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":249,\"y\":44},{\"x\":555,\"y\":44},{\"x\":555,\"y\":117},{\"x\":249,\"y\":117}],\"text\":\"behavior\"},{\"boundingBox\":[{\"x\":569,\"y\":44},{\"x\":724,\"y\":45},{\"x\":724,\"y\":116},{\"x\":569,\"y\":117}],\"text\":\"data\"},{\"boundingBox\":[{\"x\":169,\"y\":329},{\"x\":532,\"y\":330},{\"x\":532,\"y\":398},{\"x\":170,\"y\":400}],\"text\":\"Pageviews\"},{\"boundingBox\":[{\"x\":546,\"y\":330},{\"x\":617,\"y\":331},{\"x\":616,\"y\":392},{\"x\":546,\"y\":397}],\"text\":\">\"},{\"boundingBox\":[{\"x\":865,\"y\":378},{\"x\":978,\"y\":379},{\"x\":977,\"y\":444},{\"x\":865,\"y\":442}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":1152,\"y\":377},{\"x\":1376,\"y\":377},{\"x\":1376,\"y\":448},{\"x\":1152,\"y\":449}],\"text\":\"Retain\"},{\"boundingBox\":[{\"x\":1391,\"y\":377},{\"x\":1548,\"y\":376},{\"x\":1548,\"y\":449},{\"x\":1390,\"y\":448}],\"text\":\"data\"},{\"boundingBox\":[{\"x\":281,\"y\":431},{\"x\":520,\"y\":429},{\"x\":520,\"y\":504},{\"x\":281,\"y\":505}],\"text\":\"30,000\"},{\"boundingBox\":[{\"x\":344,\"y\":632},{\"x\":467,\"y\":635},{\"x\":465,\"y\":700},{\"x\":342,\"y\":698}],\"text\":\"Yes\"},{\"boundingBox\":[{\"x\":181,\"y\":888},{\"x\":482,\"y\":886},{\"x\":482,\"y\":958},{\"x\":181,\"y\":956}],\"text\":\"Purchase\"},{\"boundingBox\":[{\"x\":496,\"y\":886},{\"x\":622,\"y\":884},{\"x\":621,\"y\":957},{\"x\":495,\"y\":958}],\"text\":\">3\"},{\"boundingBox\":[{\"x\":854,\"y\":909},{\"x\":994,\"y\":914},{\"x\":992,\"y\":980},{\"x\":851,\"y\":977}],\"text\":\"Yes-\"},{\"boundingBox\":[{\"x\":1152,\"y\":912},{\"x\":1375,\"y\":910},{\"x\":1374,\"y\":982},{\"x\":1152,\"y\":983}],\"text\":\"Retain\"},{\"boundingBox\":[{\"x\":1389,\"y\":910},{\"x\":1546,\"y\":910},{\"x\":1545,\"y\":982},{\"x\":1389,\"y\":982}],\"text\":\"data\"},{\"boundingBox\":[{\"x\":376,\"y\":1000},{\"x\":419,\"y\":1002},{\"x\":416,\"y\":1051},{\"x\":374,\"y\":1049}],\"text\":\"0\"},{\"boundingBox\":[{\"x\":347,\"y\":1144},{\"x\":454,\"y\":1149},{\"x\":450,\"y\":1213},{\"x\":344,\"y\":1207}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":154,\"y\":1311},{\"x\":485,\"y\":1308},{\"x\":483,\"y\":1376},{\"x\":153,\"y\":1375}],\"text\":\"Eliminate\"},{\"boundingBox\":[{\"x\":498,\"y\":1308},{\"x\":654,\"y\":1309},{\"x\":653,\"y\":1376},{\"x\":496,\"y\":1376}],\"text\":\"data\"}]}",
        "{\"language\":\"en\",\"text\":\"Validation accuracy -Testing accuary Training accuracy 98 96 94 92 Accuracy rate/% 06 88 86 84 82 1 2 3 4 5 6 7 Number of iterations\",\"lines\":[{\"boundingBox\":[{\"x\":179,\"y\":0},{\"x\":492,\"y\":6},{\"x\":491,\"y\":36},{\"x\":179,\"y\":30}],\"text\":\"Validation accuracy\"},{\"boundingBox\":[{\"x\":495,\"y\":1},{\"x\":824,\"y\":5},{\"x\":824,\"y\":37},{\"x\":495,\"y\":33}],\"text\":\"-Testing accuary\"},{\"boundingBox\":[{\"x\":186,\"y\":56},{\"x\":458,\"y\":59},{\"x\":457,\"y\":91},{\"x\":186,\"y\":88}],\"text\":\"Training accuracy\"},{\"boundingBox\":[{\"x\":46,\"y\":122},{\"x\":80,\"y\":122},{\"x\":80,\"y\":149},{\"x\":46,\"y\":149}],\"text\":\"98\"},{\"boundingBox\":[{\"x\":46,\"y\":162},{\"x\":81,\"y\":162},{\"x\":81,\"y\":189},{\"x\":46,\"y\":189}],\"text\":\"96\"},{\"boundingBox\":[{\"x\":46,\"y\":204},{\"x\":80,\"y\":204},{\"x\":80,\"y\":230},{\"x\":45,\"y\":230}],\"text\":\"94\"},{\"boundingBox\":[{\"x\":47,\"y\":243},{\"x\":81,\"y\":245},{\"x\":80,\"y\":271},{\"x\":46,\"y\":269}],\"text\":\"92\"},{\"boundingBox\":[{\"x\":3,\"y\":426},{\"x\":3,\"y\":171},{\"x\":34,\"y\":171},{\"x\":35,\"y\":426}],\"text\":\"Accuracy rate/%\"},{\"boundingBox\":[{\"x\":76,\"y\":312},{\"x\":44,\"y\":312},{\"x\":45,\"y\":283},{\"x\":77,\"y\":284}],\"text\":\"06\"},{\"boundingBox\":[{\"x\":48,\"y\":323},{\"x\":80,\"y\":326},{\"x\":79,\"y\":353},{\"x\":46,\"y\":350}],\"text\":\"88\"},{\"boundingBox\":[{\"x\":47,\"y\":366},{\"x\":82,\"y\":368},{\"x\":80,\"y\":395},{\"x\":45,\"y\":392}],\"text\":\"86\"},{\"boundingBox\":[{\"x\":47,\"y\":409},{\"x\":80,\"y\":410},{\"x\":78,\"y\":434},{\"x\":45,\"y\":432}],\"text\":\"84\"},{\"boundingBox\":[{\"x\":48,\"y\":448},{\"x\":80,\"y\":447},{\"x\":81,\"y\":475},{\"x\":48,\"y\":475}],\"text\":\"82\"},{\"boundingBox\":[{\"x\":96,\"y\":496},{\"x\":113,\"y\":497},{\"x\":113,\"y\":514},{\"x\":96,\"y\":513}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":229,\"y\":493},{\"x\":248,\"y\":492},{\"x\":247,\"y\":515},{\"x\":229,\"y\":516}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":361,\"y\":495},{\"x\":378,\"y\":496},{\"x\":377,\"y\":515},{\"x\":359,\"y\":514}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":495,\"y\":492},{\"x\":515,\"y\":493},{\"x\":513,\"y\":515},{\"x\":494,\"y\":513}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":626,\"y\":494},{\"x\":648,\"y\":493},{\"x\":647,\"y\":514},{\"x\":625,\"y\":515}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":759,\"y\":495},{\"x\":779,\"y\":494},{\"x\":780,\"y\":511},{\"x\":759,\"y\":512}],\"text\":\"6\"},{\"boundingBox\":[{\"x\":893,\"y\":492},{\"x\":912,\"y\":494},{\"x\":911,\"y\":514},{\"x\":892,\"y\":512}],\"text\":\"7\"},{\"boundingBox\":[{\"x\":349,\"y\":538},{\"x\":664,\"y\":539},{\"x\":664,\"y\":569},{\"x\":349,\"y\":568}],\"text\":\"Number of iterations\"}],\"words\":[{\"boundingBox\":[{\"x\":191,\"y\":1},{\"x\":346,\"y\":4},{\"x\":346,\"y\":33},{\"x\":192,\"y\":31}],\"text\":\"Validation\"},{\"boundingBox\":[{\"x\":352,\"y\":4},{\"x\":493,\"y\":8},{\"x\":492,\"y\":36},{\"x\":352,\"y\":33}],\"text\":\"accuracy\"},{\"boundingBox\":[{\"x\":497,\"y\":1},{\"x\":697,\"y\":5},{\"x\":697,\"y\":36},{\"x\":498,\"y\":34}],\"text\":\"-Testing\"},{\"boundingBox\":[{\"x\":704,\"y\":5},{\"x\":824,\"y\":7},{\"x\":824,\"y\":37},{\"x\":704,\"y\":36}],\"text\":\"accuary\"},{\"boundingBox\":[{\"x\":189,\"y\":57},{\"x\":317,\"y\":57},{\"x\":317,\"y\":91},{\"x\":188,\"y\":89}],\"text\":\"Training\"},{\"boundingBox\":[{\"x\":323,\"y\":58},{\"x\":457,\"y\":62},{\"x\":458,\"y\":91},{\"x\":323,\"y\":91}],\"text\":\"accuracy\"},{\"boundingBox\":[{\"x\":46,\"y\":122},{\"x\":78,\"y\":122},{\"x\":78,\"y\":149},{\"x\":46,\"y\":149}],\"text\":\"98\"},{\"boundingBox\":[{\"x\":46,\"y\":162},{\"x\":80,\"y\":162},{\"x\":80,\"y\":189},{\"x\":46,\"y\":189}],\"text\":\"96\"},{\"boundingBox\":[{\"x\":45,\"y\":204},{\"x\":80,\"y\":204},{\"x\":80,\"y\":230},{\"x\":45,\"y\":230}],\"text\":\"94\"},{\"boundingBox\":[{\"x\":47,\"y\":243},{\"x\":80,\"y\":245},{\"x\":78,\"y\":271},{\"x\":46,\"y\":269}],\"text\":\"92\"},{\"boundingBox\":[{\"x\":4,\"y\":426},{\"x\":4,\"y\":283},{\"x\":35,\"y\":283},{\"x\":35,\"y\":426}],\"text\":\"Accuracy\"},{\"boundingBox\":[{\"x\":4,\"y\":277},{\"x\":4,\"y\":179},{\"x\":32,\"y\":179},{\"x\":35,\"y\":277}],\"text\":\"rate/%\"},{\"boundingBox\":[{\"x\":76,\"y\":312},{\"x\":44,\"y\":311},{\"x\":44,\"y\":283},{\"x\":76,\"y\":283}],\"text\":\"06\"},{\"boundingBox\":[{\"x\":50,\"y\":323},{\"x\":80,\"y\":326},{\"x\":78,\"y\":353},{\"x\":47,\"y\":350}],\"text\":\"88\"},{\"boundingBox\":[{\"x\":46,\"y\":366},{\"x\":82,\"y\":368},{\"x\":80,\"y\":395},{\"x\":45,\"y\":392}],\"text\":\"86\"},{\"boundingBox\":[{\"x\":48,\"y\":409},{\"x\":80,\"y\":410},{\"x\":78,\"y\":434},{\"x\":47,\"y\":432}],\"text\":\"84\"},{\"boundingBox\":[{\"x\":48,\"y\":447},{\"x\":79,\"y\":447},{\"x\":80,\"y\":475},{\"x\":48,\"y\":475}],\"text\":\"82\"},{\"boundingBox\":[{\"x\":99,\"y\":496},{\"x\":113,\"y\":497},{\"x\":112,\"y\":514},{\"x\":98,\"y\":513}],\"text\":\"1\"},{\"boundingBox\":[{\"x\":229,\"y\":493},{\"x\":247,\"y\":492},{\"x\":248,\"y\":514},{\"x\":230,\"y\":515}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":361,\"y\":495},{\"x\":378,\"y\":496},{\"x\":377,\"y\":515},{\"x\":360,\"y\":514}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":501,\"y\":492},{\"x\":514,\"y\":493},{\"x\":512,\"y\":515},{\"x\":499,\"y\":514}],\"text\":\"4\"},{\"boundingBox\":[{\"x\":628,\"y\":494},{\"x\":648,\"y\":493},{\"x\":649,\"y\":513},{\"x\":629,\"y\":514}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":766,\"y\":494},{\"x\":778,\"y\":494},{\"x\":779,\"y\":511},{\"x\":767,\"y\":512}],\"text\":\"6\"},{\"boundingBox\":[{\"x\":898,\"y\":492},{\"x\":913,\"y\":494},{\"x\":911,\"y\":514},{\"x\":896,\"y\":512}],\"text\":\"7\"},{\"boundingBox\":[{\"x\":351,\"y\":539},{\"x\":475,\"y\":539},{\"x\":474,\"y\":569},{\"x\":350,\"y\":569}],\"text\":\"Number\"},{\"boundingBox\":[{\"x\":481,\"y\":539},{\"x\":514,\"y\":539},{\"x\":514,\"y\":569},{\"x\":480,\"y\":569}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":520,\"y\":539},{\"x\":664,\"y\":540},{\"x\":664,\"y\":569},{\"x\":520,\"y\":569}],\"text\":\"iterations\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 01 May 2019\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":16},{\"x\":894,\"y\":16},{\"x\":894,\"y\":72},{\"x\":0,\"y\":70}],\"text\":\"Published online: 01 May 2019\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":18},{\"x\":282,\"y\":17},{\"x\":282,\"y\":71},{\"x\":0,\"y\":68}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":292,\"y\":17},{\"x\":499,\"y\":16},{\"x\":499,\"y\":73},{\"x\":292,\"y\":71}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":509,\"y\":16},{\"x\":587,\"y\":16},{\"x\":588,\"y\":73},{\"x\":509,\"y\":73}],\"text\":\"01\"},{\"boundingBox\":[{\"x\":597,\"y\":16},{\"x\":738,\"y\":16},{\"x\":739,\"y\":73},{\"x\":598,\"y\":73}],\"text\":\"May\"},{\"boundingBox\":[{\"x\":748,\"y\":16},{\"x\":892,\"y\":17},{\"x\":893,\"y\":73},{\"x\":749,\"y\":73}],\"text\":\"2019\"}]}"
      ]
    },
    {
      "@search.score": 3.8810685,
      "content": "\nQER: a new feature selection method \nfor sentiment analysis\nTuba Parlar1* , Selma Ayşe Özel2 and Fei Song3\n\nIntroduction\n“What other people think” has always been an important piece of information for most \nof us during the decision making process [1]. The Internet and social media provide a \nmajor source of information about people’s opinions. Due to the rapidly-growing num-\nber of online documents, it becomes both time-consuming and hard to obtain and ana-\nlyze the desired opinionated information. Turkey is among the top 20 countries with the \nhighest numbers of Internet users according to the Internet World Stats.1 The exploding \ngrowth in the Internet users is one of the main reasons that sentiment analysis for differ-\nent languages and domains becomes an actively-studied area for many researchers \n[2–6].\n\nSentiment analysis (SA) is a natural language processing task that classifies the senti-\nments expressed in review documents as “positive” or “negative”. In general, SA is con-\nsidered as a two-class classification problem. However, some researchers use “neutral” as \n\n1 http://www.internetworldstats.com/.\n\nAbstract \n\nSentiment analysis is about the classification of sentiments expressed in review docu-\nments. In order to improve the classification accuracy, feature selection methods are \noften used to rank features so that non-informative and noisy features with low ranks \ncan be removed. In this study, we propose a new feature selection method, called \nquery expansion ranking, which is based on query expansion term weighting meth-\nods from the field of information retrieval. We compare our proposed method with \nother widely used feature selection methods, including Chi square, information gain, \ndocument frequency difference, and optimal orthogonal centroid, using four classi-\nfiers: naïve Bayes multinomial, support vector machines, maximum entropy model-\nling, and decision trees. We test them on movie and multiple kinds of product reviews \nfor both Turkish and English languages so that we can show their performances for \ndifferent domains, languages, and classifiers. We observe that our proposed method \nachieves consistently better performance than other feature selection methods, and \nquery expansion ranking, Chi square, information gain, document frequency difference \nmethods tend to produce better results for both the English and Turkish reviews when \ntested using naïve Bayes multinomial classifier.\n\nKeywords: Sentiment analysis, Feature selection, Machine learning, Text classification\n\nOpen Access\n\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nR E S E A R C H\n\nParlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \nhttps://doi.org/10.1186/s13673-018-0135-8\n\n*Correspondence:   \ntparlar@mku.edu.tr \n1 Department \nof Mathematics, Mustafa \nKemal University, Antakya, \nHatay, Turkey\nFull list of author information \nis available at the end of the \narticle\n\n\n\nhttp://orcid.org/0000-0002-8004-6150\nhttp://www.internetworldstats.com/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-018-0135-8&domain=pdf\n\n\nPage 2 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nthe third class label. There are a number of studies about sentiment analysis that use dif-\nferent approaches for data preprocessing, feature selection, and sentiment classification \n[1, 3, 4, 6–10]. The statistical methods such as Chi square (CHI2) and information gain \n(IG) are used to eliminate unnecessary or irrelevant features so that the classification \nperformance can be improved [11]. Supervised learning methods including naïve Bayes \n(NB), support vector machines (SVM), decision trees (DT), and maximum entropy mod-\nelling (MEM) are used to classify the sentiments of the reviews.\n\nAlthough SA can be considered as a text classification task, it has some differences \nfrom the traditional topic-based text classification. For example, instead of saying: “This \ncamera is great. It takes great pictures. The LCD screen is great. I love this camera” in a \nreview document, people are more likely to write: “This camera is great. It takes breath-\ntaking pictures. The LCD screen is bright and clear. I love this camera.” [8]. As can be \nseen, sentiment-expressing words like “great” are not so frequent within a particular \nreview, but can be more frequent across different reviews, and a good feature selection \nmethod for SA should take this observation into account.\n\nIn this paper, we propose a new feature selection method, called query expansion rank-\ning (QER) which is especially developed for reducing dimensionality of feature space of \nSA problems. The aim of this study is to show that our proposed method is effective for \nSA from review texts written in different languages (e.g., Turkish, English) and domains \n(e.g., movie reviews, book reviews, kitchen appliances reviews, etc.). QER is based on \nquery expansion term weighting methods used to improve the search performance of \ninformation retrieval systems [12, 13] and to evaluate its effectiveness as a feature selec-\ntor in SA, we compare it with other common feature selection methods, including CHI2, \nIG, document frequency difference (DFD), and optimal orthogonal centroid (OCFS), \nalong with four text classifiers: naïve Bayes multinomial (NBM), SVM, DT, and MEM, \nover ten different review documents datasets. Our goal is to examine whether these fea-\nture selection methods can reduce the feature sizes and improve the classification accu-\nracy of sentiment analysis with respect to different document domains, languages, and \nclassifiers.\n\nThe rest of the paper is organized as follows. “Related work” reviews the related work \non sentiment analysis. “Methods” presents the methods that we used for our study, \nincluding the new feature selection method we proposed. “Experiments and results” \ndescribes the experimental settings, datasets, performance measures, and testing results. \nFinally, “Conclusion” concludes the paper.\n\nRelated work\nSA is an important topic in Natural Language Processing and Artificial Intelligence. \nAlso known as opinion mining, SA mines people’s opinions, sentiments, evalua-\ntions, and emotions about entities such as products, services, organizations, individu-\nals, issues, and events, as well as their related attributes. This kind of analysis has many \nuseful applications. For example, it determines a product’s popularity according to \nthe user’s reviews. If the overall sentiments are negative, further analysis may be per-\nformed to identify which features contribute to the negative ratings so companies can \nreshape their businesses. Numerous studies have been done for sentiment analysis in \ndifferent domains, languages, and approaches [3–5, 8–10, 14–17]. Among these studies, \n\n\n\nPage 3 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nthe machine learning approaches are more popular since the models can be automati-\ncally trained and improved with the training datasets. Pang et  al. [4] apply supervised \nmachine learning methods such as NB and SVM to sentiment classification. NB, SVM, \nMEM, and DT are some of the commonly used machine learning approaches [4, 7–9, \n14]. Feature selection methods are used to rank features so that non-informative features \ncan be removed to improve the classification performance [18]. Some researchers have \ninvestigated the effects of feature selection for sentiment analysis [3, 8–10, 19–25]. For \nexample, Yang and Yu [3] examine IG for feature selection and evaluate its performance \nusing NB, SVM, and C4.5 (popular implementation for DT) classifiers. Nicholls et al. [8] \ncompare their proposed DFD feature selection method against other feature selection \nmethods, including CHI2, OCFS [26], and count difference using the MEM classifier. \nAgarwal et  al. [9] investigate minimum redundancy maximum relevancy (mRMR) and \nIG methods for sentiment classification using NBM and SVM classifiers. The results \nshow that mRMR performs better than IG for feature selection, and NBM performs bet-\nter than SVM in accuracy and execution time. Abbasi et al. [22] examine a new feature \nselection method called entropy weighted genetic algorithm (EWGA) and compare the \nperformance of this method using information gain feature selection method. EWGA \nachieves a relatively high accuracy of 91.7% using SVM classifier. Xia et  al. [24] design \ntwo types of feature sets: POS based and word relation based. Their word relation based \nmethod improves an accuracy of 87.7 and 85.15% on movie and product datasets. Bai \n[25] proposes a Tabu heuristic search-enhanced Markov blanket model that provides a \nvocabulary to extract sentiment features. Their method achieves an accuracy of 92.7% \nfor the movie review dataset. Mladenovic et al. [16] propose a feature selection method \nthat is based on mapping of a large number of related features to a few features. Their \nproposed method improves the classification performance using unigram features \nwith 95% average accuracy. Zheng et  al. [27] perform comparative experiments to test \ntheir proposed improved document frequency feature selection method. Their method \nachieves significant improvement in sentiment analysis of Chinese online reviews with \nan accuracy of 97.3%.\n\nMost of the SA studies listed above focus on the English language. Only few studies \nhave been done on SA for the Turkish language [6, 10, 19, 28–31]. The Turkish language \nbelongs to the Altaic branch of the Ural-Altaic family of languages and is mainly used in \nthe Republic of Turkey. Turkish is an agglutinative language similar to Finnish and Hun-\ngarian, where a single word can be translated into a relatively longer sentence in English \n[32]. For instance, word “karşılaştırmalısın” in Turkish can be expressed as “you must \nmake (something) compare” in English. As Turkish and English have different charac-\nteristics, methods developed for SA in English need to be tested for Turkish. Among \nthe few researchers who investigate the effects of feature selection on the SA of Turkish \nreviews, Boynukalın [29] applies Weighted Log Likelihood Ratio (WLLR) to reduce fea-\nture space with NB, Complementary NB, and SVM classifiers for the emotional analysis \nusing the combinations of n-grams where sequences of n words are considered together. \nIt is shown that WLLR helps to improve the accuracy with reduced feature sizes. Akba \net  al. [19] implement and compare the performance of reduced feature sizes using two \nfeature selection methods: CHI2 and IG with NB and SVM classifiers. They show that \nfeature selection methods improve the classification accuracy.\n\n\n\nPage 4 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nOur aim is to propose a new feature selection method for the SA of Turkish and Eng-\nlish reviews. We presented an initial version of this method in [10] where we employ \nonly product review dataset in Turkish and compare our method with CHI2 and DFD \nby using only one classifier. We now extend it to more datasets for Turkish, and also \ninvestigate the performance of our method in English datasets to show that our method \nis language independent. We further include more feature selection methods especially \ndeveloped for SA and compare the performance of our proposed method using NBM, \nSVM, MEM, and DT classifiers along with statistical analysis to prove that our method is \nclassifier independent.\n\nMethods\nMachine learning algorithms\n\nFor sentiment classification, we use the Weka [33] data mining tool, which contains the \nfour classifiers we use in our experiments, i.e., NBM, SMO for SVM, J48 for C4.5, and LR \nfor MEM. We choose NBM, SVM, LR, and J48 classification methods due to the follow-\ning reasons: (i) many researchers use NBM for text classification because it is computa-\ntionally efficient [9, 10, 14] and performs well for large vocabulary sizes [34]; (ii) SVM \ntends to perform well for traditional text classification tasks [3, 4, 7, 14, 35]; (iii) LR is \nknown to be equivalent to MEM which is another method used in SA studies [8]; (iv) J48 \nis a well-known decision tree classifier for many classification problems and is used for \nSA [3, 30].\n\nFeature selection\n\nFeature Selection methods have been shown to be useful for text classification in general \nand sentiment analysis in specific [11, 18]. Such methods rank features according to cer-\ntain measures so that non-informative features can be removed, and at the same time, \nthe most valuable features can be kept in order to improve the classification accuracy \nand efficiency. In this study, we consider several feature selection methods, including \ninformation gain, Chi square, document frequency difference, optimal orthogonal cen-\ntroid, and our new query expansion ranking (QER) so that we can compare their effec-\ntiveness for the sentiment analysis.\n\nFeature sizes are selected in the range from 500 to 3000 with 500 increments, com-\npared with the total feature sizes ranging from 8000 to 18,000 for the Turkish review \ndatasets and from 8000 to 38,000 for English review datasets. In our previous study [10], \nwe observed that feature sizes up to 3000 tend to give good classification performance \nimprovement; therefore we choose these feature sizes in our experiments.\n\nInformation gain\n\nInformation gain is one of the most common feature selection methods for sentiment \nanalysis [3, 9, 19, 35], which measures the content of information obtained after knowing \nthe value of a feature in a document. The higher the information gain, the more power \nwe have to discriminate between different classes.\n\nThe content of information can be calculated by the entropy that captures the uncer-\ntainty of a probability distribution for the given classes. Given m number of classes: \nC = {c1,c2,…,cm} the entropy can be given as follows:\n\n\n\nPage 5 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nwhere P(ci) is the probability of how many documents in class ci. If an attribute A has n \ndistinct values: A = {a1,a2,…,an}, then the entropy after the attribute A is observed can be \ndefined as follows:\n\nwhere P(aj) is the probability of how many documents contain the attribute value aj, and \nP(ci|aj) is the probability of how many documents in class ci that contain the attribute \nvalue aj. Based on the definitions above, the information gain for an attribute is simply \nthe difference between the entropy values before and after the attribute is observed:\n\nFor sentiment analysis, we normally classify the reviews into positive and negative cat-\negories, and for each keyword, it either occurs or does not occur in a given document; so \nthe above formulas can be further simplified. Nevertheless, we can cut down the number \nof features in the same way by choosing the keywords that have high information gain \nscores.\n\nChi square (CHI2)\n\nChi square measures the dependence between a feature and a class. A higher score \nimplies that the related class is more dependent on the given feature. Thus, a feature with \na low score is less informative and should be removed [3, 8, 10, 19]. Using the 2-by-2 \ncontingency table for feature f and class c, where A is the number of documents in class c \nthat contains feature f, B is the number of documents in the other class that contains f, C \nis the number of documents in c that does not contain f, D is the number of documents \nin the other class that does not contain f, and N is the total number of documents, then \nthe Chi square score can be defined in the following:\n\nThe Chi square statistics can also be computed between a feature and a class in the \ndataset, which are then combined across all classes to get the scores for each feature as \nfollows:\n\nOne problem with the CHI2 method is that it may produce high scores for rare features \nas long as they are mostly used for one specific class. This is a bit counter-intuitive, since \nrare features are not frequently used in text and thus do not have a big impact for text \n\n(1)H(C) = −\nm\n∑\n\ni=1\n\nP(ci) log2 P(ci)\n\n(2)H(C|A) =\nn\n\n∑\n\nj=1\n\n(\n\n−P(aj)\n\nm\n∑\n\ni=1\n\nP(ci|aj) log2P(ci|aj)\n\n)\n\n(3)IG(A) = H(C) − H(C|A)\n\n(4)χ2\n(\n\nf , c\n)\n\n=\nN(AD − CB)2\n\n(A + C)(B + D)(A + B)(C + D)\n\n(5)χ\n2\n(f ) =\n\nm\n∑\n\ni=1\n\nP(ci)χ\n2\n(f , ci)\n\n\n\nPage 6 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nclassification. For SA, however, this is not a big issue since many sentiment-expressing \nfeatures are not frequently used within an individual review.\n\nDocument frequency difference\n\nInspired by the observation that sentiment-expressing words tends to be less frequent \nwithin a review, but more frequent across different reviews, Nicholls and Song [8] pro-\npose the DFD method that tries to differentiate the features for positive and negative \nclasses, respectively, across a document collection. More specifically, DFD is calculated \nas follows:\n\nwhere DFf+ is the number of documents in the positive class that contain feature f, DF\nf\n− \n\nis the number of documents in the negative class that contain f, and N is the total num-\nber of documents in the dataset. Note that all scores are normalized between 0 and 1; \nso they should be proportional for us to rank the features in a document collection. For \nexample, a non-sentiment word may have similar document frequencies in both posi-\ntive and negative classes, and will get a low score, but a sentiment word for the positive \nclass may have a bigger difference, resulting in a higher score. One limitation of the DFD \nmethod is that it requires an equal or nearly equal number of documents in both classes, \nwhich is more or less true for the datasets used in our experiments.\n\nOptimal orthogonal centroid (OCFS)\n\nOCFS method is an optimized form of the orthogonal centroid algorithm [26]. Docu-\nments are represented as high dimensional vectors where the weights of each dimension \ncorrespond to the importance of the related features, and a centroid is simply the aver-\nage vector for a set of document vectors. OCFS aims at finding a subset of features that \ncan make the sum of distances between all the class means maximized in the selected \nsubspace. The score of a feature f by OCFS is defined in the following [8]:\n\nwhere Nc is the number of documents in class c, N is the number of documents in the \ndataset, mc is the centroid for class c, m is the centroid for the dataset D, and m\n\nf, mc\nf are \n\nthe values of feature f in centroid m, mc respectively. The centroids of m and mc are cal-\nculated as follows:\n\nQuery expansion ranking\n\nQuery expansion ranking method is our proposed feature selection method inspired \nby the query expansion methods from the field of information retrieval (IR). Query \n\n(6)Scoref =\n|DF\n\nf\n+ − DF\n\nf\n−|\n\nN\n\n(7)Scoref =\n∑\n\nc\n\nNc\n\nN\n\n(\n\nm\nf\nc − m\n\nf\n)2\n\n(8)mc =\n∑\n\nxi∈c\nxi\n\nNc\n\n(9)m =\n∑\n\nxi∈D\nxi\n\nN\n\n\n\nPage 7 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nexpansion helps to find more relevant documents for a given query. It does so by adding \nnew terms to the query. The new terms are selected from documents that are relevant \nto the original query so that the expanded query can retrieve more relevant documents. \nMore specifically, terms from the relevant documents are extracted along with some \nscores, and those with the highest scores are included in the expanded query.\n\nWe propose a new feature selection method inspired by the query expansion technique \ndeveloped for probabilistic weighting model proposed by Harman [12]. Harman [12, 36] \nstudies how to assign scores to terms extracted from relevant documents for a given \nquery Q so that high scored terms are used to expand the original query and improve \nprecision of information retrieval strategy. In this method, first, query Q is sent to the \ninformation retrieval system, and then the system returns documents that are found as \nrelevant to the user. Then, user examines the returned documents and marks the ones \nthat are relevant with the query. After that, all the terms in the relevant documents are \nextracted and they are assigned scores by using a score formula as proposed by Har-\nman [12], and top scored k terms are chosen as the most valuable terms to expand the \nquery. Then, the expanded query Q’, which includes the terms in the original query plus \nthe k new terms that have the top-k scores, is sent to the information retrieval system to \nreturn more relevant documents to the original query Q. Equation 10 presents the score \nformula developed by Harman [12] to calculate ranking score of a term f extracted from \nthe set of relevant documents for a given query Q.\n\nwhere pf is the probability of term f in the set of relevant documents for query Q, and qf \nis the probability of term f in the set of non-relevant documents for query Q. These prob-\nability scores are computed according to Robertson and Sparck Jones [13].\n\nWe revise the above score computation method to develop an efficient feature selector \nfor SA. In our feature selection method, we propose a score formula given in Eq. 11 to \ncompute scores for features:\n\nwhere pf is the ratio of positive documents containing feature f and qf is the ratio of \nnegative documents containing feature f, which are computed according to Eqs. 12, 13, \nrespectively:\n\n(10)Scoref = log2\npf\n(\n\n1 − qf\n)\n\n(\n\n1 − pf\n)\n\nqf\n\n(11)Scoref =\npf + qf\n∣\n\n∣pf − qf\n∣\n\n∣\n\n(12)pf =\nDF\n\nf\n+ + 0.5\n\nN+ + 1.0\n\n(13)qf =\nDF\n\nf\n− + 0.5\n\nN− + 0.5\n\n\n\nPage 8 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nwhere DFf+ and DF\nf\n− are the raw counts of documents that contain f in the positive and \n\nnegative classes, respectively and N+ and N− are the numbers of documents in the \npositive and negative classes, respectively. In the probability calculations, we add small \nconstants to the numerators and denominators in Eqs.  12, 13 following Robertson and \nSparck Jones [13] who add similar constants to avoid having zero probabilities. Such a \nmethod is known as data smoothing in statistical language processing.\n\nIn QER feature selection method, scores of features are computed before the features \nhaving the lowest scores are selected and used in the classification process. When a fea-\nture has low score, the difference between the probabilities for the positive and negative \nclasses is high; therefore the feature is more class specific and more valuable for clas-\nsification process. Among the feature selection methods we considered, we notice that \nIG and OCFS are good at distinguishing multiple classes, while CHI2, DFD, and QER \nare restricted to two classes, although all of them are suitable for sentiment analysis. IG \nis considered as a greedy approach since it favors those that can maximize the informa-\ntion gain for separating the related classes. Although CHI2 tries to identify the features \nthat are dependent to a class, it can also give high values to rare features that only affect \nfew documents in a given collection. OCFS has been shown to be effective for tradi-\ntional topic-based text classification, but it depends on the distance/similarity measures \nbetween the vectors of the related documents. Since sentiment-expressing features do \nnot happen frequently within a review, as illustrated by the example in the introduction, \nthey may not be favored by the OCFS method. QER is similar to DFD in that they both \nrely on the differences of the document frequencies of a given feature between the two \nclasses. However, QER is different from DFD in that it normalizes the document fre-\nquencies of a feature in both classes into probabilities and uses the ratio of the sum over \nthe difference for these two probabilities.\n\nExperiments and results\nDatasets\n\nWe use Turkish and English review datasets in our experiments. The Turkish movie \nreviews are collected from a publicly available website (http://www.beyazperde.com) \n[30]. The dataset has 1057 positive and 978 negative reviews. The Turkish product review \ndataset is collected from an e-commerce website (http://www.hepsiburada.com) from \ndifferent domains [28]. It consists of four subsets of reviews about books, DVDs, elec-\ntronics, and kitchen appliances, each of which has 700 positive and 700 negative reviews. \nTo compare our results with existing work for sentiment analysis, we use similar datasets \nfor English reviews. The English movie review dataset is introduced by Pang and Lee [7], \nand consists of 1000 positive and 1000 negative reviews. English product review dataset \nis introduced by Blitzer et  al. [37] and also has four subsets: books, DVDs, electronics, \nand kitchen appliances, with 1000 positive and 1000 negative reviews for each subset. In \norder to keep the same dataset sizes with Turkish product reviews, we randomly select \n700 positive and 700 negative reviews from each subset of the English product reviews.\n\nPerformance evaluation\n\nThe performance of a classification system is typically evaluated by F measure, which \nis a composite score of precision and recall. Precision (P) is the number of correctly \n\nhttp://www.beyazperde.com\nhttp://www.hepsiburada.com\n\n\nPage 9 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nclassified items over the total number of classified items with respect to a class. Recall \n(R) is the number of correctly classified items over the total number of items that belong \nto a given class. Together, the F measure gives the harmonic mean of precision and \nrecall, and is calculated as follows [33]:\n\nSince we are doing multi-fold cross validations in our experiments, we use the micro-\naverage of F measure for the final classification results. This is done by adding the clas-\nsification results for all documents across all five folds before computing the final P, R, \nand the F.\n\nExperimental settings\n\nWe conduct the experiments on a MacBook Pro with 2.5  GHz Intel Core i7 processor \nand 16 GB 1600 MHz DDR3. We use Python with NLTK [38] library in our experiments. \nAfter tokenizing text into words along with case normalization, we keep some punctua-\ntion marks and stop words, as they may express sentiments (e.g., punctuation marks like \nexclamation and question marks, and stop words like “too” in “too expensive”). In addi-\ntion, we do not apply stemming as Turkish is an agglutinative language and the polarity \nof a word is often included in the suffixes. Therefore, we can have a large feature space \nand it becomes important to apply feature selection methods to reduce this space. For \nsentiment classification, we use the Weka [33] data mining tool, which contains the four \nclassifiers we use in our experiments, i.e., NBM, SMO for SVM, J48 for C4.5, and LR for \nMEM. Since our datasets are relatively small with at most a couple of thousands of docu-\nments, we apply the fivefold cross validation, which divides a dataset into five portions: \nfour of them are used for training and the remaining one for testing, and then these por-\ntions are rotated to get a total of five F measures. Table 1 the average F measures for all \nthe classifiers where the whole feature spaces are used for each dataset, except the LR \nclassifier since it requires too much memory to handle the whole feature spaces for these \ndatasets. As can be seen in Table  1, the total number of features without any reduc-\ntion ranges from 9000 to 18,000 for the Turkish review datasets, and 8,000–38,000 for \nthe English review datasets. These results form the baselines of our study and any new \nresults obtained with feature selection methods by applying five folds cross validation \ncan be compared for possible improvements.\n\n(14)F = 2 ×\nP × R\n\nP + R\n\nTable 1 Baseline results in F measure for the Turkish and English review datasets\n\nTurkish review datasets English review datasets\n\nFeatures NBM SVM J48 LR Features NBM SVM J48 LR\n\nMovie 18,578 0.8248 0.8161 0.6954 – 38,869 0.8129 0.8480 0.6769 –\n\nDVDs 11,343 0.7957 0.7320 0.6886 – 17,674 0.7836 0.7649 0.6789 –\n\nElectronics 10,911 0.8155 0.7707 0.7371 – 9010 0.7629 0.7856 0.6750 –\n\nBook 10,511 0.8317 0.7955 0.7019 – 18,306 0.7619 0.7485 0.6407 –\n\nKitchen 9447 0.7762 0.7407 0.6647 – 8076 0.8099 0.8136 0.7093 –\n\n\n\nPage 10 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nPerformance of feature selection methods for Turkish reviews\n\nWe tested five feature selection methods: QER, CHI2, IG, DFD, and OCFS on both \nTurkish and English review datasets. For each feature selection method, we tried six fea-\nture sizes at 500, 1000, 1500, 2000, 2500, and 3000, since this is the range typically con-\nsidered for text classification, and in terms of total features, we have 9000–18,000 for the \nTurkish review datasets, and 8000–38,000 for English review datasets from our baseline \nsystems. In our previous study [10], we also observed that feature sizes up to 3000 tend \nto give good classification performance. For all feature selection methods, we pick the \ntop-ranked features of a desirable size n based on the scores of the related formulas for \nthese methods. All of these settings are run against four classifiers: NBM, SVM, LR, and \nJ48, resulting in a total of 120 experiments for each review dataset. Table 2 summarizes \nthe best results for all pairs of feature selection methods and Turkish review datasets. \nFor each pair, we show the best micro-average F measure along with the correspond-\ning classifier and feature size. Also, the best results for each review dataset are given in \nbold-face.\n\nAs observed in Table  2, our new method QER is the best performer for each review \ndataset. CHI2 and IG have almost the same performance for the Turkish reviews and \nhave better results than DFD and OCFS for the movie, book, DVDs, and kitchen review \ndatasets. DFD with NBM classifier has better results than CHI2, IG, and OCFS for the \nelectronics review dataset. Also, CHI2, IG, and QER tend to work well with smaller fea-\nture sizes, while DFD and OCFS tend to favour bigger feature sizes. Note that DFD does \nreasonably well across all review datasets, which confirms our intuition that sentiment-\nexpressing words usually have low frequencies within a document, but relatively high \nfrequencies across different documents. Although OCFS is quite robust for traditional \ntopical text classification as reported in Cai and Song [39], it is not doing well for senti-\nment analysis, perhaps for the same intuition as we just explained for DFD. Once again, \nNBM remains to be the best for most of our experiments except that SVM does the best \nfor the kitchen reviews when analysed with the CHI2 and IG methods. When analysed \nby univariate ANOVA and post hoc tests for the book, DVDs, electronics, and kitchen \nreview datasets, we found that there are significant differences between three groups \n(Baseline and OCFS), (DFD, CHI2, and IG) and (QER) at 95% confidence level. Within \neach group, however, there are no significant differences. For the movie review dataset, \nthere are significant differences between two groups (Baseline and OCFS), and (DFD, \nCHI2, IG, and QER) at the 95% confidence level. Overall, feature selection methods are \nshown to be effective for sentiment analysis, improving significantly over the baseline \nresults.\n\nTo examine the effects of text classifiers, we show the best classification results for \npairs of feature selection methods and text classifiers on the electronic review dataset in \nTable 3. Note that NBM does the best for all review datasets; J48 the worst; and SVM and \nLR in between, although LR is consistently better than SVM except for the QER method. \nOne reason that the decision-tree-based solution J48 does not do well for text classifi-\ncation in general [40] and sentiment analysis in specific is that it is a greedy approach, \nalways trying to find the features that separate the given classes the most. As a result, the \nclassifier may use a much smaller set of features, even though there are many more rel-\nevant features are available. SVM typically does well for the traditional topic-based text \n\n\n\nPage 11 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nTa\nb\n\nle\n 2\n\n T\nh\n\ne\n b\n\ne\nst\n\n c\nla\n\nss\nifi\n\nca\nti\n\no\nn\n\n r\ne\n\nsu\nlt\n\ns \nfo\n\nr \np\n\na\nir\n\ns \no\n\nf \nfe\n\na\ntu\n\nre\n s\n\ne\nle\n\nct\nio\n\nn\n m\n\ne\nth\n\no\nd\n\ns \na\n\nn\nd\n\n t\nh\n\ne\n T\n\nu\nrk\n\nis\nh\n\n r\ne\n\nv\nie\n\nw\n d\n\na\nta\n\nse\nts\n\nQ\nE\n\nR\nD\n\nFD\nO\n\nC\nFS\n\nC\nH\n\nI2\nIG\n\nS\niz\n\ne\nF \n\nm\ne\n\nas\nu\n\nre\nS\n\niz\ne\n\nF \nm\n\ne\nas\n\nu\nre\n\nS\niz\n\ne\nF \n\nm\ne\n\nas\nu\n\nre\nS\n\niz\ne\n\nF \nm\n\ne\nas\n\nu\nre\n\nS\niz\n\ne\nF \n\nm\ne\n\nas\nu\n\nre\n\nM\no\n\nvi\ne\n\n30\n00\n\nN\nBM\n\n:0\n.9\n\n1\n1\n\n2\n30\n\n00\nN\n\nBM\n:0\n\n.8\n86\n\n4\n30\n\n00\nN\n\nBM\n:0\n\n.8\n44\n\n7\n15\n\n00\nN\n\nBM\n:0\n\n.8\n88\n\n3\n15\n\n00\nN\n\nBM\n:0\n\n.8\n88\n\n3\n\nD\nVD\n\ns\n15\n\n00\nN\n\nBM\n:0\n\n.9\n1\n\n3\n6\n\n30\n00\n\nN\nBM\n\n:0\n.8\n\n65\n0\n\n30\n00\n\nN\nBM\n\n:0\n.8\n\n12\n9\n\n50\n0\n\nN\nBM\n\n:0\n.8\n\n67\n1\n\n50\n0\n\nN\nBM\n\n:0\n.8\n\n67\n1\n\nEl\nec\n\ntr\no\n\nn\nic\n\ns\n15\n\n00\nN\n\nBM\n:0\n\n.8\n9\n\n9\n6\n\n15\n00\n\nN\nBM\n\n:0\n.8\n\n56\n7\n\n20\n00\n\nN\nBM\n\n:0\n.8\n\n33\n7\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n56\n4\n\n15\n00\n\nN\nBM\n\n:0\n.8\n\n55\n1\n\nBo\no\n\nk\n15\n\n00\nN\n\nBM\n:0\n\n.9\n1\n\n5\n0\n\n15\n00\n\nN\nBM\n\n:0\n.8\n\n77\n1\n\n30\n00\n\nN\nBM\n\n:0\n.8\n\n50\n6\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n86\n4\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n86\n4\n\nK\nit\n\nch\nen\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n7\n9\n\n0\n30\n\n00\nN\n\nBM\n:0\n\n.8\n31\n\n4\n30\n\n00\nN\n\nBM\n:0\n\n.8\n01\n\n7\n50\n\n0\nSV\n\nM\n:0\n\n.8\n37\n\n8\n50\n\n0\nSV\n\nM\n:0\n\n.8\n37\n\n8\n\n\n\nPage 12 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nclassification by finding a hyperplane that clearly separates the two classes [40]. In order \nto do this, we need to represent documents as weighted vectors so that we can measure \nthe distances or similarities between the documents. For sentiment analysis, however, \nwe are favouring features that have low frequencies within a document, but relatively \nhigh frequencies across different documents (as illustrated by the example of “great” in \nthe introduction), making the distance/similarity measures less effective. Both NBM and \nLR are based on the probabilities of the features in the given dataset. In particular, LR \nis equivalent to the maximum entropy modelling and is capable of handling dependent \nfeatures, whereas NBM makes the naïve assumption that all features are independent \nof each other. In our experiments, NBM does better than LR, which could be due to the \nsame reason as we just explained for SVM above.\n\nTo see the impacts of feature sizes for different feature selection methods, we plot our \nresults for the Turkish electronic review dataset in Fig.  1. Clearly, OCFS lags behind \nother feature selection methods across all feature sizes. DFD tends to do better with big-\nger feature sizes, while CHI2 and IG tend to favour smaller feature sizes. In addition, \nthe results for CHI2 and IG are sufficiently close, although they are slightly different for \ncertain feature sizes. Our new method QER does reasonably well across all other meth-\nods. For Turkish electronics review dataset, QER is the best performer and the selected \nfeatures include 7.7% of the punctuation patterns and 25% of the stop words; the features \nselected by DFD method include 61.5% of the punctuation patterns and 59% of the stop \nwords; the features selected by CHI2 method include 15% of the punctuation patterns \nand 90% of the stop words; and the features selected by OCFS method include 69.2% of \n\nTable 3 Detailed results for the Turkish electronics review dataset\n\nNBM SVM LR J48\n\nSize F measure Size F measure Size F measure Size F measure\n\nQER 1500 0.8996 2000 0.8715 1000 0.7927 2000 0.6734\n\nCHI2 1000 0.8564 1000 0.8505 500 0.7969 1000 0.7435\n\nIG 1500 0.8551 1000 0.8505 500 0.8156 1500 0.7428\n\nDFD 1500 0.8567 1500 0.8128 2500 0.7829 500 0.7399\n\nOCFS 2000 0.8337 1000 0.7729 3000 0.7643 1500 0.7371\n\nFig. 1 Detailed results of feature sizes for the Turkish electronic review dataset\n\n\n\n\n\nPage 13 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nthe punctuation patterns and 49.6% of the stop words. Therefore, CHI2 method tends \nto favor stop words but not punctuation patterns, while DFD and OCFS tend to choose \nmore punctuation patterns and fewer stop words. In addition, when we compare the fea-\ntures selected by QER and CHI2 methods, we observe that 5.7% of selected features are \nthe same, and for QER and DFD methods, there are 6.9% of the features that are com-\nmon, and for QER and OCFS methods, there are 7% of the features that are common. \nHowever, for DFD and CHI2 methods, we observe that 49.8% of the selected features are \nthe same, and for DFD and OCFS methods, there are 76.7% of the features that are com-\nmon, and for CHI2 and OCFS methods, there are 34% of the features that are common. \nNote that although we only show the results on specific datasets in Table  3 and Fig.  1, \nsimilar trends are observed for other datasets as well, and to save space these results are \nnot included.\n\nPerformance of feature selection methods for English reviews\n\nUsing similar settings as described in “Performance of feature selection methods for \nTurkish reviews”, we also carried out experiments on the English review datasets. As \nshown in Table  4, QER achieved the best performance with LR classifier for the movie \nreview dataset and NBM classifier for other datasets. CHI2 and IG achieved better per-\nformance with NBM for all five datasets. Once again, the results are basically the same \nfor CHI2 and IG, indicating that the two methods are also strongly correlated for the \nEnglish review datasets. Compared with the Turkish movie reviews, the feature size for \nthe best performer of the English movie reviews is 3000, which is achieved with QER for \nthe LR classifier. This is likely due to the bigger vocabulary of the English movie reviews \nover that of the Turkish movie reviews as can be observed in Table  1. Also compared \nwith the Turkish review datasets, DFD is not as good as CHI2 and IG for the English \nreview datasets, even though the performance is close for the kitchen reviews and gener-\nally better than OCFS. Furthermore, the best results for DFD are achieved with differ-\nent classifiers for different datasets: SVM for the movie reviews and LR for the kitchen \nreviews. Statistical analysis with univariate ANOVA and post hoc tests show similar \nresults as those for the Turkish reviews: there are significant differences between three \ngroups (Baseline and OCFS), (DFD), and (CHI2, IG, and QER) at 95% confidence level \nfor the movie, DVDs, electronic, and kitchen review datasets, but for the book review \ndataset, there are significant differences between two groups (Baseline and OCFS) and \n(DFD, CHI2, IG, and QER) at the 95% confidence level.\n\nFor text classifiers, Table  4 shows that similar trends are observed for the English \nreviews as those for the Turkish reviews, although LR and SVM can over-perform NBM \nfor some feature selection methods. For different feature sizes, similar trends are also \nobserved, as illustrated in Fig.  2. Once again, in Table  5 and Fig.  2, we only show the \nresults for specific datasets, but the trends are similar to other datasets as well.\n\nIn summary, we see some similarities between Turkish and English reviews in that for \ndata pre-processing, we should keep punctual patterns and stop words, and not per-\nform stemming, leading us to use the same setting as the baselines for further study. \nIn addition, NBM seems to be the most suitable classifier for sentiment analysis since \nsentiment-expressing words tend to have low frequencies within a document, but rela-\ntively high frequencies across different documents. For feature selection methods, our \n\n\n\nPage 14 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nTa\nb\n\nle\n 4\n\n T\nh\n\ne\n b\n\ne\nst\n\n c\nla\n\nss\nifi\n\nca\nti\n\no\nn\n\n r\ne\n\nsu\nlt\n\ns \nfo\n\nr \np\n\na\nir\n\ns \no\n\nf \nfe\n\na\ntu\n\nre\n s\n\ne\nle\n\nct\nio\n\nn\n m\n\ne\nth\n\no\nd\n\ns \na\n\nn\nd\n\n t\nh\n\ne\n E\n\nn\ng\n\nli\nsh\n\n r\ne\n\nv\nie\n\nw\n d\n\na\nta\n\nse\nts\n\nQ\nE\n\nR\nD\n\nFD\nO\n\nC\nFS\n\nC\nH\n\nI2\nIG\n\nS\niz\n\ne\nF \n\nm\ne\n\nas\nu\n\nre\nS\n\niz\ne\n\nF \nm\n\ne\nas\n\nu\nre\n\nS\niz\n\ne\nF \n\nm\ne\n\nas\nu\n\nre\nS\n\niz\ne\n\nF \nm\n\ne\nas\n\nu\nre\n\nS\niz\n\ne\nF \n\nm\ne\n\nas\nu\n\nre\n\nM\no\n\nvi\ne\n\n30\n00\n\nLR\n:0\n\n.9\n5\n\n5\n0\n\n25\n00\n\nSV\nM\n\n:0\n.8\n\n64\n0\n\n30\n00\n\nSV\nM\n\n: 0\n.8\n\n28\n5\n\n25\n00\n\nN\nBM\n\n:0\n.9\n\n15\n0\n\n25\n00\n\nN\nBM\n\n:0\n.9\n\n15\n0\n\nD\nVD\n\ns\n25\n\n00\nN\n\nBM\n:0\n\n.9\n1\n\n6\n9\n\n30\n00\n\nN\nBM\n\n:0\n.8\n\n50\n2\n\n10\n00\n\nN\nBM\n\n:0\n.7\n\n99\n6\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n96\n4\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n96\n4\n\nEl\nec\n\ntr\no\n\nn\nic\n\ns\n20\n\n00\nN\n\nBM\n:0\n\n.8\n8\n\n7\n8\n\n15\n00\n\nN\nBM\n\n:0\n.8\n\n22\n1\n\n20\n00\n\nSV\nM\n\n: 0\n.7\n\n82\n1\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n62\n1\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n62\n1\n\nBo\no\n\nk\n30\n\n00\nN\n\nBM\n:0\n\n.9\n1\n\n6\n2\n\n30\n00\n\nN\nBM\n\n:0\n.8\n\n62\n8\n\n30\n00\n\nN\nBM\n\n:0\n.7\n\n89\n9\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n87\n9\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n87\n9\n\nK\nit\n\nch\nen\n\n20\n00\n\nN\nBM\n\n:0\n.9\n\n1\n0\n\n6\n30\n\n00\nLR\n\n:0\n.8\n\n89\n3\n\n15\n00\n\nSV\nM\n\n: 0\n.8\n\n15\n7\n\n50\n0\n\nN\nBM\n\n:0\n.8\n\n96\n4\n\n50\n0\n\nN\nBM\n\n:0\n.8\n\n96\n4\n\n\n\nPage 15 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nproposed QER achieves best performances with feature sizes between 2000 and 3000. \nCHI2 and IG are strongly correlated and tend to work well with smaller feature sizes, \nwhile DFD also works reasonably well, but with bigger feature sizes. For differences, \nthe English review datasets usually have bigger vocabulary, resulting in relatively big-\nger feature sizes for feature selection. Moreover, SVM and LR can also perform well for \nsome English review datasets, while NBM looks like a dominant classifier for the Turk-\nish reviews. Finally, the performance results for the English reviews are generally higher \nthan those for the Turkish reviews, possibly related to the differences between the two \nlanguages in terms of vocabularies, writing styles, and the agglutinative property of the \nTurkish language. The limitation of QER is that it is only suitable for classifying two \nclasses since it is especially developed for sentiment analysis with the observation that \nsentiment-expressing words are usually more frequent across different reviews. The con-\ntribution of QER is that, as it is shown in the experimental results, the method is both \nlanguage and classifier independent and can select better features than other methods \nfor sentiment analysis.\n\nComparison of our proposal with the previous studies\n\nIt is generally difficult to directly compare the results of different studies since there are \noften differences in partitioning and preprocessing the datasets for training and testing, \nas shown in the studies by Pang et al. [4]. That is why we tried different combinations of \nfeature selection methods and text classifiers on multiple datasets in our research so that \n\nFig. 2 Detailed results of feature sizes for the English DVDs review dataset\n\nTable 5 Detailed results for the English DVD review dataset\n\nNBM SVM LR J48\n\nSize F measure Size F measure Size F measure Size F measure\n\nQER 2500 0.9169 3000 0.8724 2000 0.8977 2000 0.5481\n\nCHI2 1000 0.8964 500 0.8650 3000 0.6976 3000 0.6799\n\nIG 1000 0.8964 1000 0.8614 2000 0.6970 500 0.6769\n\nDFD 3000 0.8502 1000 0.8293 3000 0.7600 500 0.6771\n\nOCFS 1000 0.7996 1000 0.7714 500 0.6800 2000 0.6829\n\n\n\n\n\nPage 16 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nwe can compare their performance collectively and accurately. However, we do agree \nthat it is helpful to describe the results from the related studies so that we can put our \nresults into a suitable context. Table 6 includes a summary for comparison of our results \nwith that of the previous studies which have used the same datasets with our study. For \nthe English movie review dataset, Nicholls and Song [8] obtained a baseline accuracy \nof 79.9% with the MEM classifier, and better classification accuracies of 86.9, 85.7, and \n80.9% when combined with DFD, CHI2, and OCFS feature selection methods, respec-\ntively. Dang et al. [23] examined their proposed semantic oriented method on the prod-\nuct dataset [37]. They achieved an accuracy of 84.2% for the kitchen dataset. Also, Xia \net al. [24] improved the classification performances from 84.8 to 87.7% using their pro-\nposed word relation based feature selection method. Bai [25] improved the accuracies \nfrom baseline 84.1–92.7% using their proposed Tabu search-enhanced Markov blanket \nmodel for the movie review dataset. Pang et al. [4] obtained accuracy around 78.7% with \nNB using the document frequency of 4 to eliminate the rare features. Agarwal et al. [9] \nimproved the accuracies from baseline 82.7–89.2% using IG feature selection method \nwith Boolean NBM. Our proposed QER method showed an improvement from the \nbaseline of 81.3–91.1% with NBM in terms of F measures.\n\nFor the Turkish movie review dataset, the best classification result of 82.58% is \nobtained with the SVM classifier [30]. As shown in the previous studies, classification \naccuracy is improved by applying feature selection, and NB based classifier performs the \nbest in the majority of the cases. The proposed feature selection method is also com-\nputationally efficient and easy to implement as it only computes scores for features by \ncounting document frequencies.\n\nConclusions\nIn this paper, we proposed a new feature selection method query expansion ranking \n(QER) for the sentiment analysis and compared it with the common feature selection \nmethods for sentiment classification, including DFD and OCFS, CHI2 and IG. All of \nthese methods are tested against five datasets of Turkish reviews, using four common \n\nTable 6 Summary of related work on the sentiment analysis for the same datasets\n\nPaper Dataset Baseline accuracy (%) Best accuracies observed (%) Classifier\n\n[4] Movie 78.7 NB, SVM\n\n[7] Movie 87.1 minimum cut SVM\n\n[8] Movie\nProduct\n\n79.9\n74.3\n\n85.7 CHI2; 86.9 DFD; 80.9 OCFS\n73.7 CHI2; 75 DFD; 73.8 OCFS\n\nMEM\n\n[9] Movie\nProduct\n\n84.2\n80.9 Book; 78.9 DVD; 80.8 El\n\n91.8\n92.5 Book; 91.5 DVD; 91.8 El\nmRMR with composite features\n\nBNBM, SVM\n\n[23] Product 70.1 84.2% Kitc. semantic orientation SVM\n\n[24] Movie\nProduct\n\n84.8\n74.7 Book; 77.2 DVD; 80.8 El.; \n\n83.3 Kitc\n\n87.7\n81.8 Book; 83.8 DVD; 85.9 El.; 88.7 Kitc \n\nword relation based method\n\nNB, SVM, MEM\n\n[25] Movie 84.1 92.7% Tabu search-enhanced Markov \nblanket model\n\nNB, SVM, MEM\n\nOur study Movie\nProduct\n\n84.8\n76.2 Book; 78.4 DVD; 78.6 \n\nElect; 81.4 Kitc\n\n91.5 CHI2-IG; 87.1 DFD; 82.9 OCFS; \n95.5 91.6 Book; 91.7 DVD; 88.8 Elect; \n91.1 Kitc proposed QER\n\nNBM, SVM, MEM, DT\n\n\n\nPage 17 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\ntext classifiers, including NBM, SVM, logistic regression (LR), and decision trees (J48). \nSimilar experiments are also conducted for English reviews so that we can compare \ntheir differences with the Turkish reviews. Our results show that for all Turkish review \ndatasets, the best results are all obtained with the NBM classifier, and for some Eng-\nlish review datasets, LR and SVM have the best performance. For feature selection, our \nproposed QER method helps to achieve the best performance compared with all other \nfeature selection methods for both Turkish and English reviews. For feature selection, \nour experiments show that our proposed QER method helps to achieve the best per-\nformance among all other feature selection methods. We found that CHI2 and IG have \nalmost the same performance for the Turkish reviews and they tend to work well with \nsmaller feature sizes compared with other feature selection methods. DFD does reason-\nably well across all review datasets, but it tends to favour bigger feature sizes. This con-\nfirms our intuition that sentiment-expressing words usually have low frequencies within \na document, but relatively high frequencies across different documents. Although OCFS \nis quite robust for traditional topical text classification, it does not do well for sentiment \nanalysis since it relies on word frequencies to measure the distances between docu-\nments. Once again, NBM remains the best performer for most of our experiments when \nanalysed with QER method. Overall, feature selection methods are shown to be effective \nfor sentiment analysis, improving significantly over the baseline results.\n\nFollowing a similar process, we also carried out experiments on English review data-\nsets and NBM seems to be the most suitable classifier for sentiment analysis. For fea-\nture selection methods, CHI2 and IG are strongly correlated and tend to work well with \nsmaller feature sizes, while DFD also works reasonably well, but with bigger feature \nsizes. Our proposed query expansion ranking method achieves the best performances \nfor the English datasets as well. As for differences, the English review datasets usually \nhave a bigger vocabulary, resulting in relatively bigger feature sizes for feature selection. \nMoreover, LR and SVM also perform well for some English review datasets, while NBM \nlooks like a dominant classifier for the Turkish reviews. The performance results for the \nEnglish reviews are generally higher than those for the Turkish reviews, possibly related \nto the differences between the two languages in terms of vocabularies, writing styles, \nand the agglutinative property of the Turkish language. Finally, the experimental results \nshow that our proposal QER method is language, domain and classifier independent \nand improve the classification performance better than other FS methods for sentiment \nanalysis.\nAuthors’ contributions\nTP drafted this manuscript, conducted experiments using the datasets and analyzed the results. SAO and FS suggested \nthe methods used in this study and provided guidelines in drafting the manuscript. FS edited and corrected the manu-\nscript. All authors read and approved the final manuscript.\n\nAuthors’ information\nTP received her Ph.D. degree in Computer Engineering from Çukurova University in 2016. She received a Bachelor of \nEngineering degree in Computer Engineering from Hacettepe University, and she holds a M.Sc. in Management Infor-\nmation Sciences and a M.Sc. in Mathematics. She studied for 4 months of 2015 as a visiting researcher in University of \nGuelph, Canada with a scholarship supporting by The Scientific and Technological Research Council of Turkey (TUBITAK). \nShe is currently working as a senior lecturer and head of the Computer Technologies Department, Antakya Vocational \nSchool, Mustafa Kemal University. Her research interest is in sentiment analysis, data mining, machine learning, and \napplying text processing techniques to medical data extraction and integration.\n\nSAO received her Ph.D. and Bachelor of Science degrees both in Computer Engineering from Bilkent University, \nTurkey, in 2004 and 1996, respectively. Currently she is a professor and head of the Department of Computer Engineer-\ning, Çukurova University, Turkey. Her research interests include text mining, information retrieval systems, and applying \nbiological and nature inspired computing to text mining.\n\n\n\nPage 18 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nFS received his Ph.D. degree in Computer Science from the University of Waterloo in Canada. He is currently an \nassociate professor in the School of Computer Science, University of Guelph in Canada. His interests are mostly in Natural \nLanguage Processing, working on a wide range of topic areas, including information retrieval, text classification, topic \nmodeling, key phrase extraction, text segmentation, sentiment analysis, text summarization, and document clustering. \nMore recently, he is also interested in applying text processing techniques to privacy policy analysis and medical data \nextraction and integration.\n\nAuthor details\n1 Department of Mathematics, Mustafa Kemal University, Antakya, Hatay, Turkey. 2 Department of Computer Engineering, \nÇukurova University, Adana, Turkey. 3 School of Computer Science, University of Guelph, Guelph, Canada. \n\nAcknowledgements\nThis research is supported by TUBITAK-2214-A.\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAvailability of data and materials\nNot applicable.\n\nEthics approval and consent to participate\nNot applicable.\n\nFunding\nThis research is supported by Çukurova University Fund of Scientific Research Projects under Grant No. FDK-2015-3833, \nand Mustafa Kemal University Fund of Scientific Research Projects under Grant No. 15426.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 10 February 2018   Accepted: 16 April 2018\n\nReferences\n 1. Pang B, Lee L (2008) Opinion mining and sentiment analysis. Found Trends Inf Retr 2:1–135. https://doi.\n\norg/10.1561/1500000011\n 2. Tripathy A, Anand A, Rath SK (2017) Document-level sentiment classification using hybrid machine learning \n\napproach. Knowl Inf Syst 53:805–831. https://doi.org/10.1007/s10115-017-1055-z\n 3. Yang D-H, Yu G (2013) A method of feature selection and sentiment similarity for Chinese micro-blogs. J Inf Sci \n\n39:429–441. https://doi.org/10.1177/0165551513480308\n 4. Pang B, Lee L, Vaithyanathan S (2002) Thumbs up? In: Proceedings of the ACL-02 conference on empirical methods \n\nin natural language processing—EMNLP’02. Association for computational linguistics, Morristown, pp 79–86\n 5. Mullen T, Collier N (2004) Sentiment analysis using support vector machines with diverse information sources. Conf \n\nEmpir Methods Nat Lang Process. https://doi.org/10.3115/1219044.1219069\n 6. Kaya M, Fidan G, Toroslu IH (2012) Sentiment analysis of Turkish political news. In: 2012 IEEE/WIC/ACM international \n\nconferences on web intelligence and intelligent agent technology. IEEE, Macau, pp 174–180\n 7. Pang B, Lee L (2004) A sentimental education. In: Proceedings of the 42nd annual meeting on association for com-\n\nputational linguistics—ACL’04. Association for Computational Linguistics, Morristown, p 271–es\n 8. Nicholls C, Song F (2010) Comparison of feature selection methods for sentiment analysis. In: Advances in artificial \n\nintelligence. Springer, Berlin, pp 286–289\n 9. Agarwal B, Mittal N (2016) Prominent feature extraction for review analysis: an empirical study. J Exp Theor Artif Intell \n\n28:485–498. https://doi.org/10.1080/0952813X.2014.977830\n 10. Parlar T, Ozel SA (2016) A new feature selection method for sentiment analysis of Turkish reviews. In: International \n\nSymposium on INnovations in Intelligent SysTems and Applications (INISTA). IEEE, Sinaia, pp 1–6\n 11. Fattah MA (2017) A novel statistical feature selection approach for text categorization. J Inf Process Syst 13:1397–\n\n1409. https://doi.org/10.3745/JIPS.02.0076\n 12. Harman D (1992) Relevance feedback revisited. In: Proceedings of the 15th annual international ACM SIGIR confer-\n\nence on Research and development in information retrieval—SIGIR’92. ACM Press, New York, pp 1–10\n 13. Robertson SE, Jones KS (1976) Relevance weighting of search terms. J Am Soc Inf Sci 27:129–146. https://doi.\n\norg/10.1002/asi.4630270302\n 14. Aldoğan D, Yaslan Y (2017) A comparison study on active learning integrated ensemble approaches in sentiment \n\nanalysis. Comput Electr Eng 57:311–323. https://doi.org/10.1016/J.COMPELECENG.2016.11.015\n 15. Singh J, Singh G, Singh R (2017) Optimization of sentiment analysis using machine learning classifiers. Hum centric \n\nComput Inf Sci 7:32. https://doi.org/10.1186/s13673-017-0116-3\n 16. Mladenović M, Mitrović J, Krstev C, Vitas D (2016) Hybrid sentiment analysis framework for a morphologically rich \n\nlanguage. J Intell Inf Syst 46:599–620. https://doi.org/10.1007/s10844-015-0372-5\n 17. Asgarian E, Kahani M, Sharifi S (2018) The impact of sentiment features on the sentiment polarity classification in \n\nPersian reviews. Cognit Comput 10:117–135. https://doi.org/10.1007/s12559-017-9513-1\n\n\n\nhttps://doi.org/10.1561/1500000011\nhttps://doi.org/10.1561/1500000011\nhttps://doi.org/10.1007/s10115-017-1055-z\nhttps://doi.org/10.1177/0165551513480308\nhttps://doi.org/10.3115/1219044.1219069\nhttps://doi.org/10.1080/0952813X.2014.977830\nhttps://doi.org/10.3745/JIPS.02.0076\nhttps://doi.org/10.1002/asi.4630270302\nhttps://doi.org/10.1002/asi.4630270302\nhttps://doi.org/10.1016/J.COMPELECENG.2016.11.015\nhttps://doi.org/10.1186/s13673-017-0116-3\nhttps://doi.org/10.1007/s10844-015-0372-5\nhttps://doi.org/10.1007/s12559-017-9513-1\n\n\nPage 19 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\n 18. Guyon I, Elisseeff A (2003) An introduction to variable and feature selection. J Mach Learn Res 3:1157–1182. https://\ndoi.org/10.1016/j.aca.2011.07.027\n\n 19. Akba F, Uçan A, Sezer E, Sever H (2014) Assessment of feature selection metrics for sentiment analyses: Turkish \nmovie reviews. In: 8th European conference on data mining. Lisbon, Portugal, pp 180–184\n\n 20. Liu Y, Bi JW, Fan ZP (2017) Multi-class sentiment classification: the experimental comparisons of feature selection \nand machine learning algorithms. Expert Syst Appl 80:323–339. https://doi.org/10.1016/j.eswa.2017.03.042\n\n 21. Sagar K, Saha A (2017) Qualitative usability feature selection with ranking: a novel approach for ranking the identi-\nfied usability problematic attributes for academic websites using data-mining techniques. Hum centric Comput Inf \nSci 7:29. https://doi.org/10.1186/s13673-017-0111-8\n\n 22. Abbasi A, Chen H, Salem A (2008) Sentiment analysis in multiple languages: Feature selection for opinion classifica-\ntion in Web forums. ACM Trans Inf Syst 26:1–34. https://doi.org/10.1145/1361684.1361685\n\n 23. Dang Y, Zhang Y, Chen H (2010) A Lexicon-enhanced method for sentiment classification: an experiment on online \nproduct reviews. IEEE Intell Syst 25:46–53. https://doi.org/10.1109/MIS.2009.105\n\n 24. Xia R, Zong C, Li S (2011) Ensemble of feature sets and classification algorithms for sentiment classification. Inf Sci \n(Ny) 181:1138–1152. https://doi.org/10.1016/j.ins.2010.11.023\n\n 25. Bai X (2011) Predicting consumer sentiments from online text. Decis Support Syst 50:732–742. https://doi.\norg/10.1016/j.dss.2010.08.024\n\n 26. Yan J, Liu N, Zhang B, et al (2005) OCFS: optimal orthogonal centroid feature selection for text categorization. In: \nProceedings of the 28th annual international ACM SIGIR conference on Research and development in information \nretrieval—SIGIR’05. ACM Press, New York, p 122\n\n 27. Zheng L, Wang H, Gao S (2018) Sentimental feature selection for sentiment analysis of Chinese online reviews. Int J \nMach Learn Cybern 9:75–84. https://doi.org/10.1007/s13042-015-0347-4\n\n 28. Demirtas E, Pechenizkiy M (2013) Cross-lingual polarity detection with machine translation. In: Second international \nworkshop on issues of sentiment discovery and opinion mining—WISDOM’13. ACM Press, New York, pp 1–8\n\n 29. Boynukalin Z (2012) Emotion analysis of Turkish texts by using machine learning methods. M.Sc. Thesis, Middle East \nTechnical University\n\n 30. Sevindi BI (2013) Türkçe Metinlerde Denetimli ve Sözlük Tabanlı Duygu Analizi Yaklaşımlarının Karşılaştırılması. M.Sc. \nThesis, Gazi University\n\n 31. Parlar T, Özel SA, Song F (2018) Interactions between term weighting and feature selection methods on the senti-\nment analysis of Turkish reviews. In: Computational linguistics and intelligent text processing. CICLing 2016. Lecture \nNotes in computer Science, vol 9624. Springer, Cham, pp 335–346\n\n 32. Çakici R (2009) Wide-coverage parsing for Turkish. Ph.D. Thesis, University of Edinburgh\n 33. Witten IH, Frank E, Hall MA (2011) Data mining: practical machine learning tools and techniques. Morgan Kaufmann, \n\nBurlington\n 34. McCallum A, Nigam K (1998) A comparison of event models for naive Bayes text classification. In: AAAI/ICML-98 \n\nworkshop on learning for text categorization. pp 41–48\n 35. Zhao X, Li D, Yang B et al (2015) A two-stage feature selection method with its application. Comput Electr Eng \n\n47:114–125. https://doi.org/10.1016/J.COMPELECENG.2015.08.011\n 36. Harman D (1988) Towards interactive query expansion. In: Proceedings of the 11th annual international ACM SIGIR \n\nconference on Research and development in information retrieval—SIGIR’88. ACM Press, New York, pp 321–331\n 37. Blitzer J, Dredze M, Pereira F (2007) Biographies, bollywood, boom-boxes and blenders: domain adaptation for senti-\n\nment classification. In: 45th annual meeting-association for computational linguistics. pp 440–447\n 38. Bird S, Klein E, Loper E (2009) Natural language processing with Python. O’Reilly, Newton\n 39. Cai J, Song F (2008) Maximum entropy modeling with feature selection for text categorization. In: Lecture notes in \n\ncomputer science (including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics). pp \n549–554\n\n 40. Joachims T (1998) Text categorization with support vector machines: learning with many relevant features. Springer, \nBerlin, pp 137–142\n\nhttps://doi.org/10.1016/j.aca.2011.07.027\nhttps://doi.org/10.1016/j.aca.2011.07.027\nhttps://doi.org/10.1016/j.eswa.2017.03.042\nhttps://doi.org/10.1186/s13673-017-0111-8\nhttps://doi.org/10.1145/1361684.1361685\nhttps://doi.org/10.1109/MIS.2009.105\nhttps://doi.org/10.1016/j.ins.2010.11.023\nhttps://doi.org/10.1016/j.dss.2010.08.024\nhttps://doi.org/10.1016/j.dss.2010.08.024\nhttps://doi.org/10.1007/s13042-015-0347-4\nhttps://doi.org/10.1016/J.COMPELECENG.2015.08.011\n\n\tQER: a new feature selection method for sentiment analysis\n\tAbstract \n\tIntroduction\n\tRelated work\n\tMethods\n\tMachine learning algorithms\n\tFeature selection\n\tInformation gain\n\tChi square (CHI2)\n\tDocument frequency difference\n\tOptimal orthogonal centroid (OCFS)\n\tQuery expansion ranking\n\n\n\tExperiments and results\n\tDatasets\n\tPerformance evaluation\n\tExperimental settings\n\tPerformance of feature selection methods for Turkish reviews\n\tPerformance of feature selection methods for English reviews\n\tComparison of our proposal with the previous studies\n\n\tConclusions\n\tAuthors’ contributions\n\tReferences\n\n\n\n\n",
      "metadata_storage_path": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9saWJyYXJ5L3MxMzY3My0wMTgtMDEzNS04LnBkZg2",
      "authors": [
        "Tuba Parlar1",
        "Selma Ayşe Özel2",
        "Fei Song3",
        "R E S",
        "A R C H",
        "Mustafa",
        "Pang",
        "Yang",
        "Yu",
        "Nicholls",
        "Agarwal",
        "Abbasi",
        "Xia",
        "Bai",
        "Markov",
        "Mladenovic",
        "Zheng",
        "Boynukalın",
        "Akba",
        "lish",
        "Song",
        "Harman",
        "Har",
        "Robertson",
        "Sparck Jones",
        "Lee",
        "Blitzer",
        "Bo",
        "Dang",
        "Kitc"
      ],
      "institutions": [
        "SA",
        "Commons",
        "Kemal University",
        "QER",
        "SVM",
        "NBM",
        "EWGA",
        "DFD",
        "OCFS",
        "IG",
        "ifi",
        "FD",
        "FS",
        "BM",
        "ic",
        "SV",
        "CHI2",
        "Baseline",
        "LR",
        "NB",
        "MEM",
        "BNBM",
        "DT",
        "TP",
        "SAO"
      ],
      "key_phrases": [
        "A R C H Parlar",
        "naïve Bayes multinomial classifier",
        "Creative Commons Attribution 4.0 International License",
        "natural language processing task",
        "Selma Ayşe Özel2",
        "Text classification Open Access",
        "document frequency difference methods",
        "new feature selection method",
        "Creative Commons license",
        "other feature selection methods",
        "R E S",
        "query expansion ranking",
        "query expansion term",
        "optimal orthogonal centroid",
        "four classi- fiers",
        "support vector machines",
        "third class label",
        "decision making process",
        "two-class classification problem",
        "original author(s",
        "Internet World Stats",
        "review docu- ments",
        "Hum. Cent. Comput",
        "statistical methods",
        "senti- ments",
        "decision trees",
        "classification accuracy",
        "sentiment classification",
        "author information",
        "Tuba Parlar1",
        "Fei Song3",
        "important piece",
        "social media",
        "top 20 countries",
        "highest numbers",
        "Internet users",
        "exploding growth",
        "main reasons",
        "low ranks",
        "multiple kinds",
        "product reviews",
        "Machine learning",
        "unrestricted use",
        "appropriate credit",
        "Inf. Sci.",
        "Kemal University",
        "Full list",
        "ferent approaches",
        "data preprocessing",
        "other people",
        "The Author",
        "classification performance",
        "opinionated information",
        "information retrieval",
        "Chi square",
        "information gain",
        "sentiment analysis",
        "ent languages",
        "noisy features",
        "doi.org",
        "orcid.org",
        "irrelevant features",
        "major source",
        "online documents",
        "many researchers",
        "different domains",
        "Turkish reviews",
        "English languages",
        "QER",
        "Introduction",
        "most",
        "opinions",
        "Turkey",
        "area",
        "SA",
        "general",
        "internetworldstats",
        "Abstract",
        "sentiments",
        "order",
        "non-informative",
        "study",
        "field",
        "ling",
        "movie",
        "performances",
        "classifiers",
        "results",
        "Keywords",
        "article",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "link",
        "changes",
        "Correspondence",
        "tparlar",
        "mku",
        "1 Department",
        "Mathematics",
        "Mustafa",
        "Antakya",
        "Hatay",
        "end",
        "crossmark",
        "crossref",
        "Page",
        "19Parlar",
        "studies",
        "query expansion term weighting methods",
        "other common feature selection methods",
        "ten different review documents datasets",
        "maximum entropy mod- elling",
        "naïve Bayes multinomial",
        "traditional topic-based text classification",
        "good feature selection",
        "information retrieval systems",
        "Natural Language Processing",
        "text classification task",
        "document frequency difference",
        "machine learning methods",
        "four text classifiers",
        "machine learning approaches",
        "kitchen appliances reviews",
        "different document domains",
        "Related work SA",
        "feature space",
        "feature sizes",
        "training datasets",
        "different reviews",
        "classification accu",
        "particular review",
        "related attributes",
        "different languages",
        "LCD screen",
        "sentiment-expressing words",
        "experimental settings",
        "important topic",
        "Artificial Intelligence",
        "opinion mining",
        "useful applications",
        "negative ratings",
        "Hum. Cent",
        "movie reviews",
        "book reviews",
        "search performance",
        "performance measures",
        "testing results",
        "Numerous studies",
        "SA problems",
        "overall sentiments",
        "non-informative features",
        "great pictures",
        "NB",
        "SVM",
        "DT",
        "MEM",
        "differences",
        "example",
        "camera",
        "people",
        "observation",
        "account",
        "paper",
        "dimensionality",
        "aim",
        "proposed",
        "texts",
        "Turkish",
        "English",
        "effectiveness",
        "DFD",
        "OCFS",
        "goal",
        "racy",
        "respect",
        "rest",
        "Experiments",
        "Conclusion",
        "evalua",
        "emotions",
        "entities",
        "products",
        "services",
        "organizations",
        "als",
        "issues",
        "events",
        "kind",
        "many",
        "popularity",
        "user",
        "companies",
        "businesses",
        "Comput",
        "models",
        "cally",
        "Pang",
        "researchers",
        "effects",
        "Yang",
        "Yu",
        "Tabu heuristic search-enhanced Markov blanket model",
        "karşılaştırmalısın",
        "information gain feature selection method",
        "document frequency feature selection method",
        "minimum redundancy maximum relevancy",
        "Weighted Log Likelihood Ratio",
        "DFD feature selection method",
        "reduced feature sizes",
        "fea- ture space",
        "Machine learning algorithms",
        "product review dataset",
        "Chinese online reviews",
        "movie review dataset",
        "The Turkish language",
        "feature sets",
        "Boynukalın",
        "n words",
        "lish reviews",
        "product datasets",
        "popular implementation",
        "execution time",
        "large number",
        "comparative experiments",
        "significant improvement",
        "Altaic branch",
        "Ural-Altaic family",
        "agglutinative language",
        "longer sentence",
        "emotional analysis",
        "initial version",
        "statistical analysis",
        "IG methods",
        "word relation",
        "sentiment features",
        "related features",
        "unigram features",
        "single word",
        "one classifier",
        "two types",
        "high accuracy",
        "95% average accuracy",
        "SVM classifiers",
        "English language",
        "DT classifiers",
        "MEM classifier",
        "Complementary NB",
        "English datasets",
        "SA studies",
        "C4.5",
        "Nicholls",
        "difference",
        "Agarwal",
        "mRMR",
        "NBM",
        "Abbasi",
        "EWGA",
        "Xia",
        "Bai",
        "vocabulary",
        "Mladenovic",
        "mapping",
        "Zheng",
        "languages",
        "Republic",
        "Finnish",
        "garian",
        "instance",
        "something",
        "teristics",
        "WLLR",
        "combinations",
        "n-grams",
        "sequences",
        "Akba",
        "CHI2",
        "85.",
        "92",
        "Weka [33] data mining tool",
        "optimal orthogonal cen- troid",
        "new query expansion ranking",
        "good classification performance improvement",
        "traditional text classification tasks",
        "several feature selection methods",
        "common feature selection methods",
        "high information gain scores",
        "follow- ing reasons",
        "decision tree classifier",
        "Turkish review datasets",
        "English review datasets",
        "large vocabulary sizes",
        "many classification problems",
        "J48 classification methods",
        "total feature sizes",
        "Such methods",
        "feature f",
        "four classifiers",
        "tain measures",
        "same time",
        "effec- tiveness",
        "distinct values",
        "same way",
        "higher score",
        "low score",
        "contingency table",
        "many documents",
        "related class",
        "class c",
        "other class",
        "valuable features",
        "previous study",
        "attribute A",
        "different classes",
        "P(aj",
        "probability distribution",
        "entropy values",
        "m number",
        "attribute value",
        "experiments",
        "SMO",
        "LR",
        "specific",
        "efficiency",
        "range",
        "500 increments",
        "content",
        "power",
        "tainty",
        "definitions",
        "reviews",
        "positive",
        "egories",
        "keyword",
        "formulas",
        "dependence",
        "The Chi square statistics",
        "Query expansion ranking method",
        "Chi square score",
        "similar document frequencies",
        "high dimensional vectors",
        "Document frequency difference",
        "query expansion methods",
        "query expansion technique",
        "Optimal orthogonal centroid",
        "orthogonal centroid algorithm",
        "many sentiment-expressing features",
        "one specific class",
        "document vectors",
        "CHI2 method",
        "bigger difference",
        "document collection",
        "One problem",
        "One limitation",
        "new terms",
        "original query",
        "expanded query",
        "big impact",
        "H(C",
        "big issue",
        "sentiment word",
        "optimized form",
        "Docu- ments",
        "age vector",
        "DFD method",
        "high scores",
        "centroid m",
        "OCFS method",
        "negative class",
        "rare features",
        "individual review",
        "xi N",
        "positive class",
        "highest scores",
        "total number",
        "Nc N",
        "relevant documents",
        "equal number",
        "classes",
        "dataset",
        "text",
        "|A",
        "aj",
        "AD",
        "CB",
        "classification",
        "Song",
        "DFf",
        "weights",
        "importance",
        "subset",
        "sum",
        "distances",
        "subspace",
        "mc",
        "values",
        "centroids",
        "IR",
        "Scoref",
        "∑",
        "χ",
        "tional topic-based text classification",
        "QER feature selection method",
        "probabilistic weighting model",
        "information retrieval strategy",
        "statistical language processing",
        "informa- tion gain",
        "feature selection methods",
        "efficient feature selector",
        "information retrieval system",
        "score computation method",
        "k new terms",
        "classification process",
        "score formula",
        "ranking score",
        "k terms",
        "high scored",
        "Sparck Jones",
        "raw counts",
        "negative classes",
        "small constants",
        "similar constants",
        "data smoothing",
        "fea- ture",
        "multiple classes",
        "two classes",
        "greedy approach",
        "related classes",
        "high values",
        "distance/similarity measures",
        "Q. Equation",
        "term f",
        "negative documents",
        "related documents",
        "valuable terms",
        "zero probabilities",
        "query Q",
        "top-k scores",
        "ability scores",
        "lowest scores",
        "probability calculations",
        "sentiment-expressing features",
        "positive documents",
        "Harman",
        "precision",
        "set",
        "pf",
        "qf",
        "Robertson",
        "Eq.",
        "ratio",
        "Eqs",
        "log2",
        "DF",
        "N−",
        "numbers",
        "numerators",
        "denominators",
        "CHI",
        "collection",
        "vectors",
        "review",
        "introduction",
        "2.5  GHz Intel Core i7 processor",
        "The English movie review dataset",
        "The Turkish product review dataset",
        "The Turkish movie reviews",
        "English product review dataset",
        "English product reviews",
        "multi-fold cross validations",
        "fivefold cross validation",
        "same dataset sizes",
        "Turkish product reviews",
        "punctua- tion marks",
        "five F measures",
        "average F measures",
        "large feature space",
        "final classification results",
        "English reviews",
        "final P",
        "addi- tion",
        "classification system",
        "five folds",
        "punctuation marks",
        "question marks",
        "five portions",
        "978 negative reviews",
        "700 negative reviews",
        "1000 negative reviews",
        "document frequencies",
        "four subsets",
        "kitchen appliances",
        "existing work",
        "composite score",
        "harmonic mean",
        "Experimental settings",
        "MacBook Pro",
        "16 GB 1600 MHz",
        "NLTK [38] library",
        "case normalization",
        "docu- ments",
        "feature spaces",
        "results Datasets",
        "similar datasets",
        "two probabilities",
        "commerce website",
        "Performance evaluation",
        "classified items",
        "LR classifier",
        "beyazperde",
        "1057 positive",
        "hepsiburada",
        "books",
        "DVDs",
        "tronics",
        "700 positive",
        "Lee",
        "1000 positive",
        "Blitzer",
        "recall",
        "documents",
        "Python",
        "words",
        "exclamation",
        "polarity",
        "suffixes",
        "J48",
        "MEM.",
        "couple",
        "thousands",
        "training",
        "remaining",
        "testing",
        "Table",
        "memory",
        "features",
        "Features NBM SVM J48 LR Features",
        "NBM SVM J48 LR Movie",
        "five folds cross validation",
        "six fea- ture sizes",
        "smaller fea- ture sizes",
        "five feature selection methods",
        "best micro-average F measure",
        "bigger feature sizes",
        "post hoc tests",
        "desirable size n",
        "senti- ment analysis",
        "topical text classification",
        "good classification performance",
        "kitchen review datasets",
        "best classification results",
        "electronics review dataset",
        "top-ranked features",
        "total features",
        "Table 1 Baseline results",
        "NBM classifier",
        "best performer",
        "new method",
        "best results",
        "text classifiers",
        "possible improvements",
        "related formulas",
        "ing classifier",
        "same performance",
        "different documents",
        "univariate ANOVA",
        "significant differences",
        "three groups",
        "95% confidence level",
        "two groups",
        "new results",
        "baseline systems",
        "kitchen reviews",
        "low frequencies",
        "same intuition",
        "baselines",
        "Book",
        "Hum",
        "Cent",
        "scores",
        "settings",
        "120 experiments",
        "pairs",
        "bold-face",
        "Note",
        "expressing",
        "traditional",
        "Cai",
        "2 ×",
        "C FS C H I2",
        "Turkish electronics review dataset",
        "Turkish electronic review dataset",
        "different feature selection methods",
        "maximum entropy modelling",
        "naïve assumption",
        "text classifi- cation",
        "traditional topic-based text",
        "ger feature sizes",
        "smaller feature sizes",
        "NBM SVM LR J48",
        "review datasets",
        "smaller set",
        "One reason",
        "decision-tree-based solution",
        "ss ifi",
        "R D",
        "high frequencies",
        "same reason",
        "punctuation patterns",
        "stop words",
        "QER method",
        "IG S",
        "Detailed results",
        "evant features",
        "dependent features",
        "VD",
        "Bo",
        "hyperplane",
        "similarities",
        "great",
        "probabilities",
        "impacts",
        "Fig",
        "addition",
        "Size F measure Size F measure",
        "QER F Measure",
        "book review dataset",
        "different feature sizes",
        "English movie reviews",
        "Turkish movie reviews",
        "fewer stop words",
        "3500 4000 Feature Size",
        "CHI2 X IG",
        "different datasets",
        "specific datasets",
        "other datasets",
        "five datasets",
        "two methods",
        "fea- tures",
        "similar settings",
        "bigger vocabulary",
        "gener- ally",
        "ent classifiers",
        "Statistical analysis",
        "punctual patterns",
        "CHI2 methods",
        "similar trends",
        "OCFS methods",
        "similar results",
        "DFD methods",
        "best performance",
        "Fig.",
        "mon",
        "space",
        "Baseline",
        "summary",
        "processing",
        "English DVDs review dataset",
        "English DVD review dataset",
        "big- ger feature sizes",
        "El ec tr",
        "Size F measure",
        "3500 Feature Size",
        "other methods",
        "multiple datasets",
        "same setting",
        "best performances",
        "two languages",
        "writing styles",
        "agglutinative property",
        "con- tribution",
        "different combinations",
        "performance results",
        "experimental results",
        "suitable classifier",
        "dominant classifier",
        "previous studies",
        "different studies",
        "Turkish language",
        "FD",
        "ic",
        "ch",
        "vocabularies",
        "limitation",
        "Comparison",
        "proposal",
        "partitioning",
        "Table 5",
        "2500 3000",
        "new feature selection method query expansion ranking",
        "word relation based feature selection method",
        "Tabu search-enhanced Markov blanket model",
        "same datasets Paper Dataset Baseline accuracy",
        "word relation based method",
        "IG feature selection method",
        "English movie review dataset",
        "Turkish movie review dataset",
        "OCFS feature selection methods",
        "common feature selection",
        "lish review datasets",
        "semantic oriented method",
        "Movie 87.1 minimum cut",
        "NB based classifier",
        "best classification result",
        "uct dataset",
        "kitchen dataset",
        "four common",
        "semantic orientation",
        "related studies",
        "suitable context",
        "document frequency",
        "F measures",
        "related work",
        "logistic regression",
        "study Movie",
        "composite features",
        "classification accuracies",
        "Best accuracies",
        "Similar experiments",
        "Boolean NBM.",
        "SVM classifier",
        "IG.",
        "24] Movie",
        "80.9 OCFS",
        "73.8 OCFS",
        "82.9 OCFS",
        "comparison",
        "Dang",
        "improvement",
        "majority",
        "cases",
        "Conclusions",
        "Product",
        "80.9 Book",
        "78.9 DVD",
        "92.5 Book",
        "91.5 DVD",
        "BNBM",
        "4.2% Kitc",
        "74.7 Book",
        "77.2 DVD",
        "8 El.",
        "83.3 Kitc",
        "81.8 Book",
        "83.8 DVD",
        "76.2 Book",
        "78.4 DVD",
        "81.4 Kitc",
        "95.5 91.6 Book",
        "91.7 DVD",
        "91.1 Kitc",
        "other",
        "traditional topical text classification",
        "query expansion ranking method",
        "English review data- sets",
        "proposal QER method",
        "other FS methods",
        "word frequencies",
        "similar process",
        "baseline results",
        "Authors’ contributions",
        "reason",
        "intuition",
        "domain",
        "TP",
        "manuscript",
        "SAO",
        "guidelines"
      ],
      "merged_content": "\nQER: a new feature selection method \nfor sentiment analysis\nTuba Parlar1* , Selma Ayşe Özel2 and Fei Song3\n\nIntroduction\n“What other people think” has always been an important piece of information for most \nof us during the decision making process [1]. The Internet and social media provide a \nmajor source of information about people’s opinions. Due to the rapidly-growing num-\nber of online documents, it becomes both time-consuming and hard to obtain and ana-\nlyze the desired opinionated information. Turkey is among the top 20 countries with the \nhighest numbers of Internet users according to the Internet World Stats.1 The exploding \ngrowth in the Internet users is one of the main reasons that sentiment analysis for differ-\nent languages and domains becomes an actively-studied area for many researchers \n[2–6].\n\nSentiment analysis (SA) is a natural language processing task that classifies the senti-\nments expressed in review documents as “positive” or “negative”. In general, SA is con-\nsidered as a two-class classification problem. However, some researchers use “neutral” as \n\n1 http://www.internetworldstats.com/.\n\nAbstract \n\nSentiment analysis is about the classification of sentiments expressed in review docu-\nments. In order to improve the classification accuracy, feature selection methods are \noften used to rank features so that non-informative and noisy features with low ranks \ncan be removed. In this study, we propose a new feature selection method, called \nquery expansion ranking, which is based on query expansion term weighting meth-\nods from the field of information retrieval. We compare our proposed method with \nother widely used feature selection methods, including Chi square, information gain, \ndocument frequency difference, and optimal orthogonal centroid, using four classi-\nfiers: naïve Bayes multinomial, support vector machines, maximum entropy model-\nling, and decision trees. We test them on movie and multiple kinds of product reviews \nfor both Turkish and English languages so that we can show their performances for \ndifferent domains, languages, and classifiers. We observe that our proposed method \nachieves consistently better performance than other feature selection methods, and \nquery expansion ranking, Chi square, information gain, document frequency difference \nmethods tend to produce better results for both the English and Turkish reviews when \ntested using naïve Bayes multinomial classifier.\n\nKeywords: Sentiment analysis, Feature selection, Machine learning, Text classification\n\nOpen Access\n\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nR E S E A R C H\n\nParlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \nhttps://doi.org/10.1186/s13673-018-0135-8\n\n*Correspondence:   \ntparlar@mku.edu.tr \n1 Department \nof Mathematics, Mustafa \nKemal University, Antakya, \nHatay, Turkey\nFull list of author information \nis available at the end of the \narticle\n\n  \n\nhttp://orcid.org/0000-0002-8004-6150\nhttp://www.internetworldstats.com/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-018-0135-8&domain=pdf\n\n\nPage 2 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nthe third class label. There are a number of studies about sentiment analysis that use dif-\nferent approaches for data preprocessing, feature selection, and sentiment classification \n[1, 3, 4, 6–10]. The statistical methods such as Chi square (CHI2) and information gain \n(IG) are used to eliminate unnecessary or irrelevant features so that the classification \nperformance can be improved [11]. Supervised learning methods including naïve Bayes \n(NB), support vector machines (SVM), decision trees (DT), and maximum entropy mod-\nelling (MEM) are used to classify the sentiments of the reviews.\n\nAlthough SA can be considered as a text classification task, it has some differences \nfrom the traditional topic-based text classification. For example, instead of saying: “This \ncamera is great. It takes great pictures. The LCD screen is great. I love this camera” in a \nreview document, people are more likely to write: “This camera is great. It takes breath-\ntaking pictures. The LCD screen is bright and clear. I love this camera.” [8]. As can be \nseen, sentiment-expressing words like “great” are not so frequent within a particular \nreview, but can be more frequent across different reviews, and a good feature selection \nmethod for SA should take this observation into account.\n\nIn this paper, we propose a new feature selection method, called query expansion rank-\ning (QER) which is especially developed for reducing dimensionality of feature space of \nSA problems. The aim of this study is to show that our proposed method is effective for \nSA from review texts written in different languages (e.g., Turkish, English) and domains \n(e.g., movie reviews, book reviews, kitchen appliances reviews, etc.). QER is based on \nquery expansion term weighting methods used to improve the search performance of \ninformation retrieval systems [12, 13] and to evaluate its effectiveness as a feature selec-\ntor in SA, we compare it with other common feature selection methods, including CHI2, \nIG, document frequency difference (DFD), and optimal orthogonal centroid (OCFS), \nalong with four text classifiers: naïve Bayes multinomial (NBM), SVM, DT, and MEM, \nover ten different review documents datasets. Our goal is to examine whether these fea-\nture selection methods can reduce the feature sizes and improve the classification accu-\nracy of sentiment analysis with respect to different document domains, languages, and \nclassifiers.\n\nThe rest of the paper is organized as follows. “Related work” reviews the related work \non sentiment analysis. “Methods” presents the methods that we used for our study, \nincluding the new feature selection method we proposed. “Experiments and results” \ndescribes the experimental settings, datasets, performance measures, and testing results. \nFinally, “Conclusion” concludes the paper.\n\nRelated work\nSA is an important topic in Natural Language Processing and Artificial Intelligence. \nAlso known as opinion mining, SA mines people’s opinions, sentiments, evalua-\ntions, and emotions about entities such as products, services, organizations, individu-\nals, issues, and events, as well as their related attributes. This kind of analysis has many \nuseful applications. For example, it determines a product’s popularity according to \nthe user’s reviews. If the overall sentiments are negative, further analysis may be per-\nformed to identify which features contribute to the negative ratings so companies can \nreshape their businesses. Numerous studies have been done for sentiment analysis in \ndifferent domains, languages, and approaches [3–5, 8–10, 14–17]. Among these studies, \n\n\n\nPage 3 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nthe machine learning approaches are more popular since the models can be automati-\ncally trained and improved with the training datasets. Pang et  al. [4] apply supervised \nmachine learning methods such as NB and SVM to sentiment classification. NB, SVM, \nMEM, and DT are some of the commonly used machine learning approaches [4, 7–9, \n14]. Feature selection methods are used to rank features so that non-informative features \ncan be removed to improve the classification performance [18]. Some researchers have \ninvestigated the effects of feature selection for sentiment analysis [3, 8–10, 19–25]. For \nexample, Yang and Yu [3] examine IG for feature selection and evaluate its performance \nusing NB, SVM, and C4.5 (popular implementation for DT) classifiers. Nicholls et al. [8] \ncompare their proposed DFD feature selection method against other feature selection \nmethods, including CHI2, OCFS [26], and count difference using the MEM classifier. \nAgarwal et  al. [9] investigate minimum redundancy maximum relevancy (mRMR) and \nIG methods for sentiment classification using NBM and SVM classifiers. The results \nshow that mRMR performs better than IG for feature selection, and NBM performs bet-\nter than SVM in accuracy and execution time. Abbasi et al. [22] examine a new feature \nselection method called entropy weighted genetic algorithm (EWGA) and compare the \nperformance of this method using information gain feature selection method. EWGA \nachieves a relatively high accuracy of 91.7% using SVM classifier. Xia et  al. [24] design \ntwo types of feature sets: POS based and word relation based. Their word relation based \nmethod improves an accuracy of 87.7 and 85.15% on movie and product datasets. Bai \n[25] proposes a Tabu heuristic search-enhanced Markov blanket model that provides a \nvocabulary to extract sentiment features. Their method achieves an accuracy of 92.7% \nfor the movie review dataset. Mladenovic et al. [16] propose a feature selection method \nthat is based on mapping of a large number of related features to a few features. Their \nproposed method improves the classification performance using unigram features \nwith 95% average accuracy. Zheng et  al. [27] perform comparative experiments to test \ntheir proposed improved document frequency feature selection method. Their method \nachieves significant improvement in sentiment analysis of Chinese online reviews with \nan accuracy of 97.3%.\n\nMost of the SA studies listed above focus on the English language. Only few studies \nhave been done on SA for the Turkish language [6, 10, 19, 28–31]. The Turkish language \nbelongs to the Altaic branch of the Ural-Altaic family of languages and is mainly used in \nthe Republic of Turkey. Turkish is an agglutinative language similar to Finnish and Hun-\ngarian, where a single word can be translated into a relatively longer sentence in English \n[32]. For instance, word “karşılaştırmalısın” in Turkish can be expressed as “you must \nmake (something) compare” in English. As Turkish and English have different charac-\nteristics, methods developed for SA in English need to be tested for Turkish. Among \nthe few researchers who investigate the effects of feature selection on the SA of Turkish \nreviews, Boynukalın [29] applies Weighted Log Likelihood Ratio (WLLR) to reduce fea-\nture space with NB, Complementary NB, and SVM classifiers for the emotional analysis \nusing the combinations of n-grams where sequences of n words are considered together. \nIt is shown that WLLR helps to improve the accuracy with reduced feature sizes. Akba \net  al. [19] implement and compare the performance of reduced feature sizes using two \nfeature selection methods: CHI2 and IG with NB and SVM classifiers. They show that \nfeature selection methods improve the classification accuracy.\n\n\n\nPage 4 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nOur aim is to propose a new feature selection method for the SA of Turkish and Eng-\nlish reviews. We presented an initial version of this method in [10] where we employ \nonly product review dataset in Turkish and compare our method with CHI2 and DFD \nby using only one classifier. We now extend it to more datasets for Turkish, and also \ninvestigate the performance of our method in English datasets to show that our method \nis language independent. We further include more feature selection methods especially \ndeveloped for SA and compare the performance of our proposed method using NBM, \nSVM, MEM, and DT classifiers along with statistical analysis to prove that our method is \nclassifier independent.\n\nMethods\nMachine learning algorithms\n\nFor sentiment classification, we use the Weka [33] data mining tool, which contains the \nfour classifiers we use in our experiments, i.e., NBM, SMO for SVM, J48 for C4.5, and LR \nfor MEM. We choose NBM, SVM, LR, and J48 classification methods due to the follow-\ning reasons: (i) many researchers use NBM for text classification because it is computa-\ntionally efficient [9, 10, 14] and performs well for large vocabulary sizes [34]; (ii) SVM \ntends to perform well for traditional text classification tasks [3, 4, 7, 14, 35]; (iii) LR is \nknown to be equivalent to MEM which is another method used in SA studies [8]; (iv) J48 \nis a well-known decision tree classifier for many classification problems and is used for \nSA [3, 30].\n\nFeature selection\n\nFeature Selection methods have been shown to be useful for text classification in general \nand sentiment analysis in specific [11, 18]. Such methods rank features according to cer-\ntain measures so that non-informative features can be removed, and at the same time, \nthe most valuable features can be kept in order to improve the classification accuracy \nand efficiency. In this study, we consider several feature selection methods, including \ninformation gain, Chi square, document frequency difference, optimal orthogonal cen-\ntroid, and our new query expansion ranking (QER) so that we can compare their effec-\ntiveness for the sentiment analysis.\n\nFeature sizes are selected in the range from 500 to 3000 with 500 increments, com-\npared with the total feature sizes ranging from 8000 to 18,000 for the Turkish review \ndatasets and from 8000 to 38,000 for English review datasets. In our previous study [10], \nwe observed that feature sizes up to 3000 tend to give good classification performance \nimprovement; therefore we choose these feature sizes in our experiments.\n\nInformation gain\n\nInformation gain is one of the most common feature selection methods for sentiment \nanalysis [3, 9, 19, 35], which measures the content of information obtained after knowing \nthe value of a feature in a document. The higher the information gain, the more power \nwe have to discriminate between different classes.\n\nThe content of information can be calculated by the entropy that captures the uncer-\ntainty of a probability distribution for the given classes. Given m number of classes: \nC = {c1,c2,…,cm} the entropy can be given as follows:\n\n\n\nPage 5 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nwhere P(ci) is the probability of how many documents in class ci. If an attribute A has n \ndistinct values: A = {a1,a2,…,an}, then the entropy after the attribute A is observed can be \ndefined as follows:\n\nwhere P(aj) is the probability of how many documents contain the attribute value aj, and \nP(ci|aj) is the probability of how many documents in class ci that contain the attribute \nvalue aj. Based on the definitions above, the information gain for an attribute is simply \nthe difference between the entropy values before and after the attribute is observed:\n\nFor sentiment analysis, we normally classify the reviews into positive and negative cat-\negories, and for each keyword, it either occurs or does not occur in a given document; so \nthe above formulas can be further simplified. Nevertheless, we can cut down the number \nof features in the same way by choosing the keywords that have high information gain \nscores.\n\nChi square (CHI2)\n\nChi square measures the dependence between a feature and a class. A higher score \nimplies that the related class is more dependent on the given feature. Thus, a feature with \na low score is less informative and should be removed [3, 8, 10, 19]. Using the 2-by-2 \ncontingency table for feature f and class c, where A is the number of documents in class c \nthat contains feature f, B is the number of documents in the other class that contains f, C \nis the number of documents in c that does not contain f, D is the number of documents \nin the other class that does not contain f, and N is the total number of documents, then \nthe Chi square score can be defined in the following:\n\nThe Chi square statistics can also be computed between a feature and a class in the \ndataset, which are then combined across all classes to get the scores for each feature as \nfollows:\n\nOne problem with the CHI2 method is that it may produce high scores for rare features \nas long as they are mostly used for one specific class. This is a bit counter-intuitive, since \nrare features are not frequently used in text and thus do not have a big impact for text \n\n(1)H(C) = −\nm\n∑\n\ni=1\n\nP(ci) log2 P(ci)\n\n(2)H(C|A) =\nn\n\n∑\n\nj=1\n\n(\n\n−P(aj)\n\nm\n∑\n\ni=1\n\nP(ci|aj) log2P(ci|aj)\n\n)\n\n(3)IG(A) = H(C) − H(C|A)\n\n(4)χ2\n(\n\nf , c\n)\n\n=\nN(AD − CB)2\n\n(A + C)(B + D)(A + B)(C + D)\n\n(5)χ\n2\n(f ) =\n\nm\n∑\n\ni=1\n\nP(ci)χ\n2\n(f , ci)\n\n\n\nPage 6 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nclassification. For SA, however, this is not a big issue since many sentiment-expressing \nfeatures are not frequently used within an individual review.\n\nDocument frequency difference\n\nInspired by the observation that sentiment-expressing words tends to be less frequent \nwithin a review, but more frequent across different reviews, Nicholls and Song [8] pro-\npose the DFD method that tries to differentiate the features for positive and negative \nclasses, respectively, across a document collection. More specifically, DFD is calculated \nas follows:\n\nwhere DFf+ is the number of documents in the positive class that contain feature f, DF\nf\n− \n\nis the number of documents in the negative class that contain f, and N is the total num-\nber of documents in the dataset. Note that all scores are normalized between 0 and 1; \nso they should be proportional for us to rank the features in a document collection. For \nexample, a non-sentiment word may have similar document frequencies in both posi-\ntive and negative classes, and will get a low score, but a sentiment word for the positive \nclass may have a bigger difference, resulting in a higher score. One limitation of the DFD \nmethod is that it requires an equal or nearly equal number of documents in both classes, \nwhich is more or less true for the datasets used in our experiments.\n\nOptimal orthogonal centroid (OCFS)\n\nOCFS method is an optimized form of the orthogonal centroid algorithm [26]. Docu-\nments are represented as high dimensional vectors where the weights of each dimension \ncorrespond to the importance of the related features, and a centroid is simply the aver-\nage vector for a set of document vectors. OCFS aims at finding a subset of features that \ncan make the sum of distances between all the class means maximized in the selected \nsubspace. The score of a feature f by OCFS is defined in the following [8]:\n\nwhere Nc is the number of documents in class c, N is the number of documents in the \ndataset, mc is the centroid for class c, m is the centroid for the dataset D, and m\n\nf, mc\nf are \n\nthe values of feature f in centroid m, mc respectively. The centroids of m and mc are cal-\nculated as follows:\n\nQuery expansion ranking\n\nQuery expansion ranking method is our proposed feature selection method inspired \nby the query expansion methods from the field of information retrieval (IR). Query \n\n(6)Scoref =\n|DF\n\nf\n+ − DF\n\nf\n−|\n\nN\n\n(7)Scoref =\n∑\n\nc\n\nNc\n\nN\n\n(\n\nm\nf\nc − m\n\nf\n)2\n\n(8)mc =\n∑\n\nxi∈c\nxi\n\nNc\n\n(9)m =\n∑\n\nxi∈D\nxi\n\nN\n\n\n\nPage 7 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nexpansion helps to find more relevant documents for a given query. It does so by adding \nnew terms to the query. The new terms are selected from documents that are relevant \nto the original query so that the expanded query can retrieve more relevant documents. \nMore specifically, terms from the relevant documents are extracted along with some \nscores, and those with the highest scores are included in the expanded query.\n\nWe propose a new feature selection method inspired by the query expansion technique \ndeveloped for probabilistic weighting model proposed by Harman [12]. Harman [12, 36] \nstudies how to assign scores to terms extracted from relevant documents for a given \nquery Q so that high scored terms are used to expand the original query and improve \nprecision of information retrieval strategy. In this method, first, query Q is sent to the \ninformation retrieval system, and then the system returns documents that are found as \nrelevant to the user. Then, user examines the returned documents and marks the ones \nthat are relevant with the query. After that, all the terms in the relevant documents are \nextracted and they are assigned scores by using a score formula as proposed by Har-\nman [12], and top scored k terms are chosen as the most valuable terms to expand the \nquery. Then, the expanded query Q’, which includes the terms in the original query plus \nthe k new terms that have the top-k scores, is sent to the information retrieval system to \nreturn more relevant documents to the original query Q. Equation 10 presents the score \nformula developed by Harman [12] to calculate ranking score of a term f extracted from \nthe set of relevant documents for a given query Q.\n\nwhere pf is the probability of term f in the set of relevant documents for query Q, and qf \nis the probability of term f in the set of non-relevant documents for query Q. These prob-\nability scores are computed according to Robertson and Sparck Jones [13].\n\nWe revise the above score computation method to develop an efficient feature selector \nfor SA. In our feature selection method, we propose a score formula given in Eq. 11 to \ncompute scores for features:\n\nwhere pf is the ratio of positive documents containing feature f and qf is the ratio of \nnegative documents containing feature f, which are computed according to Eqs. 12, 13, \nrespectively:\n\n(10)Scoref = log2\npf\n(\n\n1 − qf\n)\n\n(\n\n1 − pf\n)\n\nqf\n\n(11)Scoref =\npf + qf\n∣\n\n∣pf − qf\n∣\n\n∣\n\n(12)pf =\nDF\n\nf\n+ + 0.5\n\nN+ + 1.0\n\n(13)qf =\nDF\n\nf\n− + 0.5\n\nN− + 0.5\n\n\n\nPage 8 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nwhere DFf+ and DF\nf\n− are the raw counts of documents that contain f in the positive and \n\nnegative classes, respectively and N+ and N− are the numbers of documents in the \npositive and negative classes, respectively. In the probability calculations, we add small \nconstants to the numerators and denominators in Eqs.  12, 13 following Robertson and \nSparck Jones [13] who add similar constants to avoid having zero probabilities. Such a \nmethod is known as data smoothing in statistical language processing.\n\nIn QER feature selection method, scores of features are computed before the features \nhaving the lowest scores are selected and used in the classification process. When a fea-\nture has low score, the difference between the probabilities for the positive and negative \nclasses is high; therefore the feature is more class specific and more valuable for clas-\nsification process. Among the feature selection methods we considered, we notice that \nIG and OCFS are good at distinguishing multiple classes, while CHI2, DFD, and QER \nare restricted to two classes, although all of them are suitable for sentiment analysis. IG \nis considered as a greedy approach since it favors those that can maximize the informa-\ntion gain for separating the related classes. Although CHI2 tries to identify the features \nthat are dependent to a class, it can also give high values to rare features that only affect \nfew documents in a given collection. OCFS has been shown to be effective for tradi-\ntional topic-based text classification, but it depends on the distance/similarity measures \nbetween the vectors of the related documents. Since sentiment-expressing features do \nnot happen frequently within a review, as illustrated by the example in the introduction, \nthey may not be favored by the OCFS method. QER is similar to DFD in that they both \nrely on the differences of the document frequencies of a given feature between the two \nclasses. However, QER is different from DFD in that it normalizes the document fre-\nquencies of a feature in both classes into probabilities and uses the ratio of the sum over \nthe difference for these two probabilities.\n\nExperiments and results\nDatasets\n\nWe use Turkish and English review datasets in our experiments. The Turkish movie \nreviews are collected from a publicly available website (http://www.beyazperde.com) \n[30]. The dataset has 1057 positive and 978 negative reviews. The Turkish product review \ndataset is collected from an e-commerce website (http://www.hepsiburada.com) from \ndifferent domains [28]. It consists of four subsets of reviews about books, DVDs, elec-\ntronics, and kitchen appliances, each of which has 700 positive and 700 negative reviews. \nTo compare our results with existing work for sentiment analysis, we use similar datasets \nfor English reviews. The English movie review dataset is introduced by Pang and Lee [7], \nand consists of 1000 positive and 1000 negative reviews. English product review dataset \nis introduced by Blitzer et  al. [37] and also has four subsets: books, DVDs, electronics, \nand kitchen appliances, with 1000 positive and 1000 negative reviews for each subset. In \norder to keep the same dataset sizes with Turkish product reviews, we randomly select \n700 positive and 700 negative reviews from each subset of the English product reviews.\n\nPerformance evaluation\n\nThe performance of a classification system is typically evaluated by F measure, which \nis a composite score of precision and recall. Precision (P) is the number of correctly \n\nhttp://www.beyazperde.com\nhttp://www.hepsiburada.com\n\n\nPage 9 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nclassified items over the total number of classified items with respect to a class. Recall \n(R) is the number of correctly classified items over the total number of items that belong \nto a given class. Together, the F measure gives the harmonic mean of precision and \nrecall, and is calculated as follows [33]:\n\nSince we are doing multi-fold cross validations in our experiments, we use the micro-\naverage of F measure for the final classification results. This is done by adding the clas-\nsification results for all documents across all five folds before computing the final P, R, \nand the F.\n\nExperimental settings\n\nWe conduct the experiments on a MacBook Pro with 2.5  GHz Intel Core i7 processor \nand 16 GB 1600 MHz DDR3. We use Python with NLTK [38] library in our experiments. \nAfter tokenizing text into words along with case normalization, we keep some punctua-\ntion marks and stop words, as they may express sentiments (e.g., punctuation marks like \nexclamation and question marks, and stop words like “too” in “too expensive”). In addi-\ntion, we do not apply stemming as Turkish is an agglutinative language and the polarity \nof a word is often included in the suffixes. Therefore, we can have a large feature space \nand it becomes important to apply feature selection methods to reduce this space. For \nsentiment classification, we use the Weka [33] data mining tool, which contains the four \nclassifiers we use in our experiments, i.e., NBM, SMO for SVM, J48 for C4.5, and LR for \nMEM. Since our datasets are relatively small with at most a couple of thousands of docu-\nments, we apply the fivefold cross validation, which divides a dataset into five portions: \nfour of them are used for training and the remaining one for testing, and then these por-\ntions are rotated to get a total of five F measures. Table 1 the average F measures for all \nthe classifiers where the whole feature spaces are used for each dataset, except the LR \nclassifier since it requires too much memory to handle the whole feature spaces for these \ndatasets. As can be seen in Table  1, the total number of features without any reduc-\ntion ranges from 9000 to 18,000 for the Turkish review datasets, and 8,000–38,000 for \nthe English review datasets. These results form the baselines of our study and any new \nresults obtained with feature selection methods by applying five folds cross validation \ncan be compared for possible improvements.\n\n(14)F = 2 ×\nP × R\n\nP + R\n\nTable 1 Baseline results in F measure for the Turkish and English review datasets\n\nTurkish review datasets English review datasets\n\nFeatures NBM SVM J48 LR Features NBM SVM J48 LR\n\nMovie 18,578 0.8248 0.8161 0.6954 – 38,869 0.8129 0.8480 0.6769 –\n\nDVDs 11,343 0.7957 0.7320 0.6886 – 17,674 0.7836 0.7649 0.6789 –\n\nElectronics 10,911 0.8155 0.7707 0.7371 – 9010 0.7629 0.7856 0.6750 –\n\nBook 10,511 0.8317 0.7955 0.7019 – 18,306 0.7619 0.7485 0.6407 –\n\nKitchen 9447 0.7762 0.7407 0.6647 – 8076 0.8099 0.8136 0.7093 –\n\n\n\nPage 10 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nPerformance of feature selection methods for Turkish reviews\n\nWe tested five feature selection methods: QER, CHI2, IG, DFD, and OCFS on both \nTurkish and English review datasets. For each feature selection method, we tried six fea-\nture sizes at 500, 1000, 1500, 2000, 2500, and 3000, since this is the range typically con-\nsidered for text classification, and in terms of total features, we have 9000–18,000 for the \nTurkish review datasets, and 8000–38,000 for English review datasets from our baseline \nsystems. In our previous study [10], we also observed that feature sizes up to 3000 tend \nto give good classification performance. For all feature selection methods, we pick the \ntop-ranked features of a desirable size n based on the scores of the related formulas for \nthese methods. All of these settings are run against four classifiers: NBM, SVM, LR, and \nJ48, resulting in a total of 120 experiments for each review dataset. Table 2 summarizes \nthe best results for all pairs of feature selection methods and Turkish review datasets. \nFor each pair, we show the best micro-average F measure along with the correspond-\ning classifier and feature size. Also, the best results for each review dataset are given in \nbold-face.\n\nAs observed in Table  2, our new method QER is the best performer for each review \ndataset. CHI2 and IG have almost the same performance for the Turkish reviews and \nhave better results than DFD and OCFS for the movie, book, DVDs, and kitchen review \ndatasets. DFD with NBM classifier has better results than CHI2, IG, and OCFS for the \nelectronics review dataset. Also, CHI2, IG, and QER tend to work well with smaller fea-\nture sizes, while DFD and OCFS tend to favour bigger feature sizes. Note that DFD does \nreasonably well across all review datasets, which confirms our intuition that sentiment-\nexpressing words usually have low frequencies within a document, but relatively high \nfrequencies across different documents. Although OCFS is quite robust for traditional \ntopical text classification as reported in Cai and Song [39], it is not doing well for senti-\nment analysis, perhaps for the same intuition as we just explained for DFD. Once again, \nNBM remains to be the best for most of our experiments except that SVM does the best \nfor the kitchen reviews when analysed with the CHI2 and IG methods. When analysed \nby univariate ANOVA and post hoc tests for the book, DVDs, electronics, and kitchen \nreview datasets, we found that there are significant differences between three groups \n(Baseline and OCFS), (DFD, CHI2, and IG) and (QER) at 95% confidence level. Within \neach group, however, there are no significant differences. For the movie review dataset, \nthere are significant differences between two groups (Baseline and OCFS), and (DFD, \nCHI2, IG, and QER) at the 95% confidence level. Overall, feature selection methods are \nshown to be effective for sentiment analysis, improving significantly over the baseline \nresults.\n\nTo examine the effects of text classifiers, we show the best classification results for \npairs of feature selection methods and text classifiers on the electronic review dataset in \nTable 3. Note that NBM does the best for all review datasets; J48 the worst; and SVM and \nLR in between, although LR is consistently better than SVM except for the QER method. \nOne reason that the decision-tree-based solution J48 does not do well for text classifi-\ncation in general [40] and sentiment analysis in specific is that it is a greedy approach, \nalways trying to find the features that separate the given classes the most. As a result, the \nclassifier may use a much smaller set of features, even though there are many more rel-\nevant features are available. SVM typically does well for the traditional topic-based text \n\n\n\nPage 11 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nTa\nb\n\nle\n 2\n\n T\nh\n\ne\n b\n\ne\nst\n\n c\nla\n\nss\nifi\n\nca\nti\n\no\nn\n\n r\ne\n\nsu\nlt\n\ns \nfo\n\nr \np\n\na\nir\n\ns \no\n\nf \nfe\n\na\ntu\n\nre\n s\n\ne\nle\n\nct\nio\n\nn\n m\n\ne\nth\n\no\nd\n\ns \na\n\nn\nd\n\n t\nh\n\ne\n T\n\nu\nrk\n\nis\nh\n\n r\ne\n\nv\nie\n\nw\n d\n\na\nta\n\nse\nts\n\nQ\nE\n\nR\nD\n\nFD\nO\n\nC\nFS\n\nC\nH\n\nI2\nIG\n\nS\niz\n\ne\nF \n\nm\ne\n\nas\nu\n\nre\nS\n\niz\ne\n\nF \nm\n\ne\nas\n\nu\nre\n\nS\niz\n\ne\nF \n\nm\ne\n\nas\nu\n\nre\nS\n\niz\ne\n\nF \nm\n\ne\nas\n\nu\nre\n\nS\niz\n\ne\nF \n\nm\ne\n\nas\nu\n\nre\n\nM\no\n\nvi\ne\n\n30\n00\n\nN\nBM\n\n:0\n.9\n\n1\n1\n\n2\n30\n\n00\nN\n\nBM\n:0\n\n.8\n86\n\n4\n30\n\n00\nN\n\nBM\n:0\n\n.8\n44\n\n7\n15\n\n00\nN\n\nBM\n:0\n\n.8\n88\n\n3\n15\n\n00\nN\n\nBM\n:0\n\n.8\n88\n\n3\n\nD\nVD\n\ns\n15\n\n00\nN\n\nBM\n:0\n\n.9\n1\n\n3\n6\n\n30\n00\n\nN\nBM\n\n:0\n.8\n\n65\n0\n\n30\n00\n\nN\nBM\n\n:0\n.8\n\n12\n9\n\n50\n0\n\nN\nBM\n\n:0\n.8\n\n67\n1\n\n50\n0\n\nN\nBM\n\n:0\n.8\n\n67\n1\n\nEl\nec\n\ntr\no\n\nn\nic\n\ns\n15\n\n00\nN\n\nBM\n:0\n\n.8\n9\n\n9\n6\n\n15\n00\n\nN\nBM\n\n:0\n.8\n\n56\n7\n\n20\n00\n\nN\nBM\n\n:0\n.8\n\n33\n7\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n56\n4\n\n15\n00\n\nN\nBM\n\n:0\n.8\n\n55\n1\n\nBo\no\n\nk\n15\n\n00\nN\n\nBM\n:0\n\n.9\n1\n\n5\n0\n\n15\n00\n\nN\nBM\n\n:0\n.8\n\n77\n1\n\n30\n00\n\nN\nBM\n\n:0\n.8\n\n50\n6\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n86\n4\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n86\n4\n\nK\nit\n\nch\nen\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n7\n9\n\n0\n30\n\n00\nN\n\nBM\n:0\n\n.8\n31\n\n4\n30\n\n00\nN\n\nBM\n:0\n\n.8\n01\n\n7\n50\n\n0\nSV\n\nM\n:0\n\n.8\n37\n\n8\n50\n\n0\nSV\n\nM\n:0\n\n.8\n37\n\n8\n\n\n\nPage 12 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nclassification by finding a hyperplane that clearly separates the two classes [40]. In order \nto do this, we need to represent documents as weighted vectors so that we can measure \nthe distances or similarities between the documents. For sentiment analysis, however, \nwe are favouring features that have low frequencies within a document, but relatively \nhigh frequencies across different documents (as illustrated by the example of “great” in \nthe introduction), making the distance/similarity measures less effective. Both NBM and \nLR are based on the probabilities of the features in the given dataset. In particular, LR \nis equivalent to the maximum entropy modelling and is capable of handling dependent \nfeatures, whereas NBM makes the naïve assumption that all features are independent \nof each other. In our experiments, NBM does better than LR, which could be due to the \nsame reason as we just explained for SVM above.\n\nTo see the impacts of feature sizes for different feature selection methods, we plot our \nresults for the Turkish electronic review dataset in Fig.  1. Clearly, OCFS lags behind \nother feature selection methods across all feature sizes. DFD tends to do better with big-\nger feature sizes, while CHI2 and IG tend to favour smaller feature sizes. In addition, \nthe results for CHI2 and IG are sufficiently close, although they are slightly different for \ncertain feature sizes. Our new method QER does reasonably well across all other meth-\nods. For Turkish electronics review dataset, QER is the best performer and the selected \nfeatures include 7.7% of the punctuation patterns and 25% of the stop words; the features \nselected by DFD method include 61.5% of the punctuation patterns and 59% of the stop \nwords; the features selected by CHI2 method include 15% of the punctuation patterns \nand 90% of the stop words; and the features selected by OCFS method include 69.2% of \n\nTable 3 Detailed results for the Turkish electronics review dataset\n\nNBM SVM LR J48\n\nSize F measure Size F measure Size F measure Size F measure\n\nQER 1500 0.8996 2000 0.8715 1000 0.7927 2000 0.6734\n\nCHI2 1000 0.8564 1000 0.8505 500 0.7969 1000 0.7435\n\nIG 1500 0.8551 1000 0.8505 500 0.8156 1500 0.7428\n\nDFD 1500 0.8567 1500 0.8128 2500 0.7829 500 0.7399\n\nOCFS 2000 0.8337 1000 0.7729 3000 0.7643 1500 0.7371\n\nFig. 1 Detailed results of feature sizes for the Turkish electronic review dataset\n\n 0,9200 0,9000 0,8800 QER F Measure 0,8600 CHI2 X IG 0,8400 X -x- DFD -x OCFS 0,8200 0,8000 O 500 1000 1500 2000 2500 3000 3500 4000 Feature Size \n\n\n\nPage 13 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nthe punctuation patterns and 49.6% of the stop words. Therefore, CHI2 method tends \nto favor stop words but not punctuation patterns, while DFD and OCFS tend to choose \nmore punctuation patterns and fewer stop words. In addition, when we compare the fea-\ntures selected by QER and CHI2 methods, we observe that 5.7% of selected features are \nthe same, and for QER and DFD methods, there are 6.9% of the features that are com-\nmon, and for QER and OCFS methods, there are 7% of the features that are common. \nHowever, for DFD and CHI2 methods, we observe that 49.8% of the selected features are \nthe same, and for DFD and OCFS methods, there are 76.7% of the features that are com-\nmon, and for CHI2 and OCFS methods, there are 34% of the features that are common. \nNote that although we only show the results on specific datasets in Table  3 and Fig.  1, \nsimilar trends are observed for other datasets as well, and to save space these results are \nnot included.\n\nPerformance of feature selection methods for English reviews\n\nUsing similar settings as described in “Performance of feature selection methods for \nTurkish reviews”, we also carried out experiments on the English review datasets. As \nshown in Table  4, QER achieved the best performance with LR classifier for the movie \nreview dataset and NBM classifier for other datasets. CHI2 and IG achieved better per-\nformance with NBM for all five datasets. Once again, the results are basically the same \nfor CHI2 and IG, indicating that the two methods are also strongly correlated for the \nEnglish review datasets. Compared with the Turkish movie reviews, the feature size for \nthe best performer of the English movie reviews is 3000, which is achieved with QER for \nthe LR classifier. This is likely due to the bigger vocabulary of the English movie reviews \nover that of the Turkish movie reviews as can be observed in Table  1. Also compared \nwith the Turkish review datasets, DFD is not as good as CHI2 and IG for the English \nreview datasets, even though the performance is close for the kitchen reviews and gener-\nally better than OCFS. Furthermore, the best results for DFD are achieved with differ-\nent classifiers for different datasets: SVM for the movie reviews and LR for the kitchen \nreviews. Statistical analysis with univariate ANOVA and post hoc tests show similar \nresults as those for the Turkish reviews: there are significant differences between three \ngroups (Baseline and OCFS), (DFD), and (CHI2, IG, and QER) at 95% confidence level \nfor the movie, DVDs, electronic, and kitchen review datasets, but for the book review \ndataset, there are significant differences between two groups (Baseline and OCFS) and \n(DFD, CHI2, IG, and QER) at the 95% confidence level.\n\nFor text classifiers, Table  4 shows that similar trends are observed for the English \nreviews as those for the Turkish reviews, although LR and SVM can over-perform NBM \nfor some feature selection methods. For different feature sizes, similar trends are also \nobserved, as illustrated in Fig.  2. Once again, in Table  5 and Fig.  2, we only show the \nresults for specific datasets, but the trends are similar to other datasets as well.\n\nIn summary, we see some similarities between Turkish and English reviews in that for \ndata pre-processing, we should keep punctual patterns and stop words, and not per-\nform stemming, leading us to use the same setting as the baselines for further study. \nIn addition, NBM seems to be the most suitable classifier for sentiment analysis since \nsentiment-expressing words tend to have low frequencies within a document, but rela-\ntively high frequencies across different documents. For feature selection methods, our \n\n\n\nPage 14 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nTa\nb\n\nle\n 4\n\n T\nh\n\ne\n b\n\ne\nst\n\n c\nla\n\nss\nifi\n\nca\nti\n\no\nn\n\n r\ne\n\nsu\nlt\n\ns \nfo\n\nr \np\n\na\nir\n\ns \no\n\nf \nfe\n\na\ntu\n\nre\n s\n\ne\nle\n\nct\nio\n\nn\n m\n\ne\nth\n\no\nd\n\ns \na\n\nn\nd\n\n t\nh\n\ne\n E\n\nn\ng\n\nli\nsh\n\n r\ne\n\nv\nie\n\nw\n d\n\na\nta\n\nse\nts\n\nQ\nE\n\nR\nD\n\nFD\nO\n\nC\nFS\n\nC\nH\n\nI2\nIG\n\nS\niz\n\ne\nF \n\nm\ne\n\nas\nu\n\nre\nS\n\niz\ne\n\nF \nm\n\ne\nas\n\nu\nre\n\nS\niz\n\ne\nF \n\nm\ne\n\nas\nu\n\nre\nS\n\niz\ne\n\nF \nm\n\ne\nas\n\nu\nre\n\nS\niz\n\ne\nF \n\nm\ne\n\nas\nu\n\nre\n\nM\no\n\nvi\ne\n\n30\n00\n\nLR\n:0\n\n.9\n5\n\n5\n0\n\n25\n00\n\nSV\nM\n\n:0\n.8\n\n64\n0\n\n30\n00\n\nSV\nM\n\n: 0\n.8\n\n28\n5\n\n25\n00\n\nN\nBM\n\n:0\n.9\n\n15\n0\n\n25\n00\n\nN\nBM\n\n:0\n.9\n\n15\n0\n\nD\nVD\n\ns\n25\n\n00\nN\n\nBM\n:0\n\n.9\n1\n\n6\n9\n\n30\n00\n\nN\nBM\n\n:0\n.8\n\n50\n2\n\n10\n00\n\nN\nBM\n\n:0\n.7\n\n99\n6\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n96\n4\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n96\n4\n\nEl\nec\n\ntr\no\n\nn\nic\n\ns\n20\n\n00\nN\n\nBM\n:0\n\n.8\n8\n\n7\n8\n\n15\n00\n\nN\nBM\n\n:0\n.8\n\n22\n1\n\n20\n00\n\nSV\nM\n\n: 0\n.7\n\n82\n1\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n62\n1\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n62\n1\n\nBo\no\n\nk\n30\n\n00\nN\n\nBM\n:0\n\n.9\n1\n\n6\n2\n\n30\n00\n\nN\nBM\n\n:0\n.8\n\n62\n8\n\n30\n00\n\nN\nBM\n\n:0\n.7\n\n89\n9\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n87\n9\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n87\n9\n\nK\nit\n\nch\nen\n\n20\n00\n\nN\nBM\n\n:0\n.9\n\n1\n0\n\n6\n30\n\n00\nLR\n\n:0\n.8\n\n89\n3\n\n15\n00\n\nSV\nM\n\n: 0\n.8\n\n15\n7\n\n50\n0\n\nN\nBM\n\n:0\n.8\n\n96\n4\n\n50\n0\n\nN\nBM\n\n:0\n.8\n\n96\n4\n\n\n\nPage 15 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nproposed QER achieves best performances with feature sizes between 2000 and 3000. \nCHI2 and IG are strongly correlated and tend to work well with smaller feature sizes, \nwhile DFD also works reasonably well, but with bigger feature sizes. For differences, \nthe English review datasets usually have bigger vocabulary, resulting in relatively big-\nger feature sizes for feature selection. Moreover, SVM and LR can also perform well for \nsome English review datasets, while NBM looks like a dominant classifier for the Turk-\nish reviews. Finally, the performance results for the English reviews are generally higher \nthan those for the Turkish reviews, possibly related to the differences between the two \nlanguages in terms of vocabularies, writing styles, and the agglutinative property of the \nTurkish language. The limitation of QER is that it is only suitable for classifying two \nclasses since it is especially developed for sentiment analysis with the observation that \nsentiment-expressing words are usually more frequent across different reviews. The con-\ntribution of QER is that, as it is shown in the experimental results, the method is both \nlanguage and classifier independent and can select better features than other methods \nfor sentiment analysis.\n\nComparison of our proposal with the previous studies\n\nIt is generally difficult to directly compare the results of different studies since there are \noften differences in partitioning and preprocessing the datasets for training and testing, \nas shown in the studies by Pang et al. [4]. That is why we tried different combinations of \nfeature selection methods and text classifiers on multiple datasets in our research so that \n\nFig. 2 Detailed results of feature sizes for the English DVDs review dataset\n\nTable 5 Detailed results for the English DVD review dataset\n\nNBM SVM LR J48\n\nSize F measure Size F measure Size F measure Size F measure\n\nQER 2500 0.9169 3000 0.8724 2000 0.8977 2000 0.5481\n\nCHI2 1000 0.8964 500 0.8650 3000 0.6976 3000 0.6799\n\nIG 1000 0.8964 1000 0.8614 2000 0.6970 500 0.6769\n\nDFD 3000 0.8502 1000 0.8293 3000 0.7600 500 0.6771\n\nOCFS 1000 0.7996 1000 0.7714 500 0.6800 2000 0.6829\n\n 0,9100 0,8900 0,8700 0,8500 QER F Measure 0,8300 . CHI2 - IG 0,8100 -x- DFD 0,7900 - OCFS 0,7700 0,7500 O 500 1000 1500 2000 2500 3000 3500 Feature Size \n\n\n\nPage 16 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nwe can compare their performance collectively and accurately. However, we do agree \nthat it is helpful to describe the results from the related studies so that we can put our \nresults into a suitable context. Table 6 includes a summary for comparison of our results \nwith that of the previous studies which have used the same datasets with our study. For \nthe English movie review dataset, Nicholls and Song [8] obtained a baseline accuracy \nof 79.9% with the MEM classifier, and better classification accuracies of 86.9, 85.7, and \n80.9% when combined with DFD, CHI2, and OCFS feature selection methods, respec-\ntively. Dang et al. [23] examined their proposed semantic oriented method on the prod-\nuct dataset [37]. They achieved an accuracy of 84.2% for the kitchen dataset. Also, Xia \net al. [24] improved the classification performances from 84.8 to 87.7% using their pro-\nposed word relation based feature selection method. Bai [25] improved the accuracies \nfrom baseline 84.1–92.7% using their proposed Tabu search-enhanced Markov blanket \nmodel for the movie review dataset. Pang et al. [4] obtained accuracy around 78.7% with \nNB using the document frequency of 4 to eliminate the rare features. Agarwal et al. [9] \nimproved the accuracies from baseline 82.7–89.2% using IG feature selection method \nwith Boolean NBM. Our proposed QER method showed an improvement from the \nbaseline of 81.3–91.1% with NBM in terms of F measures.\n\nFor the Turkish movie review dataset, the best classification result of 82.58% is \nobtained with the SVM classifier [30]. As shown in the previous studies, classification \naccuracy is improved by applying feature selection, and NB based classifier performs the \nbest in the majority of the cases. The proposed feature selection method is also com-\nputationally efficient and easy to implement as it only computes scores for features by \ncounting document frequencies.\n\nConclusions\nIn this paper, we proposed a new feature selection method query expansion ranking \n(QER) for the sentiment analysis and compared it with the common feature selection \nmethods for sentiment classification, including DFD and OCFS, CHI2 and IG. All of \nthese methods are tested against five datasets of Turkish reviews, using four common \n\nTable 6 Summary of related work on the sentiment analysis for the same datasets\n\nPaper Dataset Baseline accuracy (%) Best accuracies observed (%) Classifier\n\n[4] Movie 78.7 NB, SVM\n\n[7] Movie 87.1 minimum cut SVM\n\n[8] Movie\nProduct\n\n79.9\n74.3\n\n85.7 CHI2; 86.9 DFD; 80.9 OCFS\n73.7 CHI2; 75 DFD; 73.8 OCFS\n\nMEM\n\n[9] Movie\nProduct\n\n84.2\n80.9 Book; 78.9 DVD; 80.8 El\n\n91.8\n92.5 Book; 91.5 DVD; 91.8 El\nmRMR with composite features\n\nBNBM, SVM\n\n[23] Product 70.1 84.2% Kitc. semantic orientation SVM\n\n[24] Movie\nProduct\n\n84.8\n74.7 Book; 77.2 DVD; 80.8 El.; \n\n83.3 Kitc\n\n87.7\n81.8 Book; 83.8 DVD; 85.9 El.; 88.7 Kitc \n\nword relation based method\n\nNB, SVM, MEM\n\n[25] Movie 84.1 92.7% Tabu search-enhanced Markov \nblanket model\n\nNB, SVM, MEM\n\nOur study Movie\nProduct\n\n84.8\n76.2 Book; 78.4 DVD; 78.6 \n\nElect; 81.4 Kitc\n\n91.5 CHI2-IG; 87.1 DFD; 82.9 OCFS; \n95.5 91.6 Book; 91.7 DVD; 88.8 Elect; \n91.1 Kitc proposed QER\n\nNBM, SVM, MEM, DT\n\n\n\nPage 17 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\ntext classifiers, including NBM, SVM, logistic regression (LR), and decision trees (J48). \nSimilar experiments are also conducted for English reviews so that we can compare \ntheir differences with the Turkish reviews. Our results show that for all Turkish review \ndatasets, the best results are all obtained with the NBM classifier, and for some Eng-\nlish review datasets, LR and SVM have the best performance. For feature selection, our \nproposed QER method helps to achieve the best performance compared with all other \nfeature selection methods for both Turkish and English reviews. For feature selection, \nour experiments show that our proposed QER method helps to achieve the best per-\nformance among all other feature selection methods. We found that CHI2 and IG have \nalmost the same performance for the Turkish reviews and they tend to work well with \nsmaller feature sizes compared with other feature selection methods. DFD does reason-\nably well across all review datasets, but it tends to favour bigger feature sizes. This con-\nfirms our intuition that sentiment-expressing words usually have low frequencies within \na document, but relatively high frequencies across different documents. Although OCFS \nis quite robust for traditional topical text classification, it does not do well for sentiment \nanalysis since it relies on word frequencies to measure the distances between docu-\nments. Once again, NBM remains the best performer for most of our experiments when \nanalysed with QER method. Overall, feature selection methods are shown to be effective \nfor sentiment analysis, improving significantly over the baseline results.\n\nFollowing a similar process, we also carried out experiments on English review data-\nsets and NBM seems to be the most suitable classifier for sentiment analysis. For fea-\nture selection methods, CHI2 and IG are strongly correlated and tend to work well with \nsmaller feature sizes, while DFD also works reasonably well, but with bigger feature \nsizes. Our proposed query expansion ranking method achieves the best performances \nfor the English datasets as well. As for differences, the English review datasets usually \nhave a bigger vocabulary, resulting in relatively bigger feature sizes for feature selection. \nMoreover, LR and SVM also perform well for some English review datasets, while NBM \nlooks like a dominant classifier for the Turkish reviews. The performance results for the \nEnglish reviews are generally higher than those for the Turkish reviews, possibly related \nto the differences between the two languages in terms of vocabularies, writing styles, \nand the agglutinative property of the Turkish language. Finally, the experimental results \nshow that our proposal QER method is language, domain and classifier independent \nand improve the classification performance better than other FS methods for sentiment \nanalysis.\nAuthors’ contributions\nTP drafted this manuscript, conducted experiments using the datasets and analyzed the results. SAO and FS suggested \nthe methods used in this study and provided guidelines in drafting the manuscript. FS edited and corrected the manu-\nscript. All authors read and approved the final manuscript.\n\nAuthors’ information\nTP received her Ph.D. degree in Computer Engineering from Çukurova University in 2016. She received a Bachelor of \nEngineering degree in Computer Engineering from Hacettepe University, and she holds a M.Sc. in Management Infor-\nmation Sciences and a M.Sc. in Mathematics. She studied for 4 months of 2015 as a visiting researcher in University of \nGuelph, Canada with a scholarship supporting by The Scientific and Technological Research Council of Turkey (TUBITAK). \nShe is currently working as a senior lecturer and head of the Computer Technologies Department, Antakya Vocational \nSchool, Mustafa Kemal University. Her research interest is in sentiment analysis, data mining, machine learning, and \napplying text processing techniques to medical data extraction and integration.\n\nSAO received her Ph.D. and Bachelor of Science degrees both in Computer Engineering from Bilkent University, \nTurkey, in 2004 and 1996, respectively. Currently she is a professor and head of the Department of Computer Engineer-\ning, Çukurova University, Turkey. Her research interests include text mining, information retrieval systems, and applying \nbiological and nature inspired computing to text mining.\n\n\n\nPage 18 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\nFS received his Ph.D. degree in Computer Science from the University of Waterloo in Canada. He is currently an \nassociate professor in the School of Computer Science, University of Guelph in Canada. His interests are mostly in Natural \nLanguage Processing, working on a wide range of topic areas, including information retrieval, text classification, topic \nmodeling, key phrase extraction, text segmentation, sentiment analysis, text summarization, and document clustering. \nMore recently, he is also interested in applying text processing techniques to privacy policy analysis and medical data \nextraction and integration.\n\nAuthor details\n1 Department of Mathematics, Mustafa Kemal University, Antakya, Hatay, Turkey. 2 Department of Computer Engineering, \nÇukurova University, Adana, Turkey. 3 School of Computer Science, University of Guelph, Guelph, Canada. \n\nAcknowledgements\nThis research is supported by TUBITAK-2214-A.\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAvailability of data and materials\nNot applicable.\n\nEthics approval and consent to participate\nNot applicable.\n\nFunding\nThis research is supported by Çukurova University Fund of Scientific Research Projects under Grant No. FDK-2015-3833, \nand Mustafa Kemal University Fund of Scientific Research Projects under Grant No. 15426.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 10 February 2018   Accepted: 16 April 2018\n\nReferences\n 1. Pang B, Lee L (2008) Opinion mining and sentiment analysis. Found Trends Inf Retr 2:1–135. https://doi.\n\norg/10.1561/1500000011\n 2. Tripathy A, Anand A, Rath SK (2017) Document-level sentiment classification using hybrid machine learning \n\napproach. Knowl Inf Syst 53:805–831. https://doi.org/10.1007/s10115-017-1055-z\n 3. Yang D-H, Yu G (2013) A method of feature selection and sentiment similarity for Chinese micro-blogs. J Inf Sci \n\n39:429–441. https://doi.org/10.1177/0165551513480308\n 4. Pang B, Lee L, Vaithyanathan S (2002) Thumbs up? In: Proceedings of the ACL-02 conference on empirical methods \n\nin natural language processing—EMNLP’02. Association for computational linguistics, Morristown, pp 79–86\n 5. Mullen T, Collier N (2004) Sentiment analysis using support vector machines with diverse information sources. Conf \n\nEmpir Methods Nat Lang Process. https://doi.org/10.3115/1219044.1219069\n 6. Kaya M, Fidan G, Toroslu IH (2012) Sentiment analysis of Turkish political news. In: 2012 IEEE/WIC/ACM international \n\nconferences on web intelligence and intelligent agent technology. IEEE, Macau, pp 174–180\n 7. Pang B, Lee L (2004) A sentimental education. In: Proceedings of the 42nd annual meeting on association for com-\n\nputational linguistics—ACL’04. Association for Computational Linguistics, Morristown, p 271–es\n 8. Nicholls C, Song F (2010) Comparison of feature selection methods for sentiment analysis. In: Advances in artificial \n\nintelligence. Springer, Berlin, pp 286–289\n 9. Agarwal B, Mittal N (2016) Prominent feature extraction for review analysis: an empirical study. J Exp Theor Artif Intell \n\n28:485–498. https://doi.org/10.1080/0952813X.2014.977830\n 10. Parlar T, Ozel SA (2016) A new feature selection method for sentiment analysis of Turkish reviews. In: International \n\nSymposium on INnovations in Intelligent SysTems and Applications (INISTA). IEEE, Sinaia, pp 1–6\n 11. Fattah MA (2017) A novel statistical feature selection approach for text categorization. J Inf Process Syst 13:1397–\n\n1409. https://doi.org/10.3745/JIPS.02.0076\n 12. Harman D (1992) Relevance feedback revisited. In: Proceedings of the 15th annual international ACM SIGIR confer-\n\nence on Research and development in information retrieval—SIGIR’92. ACM Press, New York, pp 1–10\n 13. Robertson SE, Jones KS (1976) Relevance weighting of search terms. J Am Soc Inf Sci 27:129–146. https://doi.\n\norg/10.1002/asi.4630270302\n 14. Aldoğan D, Yaslan Y (2017) A comparison study on active learning integrated ensemble approaches in sentiment \n\nanalysis. Comput Electr Eng 57:311–323. https://doi.org/10.1016/J.COMPELECENG.2016.11.015\n 15. Singh J, Singh G, Singh R (2017) Optimization of sentiment analysis using machine learning classifiers. Hum centric \n\nComput Inf Sci 7:32. https://doi.org/10.1186/s13673-017-0116-3\n 16. Mladenović M, Mitrović J, Krstev C, Vitas D (2016) Hybrid sentiment analysis framework for a morphologically rich \n\nlanguage. J Intell Inf Syst 46:599–620. https://doi.org/10.1007/s10844-015-0372-5\n 17. Asgarian E, Kahani M, Sharifi S (2018) The impact of sentiment features on the sentiment polarity classification in \n\nPersian reviews. Cognit Comput 10:117–135. https://doi.org/10.1007/s12559-017-9513-1\n\n Published online: 09 May 2018 \n\nhttps://doi.org/10.1561/1500000011\nhttps://doi.org/10.1561/1500000011\nhttps://doi.org/10.1007/s10115-017-1055-z\nhttps://doi.org/10.1177/0165551513480308\nhttps://doi.org/10.3115/1219044.1219069\nhttps://doi.org/10.1080/0952813X.2014.977830\nhttps://doi.org/10.3745/JIPS.02.0076\nhttps://doi.org/10.1002/asi.4630270302\nhttps://doi.org/10.1002/asi.4630270302\nhttps://doi.org/10.1016/J.COMPELECENG.2016.11.015\nhttps://doi.org/10.1186/s13673-017-0116-3\nhttps://doi.org/10.1007/s10844-015-0372-5\nhttps://doi.org/10.1007/s12559-017-9513-1\n\n\nPage 19 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci.  (2018) 8:10 \n\n 18. Guyon I, Elisseeff A (2003) An introduction to variable and feature selection. J Mach Learn Res 3:1157–1182. https://\ndoi.org/10.1016/j.aca.2011.07.027\n\n 19. Akba F, Uçan A, Sezer E, Sever H (2014) Assessment of feature selection metrics for sentiment analyses: Turkish \nmovie reviews. In: 8th European conference on data mining. Lisbon, Portugal, pp 180–184\n\n 20. Liu Y, Bi JW, Fan ZP (2017) Multi-class sentiment classification: the experimental comparisons of feature selection \nand machine learning algorithms. Expert Syst Appl 80:323–339. https://doi.org/10.1016/j.eswa.2017.03.042\n\n 21. Sagar K, Saha A (2017) Qualitative usability feature selection with ranking: a novel approach for ranking the identi-\nfied usability problematic attributes for academic websites using data-mining techniques. Hum centric Comput Inf \nSci 7:29. https://doi.org/10.1186/s13673-017-0111-8\n\n 22. Abbasi A, Chen H, Salem A (2008) Sentiment analysis in multiple languages: Feature selection for opinion classifica-\ntion in Web forums. ACM Trans Inf Syst 26:1–34. https://doi.org/10.1145/1361684.1361685\n\n 23. Dang Y, Zhang Y, Chen H (2010) A Lexicon-enhanced method for sentiment classification: an experiment on online \nproduct reviews. IEEE Intell Syst 25:46–53. https://doi.org/10.1109/MIS.2009.105\n\n 24. Xia R, Zong C, Li S (2011) Ensemble of feature sets and classification algorithms for sentiment classification. Inf Sci \n(Ny) 181:1138–1152. https://doi.org/10.1016/j.ins.2010.11.023\n\n 25. Bai X (2011) Predicting consumer sentiments from online text. Decis Support Syst 50:732–742. https://doi.\norg/10.1016/j.dss.2010.08.024\n\n 26. Yan J, Liu N, Zhang B, et al (2005) OCFS: optimal orthogonal centroid feature selection for text categorization. In: \nProceedings of the 28th annual international ACM SIGIR conference on Research and development in information \nretrieval—SIGIR’05. ACM Press, New York, p 122\n\n 27. Zheng L, Wang H, Gao S (2018) Sentimental feature selection for sentiment analysis of Chinese online reviews. Int J \nMach Learn Cybern 9:75–84. https://doi.org/10.1007/s13042-015-0347-4\n\n 28. Demirtas E, Pechenizkiy M (2013) Cross-lingual polarity detection with machine translation. In: Second international \nworkshop on issues of sentiment discovery and opinion mining—WISDOM’13. ACM Press, New York, pp 1–8\n\n 29. Boynukalin Z (2012) Emotion analysis of Turkish texts by using machine learning methods. M.Sc. Thesis, Middle East \nTechnical University\n\n 30. Sevindi BI (2013) Türkçe Metinlerde Denetimli ve Sözlük Tabanlı Duygu Analizi Yaklaşımlarının Karşılaştırılması. M.Sc. \nThesis, Gazi University\n\n 31. Parlar T, Özel SA, Song F (2018) Interactions between term weighting and feature selection methods on the senti-\nment analysis of Turkish reviews. In: Computational linguistics and intelligent text processing. CICLing 2016. Lecture \nNotes in computer Science, vol 9624. Springer, Cham, pp 335–346\n\n 32. Çakici R (2009) Wide-coverage parsing for Turkish. Ph.D. Thesis, University of Edinburgh\n 33. Witten IH, Frank E, Hall MA (2011) Data mining: practical machine learning tools and techniques. Morgan Kaufmann, \n\nBurlington\n 34. McCallum A, Nigam K (1998) A comparison of event models for naive Bayes text classification. In: AAAI/ICML-98 \n\nworkshop on learning for text categorization. pp 41–48\n 35. Zhao X, Li D, Yang B et al (2015) A two-stage feature selection method with its application. Comput Electr Eng \n\n47:114–125. https://doi.org/10.1016/J.COMPELECENG.2015.08.011\n 36. Harman D (1988) Towards interactive query expansion. In: Proceedings of the 11th annual international ACM SIGIR \n\nconference on Research and development in information retrieval—SIGIR’88. ACM Press, New York, pp 321–331\n 37. Blitzer J, Dredze M, Pereira F (2007) Biographies, bollywood, boom-boxes and blenders: domain adaptation for senti-\n\nment classification. In: 45th annual meeting-association for computational linguistics. pp 440–447\n 38. Bird S, Klein E, Loper E (2009) Natural language processing with Python. O’Reilly, Newton\n 39. Cai J, Song F (2008) Maximum entropy modeling with feature selection for text categorization. In: Lecture notes in \n\ncomputer science (including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics). pp \n549–554\n\n 40. Joachims T (1998) Text categorization with support vector machines: learning with many relevant features. Springer, \nBerlin, pp 137–142\n\nhttps://doi.org/10.1016/j.aca.2011.07.027\nhttps://doi.org/10.1016/j.aca.2011.07.027\nhttps://doi.org/10.1016/j.eswa.2017.03.042\nhttps://doi.org/10.1186/s13673-017-0111-8\nhttps://doi.org/10.1145/1361684.1361685\nhttps://doi.org/10.1109/MIS.2009.105\nhttps://doi.org/10.1016/j.ins.2010.11.023\nhttps://doi.org/10.1016/j.dss.2010.08.024\nhttps://doi.org/10.1016/j.dss.2010.08.024\nhttps://doi.org/10.1007/s13042-015-0347-4\nhttps://doi.org/10.1016/J.COMPELECENG.2015.08.011\n\n\tQER: a new feature selection method for sentiment analysis\n\tAbstract \n\tIntroduction\n\tRelated work\n\tMethods\n\tMachine learning algorithms\n\tFeature selection\n\tInformation gain\n\tChi square (CHI2)\n\tDocument frequency difference\n\tOptimal orthogonal centroid (OCFS)\n\tQuery expansion ranking\n\n\n\tExperiments and results\n\tDatasets\n\tPerformance evaluation\n\tExperimental settings\n\tPerformance of feature selection methods for Turkish reviews\n\tPerformance of feature selection methods for English reviews\n\tComparison of our proposal with the previous studies\n\n\tConclusions\n\tAuthors’ contributions\n\tReferences\n\n\n\n\n",
      "text": [
        "",
        "0,9200 0,9000 0,8800 QER F Measure 0,8600 CHI2 X IG 0,8400 X -x- DFD -x OCFS 0,8200 0,8000 O 500 1000 1500 2000 2500 3000 3500 4000 Feature Size",
        "0,9100 0,8900 0,8700 0,8500 QER F Measure 0,8300 . CHI2 - IG 0,8100 -x- DFD 0,7900 - OCFS 0,7700 0,7500 O 500 1000 1500 2000 2500 3000 3500 Feature Size",
        "Published online: 09 May 2018"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"0,9200 0,9000 0,8800 QER F Measure 0,8600 CHI2 X IG 0,8400 X -x- DFD -x OCFS 0,8200 0,8000 O 500 1000 1500 2000 2500 3000 3500 4000 Feature Size\",\"lines\":[{\"boundingBox\":[{\"x\":42,\"y\":1},{\"x\":119,\"y\":1},{\"x\":120,\"y\":23},{\"x\":42,\"y\":24}],\"text\":\"0,9200\"},{\"boundingBox\":[{\"x\":42,\"y\":82},{\"x\":119,\"y\":81},{\"x\":120,\"y\":104},{\"x\":42,\"y\":106}],\"text\":\"0,9000\"},{\"boundingBox\":[{\"x\":42,\"y\":165},{\"x\":120,\"y\":164},{\"x\":120,\"y\":188},{\"x\":42,\"y\":188}],\"text\":\"0,8800\"},{\"boundingBox\":[{\"x\":1003,\"y\":195},{\"x\":1050,\"y\":195},{\"x\":1050,\"y\":216},{\"x\":1002,\"y\":216}],\"text\":\"QER\"},{\"boundingBox\":[{\"x\":1,\"y\":320},{\"x\":3,\"y\":200},{\"x\":23,\"y\":200},{\"x\":22,\"y\":320}],\"text\":\"F Measure\"},{\"boundingBox\":[{\"x\":43,\"y\":248},{\"x\":120,\"y\":247},{\"x\":120,\"y\":270},{\"x\":43,\"y\":271}],\"text\":\"0,8600\"},{\"boundingBox\":[{\"x\":1004,\"y\":244},{\"x\":1056,\"y\":244},{\"x\":1055,\"y\":265},{\"x\":1004,\"y\":264}],\"text\":\"CHI2\"},{\"boundingBox\":[{\"x\":684,\"y\":268},{\"x\":699,\"y\":268},{\"x\":699,\"y\":280},{\"x\":684,\"y\":280}],\"text\":\"X\"},{\"boundingBox\":[{\"x\":1001,\"y\":289},{\"x\":1031,\"y\":289},{\"x\":1031,\"y\":315},{\"x\":1001,\"y\":316}],\"text\":\"IG\"},{\"boundingBox\":[{\"x\":42,\"y\":330},{\"x\":119,\"y\":329},{\"x\":120,\"y\":351},{\"x\":42,\"y\":353}],\"text\":\"0,8400\"},{\"boundingBox\":[{\"x\":219,\"y\":343},{\"x\":235,\"y\":343},{\"x\":235,\"y\":355},{\"x\":219,\"y\":355}],\"text\":\"X\"},{\"boundingBox\":[{\"x\":947,\"y\":342},{\"x\":1049,\"y\":341},{\"x\":1050,\"y\":362},{\"x\":947,\"y\":363}],\"text\":\"-x- DFD\"},{\"boundingBox\":[{\"x\":947,\"y\":390},{\"x\":1062,\"y\":390},{\"x\":1061,\"y\":412},{\"x\":947,\"y\":411}],\"text\":\"-x OCFS\"},{\"boundingBox\":[{\"x\":43,\"y\":412},{\"x\":120,\"y\":412},{\"x\":120,\"y\":435},{\"x\":43,\"y\":435}],\"text\":\"0,8200\"},{\"boundingBox\":[{\"x\":42,\"y\":495},{\"x\":120,\"y\":494},{\"x\":121,\"y\":517},{\"x\":42,\"y\":519}],\"text\":\"0,8000\"},{\"boundingBox\":[{\"x\":125,\"y\":541},{\"x\":125,\"y\":520},{\"x\":137,\"y\":521},{\"x\":137,\"y\":541}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":205,\"y\":519},{\"x\":249,\"y\":519},{\"x\":248,\"y\":540},{\"x\":204,\"y\":540}],\"text\":\"500\"},{\"boundingBox\":[{\"x\":292,\"y\":520},{\"x\":352,\"y\":519},{\"x\":352,\"y\":540},{\"x\":292,\"y\":540}],\"text\":\"1000\"},{\"boundingBox\":[{\"x\":375,\"y\":516},{\"x\":447,\"y\":516},{\"x\":447,\"y\":543},{\"x\":375,\"y\":544}],\"text\":\"1500\"},{\"boundingBox\":[{\"x\":469,\"y\":515},{\"x\":538,\"y\":516},{\"x\":538,\"y\":543},{\"x\":469,\"y\":543}],\"text\":\"2000\"},{\"boundingBox\":[{\"x\":566,\"y\":516},{\"x\":637,\"y\":516},{\"x\":637,\"y\":543},{\"x\":566,\"y\":543}],\"text\":\"2500\"},{\"boundingBox\":[{\"x\":657,\"y\":516},{\"x\":730,\"y\":516},{\"x\":730,\"y\":543},{\"x\":657,\"y\":543}],\"text\":\"3000\"},{\"boundingBox\":[{\"x\":756,\"y\":518},{\"x\":846,\"y\":519},{\"x\":846,\"y\":541},{\"x\":756,\"y\":540}],\"text\":\"3500\"},{\"boundingBox\":[{\"x\":847,\"y\":519},{\"x\":907,\"y\":519},{\"x\":907,\"y\":540},{\"x\":846,\"y\":539}],\"text\":\"4000\"},{\"boundingBox\":[{\"x\":434,\"y\":569},{\"x\":576,\"y\":569},{\"x\":575,\"y\":590},{\"x\":434,\"y\":590}],\"text\":\"Feature Size\"}],\"words\":[{\"boundingBox\":[{\"x\":42,\"y\":1},{\"x\":119,\"y\":1},{\"x\":120,\"y\":23},{\"x\":43,\"y\":25}],\"text\":\"0,9200\"},{\"boundingBox\":[{\"x\":43,\"y\":83},{\"x\":119,\"y\":82},{\"x\":120,\"y\":105},{\"x\":45,\"y\":106}],\"text\":\"0,9000\"},{\"boundingBox\":[{\"x\":43,\"y\":166},{\"x\":120,\"y\":165},{\"x\":120,\"y\":188},{\"x\":43,\"y\":189}],\"text\":\"0,8800\"},{\"boundingBox\":[{\"x\":1003,\"y\":195},{\"x\":1050,\"y\":195},{\"x\":1050,\"y\":216},{\"x\":1003,\"y\":216}],\"text\":\"QER\"},{\"boundingBox\":[{\"x\":2,\"y\":320},{\"x\":2,\"y\":304},{\"x\":22,\"y\":304},{\"x\":21,\"y\":321}],\"text\":\"F\"},{\"boundingBox\":[{\"x\":2,\"y\":300},{\"x\":3,\"y\":201},{\"x\":24,\"y\":202},{\"x\":22,\"y\":301}],\"text\":\"Measure\"},{\"boundingBox\":[{\"x\":44,\"y\":249},{\"x\":120,\"y\":247},{\"x\":120,\"y\":271},{\"x\":44,\"y\":272}],\"text\":\"0,8600\"},{\"boundingBox\":[{\"x\":1004,\"y\":244},{\"x\":1056,\"y\":244},{\"x\":1056,\"y\":265},{\"x\":1004,\"y\":264}],\"text\":\"CHI2\"},{\"boundingBox\":[{\"x\":686,\"y\":268},{\"x\":698,\"y\":268},{\"x\":698,\"y\":280},{\"x\":686,\"y\":280}],\"text\":\"X\"},{\"boundingBox\":[{\"x\":1001,\"y\":289},{\"x\":1029,\"y\":289},{\"x\":1030,\"y\":316},{\"x\":1001,\"y\":316}],\"text\":\"IG\"},{\"boundingBox\":[{\"x\":42,\"y\":330},{\"x\":120,\"y\":329},{\"x\":120,\"y\":352},{\"x\":43,\"y\":354}],\"text\":\"0,8400\"},{\"boundingBox\":[{\"x\":222,\"y\":343},{\"x\":234,\"y\":343},{\"x\":234,\"y\":355},{\"x\":222,\"y\":355}],\"text\":\"X\"},{\"boundingBox\":[{\"x\":948,\"y\":343},{\"x\":999,\"y\":343},{\"x\":998,\"y\":363},{\"x\":948,\"y\":364}],\"text\":\"-x-\"},{\"boundingBox\":[{\"x\":1003,\"y\":343},{\"x\":1050,\"y\":342},{\"x\":1049,\"y\":363},{\"x\":1002,\"y\":363}],\"text\":\"DFD\"},{\"boundingBox\":[{\"x\":948,\"y\":390},{\"x\":987,\"y\":391},{\"x\":986,\"y\":412},{\"x\":947,\"y\":410}],\"text\":\"-x\"},{\"boundingBox\":[{\"x\":1004,\"y\":391},{\"x\":1062,\"y\":390},{\"x\":1062,\"y\":411},{\"x\":1003,\"y\":412}],\"text\":\"OCFS\"},{\"boundingBox\":[{\"x\":43,\"y\":413},{\"x\":120,\"y\":412},{\"x\":120,\"y\":435},{\"x\":43,\"y\":435}],\"text\":\"0,8200\"},{\"boundingBox\":[{\"x\":42,\"y\":496},{\"x\":121,\"y\":495},{\"x\":120,\"y\":518},{\"x\":43,\"y\":520}],\"text\":\"0,8000\"},{\"boundingBox\":[{\"x\":125,\"y\":539},{\"x\":125,\"y\":527},{\"x\":137,\"y\":527},{\"x\":137,\"y\":539}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":204,\"y\":519},{\"x\":249,\"y\":519},{\"x\":249,\"y\":540},{\"x\":204,\"y\":540}],\"text\":\"500\"},{\"boundingBox\":[{\"x\":292,\"y\":519},{\"x\":351,\"y\":519},{\"x\":351,\"y\":540},{\"x\":292,\"y\":540}],\"text\":\"1000\"},{\"boundingBox\":[{\"x\":382,\"y\":516},{\"x\":446,\"y\":516},{\"x\":446,\"y\":544},{\"x\":382,\"y\":544}],\"text\":\"1500\"},{\"boundingBox\":[{\"x\":475,\"y\":515},{\"x\":536,\"y\":515},{\"x\":536,\"y\":543},{\"x\":474,\"y\":543}],\"text\":\"2000\"},{\"boundingBox\":[{\"x\":568,\"y\":516},{\"x\":636,\"y\":516},{\"x\":636,\"y\":543},{\"x\":568,\"y\":543}],\"text\":\"2500\"},{\"boundingBox\":[{\"x\":661,\"y\":516},{\"x\":729,\"y\":516},{\"x\":729,\"y\":543},{\"x\":661,\"y\":543}],\"text\":\"3000\"},{\"boundingBox\":[{\"x\":757,\"y\":519},{\"x\":820,\"y\":519},{\"x\":820,\"y\":541},{\"x\":756,\"y\":541}],\"text\":\"3500\"},{\"boundingBox\":[{\"x\":849,\"y\":519},{\"x\":906,\"y\":519},{\"x\":906,\"y\":540},{\"x\":849,\"y\":540}],\"text\":\"4000\"},{\"boundingBox\":[{\"x\":435,\"y\":569},{\"x\":522,\"y\":570},{\"x\":522,\"y\":591},{\"x\":435,\"y\":590}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":526,\"y\":570},{\"x\":576,\"y\":570},{\"x\":576,\"y\":591},{\"x\":526,\"y\":591}],\"text\":\"Size\"}]}",
        "{\"language\":\"en\",\"text\":\"0,9100 0,8900 0,8700 0,8500 QER F Measure 0,8300 . CHI2 - IG 0,8100 -x- DFD 0,7900 - OCFS 0,7700 0,7500 O 500 1000 1500 2000 2500 3000 3500 Feature Size\",\"lines\":[{\"boundingBox\":[{\"x\":47,\"y\":20},{\"x\":137,\"y\":19},{\"x\":138,\"y\":46},{\"x\":47,\"y\":48}],\"text\":\"0,9100\"},{\"boundingBox\":[{\"x\":46,\"y\":85},{\"x\":137,\"y\":84},{\"x\":137,\"y\":111},{\"x\":46,\"y\":112}],\"text\":\"0,8900\"},{\"boundingBox\":[{\"x\":47,\"y\":150},{\"x\":137,\"y\":150},{\"x\":137,\"y\":176},{\"x\":47,\"y\":177}],\"text\":\"0,8700\"},{\"boundingBox\":[{\"x\":46,\"y\":214},{\"x\":136,\"y\":214},{\"x\":137,\"y\":241},{\"x\":46,\"y\":242}],\"text\":\"0,8500\"},{\"boundingBox\":[{\"x\":992,\"y\":203},{\"x\":1047,\"y\":202},{\"x\":1047,\"y\":226},{\"x\":991,\"y\":227}],\"text\":\"QER\"},{\"boundingBox\":[{\"x\":2,\"y\":350},{\"x\":3,\"y\":206},{\"x\":26,\"y\":207},{\"x\":24,\"y\":350}],\"text\":\"F Measure\"},{\"boundingBox\":[{\"x\":46,\"y\":278},{\"x\":140,\"y\":277},{\"x\":140,\"y\":308},{\"x\":46,\"y\":309}],\"text\":\"0,8300\"},{\"boundingBox\":[{\"x\":948,\"y\":259},{\"x\":1055,\"y\":259},{\"x\":1055,\"y\":285},{\"x\":948,\"y\":285}],\"text\":\". CHI2\"},{\"boundingBox\":[{\"x\":927,\"y\":317},{\"x\":1026,\"y\":315},{\"x\":1026,\"y\":343},{\"x\":927,\"y\":344}],\"text\":\"- IG\"},{\"boundingBox\":[{\"x\":46,\"y\":345},{\"x\":137,\"y\":345},{\"x\":137,\"y\":371},{\"x\":46,\"y\":372}],\"text\":\"0,8100\"},{\"boundingBox\":[{\"x\":925,\"y\":375},{\"x\":1047,\"y\":374},{\"x\":1048,\"y\":398},{\"x\":925,\"y\":399}],\"text\":\"-x- DFD\"},{\"boundingBox\":[{\"x\":47,\"y\":410},{\"x\":137,\"y\":410},{\"x\":138,\"y\":436},{\"x\":47,\"y\":437}],\"text\":\"0,7900\"},{\"boundingBox\":[{\"x\":925,\"y\":432},{\"x\":1061,\"y\":432},{\"x\":1062,\"y\":455},{\"x\":925,\"y\":456}],\"text\":\"- OCFS\"},{\"boundingBox\":[{\"x\":47,\"y\":475},{\"x\":137,\"y\":474},{\"x\":138,\"y\":501},{\"x\":47,\"y\":503}],\"text\":\"0,7700\"},{\"boundingBox\":[{\"x\":45,\"y\":538},{\"x\":142,\"y\":537},{\"x\":142,\"y\":568},{\"x\":45,\"y\":568}],\"text\":\"0,7500\"},{\"boundingBox\":[{\"x\":146,\"y\":596},{\"x\":146,\"y\":569},{\"x\":161,\"y\":569},{\"x\":160,\"y\":596}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":225,\"y\":568},{\"x\":283,\"y\":568},{\"x\":283,\"y\":598},{\"x\":224,\"y\":598}],\"text\":\"500\"},{\"boundingBox\":[{\"x\":310,\"y\":568},{\"x\":391,\"y\":568},{\"x\":392,\"y\":598},{\"x\":309,\"y\":598}],\"text\":\"1000\"},{\"boundingBox\":[{\"x\":399,\"y\":568},{\"x\":798,\"y\":568},{\"x\":798,\"y\":599},{\"x\":399,\"y\":599}],\"text\":\"1500 2000 2500 3000\"},{\"boundingBox\":[{\"x\":807,\"y\":568},{\"x\":882,\"y\":568},{\"x\":882,\"y\":598},{\"x\":807,\"y\":598}],\"text\":\"3500\"},{\"boundingBox\":[{\"x\":414,\"y\":627},{\"x\":584,\"y\":628},{\"x\":584,\"y\":653},{\"x\":414,\"y\":652}],\"text\":\"Feature Size\"}],\"words\":[{\"boundingBox\":[{\"x\":47,\"y\":20},{\"x\":137,\"y\":19},{\"x\":136,\"y\":46},{\"x\":48,\"y\":49}],\"text\":\"0,9100\"},{\"boundingBox\":[{\"x\":46,\"y\":86},{\"x\":136,\"y\":85},{\"x\":136,\"y\":112},{\"x\":48,\"y\":113}],\"text\":\"0,8900\"},{\"boundingBox\":[{\"x\":47,\"y\":151},{\"x\":137,\"y\":150},{\"x\":137,\"y\":177},{\"x\":48,\"y\":178}],\"text\":\"0,8700\"},{\"boundingBox\":[{\"x\":46,\"y\":214},{\"x\":136,\"y\":214},{\"x\":136,\"y\":241},{\"x\":47,\"y\":243}],\"text\":\"0,8500\"},{\"boundingBox\":[{\"x\":992,\"y\":203},{\"x\":1047,\"y\":202},{\"x\":1047,\"y\":226},{\"x\":993,\"y\":227}],\"text\":\"QER\"},{\"boundingBox\":[{\"x\":2,\"y\":350},{\"x\":2,\"y\":330},{\"x\":25,\"y\":329},{\"x\":24,\"y\":350}],\"text\":\"F\"},{\"boundingBox\":[{\"x\":3,\"y\":324},{\"x\":4,\"y\":207},{\"x\":27,\"y\":208},{\"x\":25,\"y\":323}],\"text\":\"Measure\"},{\"boundingBox\":[{\"x\":46,\"y\":279},{\"x\":140,\"y\":278},{\"x\":140,\"y\":309},{\"x\":47,\"y\":309}],\"text\":\"0,8300\"},{\"boundingBox\":[{\"x\":949,\"y\":261},{\"x\":969,\"y\":261},{\"x\":969,\"y\":284},{\"x\":949,\"y\":284}],\"text\":\".\"},{\"boundingBox\":[{\"x\":994,\"y\":260},{\"x\":1055,\"y\":259},{\"x\":1054,\"y\":286},{\"x\":993,\"y\":284}],\"text\":\"CHI2\"},{\"boundingBox\":[{\"x\":960,\"y\":318},{\"x\":983,\"y\":317},{\"x\":983,\"y\":342},{\"x\":959,\"y\":342}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":994,\"y\":316},{\"x\":1025,\"y\":315},{\"x\":1024,\"y\":344},{\"x\":993,\"y\":342}],\"text\":\"IG\"},{\"boundingBox\":[{\"x\":47,\"y\":346},{\"x\":137,\"y\":345},{\"x\":137,\"y\":372},{\"x\":48,\"y\":373}],\"text\":\"0,8100\"},{\"boundingBox\":[{\"x\":926,\"y\":377},{\"x\":985,\"y\":376},{\"x\":985,\"y\":398},{\"x\":926,\"y\":399}],\"text\":\"-x-\"},{\"boundingBox\":[{\"x\":993,\"y\":376},{\"x\":1048,\"y\":375},{\"x\":1048,\"y\":399},{\"x\":993,\"y\":398}],\"text\":\"DFD\"},{\"boundingBox\":[{\"x\":48,\"y\":411},{\"x\":137,\"y\":410},{\"x\":137,\"y\":437},{\"x\":48,\"y\":438}],\"text\":\"0,7900\"},{\"boundingBox\":[{\"x\":926,\"y\":433},{\"x\":945,\"y\":433},{\"x\":944,\"y\":456},{\"x\":925,\"y\":455}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":994,\"y\":434},{\"x\":1062,\"y\":432},{\"x\":1062,\"y\":454},{\"x\":994,\"y\":456}],\"text\":\"OCFS\"},{\"boundingBox\":[{\"x\":48,\"y\":476},{\"x\":137,\"y\":475},{\"x\":136,\"y\":502},{\"x\":48,\"y\":504}],\"text\":\"0,7700\"},{\"boundingBox\":[{\"x\":46,\"y\":538},{\"x\":143,\"y\":538},{\"x\":142,\"y\":569},{\"x\":46,\"y\":569}],\"text\":\"0,7500\"},{\"boundingBox\":[{\"x\":146,\"y\":595},{\"x\":146,\"y\":580},{\"x\":161,\"y\":580},{\"x\":161,\"y\":595}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":224,\"y\":568},{\"x\":282,\"y\":568},{\"x\":282,\"y\":598},{\"x\":224,\"y\":598}],\"text\":\"500\"},{\"boundingBox\":[{\"x\":319,\"y\":568},{\"x\":391,\"y\":568},{\"x\":391,\"y\":598},{\"x\":319,\"y\":598}],\"text\":\"1000\"},{\"boundingBox\":[{\"x\":416,\"y\":569},{\"x\":494,\"y\":568},{\"x\":494,\"y\":600},{\"x\":416,\"y\":600}],\"text\":\"1500\"},{\"boundingBox\":[{\"x\":514,\"y\":568},{\"x\":593,\"y\":568},{\"x\":593,\"y\":600},{\"x\":515,\"y\":600}],\"text\":\"2000\"},{\"boundingBox\":[{\"x\":613,\"y\":568},{\"x\":692,\"y\":568},{\"x\":692,\"y\":600},{\"x\":613,\"y\":600}],\"text\":\"2500\"},{\"boundingBox\":[{\"x\":710,\"y\":568},{\"x\":791,\"y\":569},{\"x\":791,\"y\":600},{\"x\":710,\"y\":600}],\"text\":\"3000\"},{\"boundingBox\":[{\"x\":809,\"y\":568},{\"x\":881,\"y\":568},{\"x\":881,\"y\":598},{\"x\":809,\"y\":598}],\"text\":\"3500\"},{\"boundingBox\":[{\"x\":416,\"y\":628},{\"x\":519,\"y\":629},{\"x\":519,\"y\":653},{\"x\":415,\"y\":652}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":524,\"y\":629},{\"x\":583,\"y\":629},{\"x\":583,\"y\":653},{\"x\":523,\"y\":653}],\"text\":\"Size\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 09 May 2018\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":15},{\"x\":896,\"y\":16},{\"x\":896,\"y\":72},{\"x\":0,\"y\":70}],\"text\":\"Published online: 09 May 2018\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":18},{\"x\":282,\"y\":16},{\"x\":282,\"y\":71},{\"x\":0,\"y\":68}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":292,\"y\":16},{\"x\":499,\"y\":16},{\"x\":499,\"y\":73},{\"x\":292,\"y\":71}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":509,\"y\":16},{\"x\":587,\"y\":16},{\"x\":588,\"y\":73},{\"x\":509,\"y\":73}],\"text\":\"09\"},{\"boundingBox\":[{\"x\":597,\"y\":16},{\"x\":738,\"y\":16},{\"x\":739,\"y\":73},{\"x\":598,\"y\":73}],\"text\":\"May\"},{\"boundingBox\":[{\"x\":748,\"y\":16},{\"x\":895,\"y\":17},{\"x\":897,\"y\":73},{\"x\":749,\"y\":73}],\"text\":\"2018\"}]}"
      ]
    },
    {
      "@search.score": 2.7504435,
      "content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nR E S E A R C H\n\nNarayanan et al. J Big Data            (2020) 7:13  \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence:   \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et  al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et  al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n\n\n\n\nPage 5 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to  ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table  2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I) −\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n• Let x significant features are identified from feature set (F ) represented as Fx ⊂ F\n• An active customer consists of significant feature having information Gain value \n\ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′x are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the  m\n\nth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using  L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1 + b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1, −1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2  G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure  3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9  GB dataset. But for DMRDF model time taken for 18  GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n9  GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nT\nim\n\ne \nT\n\nak\nen\n\n in\n s\n\nec\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24  months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope with the limitations of deep learn-\ning matrix factorization integrated with DMRDF can be adapted.\n\nAbbreviations\nDMRDF: Distributed Memory-based Resilient Dataset Filter; FIG: Feature information gain; RDD: Resilient distributed \ndataset; SVM: Support vector machine; LR: Logistic regression; LSA: Latent semantic analysis; PA: Prediction accuracy; \nP@R: Precision; R@R: Recall; MF: Matrix factorization.\n\nAcknowledgements\nNot applicable.\n\nAuthors’ contributions\nSN designed and implemented the model for Pre-launch product prediction. SN analysed and interpreted the customer \nreviews and ratings dataset regarding the pre-launch product prediction. PS supervised the design, implementation \nand analysis of the model for pre-launch product prediction. MC was a major contributor in writing the manuscript. All \nauthors read and approved the final manuscript.\n\nFunding\nNot applicable.\n\nAvailability of data and materials\nThe datasets generated and/or analysed during the current study are available in the Kaggle repository. [snap.stanford.\nedu/data/web-Amazon.html] [40] and [http://www.kaggl e.com/Promp tClou dHQ/flipk art-produ cts] [39].\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAuthor details\n1 Information Technology, School of Engineering, Cochin University of Science & Technology, Kochi 682022, India. \n2 Department of Computer Science, Cochin University of Science & Technology, Kochi 682022, India. 3 Department \nof Ship Technology, Cochin University of Science & Technology, Kochi 682022, India. \n\nReceived: 25 October 2019   Accepted: 17 February 2020\n\n\n\nhttp://www.kaggle.com/PromptCloudHQ/flipkart-products\n\n\nPage 14 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nReferences\n 1. Lau RY, Liao SY, Kwok RC, Xu K, Xia Y, Li Y. Text mining and probabilistic modeling for online review spam detection. \n\nACM Trans Manag Inform Syst. 2011;2(4):25.\n 2. Lin X, Li Y, Wang X. Social commerce research: definition, research themes and the trends. Int J Inform Manag. \n\n2017;37:190–201.\n 3. Matos CAD, Rossi CAV. Word-of-mouth communications in marketing: a meta-analytic review of the antecedents \n\nand moderators. J Acad Market Sci. 2008;36(4):578–96.\n 4. Jeon S, et al. Redundant data removal technique for efficient big data search processing. Int J Softw Eng Appl. \n\n2013;7.4:427–36.\n 5. Dave K, Lawrence S, and Pennock D. Mining the peanut gallery: opinion extraction and semantic classification of \n\nproduct reviews. WWW’2003.\n 6. Zhou Y, Wilkinson D, Schreiber R, Pan R. Large-scale parallel collaborative filtering for the netflix prize. 2008. p. \n\n337–48. https ://doi.org/10.1007/978-3-540-68880 -8_32.\n 7. Zhang KZK, Benyoucef M. Consumer behavior in social commerce: a literature review. Dec Support Syst. \n\n2016;86:95–108.\n 8. Cui Geng, Lui Hon-Kwong, Guo Xiaoning. The effect of online consumer reviews on new product sales. Int J Electron \n\nComm. 2012;17(1):39–58.\n 9. Manek AS, Shenoy PD, Mohan MC, et al. Detection of fraudulent and malicious websites by analysing user reviews \n\nfor online shopping websites. Int J Knowl Web Intell. 2016;5(3):171–89. https ://doi.org/10.1007/s1128 0-015-0381-x.\n 10. Singh S, and Singh N. Big data analytics. In: Proceedings of the 2012 international conference on communication, \n\ninformation & computing technology (ICCICT ), institute of electrical and electronics engineers (IEEE). 2012. p. 1–4. \nhttp://dx.doi.org/10.1109/iccic t.2012.63981 80.\n\n 11. Demchenko Yuri et al. Addressing big data challenges for scientific data infrastructure. In: IEEE 4th Int. conference \ncloud computing technology and science (CloudCom). 2012.\n\n 12. Sihong Xie, Guan Wang, Shuyang Lin and Yu Philip S. Review spam detection via time-series pattern discovery. In: \nACM Proceedings of the 21st international conference companion on World Wide Web. 2012. p. 635–6.\n\n 13. Koren Y, Bell R, Volinsky C. matrix factorization technique for recommender systems. Computer. 2009;8:30–7.\n 14. Salakhutdinov R, Mnih A, & Hinton G. Restricted boltzmann machines for collaborative filtering. In: Proc. of the 24th \n\nInt. conference on machine learning. 2007. p. 791–8.\n 15. Hao MA, King I, Lyu MR. Learning to recommend with explicit and implicit social relations. ACM Trans Intell Syst \n\nTechnol. 2011;2(3):29.\n 16. Bandakkanavar V, Ramesh M, Geeta V. A survey on detection of reviews using sentiment classification of methods. \n\nIJRITCC. 2014;2(2):310–4.\n 17. Gu V, and Li H. Memory or time—performance evaluation for iterative operation on hadoop and spark. In: Proc. of \n\nthe 2013 IEEE 10th Int. Con. on high-performance computing and communications. 2013. https ://doi.org/10.1109/\nhpcc.and.euc.2013.106.\n\n 18. Zhang Hanpeng, Wang Zhaohua, Chen Shengjun, Guo Chengqi. Product recommendation in online social net-\nworking communities—an empirical study of antecedents and a mediator. J Inform Manag. 2019;56(2):185–95.\n\n 19. Ghose A, Ipeirotis PG. Designing novel review ranking systems: predicting the usefulness and impact of reviews. In: \nInt Conference Electron Comm ACM. 2007. p. 303–10.\n\n 20. Chong AY, Ch’ng E, Liu MJ, Li B. Predicting consumer product demands via Big Data: the roles of online promotional \nmarketing and online reviews. Int J Prod Res. 2015;55:1–15. https ://doi.org/10.1080/00207 543.2015.10665 19.\n\n 21. Yang H, Fujimaki R, Kusumura Y, & Liu J. Online Feature Selection. In: Proceedings of the 22nd ACM SIGKDD Int. \nConference on KDD ‘16, 2016. https ://doi.org/10.1145/29396 72.29398 81.\n\n 22. Breese JS, Heckerman D, and Kadie C. Empirical analysis of predictive algorithms for collaborative filtering. In: Proc. \nof the 14th Conf. on Uncertainty in Artifical Intelligence, 1998.\n\n 23. Mukherjee A, Kumar A, Liu B, Wang J, Hsu M, Castellanos M, Ghosh R. Spotting opinion spammers using behavioral \nfootprints. In: Proc. of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining \nChicago, ACM. 2013. p. 632–40.\n\n 24. Makridakis S, Spiliotis E, Assimakopoulos V. Statistical and Machine Learning forecasting methods: concerns and \nways forward. PLoS ONE. 2018;13(3):e0194889. https ://doi.org/10.1371/journ al.pone.01948 89.\n\n 25. Imon A, Roy C, Manos C, Bhattacharjee S. Prediction of rainfall using logistic regression. Pak J Stat Oper Res. 2012. \nhttps ://doi.org/10.18187 /pjsor .v8i3.535.\n\n 26. Chen T, Zhang W, Lu Q, Chen K, Zheng Z, Yu Y. SVD Feature: a toolkit for feature-based collaborative filtering. J Mach \nLearn Res. 2012;13(1):3619–22.\n\n 27. Shi Y, Larson M, Hanjalic A. Collaborative filtering beyond the user-item matrix—a survey of the state of art and \nfuture challenges. ACM Comput Surv. 2014;47(1):3.\n\n 28. Shan H, & Banerjee A. Generalized probabilistic matrix factorizations for collaborative filtering, In Data mining \n(ICDM), IEEE 10th international conference. 2010. p. 1025–30.\n\n 29. Salakhutdinov R, & Mnih A. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. In: Proc. of \nthe 25th int. conference on machine learning. 2008. p. 880–7.\n\n 30. Crawford M, Khoshgoftaar TM, Prusa JD, Richter AN, Al Najada H. Survey of review spam detection using machine \nlearning techniques. J Big Data. 2015;2(1):23.\n\n 31. Wietsma TA, Ricci F. Product reviews in mobile decision aid systems. Francesco: PERMID; 2005. p. 15–8.\n 32. Jianguo C, et al. A disease diagnosis and treatment recommendation system based on big data mining and cloud \n\ncomputing. Inform Sci. 2018;435:124–49.\n 33. Manek AS, Shenoy PD, Mohan MC, Venugopal KR. Aspect term extraction for sentiment analysis in large movie \n\nreviews using Gini-index feature selection method and SVM classifier. World Wide Web. 2017;20:135–54. https ://doi.\norg/10.1007/s1128 0-015-0381-x.\n\n 34. Fan RE, Chang K-W, Hsieh C-J, Wang X-R, Lin C-J. LIBLINEAR: A library for large linear classification. J Mach Learn Res. \n2008;9:1871–4.\n\nhttps://doi.org/10.1007/978-3-540-68880-8_32\nhttps://doi.org/10.1007/s11280-015-0381-x\nhttp://dx.doi.org/10.1109/iccict.2012.6398180\nhttps://doi.org/10.1109/hpcc.and.euc.2013.106\nhttps://doi.org/10.1109/hpcc.and.euc.2013.106\nhttps://doi.org/10.1080/00207543.2015.1066519\nhttps://doi.org/10.1145/2939672.2939881\nhttps://doi.org/10.1371/journal.pone.0194889\nhttps://doi.org/10.18187/pjsor.v8i3.535\nhttps://doi.org/10.1007/s11280-015-0381-x\nhttps://doi.org/10.1007/s11280-015-0381-x\n\n\nPage 15 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n 35. Ribeiro MT, Singh S, and Guestrin C. Why should I trust you?: Explaining the predictions of any classifier. In: Proc. \nACMSIGKDD Int. Conf. Knowl. Discov. Data Mining. 2016. p. 1135–44.\n\n 36. Luo X, et al. An effective scheme for QoS estimation via alternating direction method-based matrix factorization. \nIEEE Trans Serv Comput. 2019;12(4):503–18.\n\n 37. Liu CL, Hsaio WH, Lee CH, Lu GC and Jou E. Movie rating and review summarization in mobile environment. In: IEEE \ntrans. systems, man and cybernetics, Part C: applications and reviews. 2012. p. 397–407.\n\n 38. Vapnik, VN. The nature of statistical learning theory, Springer, 2nd ed, 1999. Translated by Xu Jianghua, Zhang Xue-\ngong. Beijing: China Machine Press; 2000.\n\n 39. [Dataset] Flipkart-products. http://www.kaggl e.com/Promp tClou dHQ/flipk art-produ cts.\n 40. [Dataset] https ://snap.stanf ord.edu/data/web-Amazo n.html.\n 41. [Dataset] He R, McAuley J. Ups and downs: modeling the visual evolution of fashion trends with one-class collabora-\n\ntive filtering. WWW; 2016.\n 42. Popescu AM, Etzioni O. Extracting product features and opinions from reviews. 2005; EMNLP.\n 43. Zaharia M, Chowdhury M, Das T, Dave A, Ma J, McCauley M, Franklin M, Shenker S, Stoica I. Resilient distributed \n\ndatasets: A fault-tolerant abstraction for in-memory cluster computing Technical Report UCB/EECS-2011-82. UC \nBerkeley: EECS Department; 2011.\n\n 44. Davis J, Goadrich M. The relationship between precision-recall and ROC curves, In ICML. 2006. p. 233–40.\n 45. Lee JS, Lee ES. Exploring the usefulness of predicting people’s locations. Procedia Soc Beh Sci. 2014. https ://doi.\n\norg/10.1016/j.sbspr o.2014.04.451.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nhttp://www.kaggle.com/PromptCloudHQ/flipkart-products\nhttps://snap.stanford.edu/data/web-Amazon.html\nhttps://doi.org/10.1016/j.sbspro.2014.04.451\nhttps://doi.org/10.1016/j.sbspro.2014.04.451\n\n\tImproving prediction with enhanced Distributed Memory-based Resilient Dataset Filter\n\tAbstract \n\tIntroduction\n\tRelated work\n\tMethodology\n\tData collection phase\n\tDataset pre-processing\n\tResilient Distributed Dataset\n\n\tPrediction classifiers\n\tLogistic regression (LR)\n\tSupport Vector Machine (SVM)\n\n\tExperimental setup\n\n\tResults and discussions\n\tConclusion and future work\n\tAcknowledgements\n\tReferences\n\n\n\n\n",
      "metadata_storage_path": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9saWJyYXJ5L3M0MDUzNy0wMjAtMDAyOTIteS5wZGY1",
      "authors": [
        "Sandhya Narayanan1",
        "Philip Samuel2",
        "Mariamma Chacko3",
        "ket",
        "dancy",
        "R E S",
        "A R C H",
        "Narayanan",
        "Makridakis",
        "Hao",
        "Salakhutdinov",
        "Wietsma",
        "Jianguo Chen",
        "Asha",
        "Gini",
        "Luo",
        "Liu",
        "log2Ef",
        "�G",
        "Pe",
        "MC",
        "Lau RY",
        "Liao SY",
        "Kwok RC",
        "Xu K",
        "Xia Y",
        "Li Y.",
        "Lin X",
        "Li Y",
        "Wang X",
        "Matos",
        "Rossi",
        "Jeon S",
        "Dave K",
        "Lawrence S",
        "Pennock D.",
        "Zhou Y",
        "Wilkinson D",
        "Schreiber R",
        "Pan R",
        "Zhang KZK",
        "Benyoucef M.",
        "Cui Geng",
        "Lui Hon-Kwong",
        "Guo Xiaoning",
        "Manek AS",
        "Shenoy PD",
        "Mohan MC",
        "Int J Knowl",
        "Singh S",
        "Singh N.",
        "Demchenko Yuri",
        "Sihong Xie",
        "Guan Wang",
        "Shuyang Lin",
        "Yu Philip S.",
        "Koren Y",
        "Bell R",
        "Volinsky C",
        "Salakhutdinov R",
        "Mnih A",
        "Hinton G.",
        "King I",
        "Lyu",
        "Bandakkanavar V",
        "Ramesh M",
        "Geeta V",
        "Gu V",
        "Li H.",
        "Zhang Hanpeng",
        "Wang Zhaohua",
        "Chen Shengjun",
        "Guo Chengqi",
        "Ghose A",
        "Ipeirotis PG",
        "Chong AY",
        "Ch’ng E",
        "Liu MJ",
        "Li B",
        "Yang H",
        "Fujimaki R",
        "Kusumura Y",
        "Liu J.",
        "Breese JS",
        "Heckerman D",
        "Kadie C.",
        "Mukherjee A",
        "Kumar A",
        "Liu B",
        "Wang J",
        "Hsu M",
        "Castellanos M",
        "Ghosh R.",
        "Makridakis S",
        "Spiliotis E",
        "Assimakopoulos V",
        "Imon A",
        "Roy C",
        "Manos C",
        "Bhattacharjee S.",
        "Chen T",
        "Zhang W",
        "Lu Q",
        "Chen K",
        "Zheng Z",
        "Yu Y. SVD",
        "J Mach",
        "Shi Y",
        "Larson M",
        "Hanjalic A",
        "Shan H",
        "Banerjee A",
        "Mnih A.",
        "Crawford M",
        "Khoshgoftaar TM",
        "Prusa JD",
        "Richter AN",
        "Al Najada H.",
        "Ricci F",
        "Francesco",
        "Jianguo C",
        "Venugopal KR",
        "Fan RE",
        "Chang K-W",
        "Hsieh C-J",
        "Wang X-R",
        "Lin C-J",
        "Ribeiro MT",
        "Guestrin C",
        "Luo X",
        "Liu CL",
        "Hsaio WH",
        "Lee CH",
        "Lu GC",
        "Jou E.",
        "Vapnik",
        "Xu Jianghua",
        "Zhang Xue",
        "He R",
        "McAuley J.",
        "Popescu AM",
        "Etzioni O",
        "Zaharia M",
        "Chowdhury M",
        "Das T",
        "Dave A",
        "Ma J",
        "McCauley M",
        "Franklin M",
        "Shenker S",
        "Stoica I.",
        "Davis J",
        "Goadrich M",
        "Lee JS",
        "Lee ES"
      ],
      "institutions": [
        "Creative Commons",
        "iveco",
        "School of Engineering",
        "Cochin University of Science \n& Technology",
        "zon",
        "flip cart",
        "Real",
        "fk",
        "RDD",
        "DMRDF",
        "KRi",
        "KCm",
        "Amazon",
        "Intel",
        "rc",
        "PA",
        "Gini",
        "TP",
        "SVM",
        "Feature",
        "SN",
        "PS",
        "Kaggle",
        "Cochin University of Science & Technology",
        "Department of",
        "of Ship Technology",
        "Manag",
        "Int J Electron",
        "information & computing technology",
        "ICCICT",
        "institute of electrical and electronics engineers",
        "IEEE",
        "CloudCom",
        "ACM",
        "IJRITCC",
        "Int Conference Electron Comm ACM",
        "PLoS ONE",
        "Springer",
        "China Machine Press",
        "Flipkart",
        "WWW",
        "EMNLP",
        "EECS",
        "UC",
        "EECS Department",
        "Springer Nature"
      ],
      "key_phrases": [
        "Distributed Memory‑based Resilient Dataset Filter",
        "A R C H Narayanan",
        "Creative Commons Attribution 4.0 International License",
        "Distributed Memory-based Resilient Dataset Filter",
        "other third party material",
        "A Feature Information Gain",
        "big data processing technologies",
        "other online shopping sites",
        "R E S",
        "Resilient Distribution Dataset",
        "social networking sites",
        "Creative Commons licence",
        "sophisticated pre-processing techniques",
        "significant feature identification",
        "Support vector machine",
        "Redundancy Open Access",
        "J Big Data",
        "successful product launch",
        "online product recommendations",
        "consumer electronics market",
        "online product reviews",
        "distributed environment",
        "Online reviews",
        "pre-processed dataset",
        "feature modelling",
        "Online feedback",
        "retail shopping",
        "useful information",
        "price information",
        "author information",
        "Customer reviews",
        "duplicate reviews",
        "product sale",
        "product life",
        "product quality",
        "less product",
        "product pre-launch",
        "Sandhya Narayanan1",
        "Philip Samuel2",
        "Mariamma Chacko3",
        "massive volumes",
        "different applications",
        "sensor data",
        "health care",
        "enormous size",
        "unstructured data",
        "crucial thing",
        "large size",
        "communication methods",
        "direct suggestions",
        "several advantages",
        "limited time",
        "research work",
        "FIG) measure",
        "Logistic regression",
        "resilience property",
        "appropriate credit",
        "original author",
        "credit line",
        "statutory regulation",
        "copyright holder",
        "creat iveco",
        "Cochin University",
        "Full list",
        "new product",
        "1 Information Technology",
        "intended use",
        "permitted use",
        "mation source",
        "DMRDF method",
        "The Author",
        "doi.org",
        "prediction accuracy",
        "Introduction",
        "Extracting",
        "amount",
        "extensive",
        "customers",
        "impact",
        "extent",
        "ratings",
        "Abstract",
        "sustainability",
        "companies",
        "turn",
        "reliability",
        "classifiers",
        "output",
        "manufacturer",
        "design",
        "Keywords",
        "article",
        "sharing",
        "adaptation",
        "reproduction",
        "medium",
        "link",
        "changes",
        "images",
        "permission",
        "Correspondence",
        "nairsands",
        "School",
        "Engineering",
        "Science",
        "Kochi",
        "India",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "different natural language processing techniques",
        "significant data processing methods",
        "big data processing model",
        "Different feature selection methods",
        "wrapper feature selection method",
        "new prod- uct",
        "multiple forecasting field",
        "previous customer feedbacks",
        "online shopping sites",
        "structured massive volume",
        "customer review analysis",
        "spam reviews recognition",
        "many redundant reviews",
        "machine learning methods",
        "Consumer product success",
        "product pre-launch prediction",
        "future work” section",
        "user rating matrix",
        "poor quality products",
        "different criteria",
        "Different works",
        "wrapper methods",
        "art methods",
        "alternative methods",
        "statistical methods",
        "model complexity",
        "statistical analysis",
        "filter method",
        "customer reviews",
        "enhanced method",
        "unreliable data",
        "model performance",
        "shop owners",
        "other hand",
        "extra cost",
        "brand loyalty",
        "eCommerce firms",
        "other retailers",
        "large volume",
        "crucial phase",
        "univariate manner",
        "System design",
        "less accuracy",
        "unknown values",
        "improper knowledge",
        "Matrix factorization",
        "collaborative filtering",
        "two vectors",
        "low dimensionality",
        "accurate reviews",
        "duplicated reviews",
        "negative reviews",
        "Related work",
        "product reviews",
        "odology” section",
        "data pre-processing",
        "marketing strategies",
        "embedded process",
        "prediction classifiers",
        "Page",
        "15Narayanan",
        "information",
        "effort",
        "industry",
        "strategy",
        "users",
        "valuable",
        "number",
        "blogs",
        "forums",
        "awareness",
        "need",
        "ratio",
        "positive",
        "features",
        "usefulness",
        "relevance",
        "state",
        "gener",
        "ally",
        "combination",
        "distributive",
        "web",
        "order",
        "scalable",
        "failure",
        "realization",
        "paper",
        "methodology",
        "Results",
        "discussions",
        "conclusion",
        "Makridakis",
        "Author",
        "reason",
        "MF",
        "Hao",
        "item",
        "user item matrix factorization technique",
        "standard probability-based matrix factorization methods",
        "user item matrix factorization method",
        "ity related matrix factorization",
        "Gini- index impurity measure",
        "relational database management systems",
        "single value decomposition method",
        "high term frequency words",
        "probability factorization methods",
        "student user behavior",
        "Stochastic Gradient Decent",
        "mobile decision aid",
        "rule-based apriori algorithm",
        "mobile envi- ronment",
        "appropriate computing models",
        "traditional collaborative Filtering",
        "product feature identification",
        "Bayesian-based probabilistic analysis",
        "cold start problem",
        "automatic service selection",
        "Latent Semantic Analysis",
        "Gini-index feature method",
        "movie review dataset",
        "customer review datasets",
        "pre-launch product prediction",
        "Statistical methods",
        "user reviews",
        "opinion words",
        "Gini-index method",
        "filtering function",
        "movie rating",
        "review summarization",
        "customer feedback",
        "density-peaked method",
        "LSA-based) method",
        "LSA-based method",
        "squared distance",
        "big volume",
        "conventional approach",
        "implementation purpose",
        "sparsity problem",
        "various analyses",
        "recommendation issues",
        "different websites",
        "Jianguo Chen",
        "opinion extraction",
        "individual dimensions",
        "large datasets",
        "major challenge",
        "existing products",
        "different classifiers",
        "rating dataset",
        "polarity prediction",
        "product features",
        "recommender system",
        "recommendation system",
        "huge volume",
        "historical data",
        "big data",
        "disease symptoms",
        "sentimental analysis",
        "diction accuracy",
        "29 features",
        "solution",
        "squares",
        "Salakhutdinov",
        "other",
        "items",
        "limitation",
        "addition",
        "approaches",
        "Wietsma",
        "study",
        "result",
        "correlation",
        "treatment",
        "diagnosis",
        "diseases",
        "cluster",
        "Asha",
        "sentences",
        "rence",
        "document",
        "precision",
        "disadvantage",
        "Luo",
        "quality",
        "Liu",
        "authors",
        "Lack",
        "redundancy",
        "work",
        "existence",
        "Methodology",
        "phases",
        "Distributed Memory-based Resilient Dataset Filter approach",
        "Identification Redundancy Removal Data Integration Training",
        "classification algorithms Support Vector Logistic",
        "Product prelaunch prediction System Design",
        "Data collection Categorical Text Real",
        "Enhanced Feature Information Gain measure",
        "flip cart customer reviews",
        "ith customer review Ri",
        "Regression Support Vector",
        "duplicate data removal",
        "classification algorithms Logistic",
        "mobile phones product reviews",
        "port Vector Machine",
        "data collection phase",
        "JSON file format",
        "New mobile phones",
        "product rating scale",
        "categorical, real",
        "Regression Testing Dataset",
        "Dataset pre‑processing",
        "product review dataset",
        "text data",
        "Logistic Regression",
        "opinion identification",
        "data pre",
        "best information",
        "prediction classifier",
        "multivariate data",
        "pre-launch prediction",
        "Product feature",
        "input dataset",
        "Product categories",
        "feature selection",
        "processing Feature",
        "nificant feature",
        "feature instance",
        "k feature",
        "various stages",
        "dancy elimination",
        "different products",
        "Several datasets",
        "public datasets",
        "data- set",
        "seven brands",
        "two reasons",
        "unavoidable items",
        "sample set",
        "catego- rization",
        "priority weightage",
        "major role",
        "large number",
        "fea- tures",
        "total number",
        "particular review",
        "user requirements",
        "feature impurity",
        "market industry",
        "Battery life",
        "Review Content",
        "user features",
        "Figure",
        "model",
        "SVM",
        "LR",
        "zon",
        "period",
        "24 months",
        "day",
        "everyone",
        "Table",
        "ReviewID",
        "Title",
        "price",
        "camera",
        "RAM",
        "Fig.",
        "processor",
        "Average",
        "polarity",
        "opinions",
        "Senti-WordNet",
        "probability",
        "fk",
        "SR",
        "Distributed Memory-based Resilience Dataset Filter",
        "15 Rear camera 31 Finger sensor",
        "big data processing approach",
        "launch predic- tion",
        "Resilient Distributed Dataset",
        "local file system",
        "3 ReviewID 19 Product category",
        "Impurity R(I",
        "value) pair dataset",
        "memory data caching",
        "information Gain value",
        "Feature Information Gain",
        "customer review dataset",
        "Table 2 Significant Features",
        "Ef Review Feature",
        "13 Operating system",
        "input file",
        "P(R",
        "25 Front camera",
        "new dataset",
        "prior information",
        "9 Feature information",
        "Next step",
        "OR N",
        "Sim type",
        "4 Content 20 Thickness",
        "mobile phone",
        "7 Battery life",
        "10 Review type",
        "29 Network support",
        "14 Water proof",
        "Quick charging",
        "32 Internal storage",
        "machine learning",
        "pervasive requirement",
        "main actions",
        "first element",
        "main Transformations",
        "long Lineage",
        "n customers",
        "feature set",
        "active customer",
        "5 Product brand",
        "Product type",
        "11 Product display",
        "redundant reviews",
        "N reviews",
        "value) pairs",
        "null values",
        "Pi.log2Ef",
        "SR N",
        "cache chunks",
        "�G value",
        "24 Product rating",
        "reduce function",
        "SR.",
        "respect",
        "opinion",
        "No",
        "1 Author",
        "17 RAM",
        "2 Title",
        "Weight",
        "8 Price",
        "12 Processor",
        "Multi-band",
        "16 Applications",
        "RDD",
        "requirements",
        "Variety",
        "jobs",
        "point",
        "time",
        "challenge",
        "analysis",
        "systems",
        "elements",
        "saveAsSequenceFile",
        "path",
        "map",
        "groupBykey",
        "ReduceBykey",
        "fault-tolerance",
        "list",
        "Fx",
        "∑",
        "β",
        "Distributed Memory-based resilient filter score",
        "hyper plane normal vector element",
        "memory-based Resilient Dataset Filter score",
        "D dimensional input space",
        "resilient filter score value",
        "Support Vector Machine classifiers",
        "t training feature vectors",
        "one Distributed Memory-based",
        "decision hyper plane",
        "positive one class",
        "logarithmic base value",
        "logistic regression analysis",
        "machine learning method",
        "prediction variable value",
        "Logistic regression value",
        "data features relationships",
        "product failure class",
        "product success class",
        "δ score value",
        "row vector",
        "column vector",
        "Prediction classifiers",
        "significant feature",
        "corresponding vectors",
        "nearest vectors",
        "learning approaches",
        "classification method",
        "training dataset",
        "constant value",
        "probability value",
        "mth customer",
        "L1 norm",
        "second occurrence",
        "case study",
        "mobile phones",
        "logit function",
        "new skills",
        "data separation",
        "real numbers",
        "two classes",
        "th review",
        "customer review",
        "similar reviews",
        "successful products",
        "N’ number",
        "KC",
        "KR",
        "KFx",
        "KFj",
        "entry",
        "similarities",
        "The",
        "Eq.",
        "processing",
        "More",
        "market",
        "p0",
        "L0",
        "values",
        "knowledge",
        "RD",
        "XD",
        "way",
        "distance",
        "hyperplane",
        "conditions",
        "margin",
        "γ",
        "different Spark cluster configura- tions",
        "Most significant customer review features",
        "two Intel Xeon E",
        "2699V4 2.2  G Hz processors",
        "Web Server Gateway Interface",
        "customer review feature identification",
        "big data processing system",
        "software system large servers",
        "LSA-based methods processing time",
        "big data analytics",
        "Apache web server",
        "Amazon Web Services",
        "Apache Spark 2.2.1 framework",
        "Logistic regression classifiers",
        "system perfor- mance",
        "different dataset size",
        "feature information gain",
        "negative one class",
        "semantic analysis methods",
        "high-speed processing performance",
        "system response time",
        "Spark python API",
        "several case studies",
        "system design factors",
        "mobile phone sustainability",
        "prediction accuracy measurement",
        "redundant customer reviews",
        "prediction accuracy evaluation",
        "DMRDF model time",
        "proposed system",
        "separate servers",
        "prediction system",
        "failure class",
        "software components",
        "LSA-based model",
        "less time",
        "product sustainability",
        "Experimental setup",
        "PySpark version",
        "Vector Machine",
        "predic- tion",
        "major concern",
        "internal storage",
        "art techniques",
        "DMRDF approach",
        "9 GB dataset",
        "18 GB dataset",
        "9  GB dataset",
        "18  GB dataset",
        "DMRDF model 740",
        "other gini-index",
        "Product price",
        "scalability requirements",
        "other state",
        "Gini-index model",
        "application development",
        "16 GB",
        "gramming",
        "Ubuntu",
        "nodes",
        "VCPUs",
        "4 cores",
        "wtzi",
        "Support",
        "7 brands",
        "LR.",
        "graph",
        "percentage",
        "manner",
        "comparison",
        "completion",
        "latent",
        "342 s",
        "495 s",
        "156 s",
        "910 s",
        "advantage",
        "execution",
        "recall",
        "Distributed Memory-based Resilient Dataset Filter method",
        "reliable big data processing model",
        "Support Vector Machine prediction classifiers",
        "Support Vector Machine classification",
        "Classifier Support vector machine",
        "different customer review aspects",
        "customer review feature prediction",
        "Processing Time Graph",
        "big data analysis",
        "Months LSA-based DMRDF Gini-index",
        "LSA-based meth- ods",
        "Logistic Regression classifiers",
        "other two methods",
        "duplicate customer reviews",
        "Classifier Logistic regression",
        "LR classifiers",
        "redundant data",
        "feature dimensionality",
        "other methods",
        "Gini-index methods",
        "Dataset size",
        "LSA-based T",
        "Gini-index approaches",
        "LSA-based approaches",
        "significant features",
        "accuracy measures",
        "P@R",
        "R@R",
        "false negative",
        "1GB 5GB",
        "T ak",
        "SVM classifier",
        "ratings datasets",
        "performance evaluation",
        "Gini- index",
        "Technological development",
        "new challenges",
        "artificial intelligence",
        "next frontier",
        "mation Gain",
        "The DMRDF",
        "large dataset",
        "many features",
        "Performance comparison",
        "future work",
        "PA measures",
        "24  months",
        "results",
        "TP",
        "FP",
        "TN",
        "FN",
        "Eqs",
        "18GB",
        "tions",
        "Conclusion",
        "era",
        "innovation",
        "productivity",
        "implementation",
        "elimination",
        "12",
        "7",
        "ACM Trans Manag Inform Syst",
        "Int J Softw Eng Appl",
        "efficient big data search processing",
        "Int J Inform Manag.",
        "J Acad Market Sci",
        "Large-scale parallel collaborative filtering",
        "different reliable online websites",
        "Redundant data removal technique",
        "Promp tClou dHQ/flipk art",
        "other product feature identification",
        "online review spam detection",
        "Int J Electron",
        "data processing domains",
        "Dec Support Syst",
        "Feature information gain",
        "statistical prop- erties",
        "new product sales",
        "prediction model- ling",
        "Pre-launch product prediction",
        "information fusion approach",
        "memory computation method",
        "online consumer reviews",
        "ing matrix factorization",
        "time streaming predictions",
        "Latent semantic analysis",
        "Social commerce research",
        "dataset model performance",
        "Prediction accuracy",
        "meta-analytic review",
        "literature review",
        "research themes",
        "semantic classification",
        "Consumer behavior",
        "ratings dataset",
        "important role",
        "applica- tion",
        "Resilience property",
        "long lineage",
        "unified API",
        "customer comments",
        "learning algorithms",
        "major contributor",
        "current study",
        "produ cts",
        "Competing interests",
        "Author details",
        "Lau RY",
        "Liao SY",
        "Kwok RC",
        "Xu K",
        "Xia Y",
        "Li Y.",
        "Text mining",
        "probabilistic modeling",
        "Lin X",
        "Wang X",
        "Matos CAD",
        "Rossi CAV",
        "mouth communications",
        "Jeon S",
        "Dave K",
        "Lawrence S",
        "Pennock D.",
        "peanut gallery",
        "Zhou Y",
        "Wilkinson D",
        "Schreiber R",
        "Pan R.",
        "netflix prize",
        "Zhang KZK",
        "Benyoucef M.",
        "Cui Geng",
        "Lui Hon-Kwong",
        "Guo Xiaoning",
        "Proposed design",
        "final manuscript",
        "Kaggle repository",
        "Ship Technology",
        "Authors’ contributions",
        "DMRDF model",
        "Computer Science",
        "real",
        "surveys",
        "thesis",
        "sentiments",
        "limitations",
        "Abbreviations",
        "FIG",
        "LSA",
        "Precision",
        "Recall",
        "Acknowledgements",
        "SN",
        "PS",
        "MC",
        "Funding",
        "Availability",
        "materials",
        "datasets",
        "stanford",
        "Amazon",
        "2 Department",
        "3 Department",
        "25 October",
        "PromptCloudHQ",
        "flipkart-products",
        "References",
        "definition",
        "trends",
        "Word",
        "marketing",
        "antecedents",
        "moderators",
        "WWW",
        "effect",
        "Comm.",
        "17",
        "28",
        "ACM Trans Intell Syst Technol",
        "Pak J Stat Oper Res",
        "Int J Knowl Web Intell",
        "22nd ACM SIGKDD Int. Conference",
        "19th ACM SIGKDD international conference",
        "Liu J. Online Feature Selection",
        "Int Conference Electron Comm",
        "21st international conference companion",
        "2013 IEEE 10th Int. Con.",
        "Int J Prod Res",
        "IEEE 4th Int. conference",
        "novel review ranking systems",
        "Generalized probabilistic matrix factorizations",
        "IEEE 10th international conference",
        "Yu Y. SVD Feature",
        "Machine Learning forecasting methods",
        "World Wide Web",
        "J Inform Manag.",
        "ACM Comput Surv.",
        "matrix factorization technique",
        "implicit social relations",
        "Yu Philip S.",
        "online shopping websites",
        "scientific data infrastructure",
        "time-series pattern discovery",
        "Ch’ng E",
        "consumer product demands",
        "Big data analytics",
        "cloud computing technology",
        "Li H. Memory",
        "big data challenges",
        "2012 international conference",
        "feature-based collaborative filtering",
        "J Mach",
        "Wang J",
        "recommender systems",
        "Liu MJ",
        "online promotional",
        "Liu B",
        "user-item matrix",
        "malicious websites",
        "Koren Y",
        "Product recommendation",
        "Li B.",
        "online reviews",
        "Kusumura Y",
        "Knowledge discovery",
        "data mining",
        "Spiliotis E",
        "Shi Y",
        "future challenges",
        "ACM Proceedings",
        "high-performance computing",
        "Yang H",
        "Shan H",
        "Singh S",
        "Makridakis S",
        "Bhattacharjee S",
        "Manek AS",
        "Shenoy PD",
        "Mohan MC",
        "Singh N.",
        "electronics engineers",
        "dx.doi",
        "Demchenko Yuri",
        "Sihong Xie",
        "Shuyang Lin",
        "Bell R",
        "Volinsky C.",
        "Salakhutdinov R",
        "Mnih A",
        "Hinton G",
        "boltzmann machines",
        "Hao MA",
        "King I",
        "Lyu MR.",
        "Bandakkanavar V",
        "Ramesh M",
        "Geeta V.",
        "sentiment classification",
        "Gu V",
        "iterative operation",
        "Zhang Hanpeng",
        "Chen Shengjun",
        "Guo Chengqi",
        "working communities",
        "empirical study",
        "Ghose A",
        "Ipeirotis PG",
        "Chong AY",
        "Fujimaki R",
        "Breese JS",
        "Heckerman D",
        "Kadie C.",
        "Empirical analysis",
        "predictive algorithms",
        "14th Conf.",
        "Artifical Intelligence",
        "Mukherjee A",
        "Kumar A",
        "Hsu M",
        "Castellanos M",
        "Ghosh R.",
        "opinion spammers",
        "behavioral footprints",
        "Assimakopoulos V",
        "PLoS ONE",
        "Imon A",
        "Roy C",
        "Manos C",
        "logistic regression",
        "Chen T",
        "Zhang W",
        "Lu Q",
        "Chen K",
        "Zheng Z",
        "Larson M",
        "Hanjalic A.",
        "Banerjee A.",
        "Guan Wang",
        "Res.",
        "spam detection",
        "fraudulent",
        "communication",
        "ICCICT",
        "institute",
        "electrical",
        "science",
        "CloudCom",
        "Computer",
        "Proc.",
        "24th",
        "explicit",
        "survey",
        "IJRITCC",
        "hadoop",
        "spark",
        "hpcc",
        "Zhaohua",
        "mediator",
        "roles",
        "Uncertainty",
        "Chicago",
        "Statistical",
        "concerns",
        "journ",
        "Prediction",
        "rainfall",
        "pjsor",
        "toolkit",
        "ICDM",
        "memory cluster computing Technical Report UCB/EECS",
        "Mnih A. Bayesian probabilistic matrix factorization",
        "alternating direction method-based matrix factorization",
        "Markov chain Monte Carlo",
        "Al Najada H. Survey",
        "Gini-index feature selection method",
        "ACMSIGKDD Int. Conf. Knowl.",
        "Jou E. Movie rating",
        "Procedia Soc Beh Sci",
        "Introduction Related work Methodology",
        "mobile decision aid systems",
        "IEEE Trans Serv Comput",
        "Ricci F. Product reviews",
        "A disease diagnosis",
        "Stoica I. Resilient",
        "25th int. conference",
        "treatment recommendation system",
        "Aspect term extraction",
        "large linear classification",
        "Promp tClou dHQ",
        "flipk art-produ cts",
        "large movie reviews",
        "China Machine Press",
        "Data collection phase",
        "Support Vector Machine",
        "statistical learning theory",
        "machine learning techniques",
        "McAuley J. Ups",
        "big data mining",
        "cloud computing",
        "trans. systems",
        "Dave A",
        "Inform Sci",
        "mobile environment",
        "Ma J",
        "Davis J",
        "Crawford M",
        "Khoshgoftaar TM",
        "Prusa JD",
        "Richter AN",
        "Wietsma TA",
        "Jianguo C",
        "Venugopal KR",
        "sentiment analysis",
        "Fan RE",
        "Chang K-W",
        "Hsieh C-J",
        "Wang X-R",
        "Lin C-J.",
        "Ribeiro MT",
        "Guestrin C.",
        "Luo X",
        "effective scheme",
        "QoS estimation",
        "Liu CL",
        "Hsaio WH",
        "Lee CH",
        "Lu GC",
        "Part C",
        "2nd ed",
        "Xu Jianghua",
        "snap.stanf",
        "He R",
        "visual evolution",
        "fashion trends",
        "Popescu AM",
        "Etzioni O.",
        "Zaharia M",
        "Chowdhury M",
        "Das T",
        "McCauley M",
        "Franklin M",
        "Shenker S",
        "fault-tolerant abstraction",
        "EECS Department",
        "Goadrich M.",
        "ROC curves",
        "Lee JS",
        "Lee ES",
        "jurisdictional claims",
        "institutional affiliations",
        "Acknowledgements References",
        "Springer Nature",
        "Francesco",
        "PERMID",
        "LIBLINEAR",
        "library",
        "iccict",
        "euc",
        "journal",
        "pone",
        "predictions",
        "Discov.",
        "summarization",
        "cybernetics",
        "applications",
        "Vapnik",
        "VN.",
        "gong",
        "Beijing",
        "Flipkart-products",
        "kaggl",
        "downs",
        "filtering",
        "EMNLP.",
        "Berkeley",
        "relationship",
        "precision-recall",
        "ICML",
        "people",
        "locations",
        "sbspr",
        "Publisher",
        "Note",
        "regard",
        "maps",
        "Improving"
      ],
      "merged_content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nR E S E A R C H\n\nNarayanan et al. J Big Data            (2020) 7:13  \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence:   \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et  al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et  al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n  \n\n\n\nPage 5 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to  ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table  2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I) −\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n• Let x significant features are identified from feature set (F ) represented as Fx ⊂ F\n• An active customer consists of significant feature having information Gain value \n\ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′x are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the  m\n\nth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using  L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1 + b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1, −1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2  G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure  3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9  GB dataset. But for DMRDF model time taken for 18  GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n9  GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nT\nim\n\ne \nT\n\nak\nen\n\n in\n s\n\nec\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24  months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope with the limitations of deep learn-\ning matrix factorization integrated with DMRDF can be adapted.\n\nAbbreviations\nDMRDF: Distributed Memory-based Resilient Dataset Filter; FIG: Feature information gain; RDD: Resilient distributed \ndataset; SVM: Support vector machine; LR: Logistic regression; LSA: Latent semantic analysis; PA: Prediction accuracy; \nP@R: Precision; R@R: Recall; MF: Matrix factorization.\n\nAcknowledgements\nNot applicable.\n\nAuthors’ contributions\nSN designed and implemented the model for Pre-launch product prediction. SN analysed and interpreted the customer \nreviews and ratings dataset regarding the pre-launch product prediction. PS supervised the design, implementation \nand analysis of the model for pre-launch product prediction. MC was a major contributor in writing the manuscript. All \nauthors read and approved the final manuscript.\n\nFunding\nNot applicable.\n\nAvailability of data and materials\nThe datasets generated and/or analysed during the current study are available in the Kaggle repository. [snap.stanford.\nedu/data/web-Amazon.html] [40] and [http://www.kaggl e.com/Promp tClou dHQ/flipk art-produ cts] [39].\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAuthor details\n1 Information Technology, School of Engineering, Cochin University of Science & Technology, Kochi 682022, India. \n2 Department of Computer Science, Cochin University of Science & Technology, Kochi 682022, India. 3 Department \nof Ship Technology, Cochin University of Science & Technology, Kochi 682022, India. \n\nReceived: 25 October 2019   Accepted: 17 February 2020\n\n Published online: 28 February 2020 \n\nhttp://www.kaggle.com/PromptCloudHQ/flipkart-products\n\n\nPage 14 of 15Narayanan et al. J Big Data            (2020) 7:13 \n\nReferences\n 1. Lau RY, Liao SY, Kwok RC, Xu K, Xia Y, Li Y. Text mining and probabilistic modeling for online review spam detection. \n\nACM Trans Manag Inform Syst. 2011;2(4):25.\n 2. Lin X, Li Y, Wang X. Social commerce research: definition, research themes and the trends. Int J Inform Manag. \n\n2017;37:190–201.\n 3. Matos CAD, Rossi CAV. Word-of-mouth communications in marketing: a meta-analytic review of the antecedents \n\nand moderators. J Acad Market Sci. 2008;36(4):578–96.\n 4. Jeon S, et al. Redundant data removal technique for efficient big data search processing. Int J Softw Eng Appl. \n\n2013;7.4:427–36.\n 5. Dave K, Lawrence S, and Pennock D. Mining the peanut gallery: opinion extraction and semantic classification of \n\nproduct reviews. WWW’2003.\n 6. Zhou Y, Wilkinson D, Schreiber R, Pan R. Large-scale parallel collaborative filtering for the netflix prize. 2008. p. \n\n337–48. https ://doi.org/10.1007/978-3-540-68880 -8_32.\n 7. Zhang KZK, Benyoucef M. Consumer behavior in social commerce: a literature review. Dec Support Syst. \n\n2016;86:95–108.\n 8. Cui Geng, Lui Hon-Kwong, Guo Xiaoning. The effect of online consumer reviews on new product sales. Int J Electron \n\nComm. 2012;17(1):39–58.\n 9. Manek AS, Shenoy PD, Mohan MC, et al. Detection of fraudulent and malicious websites by analysing user reviews \n\nfor online shopping websites. Int J Knowl Web Intell. 2016;5(3):171–89. https ://doi.org/10.1007/s1128 0-015-0381-x.\n 10. Singh S, and Singh N. Big data analytics. In: Proceedings of the 2012 international conference on communication, \n\ninformation & computing technology (ICCICT ), institute of electrical and electronics engineers (IEEE). 2012. p. 1–4. \nhttp://dx.doi.org/10.1109/iccic t.2012.63981 80.\n\n 11. Demchenko Yuri et al. Addressing big data challenges for scientific data infrastructure. In: IEEE 4th Int. conference \ncloud computing technology and science (CloudCom). 2012.\n\n 12. Sihong Xie, Guan Wang, Shuyang Lin and Yu Philip S. Review spam detection via time-series pattern discovery. In: \nACM Proceedings of the 21st international conference companion on World Wide Web. 2012. p. 635–6.\n\n 13. Koren Y, Bell R, Volinsky C. matrix factorization technique for recommender systems. Computer. 2009;8:30–7.\n 14. Salakhutdinov R, Mnih A, & Hinton G. Restricted boltzmann machines for collaborative filtering. In: Proc. of the 24th \n\nInt. conference on machine learning. 2007. p. 791–8.\n 15. Hao MA, King I, Lyu MR. Learning to recommend with explicit and implicit social relations. ACM Trans Intell Syst \n\nTechnol. 2011;2(3):29.\n 16. Bandakkanavar V, Ramesh M, Geeta V. A survey on detection of reviews using sentiment classification of methods. \n\nIJRITCC. 2014;2(2):310–4.\n 17. Gu V, and Li H. Memory or time—performance evaluation for iterative operation on hadoop and spark. In: Proc. of \n\nthe 2013 IEEE 10th Int. Con. on high-performance computing and communications. 2013. https ://doi.org/10.1109/\nhpcc.and.euc.2013.106.\n\n 18. Zhang Hanpeng, Wang Zhaohua, Chen Shengjun, Guo Chengqi. Product recommendation in online social net-\nworking communities—an empirical study of antecedents and a mediator. J Inform Manag. 2019;56(2):185–95.\n\n 19. Ghose A, Ipeirotis PG. Designing novel review ranking systems: predicting the usefulness and impact of reviews. In: \nInt Conference Electron Comm ACM. 2007. p. 303–10.\n\n 20. Chong AY, Ch’ng E, Liu MJ, Li B. Predicting consumer product demands via Big Data: the roles of online promotional \nmarketing and online reviews. Int J Prod Res. 2015;55:1–15. https ://doi.org/10.1080/00207 543.2015.10665 19.\n\n 21. Yang H, Fujimaki R, Kusumura Y, & Liu J. Online Feature Selection. In: Proceedings of the 22nd ACM SIGKDD Int. \nConference on KDD ‘16, 2016. https ://doi.org/10.1145/29396 72.29398 81.\n\n 22. Breese JS, Heckerman D, and Kadie C. Empirical analysis of predictive algorithms for collaborative filtering. In: Proc. \nof the 14th Conf. on Uncertainty in Artifical Intelligence, 1998.\n\n 23. Mukherjee A, Kumar A, Liu B, Wang J, Hsu M, Castellanos M, Ghosh R. Spotting opinion spammers using behavioral \nfootprints. In: Proc. of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining \nChicago, ACM. 2013. p. 632–40.\n\n 24. Makridakis S, Spiliotis E, Assimakopoulos V. Statistical and Machine Learning forecasting methods: concerns and \nways forward. PLoS ONE. 2018;13(3):e0194889. https ://doi.org/10.1371/journ al.pone.01948 89.\n\n 25. Imon A, Roy C, Manos C, Bhattacharjee S. Prediction of rainfall using logistic regression. Pak J Stat Oper Res. 2012. \nhttps ://doi.org/10.18187 /pjsor .v8i3.535.\n\n 26. Chen T, Zhang W, Lu Q, Chen K, Zheng Z, Yu Y. SVD Feature: a toolkit for feature-based collaborative filtering. J Mach \nLearn Res. 2012;13(1):3619–22.\n\n 27. Shi Y, Larson M, Hanjalic A. Collaborative filtering beyond the user-item matrix—a survey of the state of art and \nfuture challenges. ACM Comput Surv. 2014;47(1):3.\n\n 28. Shan H, & Banerjee A. Generalized probabilistic matrix factorizations for collaborative filtering, In Data mining \n(ICDM), IEEE 10th international conference. 2010. p. 1025–30.\n\n 29. Salakhutdinov R, & Mnih A. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. In: Proc. of \nthe 25th int. conference on machine learning. 2008. p. 880–7.\n\n 30. Crawford M, Khoshgoftaar TM, Prusa JD, Richter AN, Al Najada H. Survey of review spam detection using machine \nlearning techniques. J Big Data. 2015;2(1):23.\n\n 31. Wietsma TA, Ricci F. Product reviews in mobile decision aid systems. Francesco: PERMID; 2005. p. 15–8.\n 32. Jianguo C, et al. A disease diagnosis and treatment recommendation system based on big data mining and cloud \n\ncomputing. Inform Sci. 2018;435:124–49.\n 33. Manek AS, Shenoy PD, Mohan MC, Venugopal KR. Aspect term extraction for sentiment analysis in large movie \n\nreviews using Gini-index feature selection method and SVM classifier. World Wide Web. 2017;20:135–54. https ://doi.\norg/10.1007/s1128 0-015-0381-x.\n\n 34. Fan RE, Chang K-W, Hsieh C-J, Wang X-R, Lin C-J. LIBLINEAR: A library for large linear classification. J Mach Learn Res. \n2008;9:1871–4.\n\nhttps://doi.org/10.1007/978-3-540-68880-8_32\nhttps://doi.org/10.1007/s11280-015-0381-x\nhttp://dx.doi.org/10.1109/iccict.2012.6398180\nhttps://doi.org/10.1109/hpcc.and.euc.2013.106\nhttps://doi.org/10.1109/hpcc.and.euc.2013.106\nhttps://doi.org/10.1080/00207543.2015.1066519\nhttps://doi.org/10.1145/2939672.2939881\nhttps://doi.org/10.1371/journal.pone.0194889\nhttps://doi.org/10.18187/pjsor.v8i3.535\nhttps://doi.org/10.1007/s11280-015-0381-x\nhttps://doi.org/10.1007/s11280-015-0381-x\n\n\nPage 15 of 15Narayanan et al. J Big Data            (2020) 7:13  \n\n 35. Ribeiro MT, Singh S, and Guestrin C. Why should I trust you?: Explaining the predictions of any classifier. In: Proc. \nACMSIGKDD Int. Conf. Knowl. Discov. Data Mining. 2016. p. 1135–44.\n\n 36. Luo X, et al. An effective scheme for QoS estimation via alternating direction method-based matrix factorization. \nIEEE Trans Serv Comput. 2019;12(4):503–18.\n\n 37. Liu CL, Hsaio WH, Lee CH, Lu GC and Jou E. Movie rating and review summarization in mobile environment. In: IEEE \ntrans. systems, man and cybernetics, Part C: applications and reviews. 2012. p. 397–407.\n\n 38. Vapnik, VN. The nature of statistical learning theory, Springer, 2nd ed, 1999. Translated by Xu Jianghua, Zhang Xue-\ngong. Beijing: China Machine Press; 2000.\n\n 39. [Dataset] Flipkart-products. http://www.kaggl e.com/Promp tClou dHQ/flipk art-produ cts.\n 40. [Dataset] https ://snap.stanf ord.edu/data/web-Amazo n.html.\n 41. [Dataset] He R, McAuley J. Ups and downs: modeling the visual evolution of fashion trends with one-class collabora-\n\ntive filtering. WWW; 2016.\n 42. Popescu AM, Etzioni O. Extracting product features and opinions from reviews. 2005; EMNLP.\n 43. Zaharia M, Chowdhury M, Das T, Dave A, Ma J, McCauley M, Franklin M, Shenker S, Stoica I. Resilient distributed \n\ndatasets: A fault-tolerant abstraction for in-memory cluster computing Technical Report UCB/EECS-2011-82. UC \nBerkeley: EECS Department; 2011.\n\n 44. Davis J, Goadrich M. The relationship between precision-recall and ROC curves, In ICML. 2006. p. 233–40.\n 45. Lee JS, Lee ES. Exploring the usefulness of predicting people’s locations. Procedia Soc Beh Sci. 2014. https ://doi.\n\norg/10.1016/j.sbspr o.2014.04.451.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nhttp://www.kaggle.com/PromptCloudHQ/flipkart-products\nhttps://snap.stanford.edu/data/web-Amazon.html\nhttps://doi.org/10.1016/j.sbspro.2014.04.451\nhttps://doi.org/10.1016/j.sbspro.2014.04.451\n\n\tImproving prediction with enhanced Distributed Memory-based Resilient Dataset Filter\n\tAbstract \n\tIntroduction\n\tRelated work\n\tMethodology\n\tData collection phase\n\tDataset pre-processing\n\tResilient Distributed Dataset\n\n\tPrediction classifiers\n\tLogistic regression (LR)\n\tSupport Vector Machine (SVM)\n\n\tExperimental setup\n\n\tResults and discussions\n\tConclusion and future work\n\tAcknowledgements\n\tReferences\n\n\n\n\n",
      "text": [
        "",
        "Published online: 28 February 2020"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"Published online: 28 February 2020\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":16},{\"x\":1020,\"y\":16},{\"x\":1020,\"y\":74},{\"x\":0,\"y\":71}],\"text\":\"Published online: 28 February 2020\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":17},{\"x\":281,\"y\":17},{\"x\":281,\"y\":70},{\"x\":0,\"y\":69}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":291,\"y\":17},{\"x\":497,\"y\":17},{\"x\":498,\"y\":72},{\"x\":291,\"y\":70}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":507,\"y\":17},{\"x\":585,\"y\":17},{\"x\":586,\"y\":72},{\"x\":508,\"y\":72}],\"text\":\"28\"},{\"boundingBox\":[{\"x\":595,\"y\":17},{\"x\":862,\"y\":17},{\"x\":864,\"y\":74},{\"x\":596,\"y\":72}],\"text\":\"February\"},{\"boundingBox\":[{\"x\":872,\"y\":17},{\"x\":1017,\"y\":17},{\"x\":1019,\"y\":75},{\"x\":874,\"y\":74}],\"text\":\"2020\"}]}"
      ]
    },
    {
      "@search.score": 1.6784654,
      "content": "\nRawnaque et al. Brain Inf.            (2020) 7:10  \nhttps://doi.org/10.1186/s40708-020-00109-x\n\nR E V I E W\n\nTechnological advancements \nand opportunities in Neuromarketing: \na systematic review\nFerdousi Sabera Rawnaque1*, Khandoker Mahmudur Rahman2, Syed Ferhat Anwar3, Ravi Vaidyanathan4, \nTom Chau5, Farhana Sarker6 and Khondaker Abdullah Al Mamun1,7\n\nAbstract \nNeuromarketing has become an academic and commercial area of interest, as the advancements in neural record-\ning techniques and interpreting algorithms have made it an effective tool for recognizing the unspoken response \nof consumers to the marketing stimuli. This article presents the very first systematic review of the technological \nadvancements in Neuromarketing field over the last 5 years. For this purpose, authors have selected and reviewed a \ntotal of 57 relevant literatures from valid databases which directly contribute to the Neuromarketing field with basic \nor empirical research findings. This review finds consumer goods as the prevalent marketing stimuli used in both \nproduct and promotion forms in these selected literatures. A trend of analyzing frontal and prefrontal alpha band sig-\nnals is observed among the consumer emotion recognition-based experiments, which corresponds to frontal alpha \nasymmetry theory. The use of electroencephalogram (EEG) is found favorable by many researchers over functional \nmagnetic resonance imaging (fMRI) in video advertisement-based Neuromarketing experiments, apparently due to \nits low cost and high time resolution advantages. Physiological response measuring techniques such as eye tracking, \nskin conductance recording, heart rate monitoring, and facial mapping have also been found in these empirical stud-\nies exclusively or in parallel with brain recordings. Alongside traditional filtering methods, independent component \nanalysis (ICA) was found most commonly in artifact removal from neural signal. In consumer response prediction and \nclassification, Artificial Neural Network (ANN), Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) \nhave performed with the highest average accuracy among other machine learning algorithms used in these litera-\ntures. The authors hope, this review will assist the future researchers with vital information in the field of Neuromarket-\ning for making novel contributions.\n\nKeywords: Neuromarketing, Neural recording, Machine learning algorithm, Brain computer interface, Marketing\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\n1 Introduction\nNeuromarketing, an application of the non-invasive \nbrain–computer interface (BCI) technology, has emerged \nas an interdisciplinary bridge between neuroscience and \nmarketing that has changed the perception of market-\ning research. Marketing is the channel between prod-\nuct and consumers which determines the ultimate sale. \n\nWithout effective marketing, a good product fails to \ninform, engage and sustain its targeted audiences [1]. \nThe expanding economy with new businesses is continu-\nously evolving with changing consumer preferences. It \nis hard for the businesses to grow and sustain without \nhaving quantitative or qualitative assessment from their \nconsumers. Newly launched products need even more \neffective marketing to successfully enter into a com-\npetitive market. However, traditional marketing renders \nonly by posteriori analysis of consumer response. Con-\nventional market research depends on surveys, focus \n\nOpen Access\n\nBrain Informatics\n\n*Correspondence:  frawnaque@umassd.edu\n1 Advanced Intelligent Multidisciplinary Systems Lab, Institute \nof Advanced Research, United International University, Dhaka, Bangladesh\nFull list of author information is available at the end of the article\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40708-020-00109-x&domain=pdf\n\n\nPage 2 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ngroup discussion, personal interviews, field trials and \nobservations for collecting consumer feedback [2]. These \napproaches have the limitations of time requirement, \nhigh cost and unreliable information, which can often \nproduce inaccurate results. In contrast to the traditional \nmarketing research techniques, Neuromarketing allows \ncapturing consumers’ unspoken cognitive and emotional \nresponse to various marketing stimuli and can forecast \nconsumers’ purchase decisions.\n\nNeuromarketing uses non-invasive brain signal record-\ning techniques to directly measure the response of a \ncustomer’s brain to the marketing stimuli, supersed-\ning the traditional survey methods [3]. Functional mag-\nnetic resonance (fMRI), electroencephalography (EEG), \nmagnetoencephalography (MEG), transcranial mag-\nnetic stimulator (TMS), positron emission tomography \n(PET), functional near-infrared spectroscopy (fNIRS) etc. \nare some examples of neural recording devices used in \nNeuromarketing research. By obtaining neuronal activ-\nity from the brain using these devices, one can explore \nthe cognitive and emotional responses (i.e., like/dislike, \napproach/withdrawal) of a customer. Different stimuli \ntrigger associated response in a human brain and the \nresponse can be tracked by monitoring the change in \nneuronal signals or brainwaves [4]. Further, the signal \nand image processing techniques and machine learning \nalgorithms have enabled the researchers to measure, ana-\nlyze and interpret the possible meanings of brainwaves. \nThis opens a new door to detect, analyze and predict \nthe buying behavior of customers in marketing research. \nNow with the help of brain–computer interface, the men-\ntal states of a customer, i.e., excitement, engagement, \nwithdrawal, stress, etc., while experiencing a market-\ning stimuli can be captured [5]. Besides these brain sig-\nnal recording techniques, Neuromarketing also utilizes \nphysiological signals, i.e., eye tracking, heart rate and \nskin conductance measurements to gather the insight of \naudience’s physiological responses due to encountering \nstimuli. These neurophysiological signals with advanced \nspectral analysis and machine learning algorithms can \nnow provide nearly accurate depiction of consumers’ \npreferences and likes/dislikes [6–8].\n\nEarly years of Neuromarketing generated a contro-\nversy between the academician and the marketers due \nto its high promises and lack of groundwork. From \nthe claim of peeping into the consumer mind to find-\ning the buy buttons of human brain, Neuromarketing \nhas long been under the scrutiny of the academicians \nand researchers [9, 10]. However, academic research in \nthis field has started to pile up and the scope of Neuro-\nmarketing to reveal and predict consumer behavior is \ngradually becoming evident. Neuromarketing Science \nand Business Association (NMSBA) was established \n\nin 2012 to bridge the gap between academicians and \nNeuromarketers, and it is promoting Neuromarket-\ning research across the world with its annual event of \nNeuromarketing World Forum [11, 12]. It may be pro-\nposed that further dialogue may continue under such a \nplatform for further industry–academia collaboration. \nEvidently, more than 150 consumer neuroscience com-\npanies are commercially operating across the globe and \nbig brands (Google, Microsoft, Unilever, etc.) are using \ntheir insights to impact their consumers in a tailored and \nefficient way. Academic research, especially the high ana-\nlytical accuracy from the engineering part of Neuromar-\nketing has garnered this breakthrough and acceptance \nover the world. Hence, reviewing the building blocks of \nNeuromarketing is essential to evaluate its scopes and \ncapacities, and to contribute new perspective in this \nfield. Numerous literature reviews have been published \nfocusing the theoretical aspect of consumer neurosci-\nence, such as marketing, business ethics, management, \npsychology, consumer behavior, etc. [13–15]. However, \nsystematic literature review from the engineering per-\nspective with a focus on neural recording tools and inter-\npretational methodologies used in this field is absent. In \nthis regard, our article sets its premises to answer the fol-\nlowing questions:\n\n– What are the types of marketing stimuli currently \nbeing used in Neuromarketing?\n\n– What are the brain regions activated by these mar-\nketing stimuli?\n\n– What is the best brain signal recording tool currently \nbeing used in Neuromarketing research?\n\n– How are these brain signals preprocessed for further \nanalysis?\n\n– And what are the current methods or techniques \nused to interpret these brain signals?\n\nThese questions will allow us to gain a comprehensive \nknowledge on the up-to-date research scopes and tech-\nniques in consumer neuroscience. After this brief intro-\nduction, our methodology of conducting this systematic \nreview will be presented, followed by the state-of-the-art \nfindings corresponding to the aforementioned questions \nand synthesis of the important results. We concluded this \nreview with relevant inference from synthesized result \nand a recommendation for future researchers.\n\n2  Methodology\nThe systematic literature review is a process in which \na body of literature is collected, screened, selected, \nreviewed and assessed with a pre-specified objective for \nthe purpose of unbiased evidence collection and to reach \nan impartial conclusion [16]. Systematic review has the \n\n\n\nPage 3 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nobligation to explicitly define its research question and to \naddress inclusion–exclusion criteria for setting the scope \nof the investigation. After exhaustive search of existing \nliteratures, articles should be selected based on their rel-\nevance, and the results of the selected studies must be \nsynthesized and assessed critically to achieve clear con-\nclusions [16].\n\nIn this systematic review, we would like to explore \nthe marketing stimuli used in Neuromarketing research \narticles over the last 5 years with their triggered brain \nregions. We would also like to focus on the technologi-\ncal tools used to capture brain signals from these regions, \nand finally deliberate on signal processing and analytical \nmethodologies used in these experiments.\n\nTherefore, the inclusion criteria defined here are  as \nfollows:\n\n– Literatures must be published in the field of Neuro-\nmarketing from 2015 to 2019.\n\n– Studies must use brain–computer interface and/or \nother physiological signal recording device in their \nNeuromarketing experiments.\n\n– Studies must have experimental findings from neu-\nral and/or biometric data used in Neuromarketing \nresearch.\n\nThe exclusion criteria for this review are set as:\n\n– Any other literature review on Neuromarketing are \nexcluded from this review.\n\n– Book chapters are excluded from this review. Since \nNeuromarketing is comparatively a new research \nfield, alongside relevant academic journal articles, \nbook chapters conducting empirical experiments \nusing BCI can only be included.\n\n– Literatures written/published in any language other \nthan English are excluded from this article.\n\nTo serve the purpose of this systematic literature \nreview, a total of 931 articles were found across the \n\ninternet by using the search item “Neuromarketing” \nand “Neuro-marketing” in valid databases. Among the \nscreened publications, Table  1 presents the database \nsource of selected 57 research articles including book \nchapters, which directly contribute to the Neuromarket-\ning field with basic or empirical research findings.\n\nAs for the aggregation of relevant existing literatures, \nthe researchers defined that the search for articles would \nbe performed in six databases—Science Direct, Emer-\nald Insight, Sage, IEEE Xplore, Wiley Online Library, \nand Taylor Francis Online. After the initial article accu-\nmulation, the articles were exhaustively screened by \nthe authors by reviewing their title, abstract, keywords \nand scope to match the objective of this research. Once \nthe studies met our aforementioned inclusion criteria, \nthey were selected for further review and critical analy-\nsis. Table  2 classifies the selected articles in terms of the \naforementioned dimensions.\n\nBy exploring the articles selected to develop this sys-\ntematic review, it was possible to successfully categorize \nthe trends and advancements in Neuromarketing field in \nfollowing dimensions:\n\n i. Marketing stimuli used in Neuromarketing \nresearch\n\n ii. Activation of the brain regions due to marketing \nstimuli\n\n iii. Neural response recording techniques\n iv. Brain signal processing in Neuromarketing\n v. Machine learning applications in Neuromarketing.\n\nSome of these Neuromarketing studies have used \neye tracking, heart rate, galvanic skin response, facial \naction coding, etc., with or without brain signal \nrecording techniques to gauge the consumer’s hidden \nresponse. As they are the response from autonomous \nnervous system (ANS), they have proven themselves \nas successful means of exploring consumer’s focus, \narousal, attention and withdrawal actions. Hence, this \nstudy includes articles those empirically used these \n\nTable 1 Number of articles found and selected\n\nName of the database Results: search “Neuromarketing” Results: search “Neuro-marketing” Articles selected\n\nScience direct 281 55 12\n\nWiley online 111 11 7\n\nEmerald insight 115 8 14\n\nIEEE 34 0 14\n\nSage 12 15 6\n\nTaylor Francis online 106 36 4\n\nTotal found: 806 Total found: 125 Total selected: 57\n\n\n\nPage 4 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ntools to answer Neuromarketing questions, since this \nstudy mainly focuses on the engineering perspective. \nInterpreting the neural data with only statistical analy-\nsis has been out of scope of this paper.\n\n3  Systematic review on the advancements \nof Neuromarketing\n\nNeuromarketing research utilizes marketing strategies in \nthe form of stimuli, and aims to invoke, capture and ana-\nlyze activities occurring in different brain regions while \n\nTable 2 Studies selected on the dimensions of this review\n\nDimensions Published articles\n\ni. Marketing stimuli used in Neuromarketing Product Chew et al. [17], Yadava et al. [18], Rojas et al. [19], Pozharliev [20], Touchette \nand Lee [21], Marques et al. [22], Shen et al. [23], Çakir et al. [24], Hubert \net al. [25], Hsu and Chen et al. [26], Hoefer et al. [27], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Wolfe et al. [31], Bosshard et al. [32], \nFehse et al. [33].\n\nPrice Çakar et al. [34], Marques et al. [22], Çakir et al. [24], Gong et al. [35], Pilelienė \nand Grigaliūnaitė [36], Hsu and Chen [26], Boccia et al. [37], Venkatraman \net al. [38], Baldo et al. [39].\n\nPromotion Soria Morillo et al. [40], Yang et al. [41], Cherubino et al. [42], Soria Morillo \net al. [43], Vasiljević et al. [44], Yang et al. [45], Pilelienė and Grigaliūnaitė \n[36], Daugherty et al. [46], Royo et al. [47], Etzold et al. [48], Chen et al. \n[49], Casado-Aranda et al. [50], Randolph and Pierquet [51], Nomura and \nMitsukura [52], Ungureanu et al. [53], Goyal and Singh [54], Oon et al. [55], \nSingh et al. [56].\n\nii. Activation of brain region due to marketing stimuli Soria Morillo et al. [40], Chew et al. [17], Cherubino et al. [42], Soria Morillo \net al. [43], Çakar et al. [34], Boksem and Smitds [57], Bhardwaj et al. [58], Ven-\nkatraman et al. [38], Touchette and Lee [21], Yang et al. [45], Marques et al. \n[22], Gong et al. [35], Gordon et al. [59], Krampe et al. [60], Hubert et al. [25], \nÇakir et al. [24], Holst and Henseler [61], Hsu and Cheng [62], Hoefer et al. \n[27], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Jain et al. \n[63], Wolfe et al. [31], Bosshard et al. [32], Fehse et al. [33].\n\niii. Neural response recording techniques EEG Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Cherubino et al. [42], \nSoria Morillo et al. [43], Yadava et al. [18], Doborjeh et al. [64], Çakar et al. \n[34], Kaur et al. [65], Baldo et al. [19], Boksem and Smitds [57], Pozharliev \net al. [20], Venkatraman [38], Touchette and Lee [21], Yang et al. [45], Pilelienė \nand Grigaliūnaitė [36], Shen et al. [23], Daugherty et al. [46], Royo et al. [47], \nGong et al. [35], Gordon et al. [59], Hsu and Chen et al. [26], Hoefer et al. [27], \nRandolph and Pierquet [51], Nomura and Mitsukura [52], Bhardwaj et al. \n[58], Fan and Touyama [66], Rakshit and Lahiri [67], Jain et al. [63],Ogino and \nMitsukura [68], Oon et al. [55], Bosshard et al. [32].\n\nfMRI Venkatraman et al. [38], Marques et al. [22], Hubert et al. [25], Hsu and Cheng \n[62], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Wolfe et al. \n[31], Fehse et al. [33].\n\nfNIRS Çakir et al. [24], Krampe et al. [60].\n\nEMG Missagila et al. [69]\n\nEye tracking Venkatraman [38], Rojas et al. [19], Pilelienė and Grigaliūnaitė [36], Çakar et al. \n[34], Ceravolo et al. [70], Ungureanu et al. [53]\n\nGalvanic skin \nresponse, \nheart rate\n\nCherubino et al. [42], Çakar et al. [34], Magdin et al. [71], Goyal and Singh [54], \nSingh et al. [56].\n\niv. Brain signal processing in Neuromarketing Cherubino et al. [42], Bhardwaj et al. [53], Venkatraman [38], Pozharliev et al. \n[20], Boksem and Smitds [57], Wriessnegger et al. [29], Fan and Touyama \n[66], Pilelienė and Grigaliūnaitė [36], Yadava et al. [18], Baldo et al. [19], \nClerico et al. [72], Chen et al. [49], Casado-Aranda et al. [50], Hsu and Cheng \n[62], Taqwa et al. [73], Bhardwaj et al. [58],Wang et al. [30], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Oon et al. [55], Fehse et al. [33],\n\nv. Machine learning applications in Neuromarketing Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Soria Morillo et al. [43], \nYadava et al. [18], Doborjeh et al. [64], Gordon [59], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Taqwa et al. [73], Bhardwaj et al. \n[58], Randolph and Pierquet [51], Fan and Touyama [66], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Ogino and Mitsukura [68], Oon \net al. [55], Singh et al. [56].\n\n\n\nPage 5 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nsubjects experience these stimuli. To conduct a system-\natic review on this matter, it is important to recall the \ninterconnection between brain functions with human \nbehavior and actions triggered by the  external stimuli. \nThe knowledge of brain anatomy and the physiologi-\ncal functions of brain areas as well as the physiological \nresponse due to external stimuli along with it, makes \nit possible to model brain activity and predict hidden \nresponse. For this purpose, current neural imaging sys-\ntems and neural recording systems have contributed \nmuch to capture the true essence of consumer prefer-\nences. This section will discuss the marketing stimuli, \ntheir targeted brain regions, neural and physiological \nsignal capturing technologies used over the last 5 years \nin Neuromarketing research. Comparing these signals \nwith their associated anatomical functionality some stud-\nies have already reached high accuracy. A number of the \nselected studies have used machine learning techniques \nto predict like/dislike and possible preference from the \ntest subjects.\n\nFor the purpose of Neuromarketing experiments, the \nfollowing literatures selected right-handed participants, \nwith normal or corrected-to-normal vision, free of cen-\ntral nervous system influencing medications and with no \nhistory of neuropathology.\n\n3.1  Marketing stimuli used in Neuromarketing\nAs Neuromarketing is a focus of marketers and consumer \nbehavior researchers, different strategies from market-\ning have been applied in Neuromarketing and they are \nbeing investigated for quantitative assessment from neu-\nrological data. Nemorin et  al. asserts that Neuromarket-\ning differentiates from any other marketing models as \nit bypasses the thinking procedures of consumers and \ndirectly enters their brain [74]. Over the last 5  years, \nNeuromarketing stimuli has been mainly in two forms—\nproducts with/without price, and promotions. Product \ncan be defined as physical object or service that meets \nthe consumer demand. In Neuromarketing, product can \nbe physical such as tasting a beverage to conceptual like \na 3D (three dimensional) image of the product. Price in \nNeuromarketing experiments is mostly seen as a stimuli \nis most of the time intermingled with product or pro-\nmotion. However, it plays an important role that deter-\nmines the decision of test subjects to buy or not to buy \nthe product [75].\n\nConsumer response to a product has been recognized \nby either physically experiencing the product or by visu-\nalizing the image of  it. To understand the user esthetics \nof 3D shapes, Chew et  al. [17], used virtual 3D bracelet \nshapes in motion and recorded the brain response of \ntest subjects with EEG with motion. As 3D visualiza-\ntion of objects for preference recognition is a new area \n\nof research, the authors used mathematical model (Gie-\nlis superformula) to create 3D bracelet-like objects. \nTheir study displayed 3D shapes appear like bracelets as \nthe product to subjects. Using the 3D shapes gave the \nauthors an advantage to produce as many of 60 bracelet \nshapes to conduct the research on. Another new prod-\nuct was the E-commerce products presented to the test \nsubjects by Yadava et al. and Çakar et al. [18, 34]. Yadava \net  al. proposed a predictive modeling framework to \nunderstand consumer choice towards E-commerce prod-\nucts in terms of “likes” and “dislikes” by analyzing EEG \nsignals. In showing E-commerce product, they showed a \ntotal of 42 product images to the test participants. These \nproduct images were mainly of apparels and accessory \nitems such as shirts, sweaters, shoes, school bags, wrist \nwatches, etc. The test participants were asked to disclose \ntheir preference in terms of likes and dislikes after view-\ning the items  [18]. Çakar et  al. used both product and \nprice to explore the experience during product search of \nfirst-time buyers in E-commerce. To motivate the partici-\npants, this research provided each participants around \n73 USD as a gift card to use during the experiment. The \ntest participants were asked to search and select three \nproducts of their interest from an e-commerce website \nand reach the maximum of their gift card limit to acti-\nvate. Test subjects often experienced negative emotion \nwhile being unable to find necessary buttons such as “add \nto cart” or “sorting options” [34]. These Neuromarketing \nexperiments on E-commerce products may help develop-\ners to build better user experience. Retail businesses lose \nlarge amount of money when they invest in the wrong \nproduct. Among retail products, shoes have thousands \nof blueprints for manufacturing. Producing thousands \nof shoes of different designs to satisfy consumers can be \nlaborious and unprofitable since a large number of the \ndesigns turn out to be failures. Baldo et  al. directly used \n30 existing image of shoe designs to show the test sub-\njects to and to choose from a mock shop showing on the \nscreen [39]. EEG signals were recorded during the whole \nshoe selection time and then subjects were asked to rate \nthe shoes in a rank of 1 to 5 of Likert scale. This experi-\nment helped realize brain response-based prediction can \nsupersede self-report-based methods, as the simulation \non sales data showed 12.1% profit growth for survey-\nbased prediction, and 36.4% profit growth for the brain \nresponse-based prediction.\n\nSimilar to the shoe experiment, Touchette and Lee [21] \nexperimented on the choice of apparel products among \nyoung adults, based on Davidson’s frontal asymmetry \ntheory. EEG signals were recorded while 34 college stu-\ndents viewed three attractive and three unattractive \napparel products on a high-resolution computer screen \nin a random order. Pozharliev et  al. [20] experimented \n\n\n\nPage 6 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\non the emotion associated with visualizing luxury brand \nproducts vs. regular brand products. The experiment dis-\nplayed 60 luxury items and 60 basic brand items to 40 \nfemale undergraduate students to recognize the brain \nresponse of seeing high emotional value (luxury) prod-\nucts in social vs. alone atmosphere. The study found \nthat, luxury brand products invoked a higher emotional \nvalue in social atmosphere which could be utilized by the \nmarketers. Bosshard et  al. and Fehse et  al. experimented \non brand images and the comparison between the brain \nresponses associated with preferred and not preferred \nbrands [32, 33]. In the study performed by Bosshard et al., \nconsumer attitude towards established brand names were \nmeasured via electroencephalography. Subjects were \nshown 120 brand names in capital white letter in Tahoma \nfont on black background and without any logo while \ntheir brain responses were recorded. On the other hand, \nFehse et al. compared the brain response of test subjects \nwhile they visualized blocks of popular vs. organic food \nbrand logos. These experiments on brand image may help \nmarketers to recognize the implicit response of consum-\ners on different types of branding.\n\nAs price is mentioned as an important factor that \ndetermines the user’s interest on purchasing a product, \na number of Neuromarketing studies have used price \nalongside the products. In the aforementioned study \nby Çakar et  al. [34] price was displayed while recording \nbrain response during first-time e-commerce user expe-\nrience. Marques et  al. [22], Çakir et  al. [24], Gong et  al. \n[35], Pilelienė and Grigaliūnaitė [36], Hsu and Chen [26], \nBoccia et al. [37], Venkatraman et al. [38], and Baldo et al. \n[39] have included price as a marketing stimuli with the \nproduct or promotional.\n\nAn interesting concept was tried by Boccia et  al. to \nrecognize the relation between corporate social respon-\nsibilities and consumer behavior. The author attempted \nto identify if consumers were willing to pay more for the \nproducts from socially or environmentally responsible \ncompany. Consumers were found to prefer the conven-\ntional companies over the socially responsible companies \ndue to lesser price. Marques et  al. [22] investigated the \ninfluence of price to compare national brand vs. own-\nlabeled branded products. In the experiment of Çakir \net  al, product then product and price were shown to \nthe subjects before decision-making time and the brain \nresponses were recorded through fNIRS [24]. Sometimes \nprice can play a passive role in the form of discounts or \ngifts in a promotional. Gong et  al. innovatively designed \nan experiment to compare consumer brain response \nassociated with promotional using discount (25% off ) vs. \ngift-giving (gift value equivalent to the discount) mar-\nketing strategies. Their study found that lower degree of \nambiguity (e.g., discounts) better motivates consumer \n\ndecision-making [35]. Hsu and Chen used price as a con-\ntrol variable in their wine tasting experiment. As price \nplays a pivotal role in purchase decision, two wines were \nselected of approximately equal price $15. Then the EEG \nsignals of test subjects were recorded during the wine \ntasting session [26].\n\nPromotion is the communication from the marketers’ \nend to influence the purchase decision of consumers [75]. \nIn Neuromarketing research, promotion is usually found \nas the TV commercials and short movies for advertise-\nment. One of the key focus of Neuromarketers is to \nevaluate the consumer engagement of advertisements. \nPredicting the engagement of advertisements before \nbroadcasting them on air, ensures higher rate of success-\nful promotions.\n\nIn 2015, Yang et  al. used six smartphone commercials \nof different brands to compare among them in terms \nof extract cognitive neurophysiological indices such as \nhappiness, surprise, and attention as well as behavio-\nral indices (memory rate, preference, etc.) [41]. A com-\nmon experimental design procedure is found among the \npromotion-based Neuromarketing experiments, that is \nsubjects are first made comfortable in the experimental \nsetting, consecutive advertisements were placed at a time \ndistance no shorter than 10  s and consecutive advertise-\nments used neutral stimuli such as white screen, green \nscenario, blank in between them to stabilize the test \nparticipants.\n\nThe Neuromarketing experiments of Soria Morillo \net  al. [40, 43] tried to find out the electrical activity of \naudience brain while viewing advertisement relevant to \naudiences’ taste. They display used 14 TV commercials \ndisplayed to their 10 test subjects for their experiment \nand predicted like or dislike response from audience \nwith the help of advanced algorithms. Cherubino et  al. \n[42] investigated cognitive and emotional changes of \ncerebral activity during the observation of TV commer-\ncials among different aged population. Among seven TV \ncommercials displayed during the experiment, one com-\nmercial with strong images was analyzed for the adults’ \nand older adults’ reaction. Other than them, Vasiljević \net  al. [44] used Nestle advertisement to measure con-\nsumer attention though pulse analysis; Daugherty et  al. \n[46] replicated an experiment of Krugman (1971) using \nboth TV advertisements and print media advertise-\nments to recognize how consumers look and think; Royo \net  al. [47] focused on consumer response while viewing \nadvertisements of sustainable product designs. For their \nexperiment, an animated commercial was made contain-\ning verbal narrative of sustainable product and an exist-\ning commercial was used to convey the visual narrative \nof conventional product. Venkatraman  et al. focused \non measuring the success of TV advertisements using \n\n\n\nPage 7 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nneuroimaging and biometric data  [38]. Randolph and \nPierquet [51] showed super bowl commercials to under-\ngraduate students to compare the class rank of the com-\nmercials and the neural response from the test subjects. \nNomura and Mitsukura [52] identified emotional states \nof audiences while watching favorable vs. unfavorable TV \ncommercials. They selected 100 TV commercials among \nwhich 50 commercials were award winning which were \nlabeled as favorable advertisements. Singh et al. [56] used \npromotion in the form of static vs. video advertisements \nto predict the success of omnichannel marketing strate-\ngies. Ungureanu et  al. [53] measured user attention and \narousal by eye tracking while surfing through web page \ncontaining static advertisements, while Goyal and Singh \n[54] utilized facial biometric sensors to model an auto-\nmated review systems for video advertisements. Oon \net al. [55] used merchandise product advertisement clips \nto recognize user preference. Singh et al. [56] used video \nadvertisements to measure visual attentions of audiences.\n\nMost of the TVC (television commercials) in these lit-\neratures had a standard time of 30 s. In Neuromarketing, \nthese TVCs were displayed in between other videos such \nas documentary film, gaming video, drama, etc., to cap-\nture the true response of consumers.\n\nSometimes Neuromarketing  is observed dealing with \nadvertisement of different purposes, such as social adver-\ntisements or gender-related advertisements. The appli-\ncation of Neuromarketing in social advertisement is to \npredict the success of these ads to reach its messages to \nthe targeted social groups [45, 49, 69]. Chen et  al. [49] \nexperimented on the neural response of adolescent audi-\nences while they are exposed to e-cigarette commercials. \nAnother social advertisement stimuli of smoking cessa-\ntion frames was used by Yang [45], to understand what \ntypes of frames (positive/negative) achieve better atten-\ntion from smokers and non-smokers. Gender plays a \nsubstantial role in advertisement industry from celebrity \nendorsement to gender-targeted marketing. Missaglia \net  al. [69] conducted a research on fast marketed con-\nsumer goods (FMCG) advertisements with celebrity vs. \nnon-celebrity female spokesperson. Casado-Aranda et al. \n[50] worked on gender-targeted advertisements using \ncongruent vs. incongruent product–voice combination. \nThese studies show us the diversity of marketing stimuli \nfor future Neuromarketing applications.\n\n3.2  Activation of brain regions due to marketing stimuli\nHuman brain is a matter of profound astonishment. \nThe anatomical development of our brain resulted in \nthe complex web of cognitive and emotional process we \nexperience every day. The evolution of vertebrate brain \nwas initially proposed by Paul D. MacLean  in his Tri-\nune Brain model  [76]. In his hypothesis, evolution of \n\nvertebrate brain is formed through three phases. First \nthe reptilian complex, which indicates the association \nof instincts with the anatomical structure basal gan-\nglia. The paleomammalian complex consists of sep-\ntum, amygdalae, hypothalamus, hippocampal complex, \nand cingulate cortex as the limbic system. These orga-\nnelles were associated with motivation and emotional \nresponse of mammalian brain. Finally, neomammalian \ncomplex consists of cerebral neocortex or the outer \nlayer of advanced mammalian brain, which is particu-\nlarly a unique feature of human brain. In the cerebral \nneocortex, we find four lobes which control our sen-\nsory, motor, emotional and cognitive processes [76]. \nThe triune brain model has been rejected by new neu-\nroscientists due to the interconnectivity of human brain \nstructures and their function. However, the anatomical \nstructure of human brain explained by this theory plays \na vital role in recognizing cognitive, emotional and \nbehavioral process.\n\nUnderstanding the anatomy of human brain has \nshowed itself indispensable in Neuromarketing \nresearch, as its functionality is deeply associated with \nthe interpretation of neural response. The outer layer of \nthe human brain is a complex system organized in four \nlobes, namely (frontal, parietal, temporal and occipital \nlobes), each having distinct functionalities for cogni-\ntive, emotional, and motor responses. The frontal lobe \nis the region where most of our thoughts and conscious \ndecisions are made  [77]. Cognitive decision-making \nmainly takes part in the prefrontal region of this lobe, \nand movement-related decisions are made in the end \npart of frontal lobe. Information about taste, touch and \nmovement is processed by the parietal lobe. The occipi-\ntal lobe is the primary center for visual processing, \nand the temporal lobe is responsible for visual memo-\nries, auditory recognition and integrating new sensory \ninformation with memories  [78]. Besides the primary \nlobes, cerebral cortex brain anatomy has gyri and sulci \nwhich create the folded appearance of the brain. The \ngyri functions on increasing surface area for informa-\ntion processing. Alongside the primary lobes, gyri of \nthese lobes can be considered as the region of interest \n(ROI) in neural imaging techniques [79].\n\nDeeper structures of the human brain consist-\ning thalamus, amygdalae, etc., produces sensory and \ninstinctual responses which are later transported to \nthe cerebral cortex. Hypothalamus works as the master \ncontrol of our autonomic system. Sleep, hunger, thirst, \nblood pressure, body temperature, sexual arousal are \ncontrolled and regulated by hypothalamus. Thalamus \non the other hand regulates sensory information, atten-\ntion and memory. Amygdalae originate our emotional \n\n\n\nPage 8 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\nresponse and hippocampus is the mainframe of our \nmemory [77].\n\nRetrieving information from brain requires diverse \ntypes of methodology. In Neuromarketing experiments, \ndifferent parts of brain are selected for retrieving dis-\ntinct information. An experiment which solely focuses \non attention might only look at the signals from frontal \nlobe, whereas experiments focusing on buyer’s motiva-\ntion might want to look at deeper structures [38].\n\nAccording to Soria Morillo et  al., brain signal acquisi-\ntion may capture neural signals either from cerebral cor-\ntex or from the deeper layer of the brain [40, 43]. Their \nexperiment on TV advertisement liking recognition ini-\ntially uses information only from prefrontal cortex using \na single electrode EEG device. Their experiment showed, \nit is possible to classify like/dislike with information col-\nlected solely from frontal lobe.\n\nSimilarly, Cherubino et  al. emphasized on the signifi-\ncance of frontal cortex (FC) and prefrontal cortex (PFC) \nin Neuromarketing studies. PFC processes the conscious \nand unconscious cognitive and emotional information. \nHence, devices using only a single sensor select PFC as \ntheir signal acquisition region  [42]. Also, ventromedial \nprefrontal cortex corresponds to motivational behaviors, \nimaging of which by fMRI or MEG can reveal purchase \nmotivations [22].\n\nNeural communication in the brain is conducted \nthrough the action potentials, or the firing of neu-\nrons  [80]. A neuronal signal is the electrochemical \ninformation that neurons send to each other. These \ninformation are acquired as signals of non-linear pat-\ntern called the brainwaves  [80]. These brainwaves are \nfurther associated with the neural signature of brain \nstates. The neural signature is divided into frequency \nbands known as rhythms, such as the delta (0.1–4  Hz), \ntheta (4–8  Hz), alpha (8–12  Hz), beta (12–30  Hz), and \ngamma (30–90  Hz). These frequency bands are related \nto different brain states, regions, functions or patholo-\ngies. Delta (δ) waves are characteristic of deep sleep and \nhave not been explored for BCI applications  [81]. Theta \n(θ) waves are enhanced during sleep in adults and often \nrelated to various brain disorders. During wakefulness \nunder relaxed conditions alpha (α) waves with moder-\nate amplitude appear spontaneously. Beta (β) waves have \nless amplitude and are strongly related to motor control \nand engagement or decision-making procedure. Gamma \n(γ) waves are associated with movement-related activ-\nity of the brain and intensely observed in invasive neural \nrecording [81].\n\nIn Neuromarketing, beta wave amplitudes are associ-\nated with reward processing which can further predict \nsuccess of a product or TVC (Boksem and Smitds) [57].\n\nFrontal alpha asymmetry is a key concept of hem-\nisphere-based like–dislike classification approach. \nWhen the brain is considered as two hemispheres, left \nand right frontal cortices show hemispheric asymme-\ntry in their activation during processing positive and \nnegative emotion. Another term for emotional engage-\nment, Approach–Withdrawal Index refers to the emo-\ntional response from Frontal Alpha Asymmetry theory \n[34]. Frontal Asymmetry Index is a marker of approach \nand avoidance. “Emotional Engagement” in Neuromar-\nketing is expressed as the power of specific frequency \nbands from left and right frontal regions. The F3/F4 and \nF7/F8 electrodes are the best candidates for these EEG \npower reception as they are positioned at the most sen-\nsitive places (International 10–20 System). The alpha \nfrequency band (8–12  Hz) is commonly used in the \nfrontal alpha asymmetry theory. However, as the alpha \nactivity corresponds with relaxation and meditation, it \nis negatively correlated with cognitive engagement.\n\nFrontal Asymmetry Index is measured from the \nequation:\n\nHigher the Frontal Asymmetry Index value, the more \napproach response is obtained from the test subjects \nand vice versa. This high or positive asymmetry score \ncan determine pleasant feeling of a test subject and vice \nversa, which was explored in the study conducted by \nTouchette and Lee [21].\n\nNeuroimaging and neural signal recording devices \nuse these locations and brain states to map the mind of \na consumer. A standard 10–20 system has been estab-\nlished, which is an internationally recognized method \nto apply the EEG sensors or electrodes on a human \nscalp. EEG electrodes under 10–20 system have let-\nters to express their location on skull such as prefron-\ntal (Fp), frontal (F), temporal (T), parietal (P), occipital \n(O), and central (C). Even number of electrodes are \nplaced on the right side of the head.\n\nOn the other hand, a test subject is placed inside an \nfMRI machine where the activities of the cortices can \nbe recorded from the hemodynamic response or blood \noxygen level-dependent (BOLD) imaging process. \nfMRI can look deeper within the spatial range from \nmillimeters to centimeters. This enables Neuromar-\nketing researchers using fMRI imaging to examine the \nresponse at putamen, thalamus, amygdalae and even in \nthe hippocampus.\n\nFunctional near-infrared spectroscopy (fNIRS) \nis another new brain imaging tool which uses the \n\nFrontal Asymmetry Index\n\n= ln\nAlpha Power of Right F4 or F8\n\nAlpha Power of Left F3 or F7\n.\n\n\n\nPage 9 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nhemodynamic responses associated with neuronal \nactivities [24, 60]. However, fNIRS has a lower spatial \nresolution than fMRI and cannot look deeper than 4 cm.\n\nAlongside brain regions associated with neural \nresponse, the human has a peripheral system which \ncorresponds to cognitive and emotional processes. Eye \nmovement, skin conductance, heart rate, facial expres-\nsion all are result of neural processes. Eye tracking is \nprimarily considered as the physiological response in \nconsumer neuroscience, however studies have suggested \neye tracking as a result of activation of the visual cortex \nor a secondary neural response [34, 36, 38, 53, 70].\n\nNeuromarketing experiments focused on the affect–\ncircumflex coordinate or valance–arousal coordinate use \nautonomic nervous system (ANS) response from sweat \nglands of hands or galvanic skin response (GSR), and car-\ndiovascular measure or heart rate (HR). GSR is viewed as \na sensitive and convenient measure for indexing changes \nin sympathetic arousal associated with emotion, cogni-\ntion and attention. On the other hand, HR correlates with \nthe emotional valence of a stimulus, e.g., the positive or \nnegative component of the emotion [34].\n\nConsidering the available regions to capture signals \nfrom, it is highly likely that Neuromarketing will expo-\nnentially improve its recognition and predictions in user \nresponse and preferences.\n\n3.3  Neural response recording techniques\nThe groundwork in Neuromarketing field is evidently \nindebted to the advancement of neuroimaging and neu-\nral recording tools. Neurophysiological tools, such as \nelectroencephalography (EEG), functional magnetic \nresonance imaging (fMRI), eye tracking, skin conduct-\nance, heart rate, etc., made it feasible to conduct the aca-\ndemic and commercial Neuromarketing research. Many \nresearch-grade neurophysiological and biometric signal \ncapturing devices are now available in the market. How-\never, some devices have cost and mobility advantages \nover the others and therefore replacing the expensive and \nimmobile devices for Neuromarketing purpose.\n\nAmong all neuroimaging devices, functional mag-\nnetic resonance imaging (fMRI) has been the most \nwidely used neuroimaging technique in Neuromar-\nketing research during the initial time of consumer \nneuroscience. The reason behind the wide acceptance \nof fMRI is that it offers the identification of cerebral \nregions associated with cognitive and emotional pro-\ncess. Combining magnetic field and radio waves, fMRI \nproduces a sequence of images of the cerebral activ-\nity by measuring the blood flow of the cerebral cor-\ntex [38]. The signal imaged in fMRI is called BOLD \n(blood oxygen level dependent) signal. This technol-\nogy also allows 3D views of the coordinates that denote \n\ncertain location, making possible to investigate deeper \nbrain structures [57]. The primary disadvantages of \nthis method are that it is very expensive and till now \nhas a poor temporal resolution. The computer screen \nused in fMRI refreshes the image every 2 to 5  s. This \nlow temporal resolution to the order of seconds due to \nthe time requirement of the cerebral blood flow’s incre-\nment after being exposed to the stimuli, makes fMRI \nunsuitable for tracking brain activities to the order of \nmilliseconds, which is required in video advertisement \nanalysis. Other disadvantage is the head of the subject \nmust remain static during the whole image recording \nprocess [62]. This restriction causes complex preproc-\nessing and movement-related artifact removal from the \nfMRI signals. A number of studies, i.e., Venkatraman \net  al. [38], Marques et  al. [22], Hubert et  al. [25], Hsu \nand Cheng [26], Chen et  al. [49], Casado-Aranda et  al. \n[50], Wang et al. [30], Wolfe et al. [31], Fehse et al. [33], \netc., have used fMRI as the neuroimaging technique \nin their Neuromarketing studies. fMRI in all studies \nrequired the test subjects to remain static and displayed \nthe subjects the images and commercials of products \nfor 3–5 s. Later the subjects had to make purchase deci-\nsion within 5  s after their exposure to the stimuli [50]. \nResearchers over the last 5  years  are found using 3-T \nfMRI scanner 3.0-T Siemens Magnetom Trio system \nMRI Scanner equipped with a 32-channel bridge head \ncoil (Hubert and Hsu and Cheng) [25, 62] and 3 Tesla \nSiemens Verio scanner (Wang et  al. [30]). Cost of an \nfMRI machine can be from $500,000 to $3 million vary-\ning on its spatiotemporal resolution.\n\nAlongside fMRI, electroencephalography (EEG) is \nanother popular tool used in Neuromarketing research. \nNumber of research in Neuromarketing using EEG \ndevices is increasing due to EEG’s cost efficiency high \ntemporal resolution and mobility advantages. The EEG \nmeasures electrical activity in the cerebral cortex, the \nouter layer of the brain. EEG devices are placed follow-\ning the 10–20 system. According to the 10–20 system, \nthe 10 and 20 refer to the actual percentage of distances \nbetween adjacent electrodes which are either 10% or 20% \nof the total front–back or right–left distance of the skull \n[82]. As EEG is portable and allows capturing signal from \ncerebral cortex with high temporal resolution, it is mainly \nused in TV commercial engagement or success analy-\nsis. EEG signal of interest in Neuromarketing are mainly \nevent-related potential (ERP), and late positive potential \n(LPP). ERP and LPP are used by Pozharliev et  al. [20] to \nmeasure the emotional value of luxury products. Çakar \net al. [34] used ERP to explore the experience of first-time \nuser of E-commerce product. Pilelienė and Grigaliūnaitė \n[36]) used ERP along with eye tracking signal to measure \nthe impact of celebrity spokesman in TVC. Shen et  al. \n\n\n\nPage 10 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\n[23] used ERP and LPP to explore the influence of rating \nreviews on online products.\n\nResearch-grade EEG devices are vastly used in Neuro-\nmarketing. Emotiv Epoc and Emotive Epoc+ were found \nas the mostly commonly used EEG devices in the review. \nThese devices were used in the studies of Yang et al. [45], \nChew et  al. [17], Soria Morillo et  al. [40], Yadava et  al. \n[18], Royo et al. [47], Jain et al. [63], and Singh et al. [56]. \nEmotive Epoc+ is a moveable, cost-effective EEG head-\nset having 14 electrodes those cover the frontal, tempo-\nral, parietal and occipital lobes with channels AF3, F7, \nF3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4. The \nacquired brain signals from Emotiv Epoc+ are highly \ndependable and have already been used in these scientific \nresearches. Another popular EEG device in Neuromar-\nketing, NeuroSky Mindwave, has only one sensor placed \non the prefrontal cortex of the head or the forehead. \nUnlike EEG devices with wet electrodes, Neurosky Mind-\nwave employs a biosensor which does not require any \nconductive medium to be applied on the test subject’s \nscalp [40]. With the help of NeroSkyLab, the provided \nscientific research tool, data viewing and analysis can be \nconducted easily by non-engineer population. In 2015, \nSoria Morillo et  al. and Ogino and Mitsukura in 2018 \nconducted Neuromarketing experiment with NeuroSky \ndevice and with the help of machine learning algorithm, \ntheir choice prediction accuracy was over 70% [40, 68]. A \n10-channel EEG device BrainAmp, from BrainProducts \nGmBh was used in the Neuromarketing experiment con-\nducted by Cherubino et  al. [42]. Another device EEGO \nSports from ANT Neuro (32 channels) was used to ana-\nlyze non-linear features of EEG signals by Oon et al. [55]. \nB-alert X10 headset from ABM consisting 9 electrode \nchannels is found in use by the experiment of Chew et al. \n[17]. 8-channel E-Prime from Neuroscan is another EEG \ndevice is used in the sales strategy experiment by Gong \net  al. and Touchette et  al. conducted their apparel liking \nexperiment with NeXus-10 biofeedback system. EEG \ndevices have different sampling rates starting from 128 \nto 512  Hz. This sampling rate determines the highest \nfrequency recordable by the EEG device. In general EEG \nhas a lower frequency spectrum, having Gamma band up \nto 90 Hz. This gives researchers advantage to choose the \nright EEG device from numerous manufacturers. Price \nof EEG devices depends mainly on the number of elec-\ntrode channels and performance. Cost of EEG device \nstarts from $99 and can go beyond $25,000, which gives \nresearchers buying flexibility.\n\nMagnetoencephalography (MEG) uses magnetic \npotentials to record brain activity at the scalp level, using \nmagnetic field sensitive detectors in the helmet placed \non the subject’s head. Magnetic field is not influenced \nby the type of tissue (blood, brain matter, bones), unlike \n\nelectrical field-based EEG, and can indicate the depth of \nthe location in the brain with high spatial and temporal \nresolution [3]. Similar to MEG, transcranial magnetic \nstimulation (TMS) uses varying magnetic field  [83] gen-\nerated by electromagnetic induction using an iron core. \nTMS can stimulate targeted part of the brain, which \nenables it to conduct social or behavioral experiments. \nTMS and MEG are also used frequently in Neuromar-\nketing experiments. However, the selected databases for \nthis review did not contain any Neuromarketing research \narticles using these technologies over the last 5 years.\n\nThe electromyography (EMG) measures electrical \nactivity produced by skeletal muscles when the mus-\ncles contracts and expands in order to move the body \n[70]. Also EMG is generated from the autonomic nerv-\nous activity related to emotional or mental activity. In \nNeuromarketing research, facial EMG is the best meas-\nure of the valence of the emotional reaction as it records \nfacial muscle movement from two different muscles, i.e., \nzygomaticus muscle and corrugator muscle. Zygomatic \nmuscle is found to react more while exposed to positive \nstimuli [70].\n\nBesides these brain signal recordings, eye tracking \nis the most popular method for analyzing consumer \nresponse. Eye tracking offers to measure visualization \ntime and gaze path across a screen in Neuromarket-\ning experiments. Besides tracking eye movement, pupil \ndilation measurement allows one to associate audi-\nence’s focus and arousal to the marketing stimuli. In the \nreviewed literatures, Tobii Pro X2-30 system from Tobii \nTechnology was found as the most popular eye track-\ning device. In 2019, Etzold et  al. used this eye tracking \ndevice to explore attention research on online book-\ning [48]. Tobii Pro can also cooperate with fMRI-based \nNeuromarketing experiment (Venkatraman [38]). Other \nthan Tobii, Eye Tribe is found in use by Çakar et al. [34]. \nUngureanu et al. used eye tracking to measure the atten-\ntion level of consumers while displaying static advertise-\nments of cars and clothing products [53]. Figure 1 depicts \nthe most popular methods of neural response recording \ni.e. EEG, fMRI and eye tracking used in the Neuromar-\nketing experiments.\n\nSome of the Neuromarketing studies used heart rate, as \none of the metrics to measure arousal and focus of the \nconsumer while they encounter TV commercial stimuli. \nHeart rate is the speed of the heartbeat and it is typically \nmeasured by electrocardiogram (EKG). An EKG meas-\nures the electrical activity of the heart using external skin \nelectrodes. Heart rate is controlled by two antagonistic \nnervous systems, i.e., the sympathetic nervous system \n(SNS) and the parasympathetic nervous system (PNS). \nAutomatic response to external stimuli is determined by \nthe sympathetic system of the body. Activation of this \n\n\n\nPage 11 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nsystem increases heart rate, causing fight or flight mode, \nwhich is an independent measure of arousal [38]. In con-\ntrast, the calm and relaxed state characterized by slower \nheart rate is controlled by the parasympathetic system. \nSlower heart rate in response to an advertisement implies \nthe increased focus on the ad, hence provides an inde-\npendent measure of attention [38]. Another physiologi-\ncal parameter, skin conductance (SC), or galvanic skin \nresponse (GSR), develops when the skin acts as an elec-\ntrical conductor due to the increased activity of the sweat \nglands from exposure to stimulus [38]. Skin conductance \n\namplitude and response latency provide direct measures \nof arousal when watching TV commercials, unlike self-\nreported measures that are often based on later memory \nrecall. Although GSR cannot independently relate to \nemotional valence, some of the Neuromarketing studies, \ni.e., Cherubino et  al. [42], Çakar et  al. [34], Ungureanu \net  al. [53], Magdin et  al. [71], Goyal and Singh [54], and \nSingh et  al. [56] have used skin conductance along with \nheart rate to measure the consumer attention and focus \non the TVC.\n\nFig. 1 Neural recording in Neuromarketing experiments: a multichannel EEG [43], b fMRI imaging [50], and c eye tracking for online booking \nappointment [48]\n\n\n\n\n\nPage 12 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\n3.4  Brain signal processing in Neuromarketing\nSince neural signals and images are highly vulnerable \nto noise and artifacts, before performing any analysis \nor interpretation it is imperative to preprocess the neu-\nral signals to increase the signal-to-noise ratio (SNR). \nNoises that commonly accompany the EEG signals \nare cardiac signals (ECG), power line interference, eye \nmovement artifact (EOG) and muscle movement arti-\nfacts (EMG). Preprocessing in Neuromarketing consists \nof filtering the signals to the frequency bands of inter-\nest, re-referencing the filtered signal to a common aver-\nage, detecting and interpolating bad channels, noise \nand artifact removal, and framing or segmentation for \nfurther machine learning process.\n\nEEG signals usually spread across its energy from \n0.5  Hz to around 90  Hz. For classification purpose, it \nis required to have energies only from the relevant fre-\nquency bands, hence EEG preprocessing commonly \nuses band pass filtering techniques. Band pass filter \nrequires two cutoff frequencies, one upper and one \nlower to pass the energy between them and blocks \nenergies from all other frequencies. Band pass filter \nused in these  Neuromarketing experiments  are basi-\ncally the digital version of the filter mostly applied by \nMATLAB and EEGLAB (a toolbox designated for EEG \nsignal processing in MATLAB). Re-referencing to a \ncommon average reference is also found common after \nband pass filtering in the studies of Yang et al. [41], Fan \nand Touyama [66] to reduce possible shifts from exter-\nnal artifacts. Power line interference is usually found \nremoved by using a notch filter at 60 Hz or 50 Hz.\n\nThe reviewed literatures had some common \napproaches in noise removal techniques. Since the noise \naccompanied with EEG signals are random in nature, \nsignal averaging is a common approach to reduce these \nnoises. Fan and Touyama [66] averaged the ERP signals \nfor noise removal. Chew et  al. [17] used ABM software \ndevelopment kit (SDK) in MATLAB to remove 5 types \nof artifacts, namely EMG, eye blinking artifact, excur-\nsions, saturations and spike. Excursion, saturation and \nspike artifacts in the EEG signals are replaced by zero val-\nues. Then they applied nearest neighbor interpolation to \nreplace those zero values. Another type of filter Savitzky–\nGolay is found in use by Yadava et  al. [18] for signal \nsmoothing. For noise and artifact removal, the  4th-order \nButterworth filter was used in the studies of Ogino and \nMitsukura [68] and Oon et al. [55].\n\nIndependent component analysis (ICA) is an approach \nto separate the statistical subcomponents of EEG sig-\nnals. ICA is found as the most sought after technique \nfor removing artifacts and noise from EEG signals in \nthese articles. Studies of Cherubino et  al. [42], Bhardwaj \net al. [53], Venkatraman et al. [38], Pozharliev et al. [20], \n\nBoksem and Smitds[57], Wriessnegger et al. [29], Fan and \nTouyama [66], Pilelienė and Grigaliūnaitė [36] all used \nindependent component analysis mostly for eye blink \nand eye movement artifact, and muscular movement \nnoise removal.\n\nNeuromarketing with fMRI studies have a different \nmethod for image preprocessing. Since the fMRI provides \na 3D image of the brain region with time information, it \nis basically a 4D signal. A 4D dataset is motion corrected \nfor any head movement, slice time corrected, spatially \nnormalized and finally smoothed to recover a denoised \nfMRI image. Wang et  al. [30] used statistical parametric \nmapping (SPM) software to preprocess their fMRI data. \nTheir raw fMRI signal was subjected to standard preproc-\nessing involving correction for head motion, slice timing \ncorrection, temporal and spatial denoising and normali-\nzation into standardized Montreal Neurological Institute \n(MNI) space. The mean fMRI signal from each region of \ninterest was extracted from voxels in a sphere of 6-mm \nradius centered at the activation point in the regional \nactivation map.\n\nfMRI scan was also used by Hubert et  al. [25] in their \nexperiment on hedonic vs. prudent shopper based on \nconsumer impulsiveness. Decision-making process with \ncognitive deliberation and the consideration of long-\nterm consequences are associated with processing in \nbrain areas such as the ventromedial prefrontal cortex \n(vmPFC) and the dorsolateral prefrontal cortex (dlPFC). \nHence, these vmPFC and dlPFC were the region of inter-\nests to capture the BOLD activation imaging [62]. Brain \nactivation through BOLD signals was used by Hsu and \nCheng [26] to investigate negative emotion after prod-\nuct harm crisis. fMRI region of interest in this study \nincluded amygdala, left calcarine, striatum, ventral teg-\nmental area (VTA) and right insula. The amygdala is \nassociated with memory and subjective evaluation, left \ncalcarine relates to human visual processing, the striatum \nis associated with goal-oriented evaluation, and reward \nevaluation, VTA relates to decision-making process and \nmotive functions, and the insula regions are involved in \nconsumer decision-making related to negative reinforce-\nment. Acquiring activation within these regions affirms \nthe relation between stimuli and cognitive response.\n\nSignal detection and segmentation is the process by \nwhich the signal of interest is detected from the origi-\nnal signal and then separated for further procedures. \nThe energy of the signal may be used as a threshold \nfor detection of the signal. Often the Neuromarketing \nexperiments contain multiple types of stimuli shown \nto the test subjects. In such cases segmentation sepa-\nrates the event-based time signals for further pro-\ncessing, example Bhardwaj et  al. [58]. Segmentation \nor framing the EEG signals to a shorter time window \n\n\n\nPage 13 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nis mostly required to process the signal in time–fre-\nquency domain [58]. Cherubino et  al. [42] segmented \ntheir acquired and filtered EEG traces to extract the \ncerebral activity during the exposure to the market-\ning stimuli. Oon et al. [55] used 1-s segmentation time \nto extract non-linear detrended fluctuation analysis \nfeatures.\n\nThe goal of feature extraction is to find the set of \nfeature that minimizes intra-class variability and maxi-\nmizes inter-class variability. So we need to extract use-\nful information from the preprocessed signal, which \ncan be spatial, spectral or temporal [45]. As the EEG \nsignal is non-stationary, the feature extraction pro-\ncedure is quite often complicated. Discrete wavelet \ntransformation (DWT) is a viable way to extract fea-\ntures from EEG signals.\n\nYadava et  al. [18] performed DWT-based four-level \nwavelet analysis to extract features from their EEG sig-\nnals and decomposed the EEG signal into delta, theta, \nalpha, beta and gamma frequency bands. Another \nfeature extraction approach, principal component \nanalysis (PCA) was used by Venkatraman et  al. [38] \nfor extracting fMRI features in their Neuromarketing \nexperiment. In 2016, Fan and Touyama applied spatial \nand temporal principal component analysis (STPCA) \nfor feature extraction from ERP P300 signal. Rakshit \nand Lahiri [67] used a different approach to extract \nfeatures from EEG signals. They used Welch method \nfor one-sided power spectral density estimate and then \napplied a 256-point DFT algorithm on hamming win-\ndow of length 50 to extract features. Chew et  al. [17] \nadopted Hadjidimitriou and Hadjileontiadis methods \nin feature extraction where the feature estimation is \nbased on the event-related synchronization and desyn-\nchronization theory.\n\nFeature selection is also popularly known as dimen-\nsionality reduction or subset selection. This is a well-\nknown concept in machine learning which is about \nselecting an optimal set of features that decreases \ndimensionality, but has the most contribution to the \nclassification accuracy. In the past few years, feature \nselection has caught the attention of most research-\ners because of the nature of high dimensionality of \nbio-signals and the low number of sample data. Selec-\ntion of the optimal feature subset is always relative to \nan evaluation function. In most cases it is the evalua-\ntion function that measures the classification accu-\nracy. Feature selection techniques can be divided into \nthree categories, namely: filter, wrapper and embed-\nded approach. Wang et  al. [30] used Recursive Cluster \nElimination (RCE) algorithm in spatiotemporal fMRI \nfeature selection. Soria Morillo et al. [40] used PCA for \nfeature reduction from their dataset. One-way analyses \n\nof variance (ANOVA) then cross-validation were also \nfound in use to identify the optimal feature set for cog-\nnitive or affective state classification by Yang et al. [41].\n\n3.5  Machine learning application in Neuromarketing\nUsing advanced neural recording method and signal pro-\ncessing tools, one can analyze EEG signals and interpret \ntheir correspondence with marketing stimuli. Frontal \nalpha asymmetry theory helped the researcher classify \nemotional approach/withdrawal response of the test sub-\njects using sub-band power of left and right hemispheric \nfrontal electrode [21]. However, classifying approach/\nwithdrawal or like/dislike without the FAA is possible, \neven possible from single electrode EEG signals. This \nrequires advanced Machine Learning algorithm appli-\ncation in Neuromarketing. Both supervised and unsu-\npervised learning methods were used in the following \nNeuromarketing experiments. Supervised learning in \nNeuromarketing uses a priori ground truth, usually the \ninterviewed response (like/dislike) from the test subjects \nas the labels. The labels help the classifier know the sig-\nnal pattern of like and dislike EEGs in the training data-\nsets. During the testing phase, like/dislike is predicted \nfrom a dataset without the labels. Researcher can hide \nthe training dataset labels from the classifier, and later \nuse it for accuracy calculation. On the other hand, unsu-\npervised learning approach used in Neuromarketing does \nnot require prior knowledge of the like/dislike labels. \nIt analyzes the signals with an aim to infer the existing \nstructures for different classes. Supervised learning usu-\nally solves either classification problem or a regression \nproblem. Support Vector Machines (SVM), Naive Bayes, \nArtificial Neural Networks (ANN), and Random Forests \n(RF) are the most common supervised learning classifi-\ners in Neuromarketing. In parallel, unsupervised learning \nin Neuromarketing has prominently the clustering type \nclassifiers, such as K-NN (k-nearest neighbors), principal \ncomponent analysis, singular value decomposition, and \nindependent component analysis (ICA).\n\nNeuromarketing researches over the last 5 years mainly \ndealt with like/dislike classi",
      "metadata_storage_path": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9saWJyYXJ5L3M0MDcwOC0wMjAtMDAxMDkteC5wZGY1",
      "authors": [
        "Rawnaque",
        "E V",
        "Ferdousi Sabera Rawnaque1",
        "Khandoker Mahmudur Rahman",
        "Syed Ferhat Anwar",
        "Ravi Vaidyanathan",
        "Tom Chau",
        "Farhana Sarker6",
        "Khondaker Abdullah Al Mamun",
        "Taylor Francis",
        "Yadava",
        "Rojas",
        "Pozharliev",
        "Lee",
        "Marques",
        "Shen",
        "Çakir",
        "Hubert",
        "Hsu",
        "Chen",
        "Hoefer",
        "Gurbuj",
        "Toga",
        "Wriessnegger",
        "Wang",
        "Wolfe",
        "Bosshard",
        "Fehse",
        "Price Çakar",
        "Gong",
        "Pilelienė",
        "Grigaliūnaitė",
        "Boccia",
        "Venkatraman",
        "Baldo",
        "Yang",
        "Cherubino",
        "Vasiljević",
        "Daugherty",
        "Royo",
        "Etzold",
        "Casado-Aranda",
        "Randolph",
        "Pierquet",
        "Nomura",
        "Mitsukura",
        "Ungureanu",
        "Goyal",
        "Singh",
        "Oon",
        "Soria Morillo",
        "Chew",
        "Çakar",
        "Boksem",
        "Smitds",
        "Bhardwaj",
        "katraman",
        "Touchette",
        "Gordon",
        "Krampe",
        "Holst",
        "Henseler",
        "Cheng",
        "Jain",
        "Doborjeh",
        "Kaur",
        "Fan",
        "Touyama",
        "Rakshit",
        "Lahiri",
        "Ogino",
        "Missagila",
        "Ceravolo",
        "Magdin",
        "Clerico",
        "Taqwa",
        "Nemorin",
        "Davidson",
        "Krugman",
        "Missaglia",
        "Paul D. MacLean",
        "Hypothalamus",
        "Thalamus"
      ],
      "institutions": [
        "Creative Commons",
        "iveco",
        "Institute \nof Advanced Research",
        "United International University",
        "Functional mag",
        "and Business Association",
        "NMSBA",
        "Google",
        "Microsoft",
        "Unilever",
        "Neuromar",
        "Neuromarket",
        "Science Direct",
        "Emer",
        "IEEE",
        "Wiley Online Library",
        "Taylor Francis Online",
        "Science",
        "Wiley",
        "Soria Morillo",
        "Nestle",
        "sumer goods",
        "FMCG",
        "PFC",
        "fMRI",
        "MEG",
        "BCI",
        "Gamma",
        "fNIRS",
        "GSR",
        "Siemens",
        "LPP",
        "TVC",
        "NeuroSky Mindwave",
        "Neurosky Mind",
        "NeroSkyLab",
        "NeuroSky",
        "BrainProducts",
        "EEGO",
        "ANT Neuro",
        "ABM",
        "Neuroscan"
      ],
      "key_phrases": [
        "R E V I E W",
        "Creative Commons Attribution 4.0 International License",
        "Khondaker Abdullah Al Mamun",
        "high time resolution advantages",
        "Physiological response measuring techniques",
        "consumer emotion recognition-based experiments",
        "other third party material",
        "neural record- ing techniques",
        "video advertisement-based Neuromarketing experiments",
        "other machine learning algorithms",
        "Creative Commons licence",
        "Support Vector Machine",
        "Ferdousi Sabera Rawnaque1",
        "Khandoker Mahmudur Rahman",
        "Syed Ferhat Anwar",
        "empirical research findings",
        "magnetic resonance imaging",
        "heart rate monitoring",
        "traditional filtering methods",
        "independent component analysis",
        "Linear Discriminant Analysis",
        "highest average accuracy",
        "market- ing research",
        "consumer response prediction",
        "Artificial Neural Network",
        "prefrontal alpha band",
        "skin conductance recording",
        "Brain computer interface",
        "brain–computer interface",
        "first systematic review",
        "prevalent marketing stimuli",
        "Neural recording",
        "unspoken response",
        "Neuromarket- ing",
        "consumer goods",
        "neural signal",
        "consumer preferences",
        "Brain Inf.",
        "brain recordings",
        "doi.org",
        "Ravi Vaidyanathan",
        "Tom Chau",
        "Farhana Sarker6",
        "commercial area",
        "last 5 years",
        "valid databases",
        "promotion forms",
        "asymmetry theory",
        "many researchers",
        "low cost",
        "eye tracking",
        "facial mapping",
        "artifact removal",
        "future researchers",
        "vital information",
        "novel contributions",
        "The Author",
        "appropriate credit",
        "original author",
        "credit line",
        "statutory regulation",
        "copyright holder",
        "creat iveco",
        "BCI) technology",
        "interdisciplinary bridge",
        "ultimate sale",
        "targeted audiences",
        "expanding economy",
        "effective marketing",
        "57 relevant literatures",
        "intended use",
        "permitted use",
        "good product",
        "new businesses",
        "Neuromarketing field",
        "Technological advancements",
        "opportunities",
        "Abstract",
        "academic",
        "interest",
        "interpreting",
        "tool",
        "consumers",
        "article",
        "purpose",
        "authors",
        "total",
        "basic",
        "trend",
        "nals",
        "electroencephalogram",
        "EEG",
        "functional",
        "fMRI",
        "parallel",
        "classification",
        "ANN",
        "SVM",
        "LDA",
        "Keywords",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "1 Introduction",
        "application",
        "invasive",
        "neuroscience",
        "perception",
        "changing",
        "non-invasive brain signal record- ing techniques",
        "1 Advanced Intelligent Multidisciplinary Systems Lab",
        "transcranial mag- netic stimulator",
        "Functional mag- netic resonance",
        "Open Access Brain Informatics",
        "market- ing stimuli",
        "image processing techniques",
        "nal recording techniques",
        "functional near-infrared spectroscopy",
        "com- petitive market",
        "United International University",
        "positron emission tomography",
        "machine learning algorithms",
        "skin conductance measurements",
        "neuronal activ- ity",
        "Different stimuli trigger",
        "marketing research techniques",
        "ventional market research",
        "traditional survey methods",
        "neural recording devices",
        "various marketing stimuli",
        "consumers’ purchase decisions",
        "Neuromarketing World Forum",
        "Advanced Research",
        "neuronal signals",
        "human brain",
        "traditional marketing",
        "Neuro- marketing",
        "academic research",
        "qualitative assessment",
        "posteriori analysis",
        "Full list",
        "author information",
        "group discussion",
        "personal interviews",
        "consumer feedback",
        "time requirement",
        "high cost",
        "unreliable information",
        "inaccurate results",
        "possible meanings",
        "new door",
        "buying behavior",
        "tal states",
        "physiological signals",
        "heart rate",
        "physiological responses",
        "spectral analysis",
        "accurate depiction",
        "Early years",
        "contro- versy",
        "high promises",
        "consumer mind",
        "buy buttons",
        "consumer behavior",
        "Business Association",
        "annual event",
        "Neuromarketing research",
        "field trials",
        "unspoken cognitive",
        "emotional responses",
        "Neuromarketing Science",
        "consumer response",
        "associated response",
        "quantitative",
        "products",
        "surveys",
        "Correspondence",
        "frawnaque",
        "umassd",
        "Institute",
        "Dhaka",
        "Bangladesh",
        "end",
        "creativecommons",
        "licenses",
        "crossmark",
        "crossref",
        "org",
        "Page",
        "19Rawnaque",
        "observations",
        "approaches",
        "limitations",
        "contrast",
        "customer",
        "electroencephalography",
        "magnetoencephalography",
        "MEG",
        "TMS",
        "fNIRS",
        "examples",
        "withdrawal",
        "change",
        "brainwaves",
        "researchers",
        "help",
        "excitement",
        "engagement",
        "stress",
        "insight",
        "audience",
        "preferences",
        "likes",
        "academician",
        "marketers",
        "due",
        "lack",
        "groundwork",
        "claim",
        "scrutiny",
        "scope",
        "NMSBA",
        "gap",
        "dialogue",
        "other physiological signal recording device",
        "best brain signal recording tool",
        "address inclusion–exclusion criteria",
        "relevant academic journal articles",
        "neural recording tools",
        "industry–academia collaboration",
        "brief intro- duction",
        "unbiased evidence collection",
        "clear con- clusions",
        "Numerous literature reviews",
        "other literature review",
        "mar- keting stimuli",
        "date research scopes",
        "systematic literature review",
        "new research field",
        "signal processing",
        "inclusion criteria",
        "Academic research",
        "relevant inference",
        "new perspective",
        "brain signals",
        "cal tools",
        "systematic review",
        "research question",
        "brain regions",
        "150 consumer neuroscience",
        "big brands",
        "efficient way",
        "lytical accuracy",
        "building blocks",
        "theoretical aspect",
        "business ethics",
        "current methods",
        "tech- niques",
        "art findings",
        "synthesized result",
        "impartial conclusion",
        "exhaustive search",
        "experimental findings",
        "neu- ral",
        "biometric data",
        "Book chapters",
        "marketing stimuli",
        "empirical experiments",
        "engineering part",
        "pretational methodologies",
        "important results",
        "lowing questions",
        "Neuromarketing experiments",
        "platform",
        "panies",
        "globe",
        "Google",
        "Microsoft",
        "Unilever",
        "insights",
        "tailored",
        "breakthrough",
        "acceptance",
        "world",
        "capacities",
        "management",
        "psychology",
        "focus",
        "regard",
        "premises",
        "types",
        "analysis",
        "techniques",
        "comprehensive",
        "knowledge",
        "methodology",
        "state",
        "synthesis",
        "recommendation",
        "body",
        "objective",
        "obligation",
        "investigation",
        "existing",
        "literatures",
        "evance",
        "studies",
        "analytical",
        "BCI",
        "language",
        "English",
        "brain signal recording techniques",
        "Neural response recording techniques",
        "Brain signal processing",
        "relevant existing literatures",
        "Machine learning applications",
        "autonomous nervous system",
        "Neuromarket- ing field",
        "different brain regions",
        "galvanic skin response",
        "Wiley Online Library",
        "Neuromarketing Product Chew",
        "neural data",
        "3  Systematic review",
        "database source",
        "book chapters",
        "six databases",
        "Taylor Francis",
        "initial article",
        "critical analy",
        "action coding",
        "successful means",
        "withdrawal actions",
        "engineering perspective",
        "statistical analy",
        "lyze activities",
        "Price Çakar",
        "Grigaliūnaitė",
        "Soria Morillo",
        "marketing strategies",
        "Neuromarketing questions",
        "search item",
        "Science Direct",
        "IEEE Xplore",
        "Table 1 Number",
        "database Results",
        "Emerald insight",
        "Neuromarketing studies",
        "following dimensions",
        "Marketing stimuli",
        "Table 2 Studies",
        "57 research articles",
        "931 articles",
        "internet",
        "Neuro-marketing",
        "publications",
        "aggregation",
        "Sage",
        "mulation",
        "title",
        "abstract",
        "keywords",
        "sis",
        "terms",
        "trends",
        "advancements",
        "Activation",
        "facial",
        "consumer",
        "arousal",
        "attention",
        "study",
        "Name",
        "tools",
        "paper",
        "form",
        "Yadava",
        "Rojas",
        "Pozharliev",
        "Touchette",
        "Lee",
        "Marques",
        "Shen",
        "Çakir",
        "Hubert",
        "Hsu",
        "Chen",
        "Hoefer",
        "Gurbuj",
        "Toga",
        "Wriessnegger",
        "Wang",
        "Wolfe",
        "Bosshard",
        "Fehse",
        "al.",
        "Gong",
        "Pilelienė",
        "Boccia",
        "Venkatraman",
        "Baldo",
        "Promotion",
        "Yang",
        "Cherubino",
        "Vasiljević",
        "Daugherty",
        "Royo",
        "Etzold",
        "Casado-Aranda",
        "Randolph",
        "Pierquet",
        "Nomura",
        "Mitsukura",
        "Ungureanu",
        "Goyal",
        "Singh",
        "Oon",
        "neural recording systems",
        "signal capturing technologies",
        "heart rate Cherubino",
        "EEG Soria Morillo",
        "brain region",
        "brain functions",
        "brain anatomy",
        "brain areas",
        "brain activity",
        "EMG Missagila",
        "Eye tracking",
        "Galvanic skin",
        "atic review",
        "human behavior",
        "cal functions",
        "true essence",
        "anatomical functionality",
        "high accuracy",
        "external stimuli",
        "fNIRS Çakir",
        "Neuromarketing Cherubino",
        "fMRI Venkatraman",
        "Chew",
        "Çakar",
        "Boksem",
        "Smitds",
        "Bhardwaj",
        "Gordon",
        "Krampe",
        "Holst",
        "Henseler",
        "Cheng",
        "Jain",
        "Doborjeh",
        "Kaur",
        "Fan",
        "Touyama",
        "Rakshit",
        "Lahiri",
        "Ogino",
        "Ceravolo",
        "Magdin",
        "Clerico",
        "Taqwa",
        "subjects",
        "matter",
        "interconnection",
        "actions",
        "physiological",
        "hidden",
        "ences",
        "section",
        "targeted",
        "signals",
        "associated",
        "number",
        "3D (three dimensional) image",
        "machine learning techniques",
        "tral nervous system",
        "predictive modeling framework",
        "3D visualiza- tion",
        "other marketing models",
        "consumer behavior researchers",
        "E-commerce prod- ucts",
        "gift card limit",
        "virtual 3D bracelet",
        "30 existing image",
        "3D shapes",
        "consumer demand",
        "Consumer response",
        "consumer choice",
        "different strategies",
        "market- ing",
        "quantitative assessment",
        "rological data",
        "thinking procedures",
        "last 5  years",
        "two forms",
        "physical object",
        "important role",
        "user esthetics",
        "new area",
        "mathematical model",
        "lis superformula",
        "school bags",
        "first-time buyers",
        "partici- pants",
        "commerce website",
        "negative emotion",
        "necessary buttons",
        "sorting options",
        "Retail businesses",
        "large amount",
        "large number",
        "mock shop",
        "right-handed participants",
        "test participants",
        "different designs",
        "shoe designs",
        "E-commerce products",
        "retail products",
        "test subjects",
        "normal vision",
        "brain response",
        "user experience",
        "42 product images",
        "product search",
        "wrong product",
        "possible preference",
        "60 bracelet",
        "medications",
        "history",
        "neuropathology",
        "Nemorin",
        "price",
        "promotions",
        "service",
        "beverage",
        "decision",
        "objects",
        "recognition",
        "bracelets",
        "advantage",
        "apparels",
        "accessory",
        "items",
        "shirts",
        "sweaters",
        "shoes",
        "wrist",
        "watches",
        "view",
        "maximum",
        "money",
        "thousands",
        "blueprints",
        "manufacturing",
        "failures",
        "3.1",
        "corporate social respon- sibilities",
        "frontal asymmetry theory",
        "female undergraduate students",
        "capital white letter",
        "luxury) prod- ucts",
        "high emotional value",
        "higher emotional value",
        "shoe selection time",
        "high-resolution computer screen",
        "60 basic brand items",
        "wine tasting experiment",
        "regular brand products",
        "luxury brand products",
        "brain response-based prediction",
        "consumer brain response",
        "60 luxury items",
        "gift value",
        "brand images",
        "brand names",
        "brand logos",
        "national brand",
        "social atmosphere",
        "apparel products",
        "branded products",
        "shoe experiment",
        "EEG signals",
        "Likert scale",
        "self-report-based methods",
        "sales data",
        "12.1% profit growth",
        "36.4% profit growth",
        "young adults",
        "random order",
        "alone atmosphere",
        "brain responses",
        "consumer attitude",
        "Tahoma font",
        "black background",
        "other hand",
        "implicit response",
        "different types",
        "important factor",
        "interesting concept",
        "tional companies",
        "responsible companies",
        "decision-making time",
        "passive role",
        "keting strategies",
        "lower degree",
        "con- trol",
        "pivotal role",
        "purchase decision",
        "two wines",
        "lesser price",
        "rank",
        "simulation",
        "choice",
        "Davidson",
        "comparison",
        "brands",
        "blocks",
        "popular",
        "experiments",
        "branding",
        "user",
        "first-time",
        "promotional",
        "relation",
        "author",
        "company",
        "socially",
        "influence",
        "discounts",
        "gifts",
        "gift-giving",
        "ambiguity",
        "40",
        "mon experimental design procedure",
        "merchandise product advertisement clips",
        "wine tasting session",
        "older adults’ reaction",
        "mated review systems",
        "different aged population",
        "facial biometric sensors",
        "TV commer- cials",
        "six smartphone commercials",
        "super bowl commercials",
        "promotion-based Neuromarketing experiments",
        "The Neuromarketing experiments",
        "sustainable product designs",
        "seven TV commercials",
        "unfavorable TV commercials",
        "cognitive neurophysiological indices",
        "consecutive advertise- ments",
        "experimental setting",
        "conventional product",
        "14 TV commercials",
        "100 TV commercials",
        "different brands",
        "ral indices",
        "Nestle advertisement",
        "television commercials",
        "consecutive advertisements",
        "TV advertisements",
        "equal price",
        "short movies",
        "key focus",
        "higher rate",
        "ful promotions",
        "memory rate",
        "time distance",
        "neutral stimuli",
        "white screen",
        "green scenario",
        "electrical activity",
        "advanced algorithms",
        "emotional changes",
        "cerebral activity",
        "strong images",
        "pulse analysis",
        "print media",
        "animated commercial",
        "verbal narrative",
        "ing commercial",
        "visual narrative",
        "graduate students",
        "class rank",
        "emotional states",
        "visual attentions",
        "lit- eratures",
        "standard time",
        "video advertisements",
        "neural response",
        "consumer engagement",
        "audience brain",
        "web page",
        "user preference",
        "audiences’ taste",
        "user attention",
        "favorable advertisements",
        "static advertisements",
        "50 commercials",
        "communication",
        "Neuromarketers",
        "air",
        "extract",
        "happiness",
        "surprise",
        "10  s",
        "participants",
        "observation",
        "Krugman",
        "success",
        "neuroimaging",
        "gies",
        "TVC",
        "30 s",
        "other",
        "videos",
        "incongruent product–voice combination",
        "The occipi- tal lobe",
        "cerebral cortex brain anatomy",
        "Paul D. MacLean",
        "increasing surface area",
        "social adver- tisements",
        "visual memo- ries",
        "neural imaging techniques",
        "informa- tion processing",
        "advanced mammalian brain",
        "triune brain model",
        "celebrity female spokesperson",
        "future Neuromarketing applications",
        "new sensory information",
        "social advertisement stimuli",
        "human brain structures",
        "visual processing",
        "social groups",
        "new neu",
        "Deeper structures",
        "atten- tion",
        "vertebrate brain",
        "documentary film",
        "gaming video",
        "different purposes",
        "appli- cation",
        "cigarette commercials",
        "smoking cessa",
        "substantial role",
        "advertisement industry",
        "gender-targeted marketing",
        "sumer goods",
        "profound astonishment",
        "complex web",
        "three phases",
        "reptilian complex",
        "paleomammalian complex",
        "hippocampal complex",
        "limbic system",
        "orga- nelles",
        "neomammalian complex",
        "outer layer",
        "unique feature",
        "vital role",
        "behavioral process",
        "complex system",
        "distinct functionalities",
        "motor responses",
        "auditory recognition",
        "folded appearance",
        "tion frames",
        "cerebral neocortex",
        "frontal lobe",
        "true response",
        "cognitive processes",
        "Cognitive decision-making",
        "gender-related advertisements",
        "gender-targeted advertisements",
        "four lobes",
        "occipital lobes",
        "primary center",
        "parietal lobe",
        "temporal lobe",
        "anatomical development",
        "movement-related decisions",
        "emotional process",
        "primary lobes",
        "prefrontal region",
        "gyri functions",
        "cognitive, emotional",
        "drama",
        "ads",
        "messages",
        "smokers",
        "endorsement",
        "Missaglia",
        "research",
        "FMCG",
        "diversity",
        "experience",
        "evolution",
        "hypothesis",
        "association",
        "instincts",
        "tum",
        "amygdalae",
        "hypothalamus",
        "motivation",
        "roscientists",
        "interconnectivity",
        "theory",
        "functionality",
        "interpretation",
        "thoughts",
        "conscious",
        "part",
        "taste",
        "touch",
        "memories",
        "sulci",
        "ROI",
        "single electrode EEG device",
        "Frontal Alpha Asymmetry theory",
        "movement-related activ- ity",
        "right frontal cortices",
        "Frontal Asymmetry Index",
        "signal acquisition region",
        "invasive neural recording",
        "emotional engage- ment",
        "specific frequency bands",
        "beta wave amplitudes",
        "dislike classification approach",
        "various brain disorders",
        "right frontal regions",
        "different brain states",
        "single sensor",
        "frontal cortex",
        "different parts",
        "Withdrawal Index",
        "neuronal signal",
        "Neural communication",
        "neural signature",
        "instinctual responses",
        "cerebral cortex",
        "autonomic system",
        "blood pressure",
        "body temperature",
        "sexual arousal",
        "diverse types",
        "deeper structures",
        "deeper layer",
        "TV advertisement",
        "recognition ini",
        "unconscious cognitive",
        "motivational behaviors",
        "purchase motivations",
        "action potentials",
        "BCI applications",
        "relaxed conditions",
        "ate amplitude",
        "less amplitude",
        "decision-making procedure",
        "key concept",
        "hem- isphere",
        "two hemispheres",
        "Neuromar- keting",
        "The F3/F4",
        "F7/F8 electrodes",
        "best candidates",
        "brain signal",
        "neural signals",
        "ing thalamus",
        "motor control",
        "reward processing",
        "tional response",
        "Emotional Engagement",
        "tinct information",
        "emotional information",
        "motiva- tion",
        "deep sleep",
        "sensory information",
        "Delta (δ) waves",
        "β) waves",
        "Hypothalamus",
        "master",
        "hunger",
        "thirst",
        "memory",
        "hippocampus",
        "mainframe",
        "buyer",
        "cance",
        "PFC",
        "devices",
        "ventromedial",
        "imaging",
        "firing",
        "rons",
        "electrochemical",
        "tern",
        "rhythms",
        "0.1–4  Hz",
        "theta",
        "8  Hz",
        "12  Hz",
        "30  Hz",
        "gamma",
        "functions",
        "adults",
        "wakefulness",
        "product",
        "based",
        "activation",
        "term",
        "marker",
        "avoidance",
        "power",
        "left",
        "α",
        "valance–arousal coordinate use autonomic nervous system",
        "oxygen level-dependent (BOLD) imaging process",
        "The alpha frequency band",
        "new brain imaging tool",
        "functional magnetic resonance imaging",
        "Frontal Asymmetry Index value",
        "frontal alpha asymmetry theory",
        "3.3  Neural response recording techniques",
        "neural signal recording devices",
        "positive asymmetry score",
        "ral recording tools",
        "secondary neural response",
        "commercial Neuromarketing research",
        "circumflex coordinate",
        "sympathetic arousal",
        "fMRI imaging",
        "neural processes",
        "biometric signal",
        "International 10–20 System",
        "alpha activity",
        "standard 10–20 system",
        "Alpha Power",
        "peripheral system",
        "brain states",
        "skin conductance",
        "Neurophysiological tools",
        "capturing devices",
        "immobile devices",
        "approach response",
        "hemodynamic response",
        "physiological response",
        "power reception",
        "sitive places",
        "pleasant feeling",
        "right side",
        "spatial range",
        "keting researchers",
        "infrared spectroscopy",
        "Right F4",
        "lower spatial",
        "emotional processes",
        "Eye movement",
        "facial expres",
        "visual cortex",
        "sweat glands",
        "diovascular measure",
        "convenient measure",
        "cogni- tion",
        "emotional valence",
        "negative component",
        "available regions",
        "research-grade neurophysiological",
        "mobility advantages",
        "Neuromarketing purpose",
        "EEG sensors",
        "cognitive engagement",
        "consumer neuroscience",
        "fMRI machine",
        "EEG electrodes",
        "relaxation",
        "meditation",
        "equation",
        "high",
        "Neuroimaging",
        "locations",
        "mind",
        "method",
        "human",
        "scalp",
        "ters",
        "skull",
        "Fp",
        "parietal",
        "occipital",
        "head",
        "activities",
        "cortices",
        "blood",
        "putamen",
        "thalamus",
        "ln",
        "F8",
        "Left",
        "neuronal",
        "resolution",
        "4 cm",
        "sion",
        "result",
        "ANS",
        "hands",
        "GSR",
        "HR",
        "sensitive",
        "stimulus",
        "predictions",
        "advancement",
        "demic",
        "cost",
        "others",
        "expensive",
        "functional mag- netic resonance imaging",
        "blood oxygen level dependent",
        "32-channel bridge head coil",
        "movement-related artifact removal",
        "total front–back",
        "right–left distance",
        "TV commercial engagement",
        "late positive potential",
        "cerebral activ- ity",
        "poor temporal resolution",
        "low temporal resolution",
        "Siemens Verio scanner",
        "image recording process",
        "cerebral blood flow",
        "eye tracking signal",
        "high temporal resolution",
        "Research-grade EEG devices",
        "spatiotemporal resolution",
        "event-related potential",
        "cerebral regions",
        "MRI Scanner",
        "neuroimaging devices",
        "neuroimaging technique",
        "initial time",
        "wide acceptance",
        "magnetic field",
        "radio waves",
        "3D views",
        "primary disadvantages",
        "computer screen",
        "video advertisement",
        "Other disadvantage",
        "purchase deci",
        "popular tool",
        "actual percentage",
        "adjacent electrodes",
        "emotional value",
        "first-time user",
        "E-commerce product",
        "celebrity spokesman",
        "TVC. Shen",
        "rating reviews",
        "Emotiv Epoc",
        "Emotive Epoc",
        "brain structures",
        "brain activities",
        "keting research",
        "luxury products",
        "online products",
        "cost efficiency",
        "EEG signal",
        "The EEG",
        "fMRI signals",
        "reason",
        "identification",
        "cognitive",
        "sequence",
        "ogy",
        "coordinates",
        "location",
        "5  s",
        "order",
        "seconds",
        "incre",
        "stimuli",
        "restriction",
        "essing",
        "commercials",
        "3–5 s",
        "exposure",
        "Researchers",
        "3-T",
        "3 Tesla",
        "10–20 system",
        "distances",
        "ERP",
        "LPP",
        "impact",
        "A 10-channel EEG device",
        "machine learning algorithm",
        "choice prediction accuracy",
        "lyze non-linear features",
        "B-alert X10 headset",
        "NeXus-10 biofeedback system",
        "scientific research tool",
        "different sampling rates",
        "sales strategy experiment",
        "apparel liking experiment",
        "Neurosky Mind- wave",
        "ANT Neuro (32 channels",
        "lower frequency spectrum",
        "popular EEG device",
        "right EEG device",
        "scientific researches",
        "Neuromarketing experiment",
        "EEG devices",
        "Emotive Epoc+",
        "NeuroSky Mindwave",
        "one sensor",
        "prefrontal cortex",
        "conductive medium",
        "test subject",
        "data viewing",
        "engineer population",
        "9 electrode channels",
        "8-channel E-Prime",
        "Gamma band",
        "numerous manufacturers",
        "wet electrodes",
        "14 electrodes",
        "ral",
        "AF",
        "FC",
        "The",
        "forehead",
        "biosensor",
        "NeroSkyLab",
        "BrainAmp",
        "BrainProducts",
        "GmBh",
        "EEGO",
        "Sports",
        "ABM",
        "use",
        "Neuroscan",
        "512  Hz",
        "90 Hz",
        "Price"
      ],
      "merged_content": "\nRawnaque et al. Brain Inf.            (2020) 7:10  \nhttps://doi.org/10.1186/s40708-020-00109-x\n\nR E V I E W\n\nTechnological advancements \nand opportunities in Neuromarketing: \na systematic review\nFerdousi Sabera Rawnaque1*, Khandoker Mahmudur Rahman2, Syed Ferhat Anwar3, Ravi Vaidyanathan4, \nTom Chau5, Farhana Sarker6 and Khondaker Abdullah Al Mamun1,7\n\nAbstract \nNeuromarketing has become an academic and commercial area of interest, as the advancements in neural record-\ning techniques and interpreting algorithms have made it an effective tool for recognizing the unspoken response \nof consumers to the marketing stimuli. This article presents the very first systematic review of the technological \nadvancements in Neuromarketing field over the last 5 years. For this purpose, authors have selected and reviewed a \ntotal of 57 relevant literatures from valid databases which directly contribute to the Neuromarketing field with basic \nor empirical research findings. This review finds consumer goods as the prevalent marketing stimuli used in both \nproduct and promotion forms in these selected literatures. A trend of analyzing frontal and prefrontal alpha band sig-\nnals is observed among the consumer emotion recognition-based experiments, which corresponds to frontal alpha \nasymmetry theory. The use of electroencephalogram (EEG) is found favorable by many researchers over functional \nmagnetic resonance imaging (fMRI) in video advertisement-based Neuromarketing experiments, apparently due to \nits low cost and high time resolution advantages. Physiological response measuring techniques such as eye tracking, \nskin conductance recording, heart rate monitoring, and facial mapping have also been found in these empirical stud-\nies exclusively or in parallel with brain recordings. Alongside traditional filtering methods, independent component \nanalysis (ICA) was found most commonly in artifact removal from neural signal. In consumer response prediction and \nclassification, Artificial Neural Network (ANN), Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) \nhave performed with the highest average accuracy among other machine learning algorithms used in these litera-\ntures. The authors hope, this review will assist the future researchers with vital information in the field of Neuromarket-\ning for making novel contributions.\n\nKeywords: Neuromarketing, Neural recording, Machine learning algorithm, Brain computer interface, Marketing\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\n1 Introduction\nNeuromarketing, an application of the non-invasive \nbrain–computer interface (BCI) technology, has emerged \nas an interdisciplinary bridge between neuroscience and \nmarketing that has changed the perception of market-\ning research. Marketing is the channel between prod-\nuct and consumers which determines the ultimate sale. \n\nWithout effective marketing, a good product fails to \ninform, engage and sustain its targeted audiences [1]. \nThe expanding economy with new businesses is continu-\nously evolving with changing consumer preferences. It \nis hard for the businesses to grow and sustain without \nhaving quantitative or qualitative assessment from their \nconsumers. Newly launched products need even more \neffective marketing to successfully enter into a com-\npetitive market. However, traditional marketing renders \nonly by posteriori analysis of consumer response. Con-\nventional market research depends on surveys, focus \n\nOpen Access\n\nBrain Informatics\n\n*Correspondence:  frawnaque@umassd.edu\n1 Advanced Intelligent Multidisciplinary Systems Lab, Institute \nof Advanced Research, United International University, Dhaka, Bangladesh\nFull list of author information is available at the end of the article\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40708-020-00109-x&domain=pdf\n\n\nPage 2 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ngroup discussion, personal interviews, field trials and \nobservations for collecting consumer feedback [2]. These \napproaches have the limitations of time requirement, \nhigh cost and unreliable information, which can often \nproduce inaccurate results. In contrast to the traditional \nmarketing research techniques, Neuromarketing allows \ncapturing consumers’ unspoken cognitive and emotional \nresponse to various marketing stimuli and can forecast \nconsumers’ purchase decisions.\n\nNeuromarketing uses non-invasive brain signal record-\ning techniques to directly measure the response of a \ncustomer’s brain to the marketing stimuli, supersed-\ning the traditional survey methods [3]. Functional mag-\nnetic resonance (fMRI), electroencephalography (EEG), \nmagnetoencephalography (MEG), transcranial mag-\nnetic stimulator (TMS), positron emission tomography \n(PET), functional near-infrared spectroscopy (fNIRS) etc. \nare some examples of neural recording devices used in \nNeuromarketing research. By obtaining neuronal activ-\nity from the brain using these devices, one can explore \nthe cognitive and emotional responses (i.e., like/dislike, \napproach/withdrawal) of a customer. Different stimuli \ntrigger associated response in a human brain and the \nresponse can be tracked by monitoring the change in \nneuronal signals or brainwaves [4]. Further, the signal \nand image processing techniques and machine learning \nalgorithms have enabled the researchers to measure, ana-\nlyze and interpret the possible meanings of brainwaves. \nThis opens a new door to detect, analyze and predict \nthe buying behavior of customers in marketing research. \nNow with the help of brain–computer interface, the men-\ntal states of a customer, i.e., excitement, engagement, \nwithdrawal, stress, etc., while experiencing a market-\ning stimuli can be captured [5]. Besides these brain sig-\nnal recording techniques, Neuromarketing also utilizes \nphysiological signals, i.e., eye tracking, heart rate and \nskin conductance measurements to gather the insight of \naudience’s physiological responses due to encountering \nstimuli. These neurophysiological signals with advanced \nspectral analysis and machine learning algorithms can \nnow provide nearly accurate depiction of consumers’ \npreferences and likes/dislikes [6–8].\n\nEarly years of Neuromarketing generated a contro-\nversy between the academician and the marketers due \nto its high promises and lack of groundwork. From \nthe claim of peeping into the consumer mind to find-\ning the buy buttons of human brain, Neuromarketing \nhas long been under the scrutiny of the academicians \nand researchers [9, 10]. However, academic research in \nthis field has started to pile up and the scope of Neuro-\nmarketing to reveal and predict consumer behavior is \ngradually becoming evident. Neuromarketing Science \nand Business Association (NMSBA) was established \n\nin 2012 to bridge the gap between academicians and \nNeuromarketers, and it is promoting Neuromarket-\ning research across the world with its annual event of \nNeuromarketing World Forum [11, 12]. It may be pro-\nposed that further dialogue may continue under such a \nplatform for further industry–academia collaboration. \nEvidently, more than 150 consumer neuroscience com-\npanies are commercially operating across the globe and \nbig brands (Google, Microsoft, Unilever, etc.) are using \ntheir insights to impact their consumers in a tailored and \nefficient way. Academic research, especially the high ana-\nlytical accuracy from the engineering part of Neuromar-\nketing has garnered this breakthrough and acceptance \nover the world. Hence, reviewing the building blocks of \nNeuromarketing is essential to evaluate its scopes and \ncapacities, and to contribute new perspective in this \nfield. Numerous literature reviews have been published \nfocusing the theoretical aspect of consumer neurosci-\nence, such as marketing, business ethics, management, \npsychology, consumer behavior, etc. [13–15]. However, \nsystematic literature review from the engineering per-\nspective with a focus on neural recording tools and inter-\npretational methodologies used in this field is absent. In \nthis regard, our article sets its premises to answer the fol-\nlowing questions:\n\n– What are the types of marketing stimuli currently \nbeing used in Neuromarketing?\n\n– What are the brain regions activated by these mar-\nketing stimuli?\n\n– What is the best brain signal recording tool currently \nbeing used in Neuromarketing research?\n\n– How are these brain signals preprocessed for further \nanalysis?\n\n– And what are the current methods or techniques \nused to interpret these brain signals?\n\nThese questions will allow us to gain a comprehensive \nknowledge on the up-to-date research scopes and tech-\nniques in consumer neuroscience. After this brief intro-\nduction, our methodology of conducting this systematic \nreview will be presented, followed by the state-of-the-art \nfindings corresponding to the aforementioned questions \nand synthesis of the important results. We concluded this \nreview with relevant inference from synthesized result \nand a recommendation for future researchers.\n\n2  Methodology\nThe systematic literature review is a process in which \na body of literature is collected, screened, selected, \nreviewed and assessed with a pre-specified objective for \nthe purpose of unbiased evidence collection and to reach \nan impartial conclusion [16]. Systematic review has the \n\n\n\nPage 3 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nobligation to explicitly define its research question and to \naddress inclusion–exclusion criteria for setting the scope \nof the investigation. After exhaustive search of existing \nliteratures, articles should be selected based on their rel-\nevance, and the results of the selected studies must be \nsynthesized and assessed critically to achieve clear con-\nclusions [16].\n\nIn this systematic review, we would like to explore \nthe marketing stimuli used in Neuromarketing research \narticles over the last 5 years with their triggered brain \nregions. We would also like to focus on the technologi-\ncal tools used to capture brain signals from these regions, \nand finally deliberate on signal processing and analytical \nmethodologies used in these experiments.\n\nTherefore, the inclusion criteria defined here are  as \nfollows:\n\n– Literatures must be published in the field of Neuro-\nmarketing from 2015 to 2019.\n\n– Studies must use brain–computer interface and/or \nother physiological signal recording device in their \nNeuromarketing experiments.\n\n– Studies must have experimental findings from neu-\nral and/or biometric data used in Neuromarketing \nresearch.\n\nThe exclusion criteria for this review are set as:\n\n– Any other literature review on Neuromarketing are \nexcluded from this review.\n\n– Book chapters are excluded from this review. Since \nNeuromarketing is comparatively a new research \nfield, alongside relevant academic journal articles, \nbook chapters conducting empirical experiments \nusing BCI can only be included.\n\n– Literatures written/published in any language other \nthan English are excluded from this article.\n\nTo serve the purpose of this systematic literature \nreview, a total of 931 articles were found across the \n\ninternet by using the search item “Neuromarketing” \nand “Neuro-marketing” in valid databases. Among the \nscreened publications, Table  1 presents the database \nsource of selected 57 research articles including book \nchapters, which directly contribute to the Neuromarket-\ning field with basic or empirical research findings.\n\nAs for the aggregation of relevant existing literatures, \nthe researchers defined that the search for articles would \nbe performed in six databases—Science Direct, Emer-\nald Insight, Sage, IEEE Xplore, Wiley Online Library, \nand Taylor Francis Online. After the initial article accu-\nmulation, the articles were exhaustively screened by \nthe authors by reviewing their title, abstract, keywords \nand scope to match the objective of this research. Once \nthe studies met our aforementioned inclusion criteria, \nthey were selected for further review and critical analy-\nsis. Table  2 classifies the selected articles in terms of the \naforementioned dimensions.\n\nBy exploring the articles selected to develop this sys-\ntematic review, it was possible to successfully categorize \nthe trends and advancements in Neuromarketing field in \nfollowing dimensions:\n\n i. Marketing stimuli used in Neuromarketing \nresearch\n\n ii. Activation of the brain regions due to marketing \nstimuli\n\n iii. Neural response recording techniques\n iv. Brain signal processing in Neuromarketing\n v. Machine learning applications in Neuromarketing.\n\nSome of these Neuromarketing studies have used \neye tracking, heart rate, galvanic skin response, facial \naction coding, etc., with or without brain signal \nrecording techniques to gauge the consumer’s hidden \nresponse. As they are the response from autonomous \nnervous system (ANS), they have proven themselves \nas successful means of exploring consumer’s focus, \narousal, attention and withdrawal actions. Hence, this \nstudy includes articles those empirically used these \n\nTable 1 Number of articles found and selected\n\nName of the database Results: search “Neuromarketing” Results: search “Neuro-marketing” Articles selected\n\nScience direct 281 55 12\n\nWiley online 111 11 7\n\nEmerald insight 115 8 14\n\nIEEE 34 0 14\n\nSage 12 15 6\n\nTaylor Francis online 106 36 4\n\nTotal found: 806 Total found: 125 Total selected: 57\n\n\n\nPage 4 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\ntools to answer Neuromarketing questions, since this \nstudy mainly focuses on the engineering perspective. \nInterpreting the neural data with only statistical analy-\nsis has been out of scope of this paper.\n\n3  Systematic review on the advancements \nof Neuromarketing\n\nNeuromarketing research utilizes marketing strategies in \nthe form of stimuli, and aims to invoke, capture and ana-\nlyze activities occurring in different brain regions while \n\nTable 2 Studies selected on the dimensions of this review\n\nDimensions Published articles\n\ni. Marketing stimuli used in Neuromarketing Product Chew et al. [17], Yadava et al. [18], Rojas et al. [19], Pozharliev [20], Touchette \nand Lee [21], Marques et al. [22], Shen et al. [23], Çakir et al. [24], Hubert \net al. [25], Hsu and Chen et al. [26], Hoefer et al. [27], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Wolfe et al. [31], Bosshard et al. [32], \nFehse et al. [33].\n\nPrice Çakar et al. [34], Marques et al. [22], Çakir et al. [24], Gong et al. [35], Pilelienė \nand Grigaliūnaitė [36], Hsu and Chen [26], Boccia et al. [37], Venkatraman \net al. [38], Baldo et al. [39].\n\nPromotion Soria Morillo et al. [40], Yang et al. [41], Cherubino et al. [42], Soria Morillo \net al. [43], Vasiljević et al. [44], Yang et al. [45], Pilelienė and Grigaliūnaitė \n[36], Daugherty et al. [46], Royo et al. [47], Etzold et al. [48], Chen et al. \n[49], Casado-Aranda et al. [50], Randolph and Pierquet [51], Nomura and \nMitsukura [52], Ungureanu et al. [53], Goyal and Singh [54], Oon et al. [55], \nSingh et al. [56].\n\nii. Activation of brain region due to marketing stimuli Soria Morillo et al. [40], Chew et al. [17], Cherubino et al. [42], Soria Morillo \net al. [43], Çakar et al. [34], Boksem and Smitds [57], Bhardwaj et al. [58], Ven-\nkatraman et al. [38], Touchette and Lee [21], Yang et al. [45], Marques et al. \n[22], Gong et al. [35], Gordon et al. [59], Krampe et al. [60], Hubert et al. [25], \nÇakir et al. [24], Holst and Henseler [61], Hsu and Cheng [62], Hoefer et al. \n[27], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Jain et al. \n[63], Wolfe et al. [31], Bosshard et al. [32], Fehse et al. [33].\n\niii. Neural response recording techniques EEG Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Cherubino et al. [42], \nSoria Morillo et al. [43], Yadava et al. [18], Doborjeh et al. [64], Çakar et al. \n[34], Kaur et al. [65], Baldo et al. [19], Boksem and Smitds [57], Pozharliev \net al. [20], Venkatraman [38], Touchette and Lee [21], Yang et al. [45], Pilelienė \nand Grigaliūnaitė [36], Shen et al. [23], Daugherty et al. [46], Royo et al. [47], \nGong et al. [35], Gordon et al. [59], Hsu and Chen et al. [26], Hoefer et al. [27], \nRandolph and Pierquet [51], Nomura and Mitsukura [52], Bhardwaj et al. \n[58], Fan and Touyama [66], Rakshit and Lahiri [67], Jain et al. [63],Ogino and \nMitsukura [68], Oon et al. [55], Bosshard et al. [32].\n\nfMRI Venkatraman et al. [38], Marques et al. [22], Hubert et al. [25], Hsu and Cheng \n[62], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Wolfe et al. \n[31], Fehse et al. [33].\n\nfNIRS Çakir et al. [24], Krampe et al. [60].\n\nEMG Missagila et al. [69]\n\nEye tracking Venkatraman [38], Rojas et al. [19], Pilelienė and Grigaliūnaitė [36], Çakar et al. \n[34], Ceravolo et al. [70], Ungureanu et al. [53]\n\nGalvanic skin \nresponse, \nheart rate\n\nCherubino et al. [42], Çakar et al. [34], Magdin et al. [71], Goyal and Singh [54], \nSingh et al. [56].\n\niv. Brain signal processing in Neuromarketing Cherubino et al. [42], Bhardwaj et al. [53], Venkatraman [38], Pozharliev et al. \n[20], Boksem and Smitds [57], Wriessnegger et al. [29], Fan and Touyama \n[66], Pilelienė and Grigaliūnaitė [36], Yadava et al. [18], Baldo et al. [19], \nClerico et al. [72], Chen et al. [49], Casado-Aranda et al. [50], Hsu and Cheng \n[62], Taqwa et al. [73], Bhardwaj et al. [58],Wang et al. [30], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Oon et al. [55], Fehse et al. [33],\n\nv. Machine learning applications in Neuromarketing Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Soria Morillo et al. [43], \nYadava et al. [18], Doborjeh et al. [64], Gordon [59], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Taqwa et al. [73], Bhardwaj et al. \n[58], Randolph and Pierquet [51], Fan and Touyama [66], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Ogino and Mitsukura [68], Oon \net al. [55], Singh et al. [56].\n\n\n\nPage 5 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nsubjects experience these stimuli. To conduct a system-\natic review on this matter, it is important to recall the \ninterconnection between brain functions with human \nbehavior and actions triggered by the  external stimuli. \nThe knowledge of brain anatomy and the physiologi-\ncal functions of brain areas as well as the physiological \nresponse due to external stimuli along with it, makes \nit possible to model brain activity and predict hidden \nresponse. For this purpose, current neural imaging sys-\ntems and neural recording systems have contributed \nmuch to capture the true essence of consumer prefer-\nences. This section will discuss the marketing stimuli, \ntheir targeted brain regions, neural and physiological \nsignal capturing technologies used over the last 5 years \nin Neuromarketing research. Comparing these signals \nwith their associated anatomical functionality some stud-\nies have already reached high accuracy. A number of the \nselected studies have used machine learning techniques \nto predict like/dislike and possible preference from the \ntest subjects.\n\nFor the purpose of Neuromarketing experiments, the \nfollowing literatures selected right-handed participants, \nwith normal or corrected-to-normal vision, free of cen-\ntral nervous system influencing medications and with no \nhistory of neuropathology.\n\n3.1  Marketing stimuli used in Neuromarketing\nAs Neuromarketing is a focus of marketers and consumer \nbehavior researchers, different strategies from market-\ning have been applied in Neuromarketing and they are \nbeing investigated for quantitative assessment from neu-\nrological data. Nemorin et  al. asserts that Neuromarket-\ning differentiates from any other marketing models as \nit bypasses the thinking procedures of consumers and \ndirectly enters their brain [74]. Over the last 5  years, \nNeuromarketing stimuli has been mainly in two forms—\nproducts with/without price, and promotions. Product \ncan be defined as physical object or service that meets \nthe consumer demand. In Neuromarketing, product can \nbe physical such as tasting a beverage to conceptual like \na 3D (three dimensional) image of the product. Price in \nNeuromarketing experiments is mostly seen as a stimuli \nis most of the time intermingled with product or pro-\nmotion. However, it plays an important role that deter-\nmines the decision of test subjects to buy or not to buy \nthe product [75].\n\nConsumer response to a product has been recognized \nby either physically experiencing the product or by visu-\nalizing the image of  it. To understand the user esthetics \nof 3D shapes, Chew et  al. [17], used virtual 3D bracelet \nshapes in motion and recorded the brain response of \ntest subjects with EEG with motion. As 3D visualiza-\ntion of objects for preference recognition is a new area \n\nof research, the authors used mathematical model (Gie-\nlis superformula) to create 3D bracelet-like objects. \nTheir study displayed 3D shapes appear like bracelets as \nthe product to subjects. Using the 3D shapes gave the \nauthors an advantage to produce as many of 60 bracelet \nshapes to conduct the research on. Another new prod-\nuct was the E-commerce products presented to the test \nsubjects by Yadava et al. and Çakar et al. [18, 34]. Yadava \net  al. proposed a predictive modeling framework to \nunderstand consumer choice towards E-commerce prod-\nucts in terms of “likes” and “dislikes” by analyzing EEG \nsignals. In showing E-commerce product, they showed a \ntotal of 42 product images to the test participants. These \nproduct images were mainly of apparels and accessory \nitems such as shirts, sweaters, shoes, school bags, wrist \nwatches, etc. The test participants were asked to disclose \ntheir preference in terms of likes and dislikes after view-\ning the items  [18]. Çakar et  al. used both product and \nprice to explore the experience during product search of \nfirst-time buyers in E-commerce. To motivate the partici-\npants, this research provided each participants around \n73 USD as a gift card to use during the experiment. The \ntest participants were asked to search and select three \nproducts of their interest from an e-commerce website \nand reach the maximum of their gift card limit to acti-\nvate. Test subjects often experienced negative emotion \nwhile being unable to find necessary buttons such as “add \nto cart” or “sorting options” [34]. These Neuromarketing \nexperiments on E-commerce products may help develop-\ners to build better user experience. Retail businesses lose \nlarge amount of money when they invest in the wrong \nproduct. Among retail products, shoes have thousands \nof blueprints for manufacturing. Producing thousands \nof shoes of different designs to satisfy consumers can be \nlaborious and unprofitable since a large number of the \ndesigns turn out to be failures. Baldo et  al. directly used \n30 existing image of shoe designs to show the test sub-\njects to and to choose from a mock shop showing on the \nscreen [39]. EEG signals were recorded during the whole \nshoe selection time and then subjects were asked to rate \nthe shoes in a rank of 1 to 5 of Likert scale. This experi-\nment helped realize brain response-based prediction can \nsupersede self-report-based methods, as the simulation \non sales data showed 12.1% profit growth for survey-\nbased prediction, and 36.4% profit growth for the brain \nresponse-based prediction.\n\nSimilar to the shoe experiment, Touchette and Lee [21] \nexperimented on the choice of apparel products among \nyoung adults, based on Davidson’s frontal asymmetry \ntheory. EEG signals were recorded while 34 college stu-\ndents viewed three attractive and three unattractive \napparel products on a high-resolution computer screen \nin a random order. Pozharliev et  al. [20] experimented \n\n\n\nPage 6 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\non the emotion associated with visualizing luxury brand \nproducts vs. regular brand products. The experiment dis-\nplayed 60 luxury items and 60 basic brand items to 40 \nfemale undergraduate students to recognize the brain \nresponse of seeing high emotional value (luxury) prod-\nucts in social vs. alone atmosphere. The study found \nthat, luxury brand products invoked a higher emotional \nvalue in social atmosphere which could be utilized by the \nmarketers. Bosshard et  al. and Fehse et  al. experimented \non brand images and the comparison between the brain \nresponses associated with preferred and not preferred \nbrands [32, 33]. In the study performed by Bosshard et al., \nconsumer attitude towards established brand names were \nmeasured via electroencephalography. Subjects were \nshown 120 brand names in capital white letter in Tahoma \nfont on black background and without any logo while \ntheir brain responses were recorded. On the other hand, \nFehse et al. compared the brain response of test subjects \nwhile they visualized blocks of popular vs. organic food \nbrand logos. These experiments on brand image may help \nmarketers to recognize the implicit response of consum-\ners on different types of branding.\n\nAs price is mentioned as an important factor that \ndetermines the user’s interest on purchasing a product, \na number of Neuromarketing studies have used price \nalongside the products. In the aforementioned study \nby Çakar et  al. [34] price was displayed while recording \nbrain response during first-time e-commerce user expe-\nrience. Marques et  al. [22], Çakir et  al. [24], Gong et  al. \n[35], Pilelienė and Grigaliūnaitė [36], Hsu and Chen [26], \nBoccia et al. [37], Venkatraman et al. [38], and Baldo et al. \n[39] have included price as a marketing stimuli with the \nproduct or promotional.\n\nAn interesting concept was tried by Boccia et  al. to \nrecognize the relation between corporate social respon-\nsibilities and consumer behavior. The author attempted \nto identify if consumers were willing to pay more for the \nproducts from socially or environmentally responsible \ncompany. Consumers were found to prefer the conven-\ntional companies over the socially responsible companies \ndue to lesser price. Marques et  al. [22] investigated the \ninfluence of price to compare national brand vs. own-\nlabeled branded products. In the experiment of Çakir \net  al, product then product and price were shown to \nthe subjects before decision-making time and the brain \nresponses were recorded through fNIRS [24]. Sometimes \nprice can play a passive role in the form of discounts or \ngifts in a promotional. Gong et  al. innovatively designed \nan experiment to compare consumer brain response \nassociated with promotional using discount (25% off ) vs. \ngift-giving (gift value equivalent to the discount) mar-\nketing strategies. Their study found that lower degree of \nambiguity (e.g., discounts) better motivates consumer \n\ndecision-making [35]. Hsu and Chen used price as a con-\ntrol variable in their wine tasting experiment. As price \nplays a pivotal role in purchase decision, two wines were \nselected of approximately equal price $15. Then the EEG \nsignals of test subjects were recorded during the wine \ntasting session [26].\n\nPromotion is the communication from the marketers’ \nend to influence the purchase decision of consumers [75]. \nIn Neuromarketing research, promotion is usually found \nas the TV commercials and short movies for advertise-\nment. One of the key focus of Neuromarketers is to \nevaluate the consumer engagement of advertisements. \nPredicting the engagement of advertisements before \nbroadcasting them on air, ensures higher rate of success-\nful promotions.\n\nIn 2015, Yang et  al. used six smartphone commercials \nof different brands to compare among them in terms \nof extract cognitive neurophysiological indices such as \nhappiness, surprise, and attention as well as behavio-\nral indices (memory rate, preference, etc.) [41]. A com-\nmon experimental design procedure is found among the \npromotion-based Neuromarketing experiments, that is \nsubjects are first made comfortable in the experimental \nsetting, consecutive advertisements were placed at a time \ndistance no shorter than 10  s and consecutive advertise-\nments used neutral stimuli such as white screen, green \nscenario, blank in between them to stabilize the test \nparticipants.\n\nThe Neuromarketing experiments of Soria Morillo \net  al. [40, 43] tried to find out the electrical activity of \naudience brain while viewing advertisement relevant to \naudiences’ taste. They display used 14 TV commercials \ndisplayed to their 10 test subjects for their experiment \nand predicted like or dislike response from audience \nwith the help of advanced algorithms. Cherubino et  al. \n[42] investigated cognitive and emotional changes of \ncerebral activity during the observation of TV commer-\ncials among different aged population. Among seven TV \ncommercials displayed during the experiment, one com-\nmercial with strong images was analyzed for the adults’ \nand older adults’ reaction. Other than them, Vasiljević \net  al. [44] used Nestle advertisement to measure con-\nsumer attention though pulse analysis; Daugherty et  al. \n[46] replicated an experiment of Krugman (1971) using \nboth TV advertisements and print media advertise-\nments to recognize how consumers look and think; Royo \net  al. [47] focused on consumer response while viewing \nadvertisements of sustainable product designs. For their \nexperiment, an animated commercial was made contain-\ning verbal narrative of sustainable product and an exist-\ning commercial was used to convey the visual narrative \nof conventional product. Venkatraman  et al. focused \non measuring the success of TV advertisements using \n\n\n\nPage 7 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nneuroimaging and biometric data  [38]. Randolph and \nPierquet [51] showed super bowl commercials to under-\ngraduate students to compare the class rank of the com-\nmercials and the neural response from the test subjects. \nNomura and Mitsukura [52] identified emotional states \nof audiences while watching favorable vs. unfavorable TV \ncommercials. They selected 100 TV commercials among \nwhich 50 commercials were award winning which were \nlabeled as favorable advertisements. Singh et al. [56] used \npromotion in the form of static vs. video advertisements \nto predict the success of omnichannel marketing strate-\ngies. Ungureanu et  al. [53] measured user attention and \narousal by eye tracking while surfing through web page \ncontaining static advertisements, while Goyal and Singh \n[54] utilized facial biometric sensors to model an auto-\nmated review systems for video advertisements. Oon \net al. [55] used merchandise product advertisement clips \nto recognize user preference. Singh et al. [56] used video \nadvertisements to measure visual attentions of audiences.\n\nMost of the TVC (television commercials) in these lit-\neratures had a standard time of 30 s. In Neuromarketing, \nthese TVCs were displayed in between other videos such \nas documentary film, gaming video, drama, etc., to cap-\nture the true response of consumers.\n\nSometimes Neuromarketing  is observed dealing with \nadvertisement of different purposes, such as social adver-\ntisements or gender-related advertisements. The appli-\ncation of Neuromarketing in social advertisement is to \npredict the success of these ads to reach its messages to \nthe targeted social groups [45, 49, 69]. Chen et  al. [49] \nexperimented on the neural response of adolescent audi-\nences while they are exposed to e-cigarette commercials. \nAnother social advertisement stimuli of smoking cessa-\ntion frames was used by Yang [45], to understand what \ntypes of frames (positive/negative) achieve better atten-\ntion from smokers and non-smokers. Gender plays a \nsubstantial role in advertisement industry from celebrity \nendorsement to gender-targeted marketing. Missaglia \net  al. [69] conducted a research on fast marketed con-\nsumer goods (FMCG) advertisements with celebrity vs. \nnon-celebrity female spokesperson. Casado-Aranda et al. \n[50] worked on gender-targeted advertisements using \ncongruent vs. incongruent product–voice combination. \nThese studies show us the diversity of marketing stimuli \nfor future Neuromarketing applications.\n\n3.2  Activation of brain regions due to marketing stimuli\nHuman brain is a matter of profound astonishment. \nThe anatomical development of our brain resulted in \nthe complex web of cognitive and emotional process we \nexperience every day. The evolution of vertebrate brain \nwas initially proposed by Paul D. MacLean  in his Tri-\nune Brain model  [76]. In his hypothesis, evolution of \n\nvertebrate brain is formed through three phases. First \nthe reptilian complex, which indicates the association \nof instincts with the anatomical structure basal gan-\nglia. The paleomammalian complex consists of sep-\ntum, amygdalae, hypothalamus, hippocampal complex, \nand cingulate cortex as the limbic system. These orga-\nnelles were associated with motivation and emotional \nresponse of mammalian brain. Finally, neomammalian \ncomplex consists of cerebral neocortex or the outer \nlayer of advanced mammalian brain, which is particu-\nlarly a unique feature of human brain. In the cerebral \nneocortex, we find four lobes which control our sen-\nsory, motor, emotional and cognitive processes [76]. \nThe triune brain model has been rejected by new neu-\nroscientists due to the interconnectivity of human brain \nstructures and their function. However, the anatomical \nstructure of human brain explained by this theory plays \na vital role in recognizing cognitive, emotional and \nbehavioral process.\n\nUnderstanding the anatomy of human brain has \nshowed itself indispensable in Neuromarketing \nresearch, as its functionality is deeply associated with \nthe interpretation of neural response. The outer layer of \nthe human brain is a complex system organized in four \nlobes, namely (frontal, parietal, temporal and occipital \nlobes), each having distinct functionalities for cogni-\ntive, emotional, and motor responses. The frontal lobe \nis the region where most of our thoughts and conscious \ndecisions are made  [77]. Cognitive decision-making \nmainly takes part in the prefrontal region of this lobe, \nand movement-related decisions are made in the end \npart of frontal lobe. Information about taste, touch and \nmovement is processed by the parietal lobe. The occipi-\ntal lobe is the primary center for visual processing, \nand the temporal lobe is responsible for visual memo-\nries, auditory recognition and integrating new sensory \ninformation with memories  [78]. Besides the primary \nlobes, cerebral cortex brain anatomy has gyri and sulci \nwhich create the folded appearance of the brain. The \ngyri functions on increasing surface area for informa-\ntion processing. Alongside the primary lobes, gyri of \nthese lobes can be considered as the region of interest \n(ROI) in neural imaging techniques [79].\n\nDeeper structures of the human brain consist-\ning thalamus, amygdalae, etc., produces sensory and \ninstinctual responses which are later transported to \nthe cerebral cortex. Hypothalamus works as the master \ncontrol of our autonomic system. Sleep, hunger, thirst, \nblood pressure, body temperature, sexual arousal are \ncontrolled and regulated by hypothalamus. Thalamus \non the other hand regulates sensory information, atten-\ntion and memory. Amygdalae originate our emotional \n\n\n\nPage 8 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\nresponse and hippocampus is the mainframe of our \nmemory [77].\n\nRetrieving information from brain requires diverse \ntypes of methodology. In Neuromarketing experiments, \ndifferent parts of brain are selected for retrieving dis-\ntinct information. An experiment which solely focuses \non attention might only look at the signals from frontal \nlobe, whereas experiments focusing on buyer’s motiva-\ntion might want to look at deeper structures [38].\n\nAccording to Soria Morillo et  al., brain signal acquisi-\ntion may capture neural signals either from cerebral cor-\ntex or from the deeper layer of the brain [40, 43]. Their \nexperiment on TV advertisement liking recognition ini-\ntially uses information only from prefrontal cortex using \na single electrode EEG device. Their experiment showed, \nit is possible to classify like/dislike with information col-\nlected solely from frontal lobe.\n\nSimilarly, Cherubino et  al. emphasized on the signifi-\ncance of frontal cortex (FC) and prefrontal cortex (PFC) \nin Neuromarketing studies. PFC processes the conscious \nand unconscious cognitive and emotional information. \nHence, devices using only a single sensor select PFC as \ntheir signal acquisition region  [42]. Also, ventromedial \nprefrontal cortex corresponds to motivational behaviors, \nimaging of which by fMRI or MEG can reveal purchase \nmotivations [22].\n\nNeural communication in the brain is conducted \nthrough the action potentials, or the firing of neu-\nrons  [80]. A neuronal signal is the electrochemical \ninformation that neurons send to each other. These \ninformation are acquired as signals of non-linear pat-\ntern called the brainwaves  [80]. These brainwaves are \nfurther associated with the neural signature of brain \nstates. The neural signature is divided into frequency \nbands known as rhythms, such as the delta (0.1–4  Hz), \ntheta (4–8  Hz), alpha (8–12  Hz), beta (12–30  Hz), and \ngamma (30–90  Hz). These frequency bands are related \nto different brain states, regions, functions or patholo-\ngies. Delta (δ) waves are characteristic of deep sleep and \nhave not been explored for BCI applications  [81]. Theta \n(θ) waves are enhanced during sleep in adults and often \nrelated to various brain disorders. During wakefulness \nunder relaxed conditions alpha (α) waves with moder-\nate amplitude appear spontaneously. Beta (β) waves have \nless amplitude and are strongly related to motor control \nand engagement or decision-making procedure. Gamma \n(γ) waves are associated with movement-related activ-\nity of the brain and intensely observed in invasive neural \nrecording [81].\n\nIn Neuromarketing, beta wave amplitudes are associ-\nated with reward processing which can further predict \nsuccess of a product or TVC (Boksem and Smitds) [57].\n\nFrontal alpha asymmetry is a key concept of hem-\nisphere-based like–dislike classification approach. \nWhen the brain is considered as two hemispheres, left \nand right frontal cortices show hemispheric asymme-\ntry in their activation during processing positive and \nnegative emotion. Another term for emotional engage-\nment, Approach–Withdrawal Index refers to the emo-\ntional response from Frontal Alpha Asymmetry theory \n[34]. Frontal Asymmetry Index is a marker of approach \nand avoidance. “Emotional Engagement” in Neuromar-\nketing is expressed as the power of specific frequency \nbands from left and right frontal regions. The F3/F4 and \nF7/F8 electrodes are the best candidates for these EEG \npower reception as they are positioned at the most sen-\nsitive places (International 10–20 System). The alpha \nfrequency band (8–12  Hz) is commonly used in the \nfrontal alpha asymmetry theory. However, as the alpha \nactivity corresponds with relaxation and meditation, it \nis negatively correlated with cognitive engagement.\n\nFrontal Asymmetry Index is measured from the \nequation:\n\nHigher the Frontal Asymmetry Index value, the more \napproach response is obtained from the test subjects \nand vice versa. This high or positive asymmetry score \ncan determine pleasant feeling of a test subject and vice \nversa, which was explored in the study conducted by \nTouchette and Lee [21].\n\nNeuroimaging and neural signal recording devices \nuse these locations and brain states to map the mind of \na consumer. A standard 10–20 system has been estab-\nlished, which is an internationally recognized method \nto apply the EEG sensors or electrodes on a human \nscalp. EEG electrodes under 10–20 system have let-\nters to express their location on skull such as prefron-\ntal (Fp), frontal (F), temporal (T), parietal (P), occipital \n(O), and central (C). Even number of electrodes are \nplaced on the right side of the head.\n\nOn the other hand, a test subject is placed inside an \nfMRI machine where the activities of the cortices can \nbe recorded from the hemodynamic response or blood \noxygen level-dependent (BOLD) imaging process. \nfMRI can look deeper within the spatial range from \nmillimeters to centimeters. This enables Neuromar-\nketing researchers using fMRI imaging to examine the \nresponse at putamen, thalamus, amygdalae and even in \nthe hippocampus.\n\nFunctional near-infrared spectroscopy (fNIRS) \nis another new brain imaging tool which uses the \n\nFrontal Asymmetry Index\n\n= ln\nAlpha Power of Right F4 or F8\n\nAlpha Power of Left F3 or F7\n.\n\n\n\nPage 9 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nhemodynamic responses associated with neuronal \nactivities [24, 60]. However, fNIRS has a lower spatial \nresolution than fMRI and cannot look deeper than 4 cm.\n\nAlongside brain regions associated with neural \nresponse, the human has a peripheral system which \ncorresponds to cognitive and emotional processes. Eye \nmovement, skin conductance, heart rate, facial expres-\nsion all are result of neural processes. Eye tracking is \nprimarily considered as the physiological response in \nconsumer neuroscience, however studies have suggested \neye tracking as a result of activation of the visual cortex \nor a secondary neural response [34, 36, 38, 53, 70].\n\nNeuromarketing experiments focused on the affect–\ncircumflex coordinate or valance–arousal coordinate use \nautonomic nervous system (ANS) response from sweat \nglands of hands or galvanic skin response (GSR), and car-\ndiovascular measure or heart rate (HR). GSR is viewed as \na sensitive and convenient measure for indexing changes \nin sympathetic arousal associated with emotion, cogni-\ntion and attention. On the other hand, HR correlates with \nthe emotional valence of a stimulus, e.g., the positive or \nnegative component of the emotion [34].\n\nConsidering the available regions to capture signals \nfrom, it is highly likely that Neuromarketing will expo-\nnentially improve its recognition and predictions in user \nresponse and preferences.\n\n3.3  Neural response recording techniques\nThe groundwork in Neuromarketing field is evidently \nindebted to the advancement of neuroimaging and neu-\nral recording tools. Neurophysiological tools, such as \nelectroencephalography (EEG), functional magnetic \nresonance imaging (fMRI), eye tracking, skin conduct-\nance, heart rate, etc., made it feasible to conduct the aca-\ndemic and commercial Neuromarketing research. Many \nresearch-grade neurophysiological and biometric signal \ncapturing devices are now available in the market. How-\never, some devices have cost and mobility advantages \nover the others and therefore replacing the expensive and \nimmobile devices for Neuromarketing purpose.\n\nAmong all neuroimaging devices, functional mag-\nnetic resonance imaging (fMRI) has been the most \nwidely used neuroimaging technique in Neuromar-\nketing research during the initial time of consumer \nneuroscience. The reason behind the wide acceptance \nof fMRI is that it offers the identification of cerebral \nregions associated with cognitive and emotional pro-\ncess. Combining magnetic field and radio waves, fMRI \nproduces a sequence of images of the cerebral activ-\nity by measuring the blood flow of the cerebral cor-\ntex [38]. The signal imaged in fMRI is called BOLD \n(blood oxygen level dependent) signal. This technol-\nogy also allows 3D views of the coordinates that denote \n\ncertain location, making possible to investigate deeper \nbrain structures [57]. The primary disadvantages of \nthis method are that it is very expensive and till now \nhas a poor temporal resolution. The computer screen \nused in fMRI refreshes the image every 2 to 5  s. This \nlow temporal resolution to the order of seconds due to \nthe time requirement of the cerebral blood flow’s incre-\nment after being exposed to the stimuli, makes fMRI \nunsuitable for tracking brain activities to the order of \nmilliseconds, which is required in video advertisement \nanalysis. Other disadvantage is the head of the subject \nmust remain static during the whole image recording \nprocess [62]. This restriction causes complex preproc-\nessing and movement-related artifact removal from the \nfMRI signals. A number of studies, i.e., Venkatraman \net  al. [38], Marques et  al. [22], Hubert et  al. [25], Hsu \nand Cheng [26], Chen et  al. [49], Casado-Aranda et  al. \n[50], Wang et al. [30], Wolfe et al. [31], Fehse et al. [33], \netc., have used fMRI as the neuroimaging technique \nin their Neuromarketing studies. fMRI in all studies \nrequired the test subjects to remain static and displayed \nthe subjects the images and commercials of products \nfor 3–5 s. Later the subjects had to make purchase deci-\nsion within 5  s after their exposure to the stimuli [50]. \nResearchers over the last 5  years  are found using 3-T \nfMRI scanner 3.0-T Siemens Magnetom Trio system \nMRI Scanner equipped with a 32-channel bridge head \ncoil (Hubert and Hsu and Cheng) [25, 62] and 3 Tesla \nSiemens Verio scanner (Wang et  al. [30]). Cost of an \nfMRI machine can be from $500,000 to $3 million vary-\ning on its spatiotemporal resolution.\n\nAlongside fMRI, electroencephalography (EEG) is \nanother popular tool used in Neuromarketing research. \nNumber of research in Neuromarketing using EEG \ndevices is increasing due to EEG’s cost efficiency high \ntemporal resolution and mobility advantages. The EEG \nmeasures electrical activity in the cerebral cortex, the \nouter layer of the brain. EEG devices are placed follow-\ning the 10–20 system. According to the 10–20 system, \nthe 10 and 20 refer to the actual percentage of distances \nbetween adjacent electrodes which are either 10% or 20% \nof the total front–back or right–left distance of the skull \n[82]. As EEG is portable and allows capturing signal from \ncerebral cortex with high temporal resolution, it is mainly \nused in TV commercial engagement or success analy-\nsis. EEG signal of interest in Neuromarketing are mainly \nevent-related potential (ERP), and late positive potential \n(LPP). ERP and LPP are used by Pozharliev et  al. [20] to \nmeasure the emotional value of luxury products. Çakar \net al. [34] used ERP to explore the experience of first-time \nuser of E-commerce product. Pilelienė and Grigaliūnaitė \n[36]) used ERP along with eye tracking signal to measure \nthe impact of celebrity spokesman in TVC. Shen et  al. \n\n\n\nPage 10 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\n[23] used ERP and LPP to explore the influence of rating \nreviews on online products.\n\nResearch-grade EEG devices are vastly used in Neuro-\nmarketing. Emotiv Epoc and Emotive Epoc+ were found \nas the mostly commonly used EEG devices in the review. \nThese devices were used in the studies of Yang et al. [45], \nChew et  al. [17], Soria Morillo et  al. [40], Yadava et  al. \n[18], Royo et al. [47], Jain et al. [63], and Singh et al. [56]. \nEmotive Epoc+ is a moveable, cost-effective EEG head-\nset having 14 electrodes those cover the frontal, tempo-\nral, parietal and occipital lobes with channels AF3, F7, \nF3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4. The \nacquired brain signals from Emotiv Epoc+ are highly \ndependable and have already been used in these scientific \nresearches. Another popular EEG device in Neuromar-\nketing, NeuroSky Mindwave, has only one sensor placed \non the prefrontal cortex of the head or the forehead. \nUnlike EEG devices with wet electrodes, Neurosky Mind-\nwave employs a biosensor which does not require any \nconductive medium to be applied on the test subject’s \nscalp [40]. With the help of NeroSkyLab, the provided \nscientific research tool, data viewing and analysis can be \nconducted easily by non-engineer population. In 2015, \nSoria Morillo et  al. and Ogino and Mitsukura in 2018 \nconducted Neuromarketing experiment with NeuroSky \ndevice and with the help of machine learning algorithm, \ntheir choice prediction accuracy was over 70% [40, 68]. A \n10-channel EEG device BrainAmp, from BrainProducts \nGmBh was used in the Neuromarketing experiment con-\nducted by Cherubino et  al. [42]. Another device EEGO \nSports from ANT Neuro (32 channels) was used to ana-\nlyze non-linear features of EEG signals by Oon et al. [55]. \nB-alert X10 headset from ABM consisting 9 electrode \nchannels is found in use by the experiment of Chew et al. \n[17]. 8-channel E-Prime from Neuroscan is another EEG \ndevice is used in the sales strategy experiment by Gong \net  al. and Touchette et  al. conducted their apparel liking \nexperiment with NeXus-10 biofeedback system. EEG \ndevices have different sampling rates starting from 128 \nto 512  Hz. This sampling rate determines the highest \nfrequency recordable by the EEG device. In general EEG \nhas a lower frequency spectrum, having Gamma band up \nto 90 Hz. This gives researchers advantage to choose the \nright EEG device from numerous manufacturers. Price \nof EEG devices depends mainly on the number of elec-\ntrode channels and performance. Cost of EEG device \nstarts from $99 and can go beyond $25,000, which gives \nresearchers buying flexibility.\n\nMagnetoencephalography (MEG) uses magnetic \npotentials to record brain activity at the scalp level, using \nmagnetic field sensitive detectors in the helmet placed \non the subject’s head. Magnetic field is not influenced \nby the type of tissue (blood, brain matter, bones), unlike \n\nelectrical field-based EEG, and can indicate the depth of \nthe location in the brain with high spatial and temporal \nresolution [3]. Similar to MEG, transcranial magnetic \nstimulation (TMS) uses varying magnetic field  [83] gen-\nerated by electromagnetic induction using an iron core. \nTMS can stimulate targeted part of the brain, which \nenables it to conduct social or behavioral experiments. \nTMS and MEG are also used frequently in Neuromar-\nketing experiments. However, the selected databases for \nthis review did not contain any Neuromarketing research \narticles using these technologies over the last 5 years.\n\nThe electromyography (EMG) measures electrical \nactivity produced by skeletal muscles when the mus-\ncles contracts and expands in order to move the body \n[70]. Also EMG is generated from the autonomic nerv-\nous activity related to emotional or mental activity. In \nNeuromarketing research, facial EMG is the best meas-\nure of the valence of the emotional reaction as it records \nfacial muscle movement from two different muscles, i.e., \nzygomaticus muscle and corrugator muscle. Zygomatic \nmuscle is found to react more while exposed to positive \nstimuli [70].\n\nBesides these brain signal recordings, eye tracking \nis the most popular method for analyzing consumer \nresponse. Eye tracking offers to measure visualization \ntime and gaze path across a screen in Neuromarket-\ning experiments. Besides tracking eye movement, pupil \ndilation measurement allows one to associate audi-\nence’s focus and arousal to the marketing stimuli. In the \nreviewed literatures, Tobii Pro X2-30 system from Tobii \nTechnology was found as the most popular eye track-\ning device. In 2019, Etzold et  al. used this eye tracking \ndevice to explore attention research on online book-\ning [48]. Tobii Pro can also cooperate with fMRI-based \nNeuromarketing experiment (Venkatraman [38]). Other \nthan Tobii, Eye Tribe is found in use by Çakar et al. [34]. \nUngureanu et al. used eye tracking to measure the atten-\ntion level of consumers while displaying static advertise-\nments of cars and clothing products [53]. Figure 1 depicts \nthe most popular methods of neural response recording \ni.e. EEG, fMRI and eye tracking used in the Neuromar-\nketing experiments.\n\nSome of the Neuromarketing studies used heart rate, as \none of the metrics to measure arousal and focus of the \nconsumer while they encounter TV commercial stimuli. \nHeart rate is the speed of the heartbeat and it is typically \nmeasured by electrocardiogram (EKG). An EKG meas-\nures the electrical activity of the heart using external skin \nelectrodes. Heart rate is controlled by two antagonistic \nnervous systems, i.e., the sympathetic nervous system \n(SNS) and the parasympathetic nervous system (PNS). \nAutomatic response to external stimuli is determined by \nthe sympathetic system of the body. Activation of this \n\n\n\nPage 11 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nsystem increases heart rate, causing fight or flight mode, \nwhich is an independent measure of arousal [38]. In con-\ntrast, the calm and relaxed state characterized by slower \nheart rate is controlled by the parasympathetic system. \nSlower heart rate in response to an advertisement implies \nthe increased focus on the ad, hence provides an inde-\npendent measure of attention [38]. Another physiologi-\ncal parameter, skin conductance (SC), or galvanic skin \nresponse (GSR), develops when the skin acts as an elec-\ntrical conductor due to the increased activity of the sweat \nglands from exposure to stimulus [38]. Skin conductance \n\namplitude and response latency provide direct measures \nof arousal when watching TV commercials, unlike self-\nreported measures that are often based on later memory \nrecall. Although GSR cannot independently relate to \nemotional valence, some of the Neuromarketing studies, \ni.e., Cherubino et  al. [42], Çakar et  al. [34], Ungureanu \net  al. [53], Magdin et  al. [71], Goyal and Singh [54], and \nSingh et  al. [56] have used skin conductance along with \nheart rate to measure the consumer attention and focus \non the TVC.\n\nFig. 1 Neural recording in Neuromarketing experiments: a multichannel EEG [43], b fMRI imaging [50], and c eye tracking for online booking \nappointment [48]\n\n a 6.0 10.0 14.0 22.0 36.0 0.0 HZ O O 20 10 O 10 -20 Power 10*log, (/V/Hz) -30 40 50 -60 5 10 15 20 25 30 35 40 45 50 Frequency (Hz) a (a) Serviontermin-Buchung 8 (9) 3 Intensity of the view: low high b C \n\n\n\nPage 12 of 19Rawnaque et al. Brain Inf.            (2020) 7:10 \n\n3.4  Brain signal processing in Neuromarketing\nSince neural signals and images are highly vulnerable \nto noise and artifacts, before performing any analysis \nor interpretation it is imperative to preprocess the neu-\nral signals to increase the signal-to-noise ratio (SNR). \nNoises that commonly accompany the EEG signals \nare cardiac signals (ECG), power line interference, eye \nmovement artifact (EOG) and muscle movement arti-\nfacts (EMG). Preprocessing in Neuromarketing consists \nof filtering the signals to the frequency bands of inter-\nest, re-referencing the filtered signal to a common aver-\nage, detecting and interpolating bad channels, noise \nand artifact removal, and framing or segmentation for \nfurther machine learning process.\n\nEEG signals usually spread across its energy from \n0.5  Hz to around 90  Hz. For classification purpose, it \nis required to have energies only from the relevant fre-\nquency bands, hence EEG preprocessing commonly \nuses band pass filtering techniques. Band pass filter \nrequires two cutoff frequencies, one upper and one \nlower to pass the energy between them and blocks \nenergies from all other frequencies. Band pass filter \nused in these  Neuromarketing experiments  are basi-\ncally the digital version of the filter mostly applied by \nMATLAB and EEGLAB (a toolbox designated for EEG \nsignal processing in MATLAB). Re-referencing to a \ncommon average reference is also found common after \nband pass filtering in the studies of Yang et al. [41], Fan \nand Touyama [66] to reduce possible shifts from exter-\nnal artifacts. Power line interference is usually found \nremoved by using a notch filter at 60 Hz or 50 Hz.\n\nThe reviewed literatures had some common \napproaches in noise removal techniques. Since the noise \naccompanied with EEG signals are random in nature, \nsignal averaging is a common approach to reduce these \nnoises. Fan and Touyama [66] averaged the ERP signals \nfor noise removal. Chew et  al. [17] used ABM software \ndevelopment kit (SDK) in MATLAB to remove 5 types \nof artifacts, namely EMG, eye blinking artifact, excur-\nsions, saturations and spike. Excursion, saturation and \nspike artifacts in the EEG signals are replaced by zero val-\nues. Then they applied nearest neighbor interpolation to \nreplace those zero values. Another type of filter Savitzky–\nGolay is found in use by Yadava et  al. [18] for signal \nsmoothing. For noise and artifact removal, the  4th-order \nButterworth filter was used in the studies of Ogino and \nMitsukura [68] and Oon et al. [55].\n\nIndependent component analysis (ICA) is an approach \nto separate the statistical subcomponents of EEG sig-\nnals. ICA is found as the most sought after technique \nfor removing artifacts and noise from EEG signals in \nthese articles. Studies of Cherubino et  al. [42], Bhardwaj \net al. [53], Venkatraman et al. [38], Pozharliev et al. [20], \n\nBoksem and Smitds[57], Wriessnegger et al. [29], Fan and \nTouyama [66], Pilelienė and Grigaliūnaitė [36] all used \nindependent component analysis mostly for eye blink \nand eye movement artifact, and muscular movement \nnoise removal.\n\nNeuromarketing with fMRI studies have a different \nmethod for image preprocessing. Since the fMRI provides \na 3D image of the brain region with time information, it \nis basically a 4D signal. A 4D dataset is motion corrected \nfor any head movement, slice time corrected, spatially \nnormalized and finally smoothed to recover a denoised \nfMRI image. Wang et  al. [30] used statistical parametric \nmapping (SPM) software to preprocess their fMRI data. \nTheir raw fMRI signal was subjected to standard preproc-\nessing involving correction for head motion, slice timing \ncorrection, temporal and spatial denoising and normali-\nzation into standardized Montreal Neurological Institute \n(MNI) space. The mean fMRI signal from each region of \ninterest was extracted from voxels in a sphere of 6-mm \nradius centered at the activation point in the regional \nactivation map.\n\nfMRI scan was also used by Hubert et  al. [25] in their \nexperiment on hedonic vs. prudent shopper based on \nconsumer impulsiveness. Decision-making process with \ncognitive deliberation and the consideration of long-\nterm consequences are associated with processing in \nbrain areas such as the ventromedial prefrontal cortex \n(vmPFC) and the dorsolateral prefrontal cortex (dlPFC). \nHence, these vmPFC and dlPFC were the region of inter-\nests to capture the BOLD activation imaging [62]. Brain \nactivation through BOLD signals was used by Hsu and \nCheng [26] to investigate negative emotion after prod-\nuct harm crisis. fMRI region of interest in this study \nincluded amygdala, left calcarine, striatum, ventral teg-\nmental area (VTA) and right insula. The amygdala is \nassociated with memory and subjective evaluation, left \ncalcarine relates to human visual processing, the striatum \nis associated with goal-oriented evaluation, and reward \nevaluation, VTA relates to decision-making process and \nmotive functions, and the insula regions are involved in \nconsumer decision-making related to negative reinforce-\nment. Acquiring activation within these regions affirms \nthe relation between stimuli and cognitive response.\n\nSignal detection and segmentation is the process by \nwhich the signal of interest is detected from the origi-\nnal signal and then separated for further procedures. \nThe energy of the signal may be used as a threshold \nfor detection of the signal. Often the Neuromarketing \nexperiments contain multiple types of stimuli shown \nto the test subjects. In such cases segmentation sepa-\nrates the event-based time signals for further pro-\ncessing, example Bhardwaj et  al. [58]. Segmentation \nor framing the EEG signals to a shorter time window \n\n\n\nPage 13 of 19Rawnaque et al. Brain Inf.            (2020) 7:10  \n\nis mostly required to process the signal in time–fre-\nquency domain [58]. Cherubino et  al. [42] segmented \ntheir acquired and filtered EEG traces to extract the \ncerebral activity during the exposure to the market-\ning stimuli. Oon et al. [55] used 1-s segmentation time \nto extract non-linear detrended fluctuation analysis \nfeatures.\n\nThe goal of feature extraction is to find the set of \nfeature that minimizes intra-class variability and maxi-\nmizes inter-class variability. So we need to extract use-\nful information from the preprocessed signal, which \ncan be spatial, spectral or temporal [45]. As the EEG \nsignal is non-stationary, the feature extraction pro-\ncedure is quite often complicated. Discrete wavelet \ntransformation (DWT) is a viable way to extract fea-\ntures from EEG signals.\n\nYadava et  al. [18] performed DWT-based four-level \nwavelet analysis to extract features from their EEG sig-\nnals and decomposed the EEG signal into delta, theta, \nalpha, beta and gamma frequency bands. Another \nfeature extraction approach, principal component \nanalysis (PCA) was used by Venkatraman et  al. [38] \nfor extracting fMRI features in their Neuromarketing \nexperiment. In 2016, Fan and Touyama applied spatial \nand temporal principal component analysis (STPCA) \nfor feature extraction from ERP P300 signal. Rakshit \nand Lahiri [67] used a different approach to extract \nfeatures from EEG signals. They used Welch method \nfor one-sided power spectral density estimate and then \napplied a 256-point DFT algorithm on hamming win-\ndow of length 50 to extract features. Chew et  al. [17] \nadopted Hadjidimitriou and Hadjileontiadis methods \nin feature extraction where the feature estimation is \nbased on the event-related synchronization and desyn-\nchronization theory.\n\nFeature selection is also popularly known as dimen-\nsionality reduction or subset selection. This is a well-\nknown concept in machine learning which is about \nselecting an optimal set of features that decreases \ndimensionality, but has the most contribution to the \nclassification accuracy. In the past few years, feature \nselection has caught the attention of most research-\ners because of the nature of high dimensionality of \nbio-signals and the low number of sample data. Selec-\ntion of the optimal feature subset is always relative to \nan evaluation function. In most cases it is the evalua-\ntion function that measures the classification accu-\nracy. Feature selection techniques can be divided into \nthree categories, namely: filter, wrapper and embed-\nded approach. Wang et  al. [30] used Recursive Cluster \nElimination (RCE) algorithm in spatiotemporal fMRI \nfeature selection. Soria Morillo et al. [40] used PCA for \nfeature reduction from their dataset. One-way analyses \n\nof variance (ANOVA) then cross-validation were also \nfound in use to identify the optimal feature set for cog-\nnitive or affective state classification by Yang et al. [41].\n\n3.5  Machine learning application in Neuromarketing\nUsing advanced neural recording method and signal pro-\ncessing tools, one can analyze EEG signals and interpret \ntheir correspondence with marketing stimuli. Frontal \nalpha asymmetry theory helped the researcher classify \nemotional approach/withdrawal response of the test sub-\njects using sub-band power of left and right hemispheric \nfrontal electrode [21]. However, classifying approach/\nwithdrawal or like/dislike without the FAA is possible, \neven possible from single electrode EEG signals. This \nrequires advanced Machine Learning algorithm appli-\ncation in Neuromarketing. Both supervised and unsu-\npervised learning methods were used in the following \nNeuromarketing experiments. Supervised learning in \nNeuromarketing uses a priori ground truth, usually the \ninterviewed response (like/dislike) from the test subjects \nas the labels. The labels help the classifier know the sig-\nnal pattern of like and dislike EEGs in the training data-\nsets. During the testing phase, like/dislike is predicted \nfrom a dataset without the labels. Researcher can hide \nthe training dataset labels from the classifier, and later \nuse it for accuracy calculation. On the other hand, unsu-\npervised learning approach used in Neuromarketing does \nnot require prior knowledge of the like/dislike labels. \nIt analyzes the signals with an aim to infer the existing \nstructures for different classes. Supervised learning usu-\nally solves either classification problem or a regression \nproblem. Support Vector Machines (SVM), Naive Bayes, \nArtificial Neural Networks (ANN), and Random Forests \n(RF) are the most common supervised learning classifi-\ners in Neuromarketing. In parallel, unsupervised learning \nin Neuromarketing has prominently the clustering type \nclassifiers, such as K-NN (k-nearest neighbors), principal \ncomponent analysis, singular value decomposition, and \nindependent component analysis (ICA).\n\nNeuromarketing researches over the last 5 years mainly \ndealt with like/dislike classi",
      "text": [
        "a 6.0 10.0 14.0 22.0 36.0 0.0 HZ O O 20 10 O 10 -20 Power 10*log, (/V/Hz) -30 40 50 -60 5 10 15 20 25 30 35 40 45 50 Frequency (Hz) a (a) Serviontermin-Buchung 8 (9) 3 Intensity of the view: low high b C",
        "Published online: 21 September 2020"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"a 6.0 10.0 14.0 22.0 36.0 0.0 HZ O O 20 10 O 10 -20 Power 10*log, (/V/Hz) -30 40 50 -60 5 10 15 20 25 30 35 40 45 50 Frequency (Hz) a (a) Serviontermin-Buchung 8 (9) 3 Intensity of the view: low high b C\",\"lines\":[{\"boundingBox\":[{\"x\":24,\"y\":8},{\"x\":60,\"y\":6},{\"x\":59,\"y\":37},{\"x\":24,\"y\":38}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":200,\"y\":4},{\"x\":230,\"y\":4},{\"x\":231,\"y\":28},{\"x\":201,\"y\":28}],\"text\":\"6.0\"},{\"boundingBox\":[{\"x\":502,\"y\":4},{\"x\":532,\"y\":4},{\"x\":532,\"y\":22},{\"x\":501,\"y\":22}],\"text\":\"10.0\"},{\"boundingBox\":[{\"x\":803,\"y\":5},{\"x\":837,\"y\":3},{\"x\":837,\"y\":27},{\"x\":803,\"y\":29}],\"text\":\"14.0\"},{\"boundingBox\":[{\"x\":1096,\"y\":2},{\"x\":1131,\"y\":5},{\"x\":1130,\"y\":24},{\"x\":1095,\"y\":22}],\"text\":\"22.0\"},{\"boundingBox\":[{\"x\":1397,\"y\":2},{\"x\":1432,\"y\":4},{\"x\":1431,\"y\":24},{\"x\":1396,\"y\":22}],\"text\":\"36.0\"},{\"boundingBox\":[{\"x\":1689,\"y\":0},{\"x\":1754,\"y\":1},{\"x\":1753,\"y\":26},{\"x\":1688,\"y\":24}],\"text\":\"0.0 HZ\"},{\"boundingBox\":[{\"x\":202,\"y\":131},{\"x\":225,\"y\":126},{\"x\":226,\"y\":141},{\"x\":204,\"y\":146}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1755,\"y\":204},{\"x\":1749,\"y\":183},{\"x\":1764,\"y\":181},{\"x\":1771,\"y\":202}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":61,\"y\":349},{\"x\":92,\"y\":347},{\"x\":91,\"y\":371},{\"x\":61,\"y\":373}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":63,\"y\":410},{\"x\":91,\"y\":410},{\"x\":91,\"y\":433},{\"x\":63,\"y\":433}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":74,\"y\":496},{\"x\":71,\"y\":468},{\"x\":88,\"y\":469},{\"x\":91,\"y\":497}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":65,\"y\":536},{\"x\":91,\"y\":539},{\"x\":89,\"y\":562},{\"x\":63,\"y\":559}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":61,\"y\":595},{\"x\":95,\"y\":601},{\"x\":90,\"y\":624},{\"x\":56,\"y\":618}],\"text\":\"-20\"},{\"boundingBox\":[{\"x\":7,\"y\":775},{\"x\":7,\"y\":498},{\"x\":39,\"y\":498},{\"x\":37,\"y\":776}],\"text\":\"Power 10*log, (/V/Hz)\"},{\"boundingBox\":[{\"x\":58,\"y\":655},{\"x\":93,\"y\":658},{\"x\":91,\"y\":683},{\"x\":56,\"y\":680}],\"text\":\"-30\"},{\"boundingBox\":[{\"x\":62,\"y\":719},{\"x\":92,\"y\":719},{\"x\":92,\"y\":742},{\"x\":62,\"y\":742}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":65,\"y\":782},{\"x\":95,\"y\":788},{\"x\":92,\"y\":808},{\"x\":61,\"y\":802}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":58,\"y\":843},{\"x\":93,\"y\":842},{\"x\":94,\"y\":863},{\"x\":58,\"y\":864}],\"text\":\"-60\"},{\"boundingBox\":[{\"x\":201,\"y\":920},{\"x\":218,\"y\":924},{\"x\":215,\"y\":945},{\"x\":198,\"y\":941}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":376,\"y\":920},{\"x\":402,\"y\":920},{\"x\":402,\"y\":946},{\"x\":376,\"y\":945}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":552,\"y\":920},{\"x\":584,\"y\":919},{\"x\":584,\"y\":944},{\"x\":552,\"y\":945}],\"text\":\"15\"},{\"boundingBox\":[{\"x\":736,\"y\":919},{\"x\":764,\"y\":919},{\"x\":763,\"y\":945},{\"x\":735,\"y\":945}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":911,\"y\":920},{\"x\":943,\"y\":918},{\"x\":943,\"y\":943},{\"x\":911,\"y\":945}],\"text\":\"25\"},{\"boundingBox\":[{\"x\":1093,\"y\":919},{\"x\":1124,\"y\":918},{\"x\":1123,\"y\":944},{\"x\":1093,\"y\":945}],\"text\":\"30\"},{\"boundingBox\":[{\"x\":1272,\"y\":917},{\"x\":1305,\"y\":916},{\"x\":1305,\"y\":944},{\"x\":1272,\"y\":945}],\"text\":\"35\"},{\"boundingBox\":[{\"x\":1454,\"y\":919},{\"x\":1483,\"y\":920},{\"x\":1481,\"y\":944},{\"x\":1452,\"y\":943}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":1633,\"y\":918},{\"x\":1666,\"y\":917},{\"x\":1666,\"y\":943},{\"x\":1633,\"y\":944}],\"text\":\"45\"},{\"boundingBox\":[{\"x\":1812,\"y\":918},{\"x\":1841,\"y\":918},{\"x\":1841,\"y\":943},{\"x\":1812,\"y\":943}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":845,\"y\":947},{\"x\":1032,\"y\":945},{\"x\":1032,\"y\":972},{\"x\":845,\"y\":973}],\"text\":\"Frequency (Hz)\"},{\"boundingBox\":[{\"x\":923,\"y\":1020},{\"x\":955,\"y\":1017},{\"x\":957,\"y\":1061},{\"x\":924,\"y\":1064}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":131,\"y\":1138},{\"x\":159,\"y\":1138},{\"x\":159,\"y\":1161},{\"x\":131,\"y\":1161}],\"text\":\"(a)\"},{\"boundingBox\":[{\"x\":1073,\"y\":1223},{\"x\":1268,\"y\":1224},{\"x\":1268,\"y\":1246},{\"x\":1073,\"y\":1245}],\"text\":\"Serviontermin-Buchung\"},{\"boundingBox\":[{\"x\":833,\"y\":1275},{\"x\":819,\"y\":1274},{\"x\":821,\"y\":1257},{\"x\":835,\"y\":1258}],\"text\":\"8\"},{\"boundingBox\":[{\"x\":158,\"y\":1475},{\"x\":128,\"y\":1474},{\"x\":127,\"y\":1452},{\"x\":157,\"y\":1452}],\"text\":\"(9)\"},{\"boundingBox\":[{\"x\":819,\"y\":1628},{\"x\":835,\"y\":1629},{\"x\":834,\"y\":1650},{\"x\":817,\"y\":1650}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":1091,\"y\":1682},{\"x\":1489,\"y\":1681},{\"x\":1489,\"y\":1723},{\"x\":1092,\"y\":1724}],\"text\":\"Intensity of the view: low\"},{\"boundingBox\":[{\"x\":1601,\"y\":1684},{\"x\":1680,\"y\":1684},{\"x\":1679,\"y\":1720},{\"x\":1599,\"y\":1720}],\"text\":\"high\"},{\"boundingBox\":[{\"x\":393,\"y\":1757},{\"x\":442,\"y\":1757},{\"x\":442,\"y\":1816},{\"x\":394,\"y\":1816}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1385,\"y\":1768},{\"x\":1429,\"y\":1764},{\"x\":1431,\"y\":1810},{\"x\":1387,\"y\":1814}],\"text\":\"C\"}],\"words\":[{\"boundingBox\":[{\"x\":25,\"y\":7},{\"x\":56,\"y\":6},{\"x\":57,\"y\":37},{\"x\":26,\"y\":38}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":200,\"y\":4},{\"x\":230,\"y\":4},{\"x\":230,\"y\":28},{\"x\":200,\"y\":28}],\"text\":\"6.0\"},{\"boundingBox\":[{\"x\":501,\"y\":4},{\"x\":531,\"y\":4},{\"x\":531,\"y\":22},{\"x\":501,\"y\":22}],\"text\":\"10.0\"},{\"boundingBox\":[{\"x\":803,\"y\":5},{\"x\":835,\"y\":3},{\"x\":837,\"y\":26},{\"x\":803,\"y\":28}],\"text\":\"14.0\"},{\"boundingBox\":[{\"x\":1096,\"y\":2},{\"x\":1131,\"y\":4},{\"x\":1129,\"y\":24},{\"x\":1095,\"y\":21}],\"text\":\"22.0\"},{\"boundingBox\":[{\"x\":1397,\"y\":2},{\"x\":1432,\"y\":4},{\"x\":1430,\"y\":24},{\"x\":1396,\"y\":22}],\"text\":\"36.0\"},{\"boundingBox\":[{\"x\":1692,\"y\":4},{\"x\":1728,\"y\":2},{\"x\":1726,\"y\":22},{\"x\":1691,\"y\":24}],\"text\":\"0.0\"},{\"boundingBox\":[{\"x\":1732,\"y\":2},{\"x\":1754,\"y\":7},{\"x\":1752,\"y\":27},{\"x\":1730,\"y\":22}],\"text\":\"HZ\"},{\"boundingBox\":[{\"x\":204,\"y\":129},{\"x\":220,\"y\":126},{\"x\":224,\"y\":142},{\"x\":208,\"y\":145}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1755,\"y\":204},{\"x\":1750,\"y\":189},{\"x\":1765,\"y\":184},{\"x\":1770,\"y\":199}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":61,\"y\":349},{\"x\":90,\"y\":347},{\"x\":91,\"y\":370},{\"x\":61,\"y\":372}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":63,\"y\":410},{\"x\":91,\"y\":410},{\"x\":91,\"y\":433},{\"x\":63,\"y\":433}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":74,\"y\":496},{\"x\":72,\"y\":479},{\"x\":89,\"y\":477},{\"x\":91,\"y\":494}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":65,\"y\":536},{\"x\":89,\"y\":539},{\"x\":87,\"y\":562},{\"x\":63,\"y\":559}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":60,\"y\":595},{\"x\":93,\"y\":601},{\"x\":89,\"y\":623},{\"x\":56,\"y\":617}],\"text\":\"-20\"},{\"boundingBox\":[{\"x\":9,\"y\":774},{\"x\":9,\"y\":697},{\"x\":37,\"y\":698},{\"x\":33,\"y\":774}],\"text\":\"Power\"},{\"boundingBox\":[{\"x\":9,\"y\":692},{\"x\":8,\"y\":609},{\"x\":39,\"y\":610},{\"x\":37,\"y\":693}],\"text\":\"10*log,\"},{\"boundingBox\":[{\"x\":8,\"y\":604},{\"x\":7,\"y\":499},{\"x\":39,\"y\":500},{\"x\":39,\"y\":605}],\"text\":\"(/V/Hz)\"},{\"boundingBox\":[{\"x\":58,\"y\":655},{\"x\":93,\"y\":658},{\"x\":91,\"y\":683},{\"x\":56,\"y\":680}],\"text\":\"-30\"},{\"boundingBox\":[{\"x\":62,\"y\":719},{\"x\":91,\"y\":719},{\"x\":91,\"y\":742},{\"x\":62,\"y\":742}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":69,\"y\":783},{\"x\":95,\"y\":788},{\"x\":92,\"y\":808},{\"x\":65,\"y\":802}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":58,\"y\":842},{\"x\":93,\"y\":842},{\"x\":93,\"y\":863},{\"x\":58,\"y\":864}],\"text\":\"-60\"},{\"boundingBox\":[{\"x\":208,\"y\":921},{\"x\":219,\"y\":924},{\"x\":214,\"y\":944},{\"x\":203,\"y\":942}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":376,\"y\":920},{\"x\":400,\"y\":920},{\"x\":400,\"y\":946},{\"x\":376,\"y\":945}],\"text\":\"10\"},{\"boundingBox\":[{\"x\":555,\"y\":920},{\"x\":583,\"y\":919},{\"x\":584,\"y\":944},{\"x\":555,\"y\":945}],\"text\":\"15\"},{\"boundingBox\":[{\"x\":735,\"y\":919},{\"x\":763,\"y\":919},{\"x\":763,\"y\":945},{\"x\":735,\"y\":945}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":913,\"y\":920},{\"x\":941,\"y\":918},{\"x\":943,\"y\":942},{\"x\":914,\"y\":944}],\"text\":\"25\"},{\"boundingBox\":[{\"x\":1093,\"y\":919},{\"x\":1122,\"y\":918},{\"x\":1122,\"y\":944},{\"x\":1093,\"y\":945}],\"text\":\"30\"},{\"boundingBox\":[{\"x\":1272,\"y\":917},{\"x\":1303,\"y\":916},{\"x\":1304,\"y\":944},{\"x\":1272,\"y\":945}],\"text\":\"35\"},{\"boundingBox\":[{\"x\":1452,\"y\":919},{\"x\":1482,\"y\":920},{\"x\":1482,\"y\":944},{\"x\":1452,\"y\":943}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":1633,\"y\":918},{\"x\":1665,\"y\":917},{\"x\":1666,\"y\":943},{\"x\":1633,\"y\":944}],\"text\":\"45\"},{\"boundingBox\":[{\"x\":1812,\"y\":918},{\"x\":1840,\"y\":918},{\"x\":1840,\"y\":943},{\"x\":1812,\"y\":943}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":846,\"y\":948},{\"x\":971,\"y\":947},{\"x\":970,\"y\":973},{\"x\":845,\"y\":973}],\"text\":\"Frequency\"},{\"boundingBox\":[{\"x\":976,\"y\":947},{\"x\":1032,\"y\":946},{\"x\":1031,\"y\":973},{\"x\":975,\"y\":973}],\"text\":\"(Hz)\"},{\"boundingBox\":[{\"x\":923,\"y\":1019},{\"x\":952,\"y\":1017},{\"x\":956,\"y\":1060},{\"x\":924,\"y\":1063}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":131,\"y\":1138},{\"x\":159,\"y\":1138},{\"x\":159,\"y\":1161},{\"x\":131,\"y\":1161}],\"text\":\"(a)\"},{\"boundingBox\":[{\"x\":1073,\"y\":1224},{\"x\":1268,\"y\":1224},{\"x\":1268,\"y\":1246},{\"x\":1076,\"y\":1246}],\"text\":\"Serviontermin-Buchung\"},{\"boundingBox\":[{\"x\":831,\"y\":1275},{\"x\":819,\"y\":1274},{\"x\":820,\"y\":1257},{\"x\":832,\"y\":1258}],\"text\":\"8\"},{\"boundingBox\":[{\"x\":157,\"y\":1475},{\"x\":127,\"y\":1474},{\"x\":127,\"y\":1452},{\"x\":157,\"y\":1452}],\"text\":\"(9)\"},{\"boundingBox\":[{\"x\":817,\"y\":1628},{\"x\":835,\"y\":1628},{\"x\":834,\"y\":1650},{\"x\":817,\"y\":1649}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":1092,\"y\":1685},{\"x\":1222,\"y\":1682},{\"x\":1222,\"y\":1722},{\"x\":1092,\"y\":1724}],\"text\":\"Intensity\"},{\"boundingBox\":[{\"x\":1230,\"y\":1682},{\"x\":1263,\"y\":1682},{\"x\":1263,\"y\":1722},{\"x\":1230,\"y\":1722}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":1271,\"y\":1682},{\"x\":1324,\"y\":1682},{\"x\":1324,\"y\":1722},{\"x\":1271,\"y\":1722}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":1332,\"y\":1682},{\"x\":1416,\"y\":1682},{\"x\":1416,\"y\":1723},{\"x\":1332,\"y\":1722}],\"text\":\"view:\"},{\"boundingBox\":[{\"x\":1424,\"y\":1682},{\"x\":1490,\"y\":1683},{\"x\":1489,\"y\":1724},{\"x\":1423,\"y\":1723}],\"text\":\"low\"},{\"boundingBox\":[{\"x\":1604,\"y\":1684},{\"x\":1678,\"y\":1684},{\"x\":1678,\"y\":1720},{\"x\":1604,\"y\":1720}],\"text\":\"high\"},{\"boundingBox\":[{\"x\":393,\"y\":1757},{\"x\":440,\"y\":1757},{\"x\":440,\"y\":1816},{\"x\":393,\"y\":1816}],\"text\":\"b\"},{\"boundingBox\":[{\"x\":1398,\"y\":1767},{\"x\":1429,\"y\":1764},{\"x\":1432,\"y\":1810},{\"x\":1402,\"y\":1813}],\"text\":\"C\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 21 September 2020\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":16},{\"x\":1069,\"y\":16},{\"x\":1069,\"y\":74},{\"x\":0,\"y\":72}],\"text\":\"Published online: 21 September 2020\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":18},{\"x\":284,\"y\":17},{\"x\":284,\"y\":72},{\"x\":0,\"y\":66}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":293,\"y\":17},{\"x\":500,\"y\":16},{\"x\":501,\"y\":74},{\"x\":293,\"y\":72}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":509,\"y\":16},{\"x\":589,\"y\":16},{\"x\":590,\"y\":74},{\"x\":510,\"y\":74}],\"text\":\"21\"},{\"boundingBox\":[{\"x\":598,\"y\":16},{\"x\":910,\"y\":17},{\"x\":912,\"y\":73},{\"x\":599,\"y\":74}],\"text\":\"September\"},{\"boundingBox\":[{\"x\":919,\"y\":17},{\"x\":1066,\"y\":17},{\"x\":1068,\"y\":70},{\"x\":921,\"y\":73}],\"text\":\"2020\"}]}"
      ]
    },
    {
      "@search.score": 1.3789501,
      "content": "\nMulti technique amalgamation \nfor enhanced information identification \nwith content based image data\nRik Das1*, Sudeep Thepade2 and Saurav Ghosh3\n\nBackground\nRecent years have witnessed the digital photo-capture devices as a ubiquity for the com-\nmon mass (Raventós et al. 2015). The low cost storage, increasing computer power and \never accessible internet have kindled the popularity of digital image acquisition. Efficient \nindexing and identification of image data from these huge image repositories has nur-\ntured new research challenges in computer vision and machine learning (Madireddy \net  al. 2014). Automatic derivation of sematically-meaningful information from image \ncontent has become imperative as the traditional text based annotation technique has \nrevealed severe limitations to fetch information from the gigantic image datasets (Walia \net  al. 2014). Conventional techniques of image recognition were based on text or key-\nwords based mapping of images which had limited image information. It was dependent \non the perception and vocabulary of the person performing the annotation. The manual \nprocess was highly time consuming and slow in nature. The aforesaid limitations have \n\nAbstract \nImage data has emerged as a resourceful foundation for information with proliferation \nof image capturing devices and social media. Diverse applications of images in areas \nincluding biomedicine, military, commerce, education have resulted in huge image \nrepositories. Semantically analogous images can be fruitfully recognized by means of \ncontent based image identification. However, the success of the technique has been \nlargely dependent on extraction of robust feature vectors from the image content. The \npaper has introduced three different techniques of content based feature extraction \nbased on image binarization, image transform and morphological operator respec-\ntively. The techniques were tested with four public datasets namely, Wang Dataset, \nOliva Torralba (OT Scene) Dataset, Corel Dataset and Caltech Dataset. The multi tech-\nnique feature extraction process was further integrated for decision fusion of image \nidentification to boost up the recognition rate. Classification result with the proposed \ntechnique has shown an average increase of 14.5 % in Precision compared to the exist-\ning techniques and the retrieval result with the introduced technique has shown an \naverage increase of 6.54 % in Precision over state-of-the art techniques.\n\nKeywords: Image classification, Image retrieval, Otsu’s threshold, Slant transform, \nMorphological operator, Fusion, t test\n\nOpen Access\n\n© 2015 Das et al. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate \nif changes were made.\n\nR E S E A R C H\n\nDas et al. SpringerPlus  (2015) 4:749 \nDOI 10.1186/s40064-015-1515-4\n\n*Correspondence:  rikdas78@\ngmail.com \n1 Department of Information \nTechnology, Xavier Institute \nof Social Service, Dr. Camil \nBulcke Path (Purulia Road), \nP.O. Box 7, Ranchi 834001, \nJharkhand, India\nFull list of author information \nis available at the end of the \narticle\n\n\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40064-015-1515-4&domain=pdf\n\n\nPage 2 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nbeen effectively handled with content based image identification which has been exer-\ncised as an effective alternative to the customary text based process (Wang et al. 2013). \nThe competence of the content based image identification technique has been depend-\nent on the extraction of robust feature vectors. Diverse low level features namely, color, \nshape, texture etc. have constituted the process of feature extraction. However, an image \ncomprises of number of features which can hardly be defined by a single feature extrac-\ntion technique (Walia et al. 2014). Therefore, three different techniques of feature extrac-\ntion namely, feature extraction with image transform, feature extraction with image \nmorphology and feature extraction with image binarization have been proposed in this \npaper to leverage fusion of multi-technique feature extraction. The recognition decision \nof three different techniques was further integrated by means of Z score normalization \nto create hybrid architecture for content based image identification. The main contribu-\ntion of the paper has been to propose fusion architecture for content based image recog-\nnition with novel techniques of feature extraction for enhanced recognition rate.\n\nThe research objectives have been enlisted as follows:\n\n  • Reducing the dimension of feature vectors.\n  • Successfully implementing fusion based method of content based image identifica-\n\ntion.\n  • Statistical validation of research results.\n  • Comparison of research results with state-of-the art techniques.\n\nThree different techniques of feature extraction using image binarization, image trans-\nforms and morphological operators have been combined to develop fusion based archi-\ntecture for content based image classification and retrieval. Hence, it is in correlation with \nresearch on binarization based feature extraction, transform based feature extraction and \nmorphology based feature extraction from images. It is also in connection with research \non multi technique fusion for content based image identification. Therefore, the following \nfour subsections have reviewed some contemporary and earlier works on these four topics.\n\nFeature extraction using image transform\n\nChange of domain of the image elements has been carried out by using image trans-\nformation to represent the image by a set of energy spectrum. An image can be repre-\nsented as series of basis images which can be formed by extrapolating the image into a \nseries of basis functions (Annadurai and Shanmugalakshmi 2011). The basis images have \nbeen populated by using orthogonal unitary matrices as image transformation opera-\ntor. This image transformation from one representation to another has advantages in \ntwo aspects. An image can be expanded in the form of a series of waveforms with the \nuse of image transforms. The transformation process has been helpful to differentiate \nthe critical components of image patterns and in making them directly accessible for \nanalysis. Moreover, the transformed image data has a compact structure useful for effi-\ncient storage and transmission. The aforesaid properties of image transforms facilitate \nradical reduction of feature vector dimension to be extracted from the images. Diverse \ntechniques of feature extraction has been proposed by exploiting the properties of image \ntransforms to extract features from images using fractional energy coefficient (Kekre and \n\n\n\nPage 3 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThepade 2009; Kekre et  al. 2010). The techniques have considered seven image trans-\nforms and fifteen fractional coefficients sets for efficient feature extraction. Original \nimages were divided into subbands by using multiple scales Biorthogonal wavelet trans-\nform and the subband coefficients were used as features for image classification (Prakash \net al. 2013). The feature spaces were reduced by applying Isomap-Hysime random aniso-\ntropic transform for classification of high dimensional data (Luo et al. 2013).\n\nImage binarization techniques for feature extraction\n\nFeature extraction from images has been largely carried out by means of image binariza-\ntion. Appropriate threshold selection has been imperative for execution of efficient image \nbinarization. Nevertheless, various factors including uneven illumination, inadequate \ncontrast etc. can have adverse effect on threshold computation (Valizadeh et  al. 2009). \nContemporary literatures on image binarization techniques have categorized three dif-\nferent techniques for threshold selection namely, mean threshold selection, local thresh-\nold selection and global threshold selection to deal with the unfavourable influences on \nthreshold selection. Enhanced classification results have been comprehended by feature \nextraction from mean threshold and multilevel mean threshold based binarized images \n(Kekre et al. 2013; Thepade et al. 2013a, b). Eventually, it has been identified that selection \nof mean threshold has not dealt with the standard deviation of the gray values and has \nconcentrated only on the average which has prevented the feature extraction techniques \nto take advantage of the spread of data to distinguish distinct features. Therefore, image \nsignature extraction was carried out with local threshold selection and global thresh-\nold selection for binarization, as the techniques were based on calculation of both mean \nand standard deviation of the gray values (Liu 2013; Yanli and Zhenxing 2012; Ramírez-\nOrtegón and Rojas 2010; Otsu 1979; Shaikh et al. 2013; Thepade et al. 2014a).\n\nUse of morphological operators for feature extraction\n\nCommercial viability of shape feature extraction has been well highlighted by systems \nlike Image Content (Flickner et  al. 1995), PicToSeek (Gevers and Smeulders 2000). \nTwo different categorization of shape descriptors namely, contour-based and region-\nbased descriptors have been elaborated in the existing literatures (Mehtre et  al. 1997; \nZhang and Lu 2004). Emphasize of the contour based descriptors has been on bound-\nary lines. Popular contour-based descriptors have embraced Fourier descriptor (Zhang \nand Lu 2003), curvature scale space (Mokhtarian and Mackworth 1992), and chain codes \n(Dubois and Glanz 1986). Feature extraction from complex shapes has been well car-\nried out by means of region-based descriptors, since the feature extraction has been per-\nformed from whole area of object (Kim and Kim 2000).\n\nFusion methodologies and multi technique feature extraction\n\nInformation recognition with image data has utilized the features extracted by means \nof diverse extraction techniques to harmonize each other for enhanced identification \nrate. Recent studies in information fusion have categorized the methodologies typically \ninto four classes, namely, early fusion, late fusion, hybrid fusion and intermediate fusion. \nEarly fusion combines the features of different techniques and produces it as a single \ninput to the learner. The process inherently increases the size of feature vector as the \n\n\n\nPage 4 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nconcentrated features easily correspond to higher dimensions. Late fusion applies sepa-\nrate learner to each feature extraction technique and fuses the decision with a combiner. \nAlthough it offers scalability in comparison to early fusion, still, it cannot explore the \nfeature level correlations, since it has to make local decisions primarily. Hybrid fusion \nmakes a mix of the two above mentioned techniques. Intermediate fusion integrates \nmultiple features by considering a joint model for decision to yield superior prediction \naccuracy (Zhu and Shyu 2015). Color and texture features were extracted by means of \n3 D color histogram and Gabor filters for fusion based image identification. The space \ncomplexity of the feature was further reduced by using genetic algorithm which has also \nobtained the optimum boundaries of numerical intervals. The process has enhanced \nsemantic retrieval by introducing feature selection technique to reduce memory con-\nsumption and to decrease retrieval process complexity (ElAlami 2011). Local descriptors \nbased on color and texture was calculated from Color moments and moments on Gabor \nfilter responses. Gradient vector flow fields were calculated to capture shape information \nin terms of edge images. The shape features were finally depicted by invariant moments. \nThe retrieval decisions with the features were fused for enhanced retrieval performance \n(Hiremath and Pujari 2007). Feature vectors comprising of color histogram and tex-\nture features based on a co-occurrence matrix were extracted from HSV color space \nto facilitate image retrieval (Yue et  al. 2011). Visually significant point features chosen \nfrom images by means of fuzzy set theoretic approach. Computation of some invariant \ncolor features from these points was performed to gauge the similarity between images \n(Banerjee et  al. 2009). Recognition process was boosted up by combining color layout \ndescriptor and Gabor texture descriptor as image signatures (Jalab 2011). Multi view \nfeatures comprising of color, texture and spatial structure descriptors have contributed \nfor increased retrieval rate (Shen and Wu 2013). Wavelet packets and Eigen values of \nGabor filters were extracted as feature vectors by the authors in (Irtaza et  al. 2013) for \nneural network architecture of image identification. The back propagation neural net-\nwork was trained on sub repository of images generated from the main image reposi-\ntory and utilizes the right neighbourhood of the query image. This kind of training was \naimed to insure correct semantic retrieval in response to query images. Higher retrieval \nresults have been apprehended with intra-class and inter-class feature extraction from \nimages (Rahimi and Moghaddam 2013). In (ElAlami 2014), extraction of color and tex-\nture features through color co-occurrence matrix (CCM) and difference between pixels \nof scan pattern (DBPSP) has been demonstrated and an artificial neural network (ANN) \nbased classifier was designed. In (Subrahmanyam et  al. 2013), content-based image \nretrieval was carried out by integrating the modified color motif co-occurrence matrix \n(MCMCM) and difference between the pixels of a scan pattern (DBPSP) features with \nequal weights. Fusion of semantic retrieval results obtained by capturing colour, shape \nand texture with the color moment (CMs), angular radial transform descriptor and edge \nhistogram descriptor (EHD) features respectively had outclassed the Precision values of \nindividual techniques (Walia et al. 2014). Six semantics of local edge bins for EHD were \nconsidered which included the vertical and the horizontal edge (0,0), 45° edge and 135° \nedge of sub-image (0,0), non directional edge of sub-image (0,0) and vertical edge of sub-\nimage at (0,1). Color histogram and spatial orientation tree has been used for unique \nfeature extraction from images for retrieval purpose (Subrahmanyam et al. 2012).\n\n\n\nPage 5 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nMethods\nThree different techniques of feature extraction have been introduced in this work namely, \nfeature extraction with image binarization, feature extraction with image transform and \nfeature extraction with morphological operator. However, there are popular feature extrac-\ntion techniques like GIST descriptor which has much greater feature dimension com-\npared to the proposed techniques in the work. GIST creates 32 feature maps of same \nsize by convolving the image with 32 Gabor filters at 4 scales, 8 orientations (Douze et al. \n2009). It averages the feature values of each region by dividing each feature map into 16 \nregions. Finally, it concatenates the 16 average value of all 32 feature maps resulting in \n16 ×  32 =  512 GIST descriptor. On the other hand, our approach has generated a fea-\nture dimension of 6 from each of the binarization and morphological technique. Feature \nextraction by applying image transform has yielded a feature size of 36. On the whole, the \nfeature size for the fusion based classifier was (6 + 36 + 6 = 48) which is far less than GIST \nand has much lesser computational overhead. Furthermore, fusion based architecture for \nclassification and retrieval have been proposed for enhanced identification rate of image \ndata. Each of the techniques of feature extraction as well as the methods for fusion based \narchitecture of classification and retrieval has been discussed in the following four subsec-\ntions and the description of datasets has been given in the fifth subsection.\n\nFeature extraction with image binarization\n\nInitially, the three color components namely, Red (R), Green (G) and Blue (B) were sepa-\nrated in each of the test images. A popular global threshold selection method named \nOtsu’s method has been applied separately on each of the color components for binari-\nzation as in Fig. 1. The above mentioned thresholding method has been largely used for \ndocument image binarzation. Otsu’s technique has been operated directly on the gray \nlevel histogram which has made it fast executable. It has been efficient to remove redun-\ndant details from the image to bring out the necessary image information. The method \nhas been considered as a non-parametric method which has considered two classes of \npixels, namely, the foreground pixels and the background pixels. It has calculated the \noptimal threshold by using the within-class variance and between-class variance. The \nseparation was carried out in such a way so that their combined intra-class variance is \nminimal (Otsu 1979; Shaikh et al. 2013). Comprehensive investigation has been carried \nout for the threshold that minimizes the intra-class variance represented by the weighted \nsum of variances of the two classes of pixels for each of the three color components.\n\nThe weighted within-class variance has been given in Eq. 1.\n\nq1(t) = ∑ \nt\ni=0P(i) where the class probabilities of different gray level pixels were estimated \n\nas shown in Eqs. 2 and 3:\n\n(1)σ2w(t) = q1(t)σ\n2\n1 (t) + q2(t)σ\n\n2\n2 (t)\n\n(2)q1(t) =\nt\n\n∑\n\ni=0\n\np(i)\n\n(3)\nq2(t) =\n\n255\n∑\n\ni=t+1\n\nP(i)\n\n\n\nPage 6 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThe class means were given as in Eqs. 4 and 5:\n\nTotal variance (σ2) = Within-class variance (σw\n2(t)) + Between-class Variance(σb\n\n2(t)).\nSince the total variance was constant and independent of t, the effect of changing \n\nthe threshold was purely to shift the contributions of the two terms back and forth. \nBetween-class variance has been given in Eq. 6\n\nThus, minimizing the within-class variance was the same as maximizing the between-\nclass variance.\n\nBinarization of the test images was carried out using the Otsu’s local threshold selec-\ntion method. The process has been repeated for all the three color components to gen-\nerate bag of words model (BoW) of features. Conventional BoW model has been based \non SIFT algorithm which has a descriptor dimension of 128 (Zhao et  al. 2015). There-\nfore, for three color components the dimension of the descriptor would have been 128 \n× 3 = 384. The size for SIFT descriptor has been huge and it has predestined problem \nfor information losses and omissions as it has been found suitable only for the stability \n\n(4)µ1(t) =\nt\n\n∑\n\ni=0\n\ni ∗ P(i)\n\nq1(t)\n\n(5)µ2(t) =\n255\n∑\n\ni=t+1\n\ni ∗ P(i)\n\nq2(t)\n\n(6)σ\n2\nb (t) = q1(t)[1 − q1(t)][µ1(t) − µ2(t)]\n\n2\n\n   \nRed Component Green Component Blue Component \n\n   \nBinarization of \n\nRed Component \nBinarzation of \n\nGreen Component \nBinarization of \n\nBlue Component \nFig. 1 Binarization using Otsu’s Threshold selection\n\n\n\n\n\n\n\n\n\nPage 7 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nof image feature point extraction and description. Furthermore, the generated SIFT \ndescriptors has to be clustered by k means clustering which has been based on alloca-\ntion of cluster members by means of comparing squared Euclidian distance. The clus-\ntering process has been helpful to generate codewords for codebook generation which \nhas been the final step of BoW. Process of k means clustering has huge computational \noverhead for calculating the squared Euclidian distance which eventually slows down \nthe BoW generation. Hence, in our approach, the grey values higher than the threshold \nwas clustered in higher intensity group and the grey values lower than the cluster was \nclustered in the lower intensity group. The mean of the two groups were calculated to \nformulate the codewords of higher intensity feature vectors and the lower intensity fea-\nture vectors respectively. Thus, each color component of a test image has been mapped \nto two codewords of higher intensity and lower intensity respectively. This has generated \nof codebook of size (3 × 2 = 6) for each image.\n\nThe algorithm for feature extraction has been stated in Algorithm 1 as follows:\n\nAlgorithm 1 \n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Calculate the local threshold value Tx for \neach pixel in each color component R,G and \nB using Otsu's Method.\n\n3. Compute binary image maps for each pixel \nfor the given image.\n\nTxjixif >=),(....1\n\nTxjixif <),(....0\n\n/*x = R, G and B */\n\n4. Generate image features for the given \nimage for each color component.\n\n/*x = R, G and B */\n\nEnd\n\n=),( jiBitmapx\n\nTx\np q\n\nqpxmean\nmean\n\nxhi >== ∑ ∑ )),((\n\nTx\np q\n\nqpxmean\nmean\n\nxlo <= ∑∑ )),((\n\n\n\nPage 8 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFeature extraction using image transform\n\nTransforms convert spatial information to frequency domain information, where cer-\ntain operations are easier to perform. Energy compaction property of transforms has \nthe capacity to pack large fraction of the average energy into a few components. This \nhas led to faster execution and efficient algorithm design. Image transforms has the \nproperty to convert the spatial domain information of an image to frequency domain \ninformation, where certain operations are easier to perform. For example, convolu-\ntion operation can be reduced to matrix multiplication in frequency domain. It has the \ncharacteristic of energy compaction which ensures that a large fraction of the average \nenergy of the image remains packed into a few components. This property has led to \nfaster execution and efficient algorithm design by drastic reduction of feature vector \nsize which is achieved by means of discarding insignificant transform coefficients as in \nFig. 2. The approach has been implemented by applying slant transform on each of the \nRed (R), Green (G) and Blue (B) color component of the image for extraction of fea-\nture vectors with smaller dimension. Slant transform has reduced the average coding \nof a monochrome image from 8 bits/pixel to 1 bit/pixel without seriously degrading the \nimage quality. It is an orthogonal transform which has also reduced the coding of color \nimages from 24–2 bits/pixel (Pratt et  al. 1974). Slant transform matrices are orthogo-\nnal and it holds all real components. Hence, it has much less computational overhead \ncompared to discrete Fourier transform. Slant transform is an unitary transform and \nfollows energy conservation. It tends to pack a large fraction of signal energy into a few \ntransform coefficients which has a significant role in reducing the feature vector for the \nimage. Let [F] be an N × N matrix of pixel values of an image and let [fi] be an N × 1 \nvector representing the ith. column of [F]. One dimensional transform of the ith. image \nline can be given by\n\n [S] = N × N unitary slant matrix.\n\n[fi] = [S][fi]\n\n0.06 % of (N*N) feature vector\n\n0.012% of (N*N) feature vector\n\n50% of (N*N) feature vector\n\nN*N feature vector\n\nFeature Vector Dimension Reduction with Partial Coefficients\n\nFig. 2 Feature extraction by applying image transform\n\n\n\n\n\n\n\nPage 9 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nA two dimensional slant transform can be performed by sequential transformations \nof row and column of [F] and the forward and inverse transform can be expressed as in \nEqs. 7 and 8.\n\nA transform operation can be conveniently represented in a series. The two dimensional \nforward and inverse transform in series form can be represented as in Eqs. 9 and 10\n\nThe algorithm for feature extraction using slant transform has been given in Algo-\nrithm 2.\n\nAlgorithm 2 \n\n(7)[ℑ] = |S|[F][S]T\n\n(8)[F] = [S]T [ℑ][S]\n\n(9)ℑ(u, v) =\nN\n∑\n\nj=1\n\nN\n∑\n\nk=1\n\nF(j, k)S(u, j)S(k, v)\n\n(10)F\n(\n\nj, k\n)\n\n=\n\nN\n∑\n\nu=1\n\nN\n∑\n\nv=1\n\nℑ(u, v)S\n(\n\nj, u\n)\n\nS(v, k)\n\nBegin\n\n1. Red, Green and Blue color components were \nextracted from a given image.\n\n2. Slant Transform was applied on each of the \ncomponent to extract feature vectors.\n\n3. The extracted feature vectors from each of the \ncomponent were stored as complete set of feature \nvectors.\n\n4. Further, partial coefficients from the entire \nfeature vector set were extracted to form the \nfeature vector database.\n\n5. Feature vector database with 100% transformed \ncoefficients and partial coefficients ranging from \n50% of the complete set of feature vectors till \n0.06% of the complete set of feature vectors were \nconstructed\n\n6. The feature vectors of the query image for the \nwhole set of feature vectors and for partial \ncoefficient of feature vectors were compared with \nthe database images for classification results.\n\n7. The fractional coefficient of feature vector \nhaving the highest classification result was \nconsidered as the feature set extracted by applying \nimage transform\n\nEnd\n\n\n\nPage 10 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nHere the features were extracted in the form of visual words. Visual words have been \ndefined as a small patch of image which can carry significant image information. The \nenergy compaction property of Slant transform has condensed noteworthy image infor-\nmation in a block of 12 elements for an image of dimension (256 × 256). Thus, the \nfeature vector extracted with slant transform was of size 12 for each color component \nwhich has given the dimension of feature vector as 36 (12  ×  3  =  36) for three color \ncomponents in each test image.\n\nFeature extraction with morphological operator\n\nHuman perception has largely been governed by shape context. It has been helpful to \nrecover the point correspondences from an image which has considerable contribution \nin feature vector formation. A variant of gray scale opening and closing operations has \nbeen termed as the top-hat transformation that has been instrumental in producing only \nthe bright peaks of an image (Sridhar 2011). It has been termed as the peak detector and \nits working process has been given as follows:\n\n1. Apply the gray scale opening operation to an image.\n2. Peak = original image—opened image.\n3. Display the peak.\n4. Exit.\n\nThe top-hat transform technique was applied on each color component Red (R), \nGreen (G) and Blue (B) of the test images for feature extraction using morphologi-\ncal operator as in Fig.  3. After applying the tophat operator, the pixels designated as \nthe foreground pixels were grouped in one cluster and were calculated with mean and \nstandard deviation to formulate the higher intensity feature vector. Similar process \nwas followed with the pixels designated as the background pixels to calculate the lower \nintensity feature vector. The feature vector extraction process has followed the bag of \nwords (BoW) methodology which has generated codewords from the cluster of fore-\nground and background pixels by calculating the mean and the standard deviation of \nboth the clusters and adding the two. Hence, codebook size for each color component \nwas two which have yielded a dimension of 6 (3 × 2 =  6) on the whole for the code-\nbook generated for three color components for each test image.\n\nThe algorithm for feature extraction using morphological operator has been given in \nAlgorithm 3.\n\n\n\nPage 11 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nAlgorithm 3 \n\nSimilarity measures\n\nDetermination of image similarity measures was performed by evaluating distance \nbetween set of image features. Higher similarity has been characterized by shorter dis-\ntance (Dunham 2009). A fusion based classifier, an artificial neural network (ANN) clas-\nsifier and a support vector machine (SVM) classifier was used for the purpose. Each of \nthe classifier types has been discussed in the following sections:\n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Apply tophat transform on each color \ncomponent\n\n3. Cluster the foreground and background \npixels obtained after the morphological \noperation     \n\n4. Generate image features xhiF.V. and xloF.V.\nfor the given image for each color \ncomponent.\n\n/*x = R, G and B */\n\nEnd\n\n∑ ∑=\np q\n\nqp\nforeground\n\nxmean\nmean\n\nxhi )),((\n\n∑ ∑=\np q\n\nqp\nforeground\n\nx\nstdev\n\nxhi )),((σ\n\n( )\nstdev\n\nxhi\nmean\n\nxhimeanxhiVF\nxhi += +\n\n..\n\n∑ ∑=\np q\n\nqp\nbackground\n\nxmean\nmean\n\nxlo )),((\n\n∑ ∑=\np q\n\nqp\nbackground\n\nx\nstdev\n\nxlo )),((σ\n\n( )\nstdev\n\nxlo\nmean\n\nxlomeanxloVF\nxlo += +\n\n..\n\n\n\nPage 12 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFusion based classifier\n\nThree different distance measures, namely, city block distance, Euclidian distance and \nmean squared error (MSE) distance metric was considered to compute the distance \nbetween query image Q and database image T as in Eqs. 11, 12 and 13\n\nwhere, Qi is the query image and Di is the database image.\nData standardization technique was followed to standardize the calculated distances \n\nfor the individual techniques with Z score normalization which was based on mean and \nstandard deviation of the computed values as in Eq.  14. The normalization process has \nbeen implemented to avoid dependence of the classification decision on a feature vec-\ntor with higher values of attributes which have the possibilities to have greater effect or \n“weight.” The process has normalized the data within a common range such as [−1, 1] or \n[0.0, 1.0].\n\nwhere, µ is the mean and σ is the standard deviation.\n\n(11)Dcityblock =\nn\n\n∑\n\ni−1\n\n|Qi − Di|\n\n(12)Deuclidian =\n\n√\n\n√\n\n√\n\n√\n\nn\n∑\n\ni=1\n\n(Qi − Di)\n2\n\n(13)DMSE =\n1\n\nn\n\nn\n∑\n\ni=1\n\n(Qi − Di)\n2\n\n(14)distn =\ndisti − µ\n\nσ\n\n   \nRed Component Green Component Blue Component \n\n   \nApplying Top-Hat \noperator on Red \n\nComponent \n\nApplying Top-Hat \noperator on Green \n\nComponent \n\nApplying Top-Hat \noperator on Blue \n\nComponent \nFig. 3 Effect of applying morphological operator\n\n\n\n\n\n\n\n\n\n\n\n\n\nPage 13 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFurther, the final distance was calculated by adding the weighted sum of individual \ndistances. The weights were calculated from the precision values of corresponding tech-\nniques. Finally, the image was classified based on the class majority of k nearest neigh-\nbors [Sridhar 2011] where value of k was\n\nThe classified image was forwarded for retrieval purpose. The image was a classified \nquery and has searched for similar images only within the class of interest. Ranking of \nthe images was done with Canberra Distance measure as in Eq.  15 and top 20 images \nwere retrieved.\n\nwhere, Qi is the query image and Di is the database image.\nThe process of fusion based classification and then retrieval with classified query has \n\nbeen illustrated in Fig. 4.\n\nArtificial neural network (ANN) classifier\n\nThe set of input features from images were mapped to an appropriate output by a feed \nforward Neural Network Classifier known as Multilayer Perceptron (MLP) as shown in \nFig. 5 (Alsmadi et al. 2009).\n\nThe back propagation technique of multi layer perceptron has a significant role in \nsupervised learning procedure. The network has been trained for optimization of clas-\nsification performance by using the procedure of back propagation. For each training \ntuple, the weights were modified so as to minimize the mean squared error between the \nnetwork prediction and the target value. These modifications have been made in the \nbackward direction through each hidden layer down to the first hidden layer. The input \nfeature vectors have been fed to the input units which comprised the input layer. The \nnumber of input units has been dependent on the summation of the number of attrib-\nutes in the feature vector dataset and the bias node. The subsequent layer has been the \nhidden layer whose number of nodes has to be determined by considering the half of the \nsummation of the number of classes and the number of attributes per class. The inputs \nthat have passed the input layer have to be weighted and fed simultaneously to the hid-\nden layer for further processing. Weighted output of the hidden layer was used as input \nto the final layer which has been named as the output layer. The number of units in the \noutput layer has been denoted by the number of class labels. The feed forward property \nof this architecture does not allow the weights to cycle back to the input units.\n\nSupport vector machine (SVM) classifier\n\nSVM transforms original training data to higher dimension by using nonlinear mapping. \nOptimal separating hyperplane has to be searched by the algorithm within this new \ndimension. Data from two different classes can readily be separated by a hyperplane by \nmeans of an appropriate nonlinear mapping to a sufficiently high dimension as in Fig. 6.\n\nk ≤\n√\n\nnumber..of ..training..ins tan ces.\n\n(15)Dcanberra =\nn\n\n∑\n\ni=1\n\n|Qi − Di|\n\n|Qi| + |Di|\n\n\n\nPage 14 of 26Das et al. SpringerPlus  (2015) 4:749 \n\n          No \n\nYes \n\nRetrieve top 20 images \n\nFeature \nExtraction \n\nwith \nBinarization \n\nFeature \nExtraction \nwith partial \nTransform \ncoefficients \n\nFeature \nExtraction \n\nwith \nMorphological \n\nOperator \n\nRank Images \nby City-Block \nDistance \n\nRank Images \nby Euclidian \nDistance \n\nRank Images      \nby MSE \nDistance\n\nFuse the distances by Z score Normalization\n\nRank the images using fused distance\n\n            Rank the images using fused distance\n\nClassify the query based on the class majority of  k nearest neighbors\n\n       Forward the classified query for retrieval from the class of interest\n\nRetrieve top 20 images\n\n    Qi     Di \n\nClassify \nquery? \n\nFig. 4 Fusion technique for image identification\n\nInput layer   Hidden Layer  Output Layer \n\nw1j\n\n                                         w2j                                        wjk\n\n                                                                   Oj\nwij\n\n                                                                                                      Ok\n\n                                       wnj\n\nOj= Output value for hidden layer  \nOk= Output Value for output layer \n\nx\n\nx\n\nx\n\nx\n\nFig. 5 Multilayer perceptron (MLP)\n\n\n\nPage 15 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nSVM has searched for the maximum separating hyperplane as shown in Fig.  6. The \nsupport vectors have been shown with thicker borders.\n\nThe algorithm was implemented using sequential minimal optimization (SMO) \n(Keerthi et  al. 2001). The operating principle of SMO has been to select two Lagrange \nmultipliers as the multipliers must obey a linear equality constraint. The two selected \nLagrange multipliers jointly optimize to find the optimal value for these multipliers and \nupdates the SVM to reflect the new optimal values.\n\nDatasets used\n\nFour different datasets namely Wang dataset, Oliva and Torralba (OT-Scene) dataset, \nCorel dataset and Caltech Dataset was used for the content based image recognition \npurpose. Each of the datasets has been described in the following subsections.\n\nWang’s dataset\n\nIt consists of 10 different categories of 1000 images and was provided by Li and Wang \n(2003). Each image is of dimension 256 × 384 or 384 × 256 and each category com-\nprises of 100 images. The different classes in this dataset are Tribals, Sea Beaches, Gothic \nStructures, Buses, Dinosaur, Elephants, Flowers, Horses, Mountains and Food. A sample \ncollage for Wang’s dataset has been given in Fig. 7.\n\nOliva and torralba (OT‑Scene) dataset\n\nThis dataset comprises of 2688 images and is divided into eight different categories. The \ndataset is provided by MIT (Walia and Pal 2014). The different categories in the dataset \nare Coast and Beach (with 360 images), Open Country (with 328 images), Forest (with \n260 images), Mountain (with 308 images), Highway (with 324 images), Street (with 410 \nimages), City Centre (with 292 images) and Tall Building (with 306 images). A sample \ncollage for OT Scene dataset is given in Fig. 8.\n\nA2\n\nA1\n\nClass 1  Class 2\n\nFig. 6 Structure of hyperplane in SVM\n\n\n\n\n\nPage 16 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nCorel dataset\n\nThe dataset comprised of 10,800 images (Liu 2013). It has 80 different categories of \nimages of dimension 80 × 120 or 120 × 80. Some of the categories are art, antique, \ncyber, dinosaur, mural, castle, lights, modern, culture, drinks, feast, fitness, dolls, avia-\ntion, balloons, bob, bonsai, bus, car, cards, decoys, dish, door, easter eggs, faces etc. A \nsample collage of the Corel dataset is given in Fig.  9. The research work has used 2500 \nimages of different categories from this dataset.\n\nCaltech dataset\n\nThe dataset includes 8127 images divided into 100 different categories (Walia and Pal \n2014). Each of the categories has different number of images with a dimension of 300 x \n200. Some of the categories are accordion, airplanes, anchor, ant, Background google, \nbarrel, bass, beaver, binocular, bonsai, brain, brontosaurus, buddha, butterfly, camera, \ncannon, car side, ceiling fan, cellphone, chair etc. A sample collage for the Caltech data-\nset has been given in Fig.  10. The research work has used 2533 images of different cat-\negories from the dataset.\n\nFig. 7 Sample collage for wang dataset\n\nFig. 8 Sample collage for OT-scene dataset\n\n\n\n\n\n\n\nPage 17 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nResults and discussions\nThe experiments were executed with Matlab version 7.11.0 (R2010b) on Intel core i5 \nprocessor with 4 GB RAM under Microsoft Windows environment. Initially the misclas-\nsification rate (MR) and F1 Score for classification with fractional coefficients of slant \ntransform were compared to each other to identify the fractional coefficient with high-\nest classification value and lowest MR. Wang dataset was used for the purpose. Further, \nthe precision and recall values for classification were determined on four different pub-\nlic datasets namely, Wang dataset, OT scene dataset Caltech dataset and Corel dataset. \nHenceforth, precision and recall values of the fused architecture for classification were \ncompared against state-of-the art techniques. The precision, recall misclassification rate \n(MR) and F1 Score were represented by Eqs. 16, 17, 18 and 19.\n\n(16)Pr ecision =\nTP\n\nTP + FP\n\n(17)TPRate/Re call =\nTP\n\nTP + FN\n\n(18)MR =\nFP + FN\n\nTP + TN + FP + FN\n\nFig. 9 Sample collage for corel dataset\n\nFig. 10 Sample collage for caltech dataset\n\n\n\n\n\n\n\nPage 18 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nTrue Positive (TP) = Number of instances classified correctly. True Negative (TN) = Num-\nber of negative results created for negative instances False Positive (FP) = Number of erro-\nneous results as positive results for negative instances False Negative (FN) =  Number of \nerroneous results as negative results for positive instances.\n\nComparison of MR and F1 Score for classification with different fractional coefficients \nof slant transform has been shown in Fig. 11.\n\nIt was observed that classification with 0.024  % of the transform coefficient has the \nhighest F1 Score and lowest MR compared to the rest. Hence, it was considered as the \nfeature vector with a dimension of 36.\n\nFurther, the precision and recall values of four public datasets have been shown in \nTable 1.\n\nHenceforth, Wang dataset was considered in order to carry out classification using \nfusion technique. The classification decision obtained for Wang dataset using three dif-\nferent feature extraction techniques were fused by means of Z score normalization and \nwere compared to classification results obtained by classifying individual techniques by \n\n(19)F1score =\n2 ∗ Pr ecision ∗ Re call\n\nPr ecision + Re call\n\nF1 Score MR\n100% feature size 0.478 0.103\n50% of feature size 0.48 0.095\n25% of feature size 0.487 0.09\n12.5% of feature size 0.489 0.09\n6.25% of feature size 0.501 0.09\n3.125% of feature size 0.528 0.089\n1.5625% of feature\n\nsize 0.53 0.089\n\n0.7813% of feature\nsize 0.532 0.088\n\n0.39% of feature size 0.536 0.088\n0.195% of feature size 0.538 0.087\n0.097% of feature size 0.539 0.087\n0.048% of feature size 0.539 0.087\n0.024% of feature size 0.54 0.086\n0.012% of feature size 0.536 0.088\n0.006% of feature size 0.534 0.089\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nV\nal\n\nue\ns \n\nComparison of MR and F1 Score for \nFractional Coeffiecnts of Slant Transform \n\nFig. 11 Comparison of MR and F1 score for partial coefficients of slant transform\n\n\n\nPage 19 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nmeans of artificial neural network (ANN) classifier and support vector machine (SVM) \nclassifier respectively. The comparisons have been shown in Fig. 12.\n\nThe comparison in Fig.  12 has clearly revealed that fusion based classification has \nshown an enhanced precision of 0.12, 0.13 and 0.067 compared to classification with \nANN classifier for feature extraction with image binarization, partial transform coef-\nficients and morphological operator respectively. The recall rate for classification with \nfusion based classification was also higher by 0.134, 0.141 and 0.08 in comparison to \nclassification with ANN classifier for feature extraction with three above mentioned \ntechniques.\n\nThe fusion based classifier has revealed an improved precision rate of 0.221, 0.204 \nand 0.118 in comparison to classification with SVM classifier for feature extraction with \nimage binarization, partial transform coefficient and morphological operator respec-\ntively as in Fig. 13. The recall value for classification with fusion based classifier was also \nhigher by 0.224, 0.21 and 0.136 compared to SVM classifier which is seen in Fig. 13.\n\nTable 1 Precision and recall values for four public datasets using three feature extraction \ntechniques\n\nFeature extraction with  \nbinarization\n\nFeature extraction with fractional \ncoefficients of slant transform\n\nFeature extraction with  \nmorphological operator\n\nWang OT scene Caltech Corel Wang OT scene Caltech Corel Wang OT scene Caltech Corel\n\nPrecision 0.609 0.41 0.49 0.534 0.555 0.449 0.454 0.527 0.728 0.607 0.523 0.711\n\nRecall 0.604 0.4 0.543 0.519 0.563 0.407 0.523 0.533 0.725 0.597 0.597 0.697\n\nPrecision Recall\nFusion Based\n\nClassifier 0.748 0.765\n\nANN Classifier\n(Feature Extraction\n\nwith Image\nBinarization)\n\n0.628 0.631\n\nANN Classifier\n(Feature Extraction\n\nwith Partial Transform\nCoefficient)\n\n0.627 0.624\n\nANN Classifier\n(Feature Extraction\nwith morphological\n\noperator)\n\n0.681 0.685\n\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\nV\nal\n\nue\ns \n\nComparison of Classification Results of \nFusion Based Classifier and Artifical Neural \n\nNetwork (ANN) Classifier \n\nFig. 12 Comparison of classification with fusion based classifier and ANN classifier\n\n\n\nPage 20 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFurther, the fusion based classification results were compared to existing techniques in \nFig. 14.\n\nIt was observed that the proposed method has outclassed the existing techniques. It \nhas an increased precision rate of 0.012, 0.108, 0.109, 0.178 and 0.228 and an enhanced \nrecall rate of 0.037, 0.125, 0.126, 0.195 and 0.245 compared to the existing techniques, \nnamely, (Thepade et  al. 2014b; Yanli and Zhenxing 2012; Ramírez-Ortegón and Rojas \n2010; Liu 2013; Shaikh et al. 2013) respectively as in Fig. 14. The proposed fusion tech-\nnique was observed to have the maximum precision and recall values compared to the \nrecent techniques cited in the literature.\n\nHenceforth, content based image retrieval was carried out with individual tech-\nniques of feature extraction and was compared to fusion based technique of retrieval in \nFig. 15. The fusion based retrieval technique comprised of classification as a precursor of \nretrieval. Comparison of fusion techniques with classified query and without classified \nquery has been shown in Fig. 16 by using a sample query.\n\nThe figure has clearly divulged that fusion technique of retrieval with classified query \nhas fetched all the images of the same category to that of the query image, whereas, \nretrieval with generic or unclassified query has three images from classes other than the \nclass of query in position 2, 15 and 19 respectively.\n\nA comparison of retrieval with individual techniques of feature extraction and fusion \nbased retrieval with classified query has been given in Fig. 15.\n\nPrecision Recall\nFusion Based\n\nClassifier 0.748 0.765\n\nSVM Classifier\n(Feature Extraction\n\nwith Image\nBinarization)\n\n0.527 0.541\n\nSVM Classifier\n(Feature Extraction\n\nwith  Partial\nTransform Coefficient)\n\n0.544 0.555\n\nSVM Classifier\n(Feature Extraction\nwith  morphological\n\noperator)\n\n0.63 0.629\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\nV\nal\n\nue\ns \n\nComparison of Classification Results of \nFusion Based Classifier and Support Vector \n\nMachine (SVM) Classifier \n\nFig. 13 Comparison of classification with fusion based classifier and SVM classifier\n\n\n\nPage 21 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nResults in Fig. 15 have shown an increase of 26.3, 34.5 and 19.5 % in precision values \nand enhancement of 5.26, 6.9 and 3.9  % in recall values for the fusion based retrieval \ntechnique with classified query in comparison to retrieval with individual feature extrac-\ntion techniques. It was clearly established that the fusion based technique has outper-\nformed the individual techniques.\n\nFurther, a paired t test was conducted to validate the statistical findings and a null \nhypothesis was formulated in Hypothesis 1 (Yıldız et al. 2011).\n\nHypothesis 1: There is no significant difference among the Precision values of fusion \nbased retrieval with classified query with respect to individual retrieval techniques\n\nThe p values for the paired t test have been enlisted in Table  2. The precision value \nof fusion based retrieval with classified query was compared to that of the individual \nretrieval techniques to obtain the computed values in Table 2.\n\nThe p values have clearly indicated significant difference in precision values of the fusion \nbased retrieval technique with classified query compared to the existing techniques of \nretrieval. Hence, the null hypothesis was rejected and the proposed fusion technique with \nclassified query has been found to boost the precision values with statistical significance.\n\nFinally, the precision and recall values of the proposed fusion technique were com-\npared to existing fusion based retrieval techniques. The results have been displayed in \nFig. 17.\n\nPrecision Recall\nProposed Fusion\n\nTechnique 0.748 0.765\n\nThepade et al. (2014b) 0.736 0.728\n(Yanli Y. and\n\nZhenxing Z., 2012) 0.64 0.64\n\n(Ramírez-Ortegón, \nM.A. And Rojas R., \n\n2010) \n0.63 0.63\n\n(Liu.C, 2013) 0.57 0.57\n(Shaikh, 2013) 0.52 0.52\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\nV\nal\n\nue\ns \n\nComparison of Precision and Recall values \nfor Classification \n\nFig. 14 Comparison of classification results of proposed technique with respect to existing techniques\n\n\n\nPage 22 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nPrecision Recall\nRetrieval with feature\n\nextraction with\nBinarization\n\n49.7 9.94\n\nRetrieval with feature\nextraction with\n\nFractional Coefficient\nof Slant Transform\n\n41.5 8.3\n\nRetrieval with feature\nextraction with\nMorphological\n\nOperator\n\n56.5 11.3\n\nFusion Based retrieval\nwith classified query 76 15.2\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\n70\n\n80\n\nV\nal\n\nue\ns \n\nComparison of Precision and Recall for \nFusion Based Retrieval and Individual \n\nRetrieval Technique \n\nFig. 15 Comparison of precision and recall with fusion based retrieval technique and individual retrieval \ntechnique\n\nRetrieval with \nclassified query \n\nRetrieval with generic \nquery \n\nFig. 16 Comparison of fusion based retrieval with classified and generic query\n\n\n\n\n\n\n\nPage 23 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThe comparison in Fig.  17 has clearly established the superiority of the proposed \nfusion based retrieval technique with respect to existing fusion based technique of \nretrieval. The proposed retrieval technique has improved precision of 1.98, 3.2, 3.3, 3.49, \n17.8, 21.1 and 26.31  % and superior recall of 0.4, 0.64, 0.66, 0.7, 3.56, 4.22 and 5.26  % \ncompared to the existing fusion based techniques mentioned in Fig. 13.\n\nHenceforth, the proposed method was compared to the semantic retrieval techniques \nin Fig. 18.\n\nTable 2 Statistical validation with paired t test\n\np value Significance\n\nRetrieval by feature extraction with image transform 0.0013 Significant\n\nRetrieval by feature extraction with image binarization 0.0076 Significant\n\nRetrieval by feature extraction with morphological operator 0.0452 Significant\n\n  \n  \n\n  \n\nPrecision Recall\nProposed 76 15.2\nSubrahmanyam et al.\n\n2013 74.02 14.80\n\nShen & Wu (2013) 72.8 14.56\nBanerjee et al. (2009) 72.7 14.54\nSubrahmanyam et al.\n\n2012 72.51 14.50\n\nJalab (2011) 58.2 11.64\nHiremath & Pujari\n\n(2007) 54.9 10.98\n\nRahimi & Moghaddam\n(2013) 49.69 9.94\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\n70\n\n80\n\nV\nal\n\nue\ns \n\nComparison of Retrieval Performance with \nFusion based Techniques \n\nFig. 17 Comparison of retrieval with the proposed technique compared to state-of-the art fusion tech-\nniques\n\n\n\nPage 24 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThe comparison shown in Fig.  18 has revealed an enhanced precision rate of 0.2, 0.5 \nand 2.1  % and increased recall rate of 0.04, 0.1 and 0.6  % respectively for the proposed \nmethod with respect to the existing semantic retrieval techniques.\n\nTherefore, the research work has fulfilled the following objectives:\n\n  • It has reduced the dimension of feature vectors.\n  • It has successfully implemented fusion based method of content based image identi-\n\nfication.\n  • The research results have shown statistical significance.\n  • The research results have outperformed the results of state-of-the art techniques.\n\nConclusions\nIn depth analysis of feature extraction techniques have been exercised in this research \nwork. Three different techniques of feature extraction comprising of image binariza-\ntion, fractional coefficients of image transforms and morphological operations has been \nimplemented to extract features from the images. The extracted features with multiple \ntechniques were used for fusion based identification process. The proposed method of \nfusion has divulged statistical significance with respect to the individual techniques. \nThe retrieval technique was implemented with classification as a precursor. The classi-\nfication technique was used to classify the query image for retrieval. The method has \n\nPrecision Recall\nProposed 76 15.2\nWalia et al. (2014) 75.8 15.16\nIrtaza et al. (2013) 75.5 15.10\nAlami (2011) 73.9 14.78\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\n70\n\n80\n\nV\nal\n\nue\ns \n\nComparison of Proposed Technique with \nSemantic Retrieval Technique \n\nFig. 18 Comparison of retrieval with the proposed technique to semantic retrieval techniques\n\n\n\nPage 25 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nshown better performance compared to generic query based method of retrieval. Thus, \nthe importance of classification was established in limiting the computational overhead \nfor content based image identification. Finally, image identification with the proposed \ntechnique has surpassed the state-of-the art methods for content based image recogni-\ntion. The work may be extended towards content based image recognition in the field of \nmilitary, media, medical science, journalism, e commerce and many more.\nAuthor’s contributions\nRD and ST have designed the feature extraction techniques and the classification and retrieval techniques. RD and SG \nhave planned the statistical test and conclusion. RD wrote the manuscript. All the authors have read and approved the \nfinal manuscript.\n\nAuthor details\n1 Department of Information Technology, Xavier Institute of Social Service, Dr. Camil Bulcke Path (Purulia Road), P.O. \nBox 7, Ranchi 834001, Jharkhand, India. 2 Pimpri Chinchwad College of Engineering, Akrudi, Sec-26,Pradhikaran, Nigdi, \nPune 411033, Maharashtra, India. 3 A.K. Choudhury School of Information Technology, University of Calcutta, 92, APC \nRoad, Kolkata 700009, West Bengal, India. \n\nAcknowledgements\nThe authors acknowledge Late Dr. H.B. Kekre for encouraging the experimental process. The authors also acknowledge \nDr. Rohit Vishal Kumar and Dr. Subhajit Bhattacharya for explaining the statistical techniques.\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nReceived: 12 September 2015   Accepted: 5 November 2015\n\nReferences\nAlsmadi MK, Omar KB, Noah SA, Almarashdah I (2009) Performance comparison of multi-layer perceptron (Back Propaga-\n\ntion, Delta Rule and Perceptron) algorithms in neural networks. 2009 IEEE International Advance Computing Confer-\nence, IACC 2009, 7: pp 296–299\n\nAnnadurai S, Shanmugalakshmi R (2011) Image transforms, fundamentals of digital image processing. Dorling Kindersley \n(India) Pvt. Ltd., pp 31–66\n\nBanerjee M, Kundu MK, Maji P (2009) Content- based image retrieval using visually significant point features. Fuzzy Sets \nSyst 160:3323–3341\n\nDouze M, Jégou H, Singh H, Amsaleg L, Schmid C (2009) Evaluation of GIST descriptors for web-scale image search. In \nACM International Conference on Image and Video Retrieval, pp 0–7\n\nDubois SR, Glanz FH (1986) An autoregressive model approach to two-dimensional shape classification. IEEE Trans Pat-\ntern Anal Mach Intell 8(1):55–66\n\nDunham MH (2009) Data Mining Introductory and Advanced Topics: Pearson Education, p 127\nElAlami ME (2011) A novel image retrieval model based on the most relevant features. Knowl-Based Syst 24:23–32\nElAlami ME (2014) A new matching strategy for content based image retrieval system. Appl Soft Comput J 14:407–418\nFlickner M, Sawhney H, Niblack W, Ashley J, Huang Q, Dom B, Gorkani M et al (1995) Query by image and video content: \n\nthe QBIC system. Computer 28(9):23–32 IEEE\nGevers T, Smeulders AW (2000) PicToSeek: combining color and shape invariant features for image retrieval. IEEE Trans \n\nImage Proc Publ IEEE Signal Proc Soc 9(1):102–119\nHiremath PS, Pujari J (2007) Content based image retrieval based on color, texture and shape features using image and \n\nits complement. Int J Computer Sci Secur 1:25–35\nIrtaza A, Jaffar MA, Aleisa E, Choi TS (2013) Embedding neural networks for semantic association in content based image \n\nretrieval. Multimed Tool Appl 72(2):1911–1931\nJalab HA (2011) Image retrieval system based on color layout descriptor and Gabor filters. 2011 IEEE Conference on Open \n\nSystems. pp 32–36\nKeerthi SS, Shevade SK, Bhattacharyya C, Murthy KRK (2001) Improvements to Plattʼs SMO Algorithm for SVM classifier \n\ndesign. Neural Comput 13:637–649\nKekre HB, Thepade S (2009) Improving the performance of image retrieval using partial coefficients of transformed \n\nimage. Int J Inf Retr Ser Publ 2(1):72–79\nKekre HB, Thepade S, Maloo A (2010) Image Retrieval using Fractional Coefficients of Transformed Image using DCT and \n\nWalsh Transform‖. Int J Eng Sci Technol (IJEST ) 2(4):362–371\nKekre HB, Thepade S, Das R, Ghosh S (2013) Multilevel block truncation coding with diverse colour spaces for image clas-\n\nsification. In: IEEE-International conference on Advances in Technology and Engineering (ICATE), pp 1–7\nKim WY, Kim YS (2000) Region-based shape descriptor using Zernike moments. Sig Process Image Commun 16:95–102\nLi J, Wang JZ (2003) Automatic linguistic indexing of pictures by a statistical modeling approach. IEEE Trans Pattern Anal \n\nMach Intell 25:1075–1088\nLiu C (2013) A new finger vein feature extraction algorithm, In: IEEE 6th. International Congress on Image and Signal \n\nProcessing (CISP), pp 395–399\n\n\n\n\n\nPage 26 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nLuo H, Lina Y, Haoliang Y, Yuan YT (2013) Dimension reduction with randomized anisotropic transform for hyperspectral \nimage classification. In: 2013 IEEE International Conference on Cybernetics, CYBCONF 2013, pp 156–161\n\nMadireddy RM, Gottumukkala PSV, Murthy PD, Chittipothula S (2014) A modified shape context method for shape based \nobject retrieval. SpringerPlus 3:674. doi:10.1186/2193-1801-3-674\n\nMehtre BM, Kankanhalli MS, Lee Wing Foon (1997) Shape measures for content based image retrieval: a comparison. Inf \nProcess Manage 33:319–337\n\nMokhtarian F, Mackworth AK (1992) A theory of multiscale, curvature-based shape representation for planar curves. IEEE \nTrans Pattern Anal Mach Intell 14:789–805\n\nOtsu N (1979) A threshold selection method from gray- level histogram IEEE transactions on systems. Man Cybern \n9:62–66\n\nPrakash O, Khare M, Srivastava RK, Khare A (2013) Multiclass image classification using multiscale biorthogonal wavelet \ntransform, In: IEEE Second International Conference on Information Processing (ICIIP), pp 131–135\n\nPratt W, Chen WH, Welch L (1974) Slant transform image coding. IEEE Transactions on Communications 22\nRahimi M and Moghaddam ME (2013) A content based image retrieval system based on color ton distributed descrip-\n\ntors. Signal Image Video Process 9(3):691–704. http://dx.doi.org/10.1007/s11760-013-0506-6\nRamírez-Ortegón MA and Rojas R (2010) Unsupervised evaluation methods based on local gray-intensity variances \n\nfor binarization of historical documents. Proceedings—International Conference on Pattern Recognition, pp \n2029–2032\n\nRaventós A, Quijada R, Torres L, Tarrés F (2015) Automatic summarization of soccer highlights using audio- visual descrip-\ntors. SpringerPlus 4:301. doi:10.1186/s40064-015-1065-9\n\nShaikh SH, Maiti AK, Chaki N (2013) A new image binarization method using iterative partitioning. Mach Vis Appl \n24(2):337–350\n\nShen GL and Wu XJ (2013) Content based image retrieval by combining color texture and CENTRIST, In: IEEE international \nworkshop on signal processing, vol 1, pp 1–4\n\nSridhar S (2011) Image features representation and description digital image processing. India Oxford University Press, \nNew Delhi, pp 483–486\n\nSubrahmanyam M, Maheshwari RP, Balasubramanian R (2012) Expert system design using wavelet and color vocabulary \ntrees for image retrieval. Expert Syst Appl 39:5104–5114\n\nSubrahmanyam M, Wu QMJ, Maheshwari RP, Balasubramanian R (2013) Modified color motif co- occurrence matrix for \nimage indexing and retrieval. Comput Electr Eng 39:762–774\n\nThepade S, Das R, Ghosh S (2013a) Advances in computing, communication and control. Image classification \nusing advanced block truncation coding with ternary image maps, vol 361. Springer, Berlin, pp 500–509. \ndoi:10.1007/978-3-642-36321-4_48\n\nThepade S, Das R, Ghosh S (2013b) Performance comparison of feature vector extraction techniques in RGB color space \nusing block truncation coding or content based image classification with discrete classifiers. In: India Conference \n(INDICON), IEEE, pp 1–6. doi: 10.1109/INDCON.2013.6726053\n\nThepade S, Das R, Ghosh S (2014a) A novel feature extraction technique using binarization of bit planes for content \nbased image classification. J Eng. doi:10.1155/2014/439218\n\nThepade S, Das R, Ghosh S (2014b) Feature extraction with ordered mean values for content based image classification. \nAdv Comput Eng 2014. doi:10.1155/2014/454876\n\nValizadeh M, Armanfard N, Komeili M, Kabir E (2009) A novel hybrid algorithm for binarization of badly illuminated docu-\nment images. 2009 14th International CSI Computer Conference, CSICC 2009, pp 121–126\n\nWalia E, Pal A (2014) Fusion framework for effective color image retrieval. J Vis Commun Image Represent \n25(6):1335–1348\n\nWalia E, Vesal S, Pal A (2014) An Effective and Fast Hybrid Framework for Color Image Retrieval. Sens Imaging 15:93. doi: \n10.1007/s11220-014-0093-9\n\nWang X, Bian W, Tao D (2013) Grassmannian regularized structured multi-view embedding for image classification. IEEE \nTrans Image Process 22(7):2646–2660\n\nYanli Y and Zhenxing Z (2012) A novel local threshold binarization method for QR image, In: IET International Conference \non Automatic Control and Artificial Intelligence (ACAI), pp 224–227\n\nYıldız OT, Aslan O, Alpaydın E (2011) Multivariate statistical tests for comparing classi-fication algorithms. Lect Notes \nComp Sci, vol 6683, Springer, Berlin, pp 1–15\n\nYue J, Li Z, Liu L, Fu Z (2011) Content-based image retrieval using color and texture fused features. Math Comput Model \n54:1121–1127\n\nZhang D, Lu G (2003) A comparative study of curvature scale space and Fourier descriptors for shape- based image \nretrieval. J Vis Commun Image Represent 14:39–57\n\nZhang D, Lu G (2004) Review of shape representation and description techniques. Pattern Recogn 37:1–19\nZhao C, Li X, Cang Y (2015) Bisecting k-means clustering based face recognition using block-based bag of words model. \n\nOptik Int J Light Electron Optics 126(19):1761–1766\nZhu Q, Shyu M-L (2015) sparse linear integration of content and context modalities for semantic concept retrieval. IEEE \n\nTrans Emerg Top Comput 3(2):152–160\n\nhttp://dx.doi.org/10.1186/2193-1801-3-674\nhttp://dx.doi.org/10.1007/s11760-013-0506-6\nhttp://dx.doi.org/10.1186/s40064-015-1065-9\nhttp://dx.doi.org/10.1007/978-3-642-36321-4_48\nhttp://dx.doi.org/10.1109/INDCON.2013.6726053\nhttp://dx.doi.org/10.1155/2014/439218\nhttp://dx.doi.org/10.1155/2014/454876\nhttp://dx.doi.org/10.1007/s11220-014-0093-9\n\n\tMulti technique amalgamation for enhanced information identification with content based image data\n\tAbstract \n\tBackground\n\tFeature extraction using image transform\n\tImage binarization techniques for feature extraction\n\tUse of morphological operators for feature extraction\n\tFusion methodologies and multi technique feature extraction\n\n\tMethods\n\tFeature extraction with image binarization\n\tFeature extraction using image transform\n\tFeature extraction with morphological operator\n\tSimilarity measures\n\tFusion based classifier\n\tArtificial neural network (ANN) classifier\n\tSupport vector machine (SVM) classifier\n\tDatasets used\n\tWang’s dataset\n\tOliva and torralba (OT-Scene) dataset\n\tCorel dataset\n\tCaltech dataset\n\n\tResults and discussions\n\tConclusions\n\tAuthor’s contributions\n\tReferences\n\n\n\n\n",
      "metadata_storage_path": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9saWJyYXJ5L3M0MDA2NC0wMTUtMTUxNS00LnBkZg2",
      "authors": [
        "Rik Das1",
        "Sudeep Thepade2",
        "Saurav Ghosh3",
        "Raventós",
        "Madireddy",
        "Oliva Torralba",
        "R E S",
        "A R C H",
        "Das",
        "rikdas78",
        "Camil",
        "Wang",
        "Annadurai",
        "Shanmugalakshmi",
        "Kekre",
        "Prakash",
        "Luo",
        "Valizadeh",
        "Thepade",
        "Liu",
        "Yanli",
        "Zhenxing",
        "Ramírez",
        "Ortegón",
        "Rojas",
        "Otsu",
        "Shaikh",
        "Flickner",
        "Gevers",
        "Smeulders",
        "Mehtre",
        "Zhang",
        "Lu",
        "ary",
        "Fourier",
        "Mokhtarian",
        "Mackworth",
        "Dubois",
        "Glanz",
        "Kim",
        "Zhu",
        "Shyu",
        "ElAlami",
        "Gabor",
        "Hiremath",
        "Pujari",
        "Yue",
        "Banerjee",
        "Jalab",
        "Shen",
        "Wu",
        "Irtaza",
        "Rahimi",
        "Moghaddam",
        "Subrahmanyam",
        "Walia",
        "Douze",
        "Zhao",
        "Txjixif",
        "Pratt",
        "Sridhar",
        "xmean",
        "xhi",
        "Qi",
        "Alsmadi",
        "Euclidian",
        "Qi     Di",
        "Keerthi",
        "Oliva",
        "Torralba",
        "Li",
        "torralba",
        "Pal",
        "MR. Wang",
        "Ramírez-Ortegón",
        "Yıldız",
        "Yanli Y.",
        "Zhenxing Z.",
        "Rojas R."
      ],
      "institutions": [
        "Corel",
        "Caltech",
        "Commons",
        "SpringerPlus",
        "gmail.com",
        "Department of Information",
        "Xavier Institute",
        "ElAlami",
        "MCMCM",
        "GIST",
        "Otsu",
        "BoW",
        "xloF.",
        "stdev",
        "Dcityblock",
        "DMSE",
        "City-Block",
        "MSE",
        "wij",
        "wnj",
        "SVM",
        "SMO",
        "MIT",
        "google",
        "Intel",
        "Microsoft",
        "TP",
        "FP",
        "FN",
        "caltech",
        "ANN",
        "Partial Transform"
      ],
      "key_phrases": [
        "multi tech- nique feature extraction process",
        "content based image data Rik Das1",
        "A R C H Das",
        "Creative Commons Attribution 4.0 International License",
        "traditional text based annotation technique",
        "Dr. Camil Bulcke Path",
        "customary text based process",
        "Diverse low level features",
        "content based feature extraction",
        "content based image identification",
        "Creative Commons license",
        "robust feature vectors",
        "Multi technique amalgamation",
        "R E S",
        "low cost storage",
        "digital photo-capture devices",
        "new research challenges",
        "four public datasets",
        "test Open Access",
        "P.O. Box",
        "digital image acquisition",
        "huge image repositories",
        "gigantic image datasets",
        "image capturing devices",
        "OT Scene) Dataset",
        "three different techniques",
        "exist- ing techniques",
        "original author(s",
        "Semantically analogous images",
        "image identification technique",
        "enhanced information identification",
        "manual process",
        "image content",
        "Diverse applications",
        "Raventós",
        "image recognition",
        "image binarization",
        "image transform",
        "Image classification",
        "Image retrieval",
        "image information",
        "author information",
        "Corel Dataset",
        "Caltech Dataset",
        "Conventional techniques",
        "art techniques",
        "Sudeep Thepade2",
        "Saurav Ghosh3",
        "Recent years",
        "computer power",
        "accessible internet",
        "Efficient indexing",
        "computer vision",
        "machine learning",
        "Automatic derivation",
        "severe limitations",
        "aforesaid limitations",
        "resourceful foundation",
        "social media",
        "morphological operator",
        "Oliva Torralba",
        "recognition rate",
        "Classification result",
        "average increase",
        "retrieval result",
        "Slant transform",
        "unrestricted use",
        "appropriate credit",
        "Xavier Institute",
        "Social Service",
        "Purulia Road",
        "Full list",
        "effective alternative",
        "meaningful information",
        "Information Technology",
        "Wang Dataset",
        "decision fusion",
        "Background",
        "ubiquity",
        "mass",
        "popularity",
        "Madireddy",
        "Walia",
        "words",
        "mapping",
        "perception",
        "vocabulary",
        "person",
        "nature",
        "Abstract",
        "proliferation",
        "areas",
        "biomedicine",
        "military",
        "commerce",
        "education",
        "means",
        "success",
        "paper",
        "Precision",
        "state",
        "Otsu",
        "threshold",
        "article",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "link",
        "changes",
        "SpringerPlus",
        "DOI",
        "Correspondence",
        "1 Department",
        "Ranchi",
        "Jharkhand",
        "India",
        "end",
        "crossmark",
        "crossref",
        "org",
        "Page",
        "26Das",
        "competence",
        "color",
        "content based image recog",
        "binarization based feature extraction",
        "content based image classification",
        "feature extraction Feature extraction",
        "Z score normalization",
        "orthogonal unitary matrices",
        "Appropriate threshold selection",
        "fusion based method",
        "single feature extrac",
        "fractional energy coefficient",
        "fifteen fractional coefficients",
        "high dimensional data",
        "main contribu- tion",
        "multi-technique feature extraction",
        "efficient feature extraction",
        "feature extrac- tion",
        "efficient image binarization",
        "feature vector dimension",
        "image trans- formation",
        "multi technique fusion",
        "image binariza- tion",
        "Image binarization techniques",
        "feature vectors",
        "feature spaces",
        "image data",
        "energy spectrum",
        "subband coefficients",
        "novel techniques",
        "Diverse techniques",
        "image identification",
        "image elements",
        "image patterns",
        "seven image",
        "recognition decision",
        "hybrid architecture",
        "fusion architecture",
        "Statistical validation",
        "trans- forms",
        "morphological operators",
        "archi- tecture",
        "four subsections",
        "earlier works",
        "four topics",
        "basis functions",
        "one representation",
        "two aspects",
        "critical components",
        "compact structure",
        "cient storage",
        "radical reduction",
        "multiple scales",
        "tropic transform",
        "various factors",
        "uneven illumination",
        "research objectives",
        "research results",
        "transformation process",
        "aforesaid properties",
        "basis images",
        "Original images",
        "shape",
        "texture",
        "number",
        "features",
        "morphology",
        "Comparison",
        "retrieval",
        "correlation",
        "connection",
        "contemporary",
        "Change",
        "domain",
        "set",
        "series",
        "Annadurai",
        "Shanmugalakshmi",
        "advantages",
        "waveforms",
        "use",
        "analysis",
        "transmission",
        "Kekre",
        "Thepade",
        "subbands",
        "Prakash",
        "Luo",
        "execution",
        "inadequate",
        "Ramírez- Ortegón",
        "Gradient vector flow fields",
        "multi technique feature extraction",
        "fusion based image identification",
        "superior prediction accuracy",
        "feature level correlations",
        "image signature extraction",
        "feature extraction technique",
        "Popular contour-based descriptors",
        "Enhanced classification results",
        "curvature scale space",
        "feature selection technique",
        "region- based descriptors",
        "3 D color histogram",
        "diverse extraction techniques",
        "Two different categorization",
        "global threshold selection",
        "shape feature extraction",
        "multilevel mean threshold",
        "image binarization techniques",
        "local threshold selection",
        "retrieval process complexity",
        "feature vector",
        "identification rate",
        "Image Content",
        "different techniques",
        "shape descriptors",
        "semantic retrieval",
        "Local descriptors",
        "local decisions",
        "shape information",
        "region-based descriptors",
        "information fusion",
        "early fusion",
        "late fusion",
        "hybrid fusion",
        "intermediate fusion",
        "adverse effect",
        "Contemporary literatures",
        "unfavourable influences",
        "binarized images",
        "standard deviation",
        "gray values",
        "Commercial viability",
        "existing literatures",
        "ary lines",
        "Fourier descriptor",
        "chain codes",
        "complex shapes",
        "Information recognition",
        "Recent studies",
        "four classes",
        "higher dimensions",
        "joint model",
        "genetic algorithm",
        "optimum boundaries",
        "numerical intervals",
        "memory con",
        "filter responses",
        "distinct features",
        "concentrated features",
        "multiple features",
        "Fusion methodologies",
        "rate learner",
        "Gabor filters",
        "Color moments",
        "texture features",
        "contrast",
        "computation",
        "Valizadeh",
        "average",
        "advantage",
        "spread",
        "calculation",
        "Liu",
        "Yanli",
        "Zhenxing",
        "Rojas",
        "Shaikh",
        "Use",
        "systems",
        "Flickner",
        "PicToSeek",
        "Gevers",
        "Smeulders",
        "Mehtre",
        "Zhang",
        "Emphasize",
        "Mokhtarian",
        "Mackworth",
        "Dubois",
        "Glanz",
        "area",
        "object",
        "Kim",
        "single",
        "input",
        "sepa",
        "combiner",
        "scalability",
        "comparison",
        "mix",
        "Zhu",
        "Shyu",
        "sumption",
        "ElAlami",
        "back propagation neural net- work",
        "fuzzy set theoretic approach",
        "angular radial transform descriptor",
        "artificial neural network",
        "spatial structure descriptors",
        "spatial orientation tree",
        "lesser computational overhead",
        "neural network architecture",
        "correct semantic retrieval",
        "Higher retrieval results",
        "semantic retrieval results",
        "local edge bins",
        "non directional edge",
        "HSV color space",
        "significant point features",
        "Three different techniques",
        "color layout descriptor",
        "greater feature dimension",
        "content-based image retrieval",
        "tex- ture features",
        "edge histogram descriptor",
        "inter-class feature extraction",
        "Gabor texture descriptor",
        "fusion based classifier",
        "invariant color features",
        "invariant moments",
        "retrieval decisions",
        "retrieval performance",
        "retrieval rate",
        "retrieval purpose",
        "color histogram",
        "GIST descriptor",
        "horizontal edge",
        "Feature vectors",
        "popular feature",
        "32 feature maps",
        "feature values",
        "feature size",
        "color co",
        "color motif",
        "color moment",
        "occurrence matrix",
        "Recognition process",
        "image signatures",
        "Multi view",
        "Wavelet packets",
        "Eigen values",
        "sub repository",
        "main image",
        "right neighbourhood",
        "query image",
        "scan pattern",
        "equal weights",
        "Precision values",
        "individual techniques",
        "Six semantics",
        "sub- image",
        "tion techniques",
        "same size",
        "16 average value",
        "other hand",
        "morphological technique",
        "vertical edge",
        "shape features",
        "EHD) features",
        "edge images",
        "45° edge",
        "135° edge",
        "Hiremath",
        "Pujari",
        "Yue",
        "points",
        "similarity",
        "Banerjee",
        "Jalab",
        "increased",
        "Shen",
        "Wu",
        "authors",
        "Irtaza",
        "kind",
        "training",
        "response",
        "intra-class",
        "Rahimi",
        "Moghaddam",
        "CCM",
        "difference",
        "pixels",
        "DBPSP",
        "ANN",
        "Subrahmanyam",
        "MCMCM",
        "colour",
        "CMs",
        "sub-image",
        "unique",
        "Methods",
        "4 scales",
        "8 orientations",
        "Douze",
        "region",
        "Red Component Green Component Blue Component",
        "popular global threshold selection method",
        "local threshold selec- tion method",
        "image feature point extraction",
        "different gray level pixels",
        "four subsec- tions",
        "three color components",
        "necessary image information",
        "squared Euclidian distance",
        "document image binarzation",
        "Conventional BoW model",
        "alloca- tion",
        "feature extraction",
        "level histogram",
        "thresholding method",
        "parametric method",
        "optimal threshold",
        "words model",
        "information losses",
        "fifth subsection",
        "test images",
        "binari- zation",
        "dant details",
        "two classes",
        "foreground pixels",
        "background pixels",
        "class variance",
        "The separation",
        "Comprehensive investigation",
        "class probabilities",
        "Total variance",
        "two terms",
        "SIFT algorithm",
        "SIFT descriptors",
        "cluster members",
        "codebook generation",
        "final step",
        "huge computational",
        "BoW generation",
        "class means",
        "tering process",
        "descriptor dimension",
        "classification",
        "techniques",
        "methods",
        "fusion",
        "architecture",
        "following",
        "description",
        "datasets",
        "Fig.",
        "way",
        "combined",
        "sum",
        "variances",
        "Eq.",
        "Eqs.",
        "q2",
        "effect",
        "contributions",
        "bag",
        "Zhao",
        "size",
        "problem",
        "omissions",
        "stability",
        "clustering",
        "codewords",
        "overhead",
        "∑",
        "σ",
        "N*N feature vector Feature Vector Dimension Reduction",
        "Compute binary image maps",
        "N unitary slant matrix",
        "Blue (B) color component",
        "N × N matrix",
        "higher intensity feature vectors",
        "two dimensional slant transform",
        "N × 1 vector",
        "higher intensity group",
        "three different color",
        "discrete Fourier transform",
        "One dimensional transform",
        "lower intensity group",
        "Slant transform matrices",
        "frequency domain information",
        "local threshold value",
        "spatial domain information",
        "efficient algorithm design",
        "insignificant transform coefficients",
        "drastic reduction",
        "smaller dimension",
        "Energy compaction property",
        "unitary transform",
        "spatial information",
        "matrix multiplication",
        "two groups",
        "orthogonal transform",
        "inverse transform",
        "transform operation",
        "grey values",
        "two codewords",
        "large fraction",
        "faster execution",
        "tion operation",
        "computational overhead",
        "energy conservation",
        "signal energy",
        "significant role",
        "Partial Coefficients",
        "sequential transformations",
        "average energy",
        "test image",
        "image features",
        "monochrome image",
        "image quality",
        "image line",
        "components R",
        "tain operations",
        "real components",
        "pixel values",
        "average coding",
        "Transforms",
        "approach",
        "cluster",
        "mean",
        "codebook",
        "Tx",
        "Method",
        "End",
        "xhi",
        "cer",
        "capacity",
        "example",
        "characteristic",
        "Green",
        "8 bits",
        "1 bit",
        "images",
        "24–2 bits",
        "Pratt",
        "less",
        "column",
        "row",
        "forward",
        "Eqs",
        "The energy compaction property",
        "gray scale opening operation",
        "lower intensity feature vector",
        "higher intensity feature vector",
        "entire feature vector set",
        "feature vector extraction process",
        "shorter dis- tance",
        "feature vector formation",
        "highest classification result",
        "Blue color components",
        "feature vector database",
        "hat transform technique",
        "significant image information",
        "image transform End",
        "image similarity measures",
        "Higher similarity",
        "database images",
        "working process",
        "Similar process",
        "classification results",
        "hat transformation",
        "slant transform",
        "complete set",
        "partial coefficient",
        "fractional coefficient",
        "small patch",
        "Human perception",
        "shape context",
        "point correspondences",
        "considerable contribution",
        "closing operations",
        "bright peaks",
        "tophat operator",
        "code- book",
        "noteworthy image",
        "original image",
        "visual words",
        "series form",
        "one cluster",
        "codebook size",
        "peak detector",
        "Red, Green",
        "coefficients",
        "2. Peak",
        "algorithm",
        "query",
        "block",
        "12 elements",
        "dimension",
        "variant",
        "Sridhar",
        "Exit",
        "BoW",
        "methodology",
        "clusters",
        "Determination",
        "distance",
        "Dunham",
        "σ   Red Component Green Component Blue Component",
        "Three different distance measures",
        "forward Neural Network Classifier",
        "support vector machine",
        "nearest neigh- bors",
        "city block distance",
        "MSE) distance metric",
        "Canberra Distance measure",
        "fusion based classification",
        "first hidden layer",
        "input feature vectors",
        "mean squared error",
        "Data standardization technique",
        "back propagation technique",
        "supervised learning procedure",
        "multi layer perceptron",
        "database image T",
        "query image Q",
        "color component",
        "SVM) classifier",
        "classifier types",
        "network prediction",
        "Euclidian distance",
        "final distance",
        "classification decision",
        "Multilayer Perceptron",
        "input layer",
        "input features",
        "input units",
        "classified query",
        "clas- sifier",
        "following sections",
        "tophat transform",
        "morphological operation",
        "xloF.V.",
        "common range",
        "Top-Hat operator",
        "weighted sum",
        "appropriate output",
        "sification performance",
        "training tuple",
        "backward direction",
        "normalization process",
        "classified image",
        "higher values",
        "precision values",
        "stdev xhi",
        "qp background",
        "xlomeanxloVF xlo",
        "greater effect",
        "individual distances",
        "class majority",
        "similar images",
        "top 20 images",
        "target value",
        "stdev xlo",
        "foreground",
        "xmean",
        "xhimeanxhiVF",
        "Qi",
        "attributes",
        "possibilities",
        "Dcityblock",
        "Deuclidian",
        "DMSE",
        "distn",
        "disti",
        "weights",
        "interest",
        "Ranking",
        "feed",
        "MLP",
        "Alsmadi",
        "optimization",
        "modifications",
        "The",
        "µ",
        "15",
        "Hidden Layer  Output Layer w1j                                         w2j                                        wjk",
        "partial Transform coefficients Feature Extraction",
        "content based image recognition",
        "Morphological Operator Rank Images",
        "top 20 images Feature Extraction",
        "image identification Input layer",
        "Binarization Feature Extraction",
        "Z score Normalization",
        "sequential minimal optimization",
        "linear equality constraint",
        "feature vector dataset",
        "Support vector machine",
        "maximum separating hyperplane",
        "new optimal values",
        "eight different categories",
        "Optimal separating hyperplane",
        "appropriate nonlinear mapping",
        "OT‑Scene) dataset",
        "Distance Rank Images",
        "Four different datasets",
        "two different classes",
        "original training data",
        "10 different categories",
        "subsequent layer",
        "final layer",
        "Weighted output",
        "Output value",
        "support vectors",
        "two Lagrange",
        "new dimension",
        "fused distance",
        "bias node",
        "forward property",
        "nearest neighbors",
        "Fusion technique",
        "Multilayer perceptron",
        "thicker borders",
        "operating principle",
        "following subsections",
        "Sea Beaches",
        "sample collage",
        "OT-Scene) dataset",
        "Corel dataset",
        "higher dimension",
        "high dimension",
        "Classify query",
        "class labels",
        "The dataset",
        "Lagrange multipliers",
        "Wang dataset",
        "Qi     Di",
        "1000 images",
        "100 images",
        "2688 images",
        "summation",
        "utes",
        "nodes",
        "half",
        "inputs",
        "processing",
        "Dcanberra",
        "City-Block",
        "Euclidian",
        "MSE",
        "distances",
        "Oj",
        "wij",
        "wnj",
        "SMO",
        "Keerthi",
        "Oliva",
        "Torralba",
        "purpose",
        "category",
        "prises",
        "Tribals",
        "Gothic",
        "Structures",
        "Buses",
        "Dinosaur",
        "Elephants",
        "Flowers",
        "Horses",
        "Mountains",
        "Food",
        "MIT",
        "Pal",
        "Intel core i5 processor",
        "ferent feature extraction techniques",
        "lowest MR. Wang dataset",
        "Microsoft Windows environment",
        "Caltech data- set",
        "OT Scene dataset",
        "recall misclassification rate",
        "highest F1 Score",
        "est classification value",
        "A sample collage",
        "different fractional coefficients",
        "True Positive (TP",
        "True Negative",
        "recall values",
        "Caltech dataset",
        "Open Country",
        "City Centre",
        "Tall Building",
        "Class 1  Class 2",
        "Large Margin",
        "avia- tion",
        "easter eggs",
        "research work",
        "Background google",
        "ceiling fan",
        "nan Zins Cavern",
        "Matlab version",
        "4 GB RAM",
        "fused architecture",
        "Pr ecision",
        "TPRate/Re call",
        "RE-USE metani",
        "SENTES MASa",
        "transform coefficient",
        "fusion technique",
        "OT-scene dataset",
        "neous results",
        "positive results",
        "positive instances",
        "different categories",
        "negative results",
        "negative instances",
        "car side",
        "different number",
        "Coast",
        "Beach",
        "360 images",
        "328 images",
        "Forest",
        "260 images",
        "Mountain",
        "308 images",
        "Highway",
        "324 images",
        "Street",
        "292 images",
        "306 images",
        "A2",
        "A1",
        "6 Structure",
        "hyperplane",
        "SVM",
        "10,800 images",
        "cyber",
        "dinosaur",
        "mural",
        "castle",
        "lights",
        "culture",
        "drinks",
        "feast",
        "fitness",
        "dolls",
        "balloons",
        "bob",
        "bonsai",
        "bus",
        "cards",
        "decoys",
        "dish",
        "door",
        "faces",
        "8127 images",
        "300 x",
        "accordion",
        "airplanes",
        "anchor",
        "barrel",
        "bass",
        "beaver",
        "binocular",
        "brain",
        "brontosaurus",
        "buddha",
        "butterfly",
        "camera",
        "cannon",
        "cellphone",
        "chair",
        "2533 images",
        "discussions",
        "experiments",
        "R2010b",
        "precision",
        "FP",
        "FN",
        "Table",
        "order",
        "Caltech Corel Wang OT scene",
        "morphological operator Wang OT scene",
        "three feature extraction techniques Feature extraction",
        "content based image retrieval",
        "fusion based classification results",
        "Artifical Neural Network",
        "Ramírez-Ortegón",
        "fusion tech- nique",
        "partial transform coefficient",
        "binarization Feature extraction",
        "F1 Score MR",
        "partial coefficients",
        "existing techniques",
        "recent techniques",
        "Slant Transform",
        "ANN) classifier",
        "ANN classifier",
        "100% feature size",
        "Re call",
        "Fractional Coeffiecnts",
        "fractional coefficients",
        "recall rate",
        "recall value",
        "SVM classifier",
        "enhanced precision",
        "precision rate",
        "maximum precision",
        "Precision Recall",
        "F1score",
        "Table 1",
        "method",
        "literature",
        "0.0",
        "0.1",
        "individual feature extrac- tion techniques",
        "Precision Recall Fusion Based",
        "fusion based retrieval technique",
        "Support Vector Machine",
        "Partial Transform Coefficient",
        "paired t test",
        "Yıldız",
        "fusion based technique",
        "Fusion Based Classifier",
        "individual retrieval techniques",
        "fusion techniques",
        "Yanli Y.",
        "Zhenxing Z.",
        "Fractional Coefficient",
        "existing fusion",
        "SVM Classifier",
        "SVM) Classifier",
        "same category",
        "statistical findings",
        "significant difference",
        "statistical significance",
        "M.A.",
        "Rojas R.",
        "Liu.C",
        "sample query",
        "precision value",
        "p values",
        "three images",
        "Image Binarization",
        "null hypothesis",
        "precursor",
        "figure",
        "generic",
        "classes",
        "position",
        "increase",
        "enhancement",
        "respect",
        "26.",
        "4.5",
        "5.26",
        "6.9",
        "existing fusion based techniques",
        "existing semantic retrieval techniques",
        "content based image",
        "individual retrieval technique",
        "feature extraction techniques",
        "enhanced precision rate",
        "generic query",
        "Retrieval Performance",
        "following objectives",
        "identi- fication",
        "depth analysis",
        "superior recall",
        "superiority",
        "value",
        "Conclusions",
        "0.5"
      ],
      "merged_content": "\nMulti technique amalgamation \nfor enhanced information identification \nwith content based image data\nRik Das1*, Sudeep Thepade2 and Saurav Ghosh3\n\nBackground\nRecent years have witnessed the digital photo-capture devices as a ubiquity for the com-\nmon mass (Raventós et al. 2015). The low cost storage, increasing computer power and \never accessible internet have kindled the popularity of digital image acquisition. Efficient \nindexing and identification of image data from these huge image repositories has nur-\ntured new research challenges in computer vision and machine learning (Madireddy \net  al. 2014). Automatic derivation of sematically-meaningful information from image \ncontent has become imperative as the traditional text based annotation technique has \nrevealed severe limitations to fetch information from the gigantic image datasets (Walia \net  al. 2014). Conventional techniques of image recognition were based on text or key-\nwords based mapping of images which had limited image information. It was dependent \non the perception and vocabulary of the person performing the annotation. The manual \nprocess was highly time consuming and slow in nature. The aforesaid limitations have \n\nAbstract \nImage data has emerged as a resourceful foundation for information with proliferation \nof image capturing devices and social media. Diverse applications of images in areas \nincluding biomedicine, military, commerce, education have resulted in huge image \nrepositories. Semantically analogous images can be fruitfully recognized by means of \ncontent based image identification. However, the success of the technique has been \nlargely dependent on extraction of robust feature vectors from the image content. The \npaper has introduced three different techniques of content based feature extraction \nbased on image binarization, image transform and morphological operator respec-\ntively. The techniques were tested with four public datasets namely, Wang Dataset, \nOliva Torralba (OT Scene) Dataset, Corel Dataset and Caltech Dataset. The multi tech-\nnique feature extraction process was further integrated for decision fusion of image \nidentification to boost up the recognition rate. Classification result with the proposed \ntechnique has shown an average increase of 14.5 % in Precision compared to the exist-\ning techniques and the retrieval result with the introduced technique has shown an \naverage increase of 6.54 % in Precision over state-of-the art techniques.\n\nKeywords: Image classification, Image retrieval, Otsu’s threshold, Slant transform, \nMorphological operator, Fusion, t test\n\nOpen Access\n\n© 2015 Das et al. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate \nif changes were made.\n\nR E S E A R C H\n\nDas et al. SpringerPlus  (2015) 4:749 \nDOI 10.1186/s40064-015-1515-4\n\n*Correspondence:  rikdas78@\ngmail.com \n1 Department of Information \nTechnology, Xavier Institute \nof Social Service, Dr. Camil \nBulcke Path (Purulia Road), \nP.O. Box 7, Ranchi 834001, \nJharkhand, India\nFull list of author information \nis available at the end of the \narticle\n\n  \n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40064-015-1515-4&domain=pdf\n\n\nPage 2 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nbeen effectively handled with content based image identification which has been exer-\ncised as an effective alternative to the customary text based process (Wang et al. 2013). \nThe competence of the content based image identification technique has been depend-\nent on the extraction of robust feature vectors. Diverse low level features namely, color, \nshape, texture etc. have constituted the process of feature extraction. However, an image \ncomprises of number of features which can hardly be defined by a single feature extrac-\ntion technique (Walia et al. 2014). Therefore, three different techniques of feature extrac-\ntion namely, feature extraction with image transform, feature extraction with image \nmorphology and feature extraction with image binarization have been proposed in this \npaper to leverage fusion of multi-technique feature extraction. The recognition decision \nof three different techniques was further integrated by means of Z score normalization \nto create hybrid architecture for content based image identification. The main contribu-\ntion of the paper has been to propose fusion architecture for content based image recog-\nnition with novel techniques of feature extraction for enhanced recognition rate.\n\nThe research objectives have been enlisted as follows:\n\n  • Reducing the dimension of feature vectors.\n  • Successfully implementing fusion based method of content based image identifica-\n\ntion.\n  • Statistical validation of research results.\n  • Comparison of research results with state-of-the art techniques.\n\nThree different techniques of feature extraction using image binarization, image trans-\nforms and morphological operators have been combined to develop fusion based archi-\ntecture for content based image classification and retrieval. Hence, it is in correlation with \nresearch on binarization based feature extraction, transform based feature extraction and \nmorphology based feature extraction from images. It is also in connection with research \non multi technique fusion for content based image identification. Therefore, the following \nfour subsections have reviewed some contemporary and earlier works on these four topics.\n\nFeature extraction using image transform\n\nChange of domain of the image elements has been carried out by using image trans-\nformation to represent the image by a set of energy spectrum. An image can be repre-\nsented as series of basis images which can be formed by extrapolating the image into a \nseries of basis functions (Annadurai and Shanmugalakshmi 2011). The basis images have \nbeen populated by using orthogonal unitary matrices as image transformation opera-\ntor. This image transformation from one representation to another has advantages in \ntwo aspects. An image can be expanded in the form of a series of waveforms with the \nuse of image transforms. The transformation process has been helpful to differentiate \nthe critical components of image patterns and in making them directly accessible for \nanalysis. Moreover, the transformed image data has a compact structure useful for effi-\ncient storage and transmission. The aforesaid properties of image transforms facilitate \nradical reduction of feature vector dimension to be extracted from the images. Diverse \ntechniques of feature extraction has been proposed by exploiting the properties of image \ntransforms to extract features from images using fractional energy coefficient (Kekre and \n\n\n\nPage 3 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThepade 2009; Kekre et  al. 2010). The techniques have considered seven image trans-\nforms and fifteen fractional coefficients sets for efficient feature extraction. Original \nimages were divided into subbands by using multiple scales Biorthogonal wavelet trans-\nform and the subband coefficients were used as features for image classification (Prakash \net al. 2013). The feature spaces were reduced by applying Isomap-Hysime random aniso-\ntropic transform for classification of high dimensional data (Luo et al. 2013).\n\nImage binarization techniques for feature extraction\n\nFeature extraction from images has been largely carried out by means of image binariza-\ntion. Appropriate threshold selection has been imperative for execution of efficient image \nbinarization. Nevertheless, various factors including uneven illumination, inadequate \ncontrast etc. can have adverse effect on threshold computation (Valizadeh et  al. 2009). \nContemporary literatures on image binarization techniques have categorized three dif-\nferent techniques for threshold selection namely, mean threshold selection, local thresh-\nold selection and global threshold selection to deal with the unfavourable influences on \nthreshold selection. Enhanced classification results have been comprehended by feature \nextraction from mean threshold and multilevel mean threshold based binarized images \n(Kekre et al. 2013; Thepade et al. 2013a, b). Eventually, it has been identified that selection \nof mean threshold has not dealt with the standard deviation of the gray values and has \nconcentrated only on the average which has prevented the feature extraction techniques \nto take advantage of the spread of data to distinguish distinct features. Therefore, image \nsignature extraction was carried out with local threshold selection and global thresh-\nold selection for binarization, as the techniques were based on calculation of both mean \nand standard deviation of the gray values (Liu 2013; Yanli and Zhenxing 2012; Ramírez-\nOrtegón and Rojas 2010; Otsu 1979; Shaikh et al. 2013; Thepade et al. 2014a).\n\nUse of morphological operators for feature extraction\n\nCommercial viability of shape feature extraction has been well highlighted by systems \nlike Image Content (Flickner et  al. 1995), PicToSeek (Gevers and Smeulders 2000). \nTwo different categorization of shape descriptors namely, contour-based and region-\nbased descriptors have been elaborated in the existing literatures (Mehtre et  al. 1997; \nZhang and Lu 2004). Emphasize of the contour based descriptors has been on bound-\nary lines. Popular contour-based descriptors have embraced Fourier descriptor (Zhang \nand Lu 2003), curvature scale space (Mokhtarian and Mackworth 1992), and chain codes \n(Dubois and Glanz 1986). Feature extraction from complex shapes has been well car-\nried out by means of region-based descriptors, since the feature extraction has been per-\nformed from whole area of object (Kim and Kim 2000).\n\nFusion methodologies and multi technique feature extraction\n\nInformation recognition with image data has utilized the features extracted by means \nof diverse extraction techniques to harmonize each other for enhanced identification \nrate. Recent studies in information fusion have categorized the methodologies typically \ninto four classes, namely, early fusion, late fusion, hybrid fusion and intermediate fusion. \nEarly fusion combines the features of different techniques and produces it as a single \ninput to the learner. The process inherently increases the size of feature vector as the \n\n\n\nPage 4 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nconcentrated features easily correspond to higher dimensions. Late fusion applies sepa-\nrate learner to each feature extraction technique and fuses the decision with a combiner. \nAlthough it offers scalability in comparison to early fusion, still, it cannot explore the \nfeature level correlations, since it has to make local decisions primarily. Hybrid fusion \nmakes a mix of the two above mentioned techniques. Intermediate fusion integrates \nmultiple features by considering a joint model for decision to yield superior prediction \naccuracy (Zhu and Shyu 2015). Color and texture features were extracted by means of \n3 D color histogram and Gabor filters for fusion based image identification. The space \ncomplexity of the feature was further reduced by using genetic algorithm which has also \nobtained the optimum boundaries of numerical intervals. The process has enhanced \nsemantic retrieval by introducing feature selection technique to reduce memory con-\nsumption and to decrease retrieval process complexity (ElAlami 2011). Local descriptors \nbased on color and texture was calculated from Color moments and moments on Gabor \nfilter responses. Gradient vector flow fields were calculated to capture shape information \nin terms of edge images. The shape features were finally depicted by invariant moments. \nThe retrieval decisions with the features were fused for enhanced retrieval performance \n(Hiremath and Pujari 2007). Feature vectors comprising of color histogram and tex-\nture features based on a co-occurrence matrix were extracted from HSV color space \nto facilitate image retrieval (Yue et  al. 2011). Visually significant point features chosen \nfrom images by means of fuzzy set theoretic approach. Computation of some invariant \ncolor features from these points was performed to gauge the similarity between images \n(Banerjee et  al. 2009). Recognition process was boosted up by combining color layout \ndescriptor and Gabor texture descriptor as image signatures (Jalab 2011). Multi view \nfeatures comprising of color, texture and spatial structure descriptors have contributed \nfor increased retrieval rate (Shen and Wu 2013). Wavelet packets and Eigen values of \nGabor filters were extracted as feature vectors by the authors in (Irtaza et  al. 2013) for \nneural network architecture of image identification. The back propagation neural net-\nwork was trained on sub repository of images generated from the main image reposi-\ntory and utilizes the right neighbourhood of the query image. This kind of training was \naimed to insure correct semantic retrieval in response to query images. Higher retrieval \nresults have been apprehended with intra-class and inter-class feature extraction from \nimages (Rahimi and Moghaddam 2013). In (ElAlami 2014), extraction of color and tex-\nture features through color co-occurrence matrix (CCM) and difference between pixels \nof scan pattern (DBPSP) has been demonstrated and an artificial neural network (ANN) \nbased classifier was designed. In (Subrahmanyam et  al. 2013), content-based image \nretrieval was carried out by integrating the modified color motif co-occurrence matrix \n(MCMCM) and difference between the pixels of a scan pattern (DBPSP) features with \nequal weights. Fusion of semantic retrieval results obtained by capturing colour, shape \nand texture with the color moment (CMs), angular radial transform descriptor and edge \nhistogram descriptor (EHD) features respectively had outclassed the Precision values of \nindividual techniques (Walia et al. 2014). Six semantics of local edge bins for EHD were \nconsidered which included the vertical and the horizontal edge (0,0), 45° edge and 135° \nedge of sub-image (0,0), non directional edge of sub-image (0,0) and vertical edge of sub-\nimage at (0,1). Color histogram and spatial orientation tree has been used for unique \nfeature extraction from images for retrieval purpose (Subrahmanyam et al. 2012).\n\n\n\nPage 5 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nMethods\nThree different techniques of feature extraction have been introduced in this work namely, \nfeature extraction with image binarization, feature extraction with image transform and \nfeature extraction with morphological operator. However, there are popular feature extrac-\ntion techniques like GIST descriptor which has much greater feature dimension com-\npared to the proposed techniques in the work. GIST creates 32 feature maps of same \nsize by convolving the image with 32 Gabor filters at 4 scales, 8 orientations (Douze et al. \n2009). It averages the feature values of each region by dividing each feature map into 16 \nregions. Finally, it concatenates the 16 average value of all 32 feature maps resulting in \n16 ×  32 =  512 GIST descriptor. On the other hand, our approach has generated a fea-\nture dimension of 6 from each of the binarization and morphological technique. Feature \nextraction by applying image transform has yielded a feature size of 36. On the whole, the \nfeature size for the fusion based classifier was (6 + 36 + 6 = 48) which is far less than GIST \nand has much lesser computational overhead. Furthermore, fusion based architecture for \nclassification and retrieval have been proposed for enhanced identification rate of image \ndata. Each of the techniques of feature extraction as well as the methods for fusion based \narchitecture of classification and retrieval has been discussed in the following four subsec-\ntions and the description of datasets has been given in the fifth subsection.\n\nFeature extraction with image binarization\n\nInitially, the three color components namely, Red (R), Green (G) and Blue (B) were sepa-\nrated in each of the test images. A popular global threshold selection method named \nOtsu’s method has been applied separately on each of the color components for binari-\nzation as in Fig. 1. The above mentioned thresholding method has been largely used for \ndocument image binarzation. Otsu’s technique has been operated directly on the gray \nlevel histogram which has made it fast executable. It has been efficient to remove redun-\ndant details from the image to bring out the necessary image information. The method \nhas been considered as a non-parametric method which has considered two classes of \npixels, namely, the foreground pixels and the background pixels. It has calculated the \noptimal threshold by using the within-class variance and between-class variance. The \nseparation was carried out in such a way so that their combined intra-class variance is \nminimal (Otsu 1979; Shaikh et al. 2013). Comprehensive investigation has been carried \nout for the threshold that minimizes the intra-class variance represented by the weighted \nsum of variances of the two classes of pixels for each of the three color components.\n\nThe weighted within-class variance has been given in Eq. 1.\n\nq1(t) = ∑ \nt\ni=0P(i) where the class probabilities of different gray level pixels were estimated \n\nas shown in Eqs. 2 and 3:\n\n(1)σ2w(t) = q1(t)σ\n2\n1 (t) + q2(t)σ\n\n2\n2 (t)\n\n(2)q1(t) =\nt\n\n∑\n\ni=0\n\np(i)\n\n(3)\nq2(t) =\n\n255\n∑\n\ni=t+1\n\nP(i)\n\n\n\nPage 6 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThe class means were given as in Eqs. 4 and 5:\n\nTotal variance (σ2) = Within-class variance (σw\n2(t)) + Between-class Variance(σb\n\n2(t)).\nSince the total variance was constant and independent of t, the effect of changing \n\nthe threshold was purely to shift the contributions of the two terms back and forth. \nBetween-class variance has been given in Eq. 6\n\nThus, minimizing the within-class variance was the same as maximizing the between-\nclass variance.\n\nBinarization of the test images was carried out using the Otsu’s local threshold selec-\ntion method. The process has been repeated for all the three color components to gen-\nerate bag of words model (BoW) of features. Conventional BoW model has been based \non SIFT algorithm which has a descriptor dimension of 128 (Zhao et  al. 2015). There-\nfore, for three color components the dimension of the descriptor would have been 128 \n× 3 = 384. The size for SIFT descriptor has been huge and it has predestined problem \nfor information losses and omissions as it has been found suitable only for the stability \n\n(4)µ1(t) =\nt\n\n∑\n\ni=0\n\ni ∗ P(i)\n\nq1(t)\n\n(5)µ2(t) =\n255\n∑\n\ni=t+1\n\ni ∗ P(i)\n\nq2(t)\n\n(6)σ\n2\nb (t) = q1(t)[1 − q1(t)][µ1(t) − µ2(t)]\n\n2\n\n   \nRed Component Green Component Blue Component \n\n   \nBinarization of \n\nRed Component \nBinarzation of \n\nGreen Component \nBinarization of \n\nBlue Component \nFig. 1 Binarization using Otsu’s Threshold selection\n\n  \n\n  \n\n  \n\n\n\nPage 7 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nof image feature point extraction and description. Furthermore, the generated SIFT \ndescriptors has to be clustered by k means clustering which has been based on alloca-\ntion of cluster members by means of comparing squared Euclidian distance. The clus-\ntering process has been helpful to generate codewords for codebook generation which \nhas been the final step of BoW. Process of k means clustering has huge computational \noverhead for calculating the squared Euclidian distance which eventually slows down \nthe BoW generation. Hence, in our approach, the grey values higher than the threshold \nwas clustered in higher intensity group and the grey values lower than the cluster was \nclustered in the lower intensity group. The mean of the two groups were calculated to \nformulate the codewords of higher intensity feature vectors and the lower intensity fea-\nture vectors respectively. Thus, each color component of a test image has been mapped \nto two codewords of higher intensity and lower intensity respectively. This has generated \nof codebook of size (3 × 2 = 6) for each image.\n\nThe algorithm for feature extraction has been stated in Algorithm 1 as follows:\n\nAlgorithm 1 \n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Calculate the local threshold value Tx for \neach pixel in each color component R,G and \nB using Otsu's Method.\n\n3. Compute binary image maps for each pixel \nfor the given image.\n\nTxjixif >=),(....1\n\nTxjixif <),(....0\n\n/*x = R, G and B */\n\n4. Generate image features for the given \nimage for each color component.\n\n/*x = R, G and B */\n\nEnd\n\n=),( jiBitmapx\n\nTx\np q\n\nqpxmean\nmean\n\nxhi >== ∑ ∑ )),((\n\nTx\np q\n\nqpxmean\nmean\n\nxlo <= ∑∑ )),((\n\n\n\nPage 8 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFeature extraction using image transform\n\nTransforms convert spatial information to frequency domain information, where cer-\ntain operations are easier to perform. Energy compaction property of transforms has \nthe capacity to pack large fraction of the average energy into a few components. This \nhas led to faster execution and efficient algorithm design. Image transforms has the \nproperty to convert the spatial domain information of an image to frequency domain \ninformation, where certain operations are easier to perform. For example, convolu-\ntion operation can be reduced to matrix multiplication in frequency domain. It has the \ncharacteristic of energy compaction which ensures that a large fraction of the average \nenergy of the image remains packed into a few components. This property has led to \nfaster execution and efficient algorithm design by drastic reduction of feature vector \nsize which is achieved by means of discarding insignificant transform coefficients as in \nFig. 2. The approach has been implemented by applying slant transform on each of the \nRed (R), Green (G) and Blue (B) color component of the image for extraction of fea-\nture vectors with smaller dimension. Slant transform has reduced the average coding \nof a monochrome image from 8 bits/pixel to 1 bit/pixel without seriously degrading the \nimage quality. It is an orthogonal transform which has also reduced the coding of color \nimages from 24–2 bits/pixel (Pratt et  al. 1974). Slant transform matrices are orthogo-\nnal and it holds all real components. Hence, it has much less computational overhead \ncompared to discrete Fourier transform. Slant transform is an unitary transform and \nfollows energy conservation. It tends to pack a large fraction of signal energy into a few \ntransform coefficients which has a significant role in reducing the feature vector for the \nimage. Let [F] be an N × N matrix of pixel values of an image and let [fi] be an N × 1 \nvector representing the ith. column of [F]. One dimensional transform of the ith. image \nline can be given by\n\n [S] = N × N unitary slant matrix.\n\n[fi] = [S][fi]\n\n0.06 % of (N*N) feature vector\n\n0.012% of (N*N) feature vector\n\n50% of (N*N) feature vector\n\nN*N feature vector\n\nFeature Vector Dimension Reduction with Partial Coefficients\n\nFig. 2 Feature extraction by applying image transform\n\n  \n\n  \n\n\n\nPage 9 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nA two dimensional slant transform can be performed by sequential transformations \nof row and column of [F] and the forward and inverse transform can be expressed as in \nEqs. 7 and 8.\n\nA transform operation can be conveniently represented in a series. The two dimensional \nforward and inverse transform in series form can be represented as in Eqs. 9 and 10\n\nThe algorithm for feature extraction using slant transform has been given in Algo-\nrithm 2.\n\nAlgorithm 2 \n\n(7)[ℑ] = |S|[F][S]T\n\n(8)[F] = [S]T [ℑ][S]\n\n(9)ℑ(u, v) =\nN\n∑\n\nj=1\n\nN\n∑\n\nk=1\n\nF(j, k)S(u, j)S(k, v)\n\n(10)F\n(\n\nj, k\n)\n\n=\n\nN\n∑\n\nu=1\n\nN\n∑\n\nv=1\n\nℑ(u, v)S\n(\n\nj, u\n)\n\nS(v, k)\n\nBegin\n\n1. Red, Green and Blue color components were \nextracted from a given image.\n\n2. Slant Transform was applied on each of the \ncomponent to extract feature vectors.\n\n3. The extracted feature vectors from each of the \ncomponent were stored as complete set of feature \nvectors.\n\n4. Further, partial coefficients from the entire \nfeature vector set were extracted to form the \nfeature vector database.\n\n5. Feature vector database with 100% transformed \ncoefficients and partial coefficients ranging from \n50% of the complete set of feature vectors till \n0.06% of the complete set of feature vectors were \nconstructed\n\n6. The feature vectors of the query image for the \nwhole set of feature vectors and for partial \ncoefficient of feature vectors were compared with \nthe database images for classification results.\n\n7. The fractional coefficient of feature vector \nhaving the highest classification result was \nconsidered as the feature set extracted by applying \nimage transform\n\nEnd\n\n\n\nPage 10 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nHere the features were extracted in the form of visual words. Visual words have been \ndefined as a small patch of image which can carry significant image information. The \nenergy compaction property of Slant transform has condensed noteworthy image infor-\nmation in a block of 12 elements for an image of dimension (256 × 256). Thus, the \nfeature vector extracted with slant transform was of size 12 for each color component \nwhich has given the dimension of feature vector as 36 (12  ×  3  =  36) for three color \ncomponents in each test image.\n\nFeature extraction with morphological operator\n\nHuman perception has largely been governed by shape context. It has been helpful to \nrecover the point correspondences from an image which has considerable contribution \nin feature vector formation. A variant of gray scale opening and closing operations has \nbeen termed as the top-hat transformation that has been instrumental in producing only \nthe bright peaks of an image (Sridhar 2011). It has been termed as the peak detector and \nits working process has been given as follows:\n\n1. Apply the gray scale opening operation to an image.\n2. Peak = original image—opened image.\n3. Display the peak.\n4. Exit.\n\nThe top-hat transform technique was applied on each color component Red (R), \nGreen (G) and Blue (B) of the test images for feature extraction using morphologi-\ncal operator as in Fig.  3. After applying the tophat operator, the pixels designated as \nthe foreground pixels were grouped in one cluster and were calculated with mean and \nstandard deviation to formulate the higher intensity feature vector. Similar process \nwas followed with the pixels designated as the background pixels to calculate the lower \nintensity feature vector. The feature vector extraction process has followed the bag of \nwords (BoW) methodology which has generated codewords from the cluster of fore-\nground and background pixels by calculating the mean and the standard deviation of \nboth the clusters and adding the two. Hence, codebook size for each color component \nwas two which have yielded a dimension of 6 (3 × 2 =  6) on the whole for the code-\nbook generated for three color components for each test image.\n\nThe algorithm for feature extraction using morphological operator has been given in \nAlgorithm 3.\n\n\n\nPage 11 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nAlgorithm 3 \n\nSimilarity measures\n\nDetermination of image similarity measures was performed by evaluating distance \nbetween set of image features. Higher similarity has been characterized by shorter dis-\ntance (Dunham 2009). A fusion based classifier, an artificial neural network (ANN) clas-\nsifier and a support vector machine (SVM) classifier was used for the purpose. Each of \nthe classifier types has been discussed in the following sections:\n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Apply tophat transform on each color \ncomponent\n\n3. Cluster the foreground and background \npixels obtained after the morphological \noperation     \n\n4. Generate image features xhiF.V. and xloF.V.\nfor the given image for each color \ncomponent.\n\n/*x = R, G and B */\n\nEnd\n\n∑ ∑=\np q\n\nqp\nforeground\n\nxmean\nmean\n\nxhi )),((\n\n∑ ∑=\np q\n\nqp\nforeground\n\nx\nstdev\n\nxhi )),((σ\n\n( )\nstdev\n\nxhi\nmean\n\nxhimeanxhiVF\nxhi += +\n\n..\n\n∑ ∑=\np q\n\nqp\nbackground\n\nxmean\nmean\n\nxlo )),((\n\n∑ ∑=\np q\n\nqp\nbackground\n\nx\nstdev\n\nxlo )),((σ\n\n( )\nstdev\n\nxlo\nmean\n\nxlomeanxloVF\nxlo += +\n\n..\n\n\n\nPage 12 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFusion based classifier\n\nThree different distance measures, namely, city block distance, Euclidian distance and \nmean squared error (MSE) distance metric was considered to compute the distance \nbetween query image Q and database image T as in Eqs. 11, 12 and 13\n\nwhere, Qi is the query image and Di is the database image.\nData standardization technique was followed to standardize the calculated distances \n\nfor the individual techniques with Z score normalization which was based on mean and \nstandard deviation of the computed values as in Eq.  14. The normalization process has \nbeen implemented to avoid dependence of the classification decision on a feature vec-\ntor with higher values of attributes which have the possibilities to have greater effect or \n“weight.” The process has normalized the data within a common range such as [−1, 1] or \n[0.0, 1.0].\n\nwhere, µ is the mean and σ is the standard deviation.\n\n(11)Dcityblock =\nn\n\n∑\n\ni−1\n\n|Qi − Di|\n\n(12)Deuclidian =\n\n√\n\n√\n\n√\n\n√\n\nn\n∑\n\ni=1\n\n(Qi − Di)\n2\n\n(13)DMSE =\n1\n\nn\n\nn\n∑\n\ni=1\n\n(Qi − Di)\n2\n\n(14)distn =\ndisti − µ\n\nσ\n\n   \nRed Component Green Component Blue Component \n\n   \nApplying Top-Hat \noperator on Red \n\nComponent \n\nApplying Top-Hat \noperator on Green \n\nComponent \n\nApplying Top-Hat \noperator on Blue \n\nComponent \nFig. 3 Effect of applying morphological operator\n\n  \n\n  \n\n  \n\n  \n\n  \n\n\n\nPage 13 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFurther, the final distance was calculated by adding the weighted sum of individual \ndistances. The weights were calculated from the precision values of corresponding tech-\nniques. Finally, the image was classified based on the class majority of k nearest neigh-\nbors [Sridhar 2011] where value of k was\n\nThe classified image was forwarded for retrieval purpose. The image was a classified \nquery and has searched for similar images only within the class of interest. Ranking of \nthe images was done with Canberra Distance measure as in Eq.  15 and top 20 images \nwere retrieved.\n\nwhere, Qi is the query image and Di is the database image.\nThe process of fusion based classification and then retrieval with classified query has \n\nbeen illustrated in Fig. 4.\n\nArtificial neural network (ANN) classifier\n\nThe set of input features from images were mapped to an appropriate output by a feed \nforward Neural Network Classifier known as Multilayer Perceptron (MLP) as shown in \nFig. 5 (Alsmadi et al. 2009).\n\nThe back propagation technique of multi layer perceptron has a significant role in \nsupervised learning procedure. The network has been trained for optimization of clas-\nsification performance by using the procedure of back propagation. For each training \ntuple, the weights were modified so as to minimize the mean squared error between the \nnetwork prediction and the target value. These modifications have been made in the \nbackward direction through each hidden layer down to the first hidden layer. The input \nfeature vectors have been fed to the input units which comprised the input layer. The \nnumber of input units has been dependent on the summation of the number of attrib-\nutes in the feature vector dataset and the bias node. The subsequent layer has been the \nhidden layer whose number of nodes has to be determined by considering the half of the \nsummation of the number of classes and the number of attributes per class. The inputs \nthat have passed the input layer have to be weighted and fed simultaneously to the hid-\nden layer for further processing. Weighted output of the hidden layer was used as input \nto the final layer which has been named as the output layer. The number of units in the \noutput layer has been denoted by the number of class labels. The feed forward property \nof this architecture does not allow the weights to cycle back to the input units.\n\nSupport vector machine (SVM) classifier\n\nSVM transforms original training data to higher dimension by using nonlinear mapping. \nOptimal separating hyperplane has to be searched by the algorithm within this new \ndimension. Data from two different classes can readily be separated by a hyperplane by \nmeans of an appropriate nonlinear mapping to a sufficiently high dimension as in Fig. 6.\n\nk ≤\n√\n\nnumber..of ..training..ins tan ces.\n\n(15)Dcanberra =\nn\n\n∑\n\ni=1\n\n|Qi − Di|\n\n|Qi| + |Di|\n\n\n\nPage 14 of 26Das et al. SpringerPlus  (2015) 4:749 \n\n          No \n\nYes \n\nRetrieve top 20 images \n\nFeature \nExtraction \n\nwith \nBinarization \n\nFeature \nExtraction \nwith partial \nTransform \ncoefficients \n\nFeature \nExtraction \n\nwith \nMorphological \n\nOperator \n\nRank Images \nby City-Block \nDistance \n\nRank Images \nby Euclidian \nDistance \n\nRank Images      \nby MSE \nDistance\n\nFuse the distances by Z score Normalization\n\nRank the images using fused distance\n\n            Rank the images using fused distance\n\nClassify the query based on the class majority of  k nearest neighbors\n\n       Forward the classified query for retrieval from the class of interest\n\nRetrieve top 20 images\n\n    Qi     Di \n\nClassify \nquery? \n\nFig. 4 Fusion technique for image identification\n\nInput layer   Hidden Layer  Output Layer \n\nw1j\n\n                                         w2j                                        wjk\n\n                                                                   Oj\nwij\n\n                                                                                                      Ok\n\n                                       wnj\n\nOj= Output value for hidden layer  \nOk= Output Value for output layer \n\nx\n\nx\n\nx\n\nx\n\nFig. 5 Multilayer perceptron (MLP)\n\n\n\nPage 15 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nSVM has searched for the maximum separating hyperplane as shown in Fig.  6. The \nsupport vectors have been shown with thicker borders.\n\nThe algorithm was implemented using sequential minimal optimization (SMO) \n(Keerthi et  al. 2001). The operating principle of SMO has been to select two Lagrange \nmultipliers as the multipliers must obey a linear equality constraint. The two selected \nLagrange multipliers jointly optimize to find the optimal value for these multipliers and \nupdates the SVM to reflect the new optimal values.\n\nDatasets used\n\nFour different datasets namely Wang dataset, Oliva and Torralba (OT-Scene) dataset, \nCorel dataset and Caltech Dataset was used for the content based image recognition \npurpose. Each of the datasets has been described in the following subsections.\n\nWang’s dataset\n\nIt consists of 10 different categories of 1000 images and was provided by Li and Wang \n(2003). Each image is of dimension 256 × 384 or 384 × 256 and each category com-\nprises of 100 images. The different classes in this dataset are Tribals, Sea Beaches, Gothic \nStructures, Buses, Dinosaur, Elephants, Flowers, Horses, Mountains and Food. A sample \ncollage for Wang’s dataset has been given in Fig. 7.\n\nOliva and torralba (OT‑Scene) dataset\n\nThis dataset comprises of 2688 images and is divided into eight different categories. The \ndataset is provided by MIT (Walia and Pal 2014). The different categories in the dataset \nare Coast and Beach (with 360 images), Open Country (with 328 images), Forest (with \n260 images), Mountain (with 308 images), Highway (with 324 images), Street (with 410 \nimages), City Centre (with 292 images) and Tall Building (with 306 images). A sample \ncollage for OT Scene dataset is given in Fig. 8.\n\nA2\n\nA1\n\nClass 1  Class 2\n\nFig. 6 Structure of hyperplane in SVM\n\n O O O O Large Margin O O O \n\n\n\nPage 16 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nCorel dataset\n\nThe dataset comprised of 10,800 images (Liu 2013). It has 80 different categories of \nimages of dimension 80 × 120 or 120 × 80. Some of the categories are art, antique, \ncyber, dinosaur, mural, castle, lights, modern, culture, drinks, feast, fitness, dolls, avia-\ntion, balloons, bob, bonsai, bus, car, cards, decoys, dish, door, easter eggs, faces etc. A \nsample collage of the Corel dataset is given in Fig.  9. The research work has used 2500 \nimages of different categories from this dataset.\n\nCaltech dataset\n\nThe dataset includes 8127 images divided into 100 different categories (Walia and Pal \n2014). Each of the categories has different number of images with a dimension of 300 x \n200. Some of the categories are accordion, airplanes, anchor, ant, Background google, \nbarrel, bass, beaver, binocular, bonsai, brain, brontosaurus, buddha, butterfly, camera, \ncannon, car side, ceiling fan, cellphone, chair etc. A sample collage for the Caltech data-\nset has been given in Fig.  10. The research work has used 2533 images of different cat-\negories from the dataset.\n\nFig. 7 Sample collage for wang dataset\n\nFig. 8 Sample collage for OT-scene dataset\n\n  \n\n nan Zins Cavern \n\n\n\nPage 17 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nResults and discussions\nThe experiments were executed with Matlab version 7.11.0 (R2010b) on Intel core i5 \nprocessor with 4 GB RAM under Microsoft Windows environment. Initially the misclas-\nsification rate (MR) and F1 Score for classification with fractional coefficients of slant \ntransform were compared to each other to identify the fractional coefficient with high-\nest classification value and lowest MR. Wang dataset was used for the purpose. Further, \nthe precision and recall values for classification were determined on four different pub-\nlic datasets namely, Wang dataset, OT scene dataset Caltech dataset and Corel dataset. \nHenceforth, precision and recall values of the fused architecture for classification were \ncompared against state-of-the art techniques. The precision, recall misclassification rate \n(MR) and F1 Score were represented by Eqs. 16, 17, 18 and 19.\n\n(16)Pr ecision =\nTP\n\nTP + FP\n\n(17)TPRate/Re call =\nTP\n\nTP + FN\n\n(18)MR =\nFP + FN\n\nTP + TN + FP + FN\n\nFig. 9 Sample collage for corel dataset\n\nFig. 10 Sample collage for caltech dataset\n\n  \n\n RE-USE metani SITED SENTES MASa \n\n\n\nPage 18 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nTrue Positive (TP) = Number of instances classified correctly. True Negative (TN) = Num-\nber of negative results created for negative instances False Positive (FP) = Number of erro-\nneous results as positive results for negative instances False Negative (FN) =  Number of \nerroneous results as negative results for positive instances.\n\nComparison of MR and F1 Score for classification with different fractional coefficients \nof slant transform has been shown in Fig. 11.\n\nIt was observed that classification with 0.024  % of the transform coefficient has the \nhighest F1 Score and lowest MR compared to the rest. Hence, it was considered as the \nfeature vector with a dimension of 36.\n\nFurther, the precision and recall values of four public datasets have been shown in \nTable 1.\n\nHenceforth, Wang dataset was considered in order to carry out classification using \nfusion technique. The classification decision obtained for Wang dataset using three dif-\nferent feature extraction techniques were fused by means of Z score normalization and \nwere compared to classification results obtained by classifying individual techniques by \n\n(19)F1score =\n2 ∗ Pr ecision ∗ Re call\n\nPr ecision + Re call\n\nF1 Score MR\n100% feature size 0.478 0.103\n50% of feature size 0.48 0.095\n25% of feature size 0.487 0.09\n12.5% of feature size 0.489 0.09\n6.25% of feature size 0.501 0.09\n3.125% of feature size 0.528 0.089\n1.5625% of feature\n\nsize 0.53 0.089\n\n0.7813% of feature\nsize 0.532 0.088\n\n0.39% of feature size 0.536 0.088\n0.195% of feature size 0.538 0.087\n0.097% of feature size 0.539 0.087\n0.048% of feature size 0.539 0.087\n0.024% of feature size 0.54 0.086\n0.012% of feature size 0.536 0.088\n0.006% of feature size 0.534 0.089\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nV\nal\n\nue\ns \n\nComparison of MR and F1 Score for \nFractional Coeffiecnts of Slant Transform \n\nFig. 11 Comparison of MR and F1 score for partial coefficients of slant transform\n\n\n\nPage 19 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nmeans of artificial neural network (ANN) classifier and support vector machine (SVM) \nclassifier respectively. The comparisons have been shown in Fig. 12.\n\nThe comparison in Fig.  12 has clearly revealed that fusion based classification has \nshown an enhanced precision of 0.12, 0.13 and 0.067 compared to classification with \nANN classifier for feature extraction with image binarization, partial transform coef-\nficients and morphological operator respectively. The recall rate for classification with \nfusion based classification was also higher by 0.134, 0.141 and 0.08 in comparison to \nclassification with ANN classifier for feature extraction with three above mentioned \ntechniques.\n\nThe fusion based classifier has revealed an improved precision rate of 0.221, 0.204 \nand 0.118 in comparison to classification with SVM classifier for feature extraction with \nimage binarization, partial transform coefficient and morphological operator respec-\ntively as in Fig. 13. The recall value for classification with fusion based classifier was also \nhigher by 0.224, 0.21 and 0.136 compared to SVM classifier which is seen in Fig. 13.\n\nTable 1 Precision and recall values for four public datasets using three feature extraction \ntechniques\n\nFeature extraction with  \nbinarization\n\nFeature extraction with fractional \ncoefficients of slant transform\n\nFeature extraction with  \nmorphological operator\n\nWang OT scene Caltech Corel Wang OT scene Caltech Corel Wang OT scene Caltech Corel\n\nPrecision 0.609 0.41 0.49 0.534 0.555 0.449 0.454 0.527 0.728 0.607 0.523 0.711\n\nRecall 0.604 0.4 0.543 0.519 0.563 0.407 0.523 0.533 0.725 0.597 0.597 0.697\n\nPrecision Recall\nFusion Based\n\nClassifier 0.748 0.765\n\nANN Classifier\n(Feature Extraction\n\nwith Image\nBinarization)\n\n0.628 0.631\n\nANN Classifier\n(Feature Extraction\n\nwith Partial Transform\nCoefficient)\n\n0.627 0.624\n\nANN Classifier\n(Feature Extraction\nwith morphological\n\noperator)\n\n0.681 0.685\n\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\nV\nal\n\nue\ns \n\nComparison of Classification Results of \nFusion Based Classifier and Artifical Neural \n\nNetwork (ANN) Classifier \n\nFig. 12 Comparison of classification with fusion based classifier and ANN classifier\n\n\n\nPage 20 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nFurther, the fusion based classification results were compared to existing techniques in \nFig. 14.\n\nIt was observed that the proposed method has outclassed the existing techniques. It \nhas an increased precision rate of 0.012, 0.108, 0.109, 0.178 and 0.228 and an enhanced \nrecall rate of 0.037, 0.125, 0.126, 0.195 and 0.245 compared to the existing techniques, \nnamely, (Thepade et  al. 2014b; Yanli and Zhenxing 2012; Ramírez-Ortegón and Rojas \n2010; Liu 2013; Shaikh et al. 2013) respectively as in Fig. 14. The proposed fusion tech-\nnique was observed to have the maximum precision and recall values compared to the \nrecent techniques cited in the literature.\n\nHenceforth, content based image retrieval was carried out with individual tech-\nniques of feature extraction and was compared to fusion based technique of retrieval in \nFig. 15. The fusion based retrieval technique comprised of classification as a precursor of \nretrieval. Comparison of fusion techniques with classified query and without classified \nquery has been shown in Fig. 16 by using a sample query.\n\nThe figure has clearly divulged that fusion technique of retrieval with classified query \nhas fetched all the images of the same category to that of the query image, whereas, \nretrieval with generic or unclassified query has three images from classes other than the \nclass of query in position 2, 15 and 19 respectively.\n\nA comparison of retrieval with individual techniques of feature extraction and fusion \nbased retrieval with classified query has been given in Fig. 15.\n\nPrecision Recall\nFusion Based\n\nClassifier 0.748 0.765\n\nSVM Classifier\n(Feature Extraction\n\nwith Image\nBinarization)\n\n0.527 0.541\n\nSVM Classifier\n(Feature Extraction\n\nwith  Partial\nTransform Coefficient)\n\n0.544 0.555\n\nSVM Classifier\n(Feature Extraction\nwith  morphological\n\noperator)\n\n0.63 0.629\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\nV\nal\n\nue\ns \n\nComparison of Classification Results of \nFusion Based Classifier and Support Vector \n\nMachine (SVM) Classifier \n\nFig. 13 Comparison of classification with fusion based classifier and SVM classifier\n\n\n\nPage 21 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nResults in Fig. 15 have shown an increase of 26.3, 34.5 and 19.5 % in precision values \nand enhancement of 5.26, 6.9 and 3.9  % in recall values for the fusion based retrieval \ntechnique with classified query in comparison to retrieval with individual feature extrac-\ntion techniques. It was clearly established that the fusion based technique has outper-\nformed the individual techniques.\n\nFurther, a paired t test was conducted to validate the statistical findings and a null \nhypothesis was formulated in Hypothesis 1 (Yıldız et al. 2011).\n\nHypothesis 1: There is no significant difference among the Precision values of fusion \nbased retrieval with classified query with respect to individual retrieval techniques\n\nThe p values for the paired t test have been enlisted in Table  2. The precision value \nof fusion based retrieval with classified query was compared to that of the individual \nretrieval techniques to obtain the computed values in Table 2.\n\nThe p values have clearly indicated significant difference in precision values of the fusion \nbased retrieval technique with classified query compared to the existing techniques of \nretrieval. Hence, the null hypothesis was rejected and the proposed fusion technique with \nclassified query has been found to boost the precision values with statistical significance.\n\nFinally, the precision and recall values of the proposed fusion technique were com-\npared to existing fusion based retrieval techniques. The results have been displayed in \nFig. 17.\n\nPrecision Recall\nProposed Fusion\n\nTechnique 0.748 0.765\n\nThepade et al. (2014b) 0.736 0.728\n(Yanli Y. and\n\nZhenxing Z., 2012) 0.64 0.64\n\n(Ramírez-Ortegón, \nM.A. And Rojas R., \n\n2010) \n0.63 0.63\n\n(Liu.C, 2013) 0.57 0.57\n(Shaikh, 2013) 0.52 0.52\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\nV\nal\n\nue\ns \n\nComparison of Precision and Recall values \nfor Classification \n\nFig. 14 Comparison of classification results of proposed technique with respect to existing techniques\n\n\n\nPage 22 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nPrecision Recall\nRetrieval with feature\n\nextraction with\nBinarization\n\n49.7 9.94\n\nRetrieval with feature\nextraction with\n\nFractional Coefficient\nof Slant Transform\n\n41.5 8.3\n\nRetrieval with feature\nextraction with\nMorphological\n\nOperator\n\n56.5 11.3\n\nFusion Based retrieval\nwith classified query 76 15.2\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\n70\n\n80\n\nV\nal\n\nue\ns \n\nComparison of Precision and Recall for \nFusion Based Retrieval and Individual \n\nRetrieval Technique \n\nFig. 15 Comparison of precision and recall with fusion based retrieval technique and individual retrieval \ntechnique\n\nRetrieval with \nclassified query \n\nRetrieval with generic \nquery \n\nFig. 16 Comparison of fusion based retrieval with classified and generic query\n\n  \n\n  \n\n\n\nPage 23 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThe comparison in Fig.  17 has clearly established the superiority of the proposed \nfusion based retrieval technique with respect to existing fusion based technique of \nretrieval. The proposed retrieval technique has improved precision of 1.98, 3.2, 3.3, 3.49, \n17.8, 21.1 and 26.31  % and superior recall of 0.4, 0.64, 0.66, 0.7, 3.56, 4.22 and 5.26  % \ncompared to the existing fusion based techniques mentioned in Fig. 13.\n\nHenceforth, the proposed method was compared to the semantic retrieval techniques \nin Fig. 18.\n\nTable 2 Statistical validation with paired t test\n\np value Significance\n\nRetrieval by feature extraction with image transform 0.0013 Significant\n\nRetrieval by feature extraction with image binarization 0.0076 Significant\n\nRetrieval by feature extraction with morphological operator 0.0452 Significant\n\n  \n  \n\n  \n\nPrecision Recall\nProposed 76 15.2\nSubrahmanyam et al.\n\n2013 74.02 14.80\n\nShen & Wu (2013) 72.8 14.56\nBanerjee et al. (2009) 72.7 14.54\nSubrahmanyam et al.\n\n2012 72.51 14.50\n\nJalab (2011) 58.2 11.64\nHiremath & Pujari\n\n(2007) 54.9 10.98\n\nRahimi & Moghaddam\n(2013) 49.69 9.94\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\n70\n\n80\n\nV\nal\n\nue\ns \n\nComparison of Retrieval Performance with \nFusion based Techniques \n\nFig. 17 Comparison of retrieval with the proposed technique compared to state-of-the art fusion tech-\nniques\n\n\n\nPage 24 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nThe comparison shown in Fig.  18 has revealed an enhanced precision rate of 0.2, 0.5 \nand 2.1  % and increased recall rate of 0.04, 0.1 and 0.6  % respectively for the proposed \nmethod with respect to the existing semantic retrieval techniques.\n\nTherefore, the research work has fulfilled the following objectives:\n\n  • It has reduced the dimension of feature vectors.\n  • It has successfully implemented fusion based method of content based image identi-\n\nfication.\n  • The research results have shown statistical significance.\n  • The research results have outperformed the results of state-of-the art techniques.\n\nConclusions\nIn depth analysis of feature extraction techniques have been exercised in this research \nwork. Three different techniques of feature extraction comprising of image binariza-\ntion, fractional coefficients of image transforms and morphological operations has been \nimplemented to extract features from the images. The extracted features with multiple \ntechniques were used for fusion based identification process. The proposed method of \nfusion has divulged statistical significance with respect to the individual techniques. \nThe retrieval technique was implemented with classification as a precursor. The classi-\nfication technique was used to classify the query image for retrieval. The method has \n\nPrecision Recall\nProposed 76 15.2\nWalia et al. (2014) 75.8 15.16\nIrtaza et al. (2013) 75.5 15.10\nAlami (2011) 73.9 14.78\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\n70\n\n80\n\nV\nal\n\nue\ns \n\nComparison of Proposed Technique with \nSemantic Retrieval Technique \n\nFig. 18 Comparison of retrieval with the proposed technique to semantic retrieval techniques\n\n\n\nPage 25 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nshown better performance compared to generic query based method of retrieval. Thus, \nthe importance of classification was established in limiting the computational overhead \nfor content based image identification. Finally, image identification with the proposed \ntechnique has surpassed the state-of-the art methods for content based image recogni-\ntion. The work may be extended towards content based image recognition in the field of \nmilitary, media, medical science, journalism, e commerce and many more.\nAuthor’s contributions\nRD and ST have designed the feature extraction techniques and the classification and retrieval techniques. RD and SG \nhave planned the statistical test and conclusion. RD wrote the manuscript. All the authors have read and approved the \nfinal manuscript.\n\nAuthor details\n1 Department of Information Technology, Xavier Institute of Social Service, Dr. Camil Bulcke Path (Purulia Road), P.O. \nBox 7, Ranchi 834001, Jharkhand, India. 2 Pimpri Chinchwad College of Engineering, Akrudi, Sec-26,Pradhikaran, Nigdi, \nPune 411033, Maharashtra, India. 3 A.K. Choudhury School of Information Technology, University of Calcutta, 92, APC \nRoad, Kolkata 700009, West Bengal, India. \n\nAcknowledgements\nThe authors acknowledge Late Dr. H.B. Kekre for encouraging the experimental process. The authors also acknowledge \nDr. Rohit Vishal Kumar and Dr. Subhajit Bhattacharya for explaining the statistical techniques.\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nReceived: 12 September 2015   Accepted: 5 November 2015\n\nReferences\nAlsmadi MK, Omar KB, Noah SA, Almarashdah I (2009) Performance comparison of multi-layer perceptron (Back Propaga-\n\ntion, Delta Rule and Perceptron) algorithms in neural networks. 2009 IEEE International Advance Computing Confer-\nence, IACC 2009, 7: pp 296–299\n\nAnnadurai S, Shanmugalakshmi R (2011) Image transforms, fundamentals of digital image processing. Dorling Kindersley \n(India) Pvt. Ltd., pp 31–66\n\nBanerjee M, Kundu MK, Maji P (2009) Content- based image retrieval using visually significant point features. Fuzzy Sets \nSyst 160:3323–3341\n\nDouze M, Jégou H, Singh H, Amsaleg L, Schmid C (2009) Evaluation of GIST descriptors for web-scale image search. In \nACM International Conference on Image and Video Retrieval, pp 0–7\n\nDubois SR, Glanz FH (1986) An autoregressive model approach to two-dimensional shape classification. IEEE Trans Pat-\ntern Anal Mach Intell 8(1):55–66\n\nDunham MH (2009) Data Mining Introductory and Advanced Topics: Pearson Education, p 127\nElAlami ME (2011) A novel image retrieval model based on the most relevant features. Knowl-Based Syst 24:23–32\nElAlami ME (2014) A new matching strategy for content based image retrieval system. Appl Soft Comput J 14:407–418\nFlickner M, Sawhney H, Niblack W, Ashley J, Huang Q, Dom B, Gorkani M et al (1995) Query by image and video content: \n\nthe QBIC system. Computer 28(9):23–32 IEEE\nGevers T, Smeulders AW (2000) PicToSeek: combining color and shape invariant features for image retrieval. IEEE Trans \n\nImage Proc Publ IEEE Signal Proc Soc 9(1):102–119\nHiremath PS, Pujari J (2007) Content based image retrieval based on color, texture and shape features using image and \n\nits complement. Int J Computer Sci Secur 1:25–35\nIrtaza A, Jaffar MA, Aleisa E, Choi TS (2013) Embedding neural networks for semantic association in content based image \n\nretrieval. Multimed Tool Appl 72(2):1911–1931\nJalab HA (2011) Image retrieval system based on color layout descriptor and Gabor filters. 2011 IEEE Conference on Open \n\nSystems. pp 32–36\nKeerthi SS, Shevade SK, Bhattacharyya C, Murthy KRK (2001) Improvements to Plattʼs SMO Algorithm for SVM classifier \n\ndesign. Neural Comput 13:637–649\nKekre HB, Thepade S (2009) Improving the performance of image retrieval using partial coefficients of transformed \n\nimage. Int J Inf Retr Ser Publ 2(1):72–79\nKekre HB, Thepade S, Maloo A (2010) Image Retrieval using Fractional Coefficients of Transformed Image using DCT and \n\nWalsh Transform‖. Int J Eng Sci Technol (IJEST ) 2(4):362–371\nKekre HB, Thepade S, Das R, Ghosh S (2013) Multilevel block truncation coding with diverse colour spaces for image clas-\n\nsification. In: IEEE-International conference on Advances in Technology and Engineering (ICATE), pp 1–7\nKim WY, Kim YS (2000) Region-based shape descriptor using Zernike moments. Sig Process Image Commun 16:95–102\nLi J, Wang JZ (2003) Automatic linguistic indexing of pictures by a statistical modeling approach. IEEE Trans Pattern Anal \n\nMach Intell 25:1075–1088\nLiu C (2013) A new finger vein feature extraction algorithm, In: IEEE 6th. International Congress on Image and Signal \n\nProcessing (CISP), pp 395–399\n\n Published online: 01 December 2015 \n\n\n\nPage 26 of 26Das et al. SpringerPlus  (2015) 4:749 \n\nLuo H, Lina Y, Haoliang Y, Yuan YT (2013) Dimension reduction with randomized anisotropic transform for hyperspectral \nimage classification. In: 2013 IEEE International Conference on Cybernetics, CYBCONF 2013, pp 156–161\n\nMadireddy RM, Gottumukkala PSV, Murthy PD, Chittipothula S (2014) A modified shape context method for shape based \nobject retrieval. SpringerPlus 3:674. doi:10.1186/2193-1801-3-674\n\nMehtre BM, Kankanhalli MS, Lee Wing Foon (1997) Shape measures for content based image retrieval: a comparison. Inf \nProcess Manage 33:319–337\n\nMokhtarian F, Mackworth AK (1992) A theory of multiscale, curvature-based shape representation for planar curves. IEEE \nTrans Pattern Anal Mach Intell 14:789–805\n\nOtsu N (1979) A threshold selection method from gray- level histogram IEEE transactions on systems. Man Cybern \n9:62–66\n\nPrakash O, Khare M, Srivastava RK, Khare A (2013) Multiclass image classification using multiscale biorthogonal wavelet \ntransform, In: IEEE Second International Conference on Information Processing (ICIIP), pp 131–135\n\nPratt W, Chen WH, Welch L (1974) Slant transform image coding. IEEE Transactions on Communications 22\nRahimi M and Moghaddam ME (2013) A content based image retrieval system based on color ton distributed descrip-\n\ntors. Signal Image Video Process 9(3):691–704. http://dx.doi.org/10.1007/s11760-013-0506-6\nRamírez-Ortegón MA and Rojas R (2010) Unsupervised evaluation methods based on local gray-intensity variances \n\nfor binarization of historical documents. Proceedings—International Conference on Pattern Recognition, pp \n2029–2032\n\nRaventós A, Quijada R, Torres L, Tarrés F (2015) Automatic summarization of soccer highlights using audio- visual descrip-\ntors. SpringerPlus 4:301. doi:10.1186/s40064-015-1065-9\n\nShaikh SH, Maiti AK, Chaki N (2013) A new image binarization method using iterative partitioning. Mach Vis Appl \n24(2):337–350\n\nShen GL and Wu XJ (2013) Content based image retrieval by combining color texture and CENTRIST, In: IEEE international \nworkshop on signal processing, vol 1, pp 1–4\n\nSridhar S (2011) Image features representation and description digital image processing. India Oxford University Press, \nNew Delhi, pp 483–486\n\nSubrahmanyam M, Maheshwari RP, Balasubramanian R (2012) Expert system design using wavelet and color vocabulary \ntrees for image retrieval. Expert Syst Appl 39:5104–5114\n\nSubrahmanyam M, Wu QMJ, Maheshwari RP, Balasubramanian R (2013) Modified color motif co- occurrence matrix for \nimage indexing and retrieval. Comput Electr Eng 39:762–774\n\nThepade S, Das R, Ghosh S (2013a) Advances in computing, communication and control. Image classification \nusing advanced block truncation coding with ternary image maps, vol 361. Springer, Berlin, pp 500–509. \ndoi:10.1007/978-3-642-36321-4_48\n\nThepade S, Das R, Ghosh S (2013b) Performance comparison of feature vector extraction techniques in RGB color space \nusing block truncation coding or content based image classification with discrete classifiers. In: India Conference \n(INDICON), IEEE, pp 1–6. doi: 10.1109/INDCON.2013.6726053\n\nThepade S, Das R, Ghosh S (2014a) A novel feature extraction technique using binarization of bit planes for content \nbased image classification. J Eng. doi:10.1155/2014/439218\n\nThepade S, Das R, Ghosh S (2014b) Feature extraction with ordered mean values for content based image classification. \nAdv Comput Eng 2014. doi:10.1155/2014/454876\n\nValizadeh M, Armanfard N, Komeili M, Kabir E (2009) A novel hybrid algorithm for binarization of badly illuminated docu-\nment images. 2009 14th International CSI Computer Conference, CSICC 2009, pp 121–126\n\nWalia E, Pal A (2014) Fusion framework for effective color image retrieval. J Vis Commun Image Represent \n25(6):1335–1348\n\nWalia E, Vesal S, Pal A (2014) An Effective and Fast Hybrid Framework for Color Image Retrieval. Sens Imaging 15:93. doi: \n10.1007/s11220-014-0093-9\n\nWang X, Bian W, Tao D (2013) Grassmannian regularized structured multi-view embedding for image classification. IEEE \nTrans Image Process 22(7):2646–2660\n\nYanli Y and Zhenxing Z (2012) A novel local threshold binarization method for QR image, In: IET International Conference \non Automatic Control and Artificial Intelligence (ACAI), pp 224–227\n\nYıldız OT, Aslan O, Alpaydın E (2011) Multivariate statistical tests for comparing classi-fication algorithms. Lect Notes \nComp Sci, vol 6683, Springer, Berlin, pp 1–15\n\nYue J, Li Z, Liu L, Fu Z (2011) Content-based image retrieval using color and texture fused features. Math Comput Model \n54:1121–1127\n\nZhang D, Lu G (2003) A comparative study of curvature scale space and Fourier descriptors for shape- based image \nretrieval. J Vis Commun Image Represent 14:39–57\n\nZhang D, Lu G (2004) Review of shape representation and description techniques. Pattern Recogn 37:1–19\nZhao C, Li X, Cang Y (2015) Bisecting k-means clustering based face recognition using block-based bag of words model. \n\nOptik Int J Light Electron Optics 126(19):1761–1766\nZhu Q, Shyu M-L (2015) sparse linear integration of content and context modalities for semantic concept retrieval. IEEE \n\nTrans Emerg Top Comput 3(2):152–160\n\nhttp://dx.doi.org/10.1186/2193-1801-3-674\nhttp://dx.doi.org/10.1007/s11760-013-0506-6\nhttp://dx.doi.org/10.1186/s40064-015-1065-9\nhttp://dx.doi.org/10.1007/978-3-642-36321-4_48\nhttp://dx.doi.org/10.1109/INDCON.2013.6726053\nhttp://dx.doi.org/10.1155/2014/439218\nhttp://dx.doi.org/10.1155/2014/454876\nhttp://dx.doi.org/10.1007/s11220-014-0093-9\n\n\tMulti technique amalgamation for enhanced information identification with content based image data\n\tAbstract \n\tBackground\n\tFeature extraction using image transform\n\tImage binarization techniques for feature extraction\n\tUse of morphological operators for feature extraction\n\tFusion methodologies and multi technique feature extraction\n\n\tMethods\n\tFeature extraction with image binarization\n\tFeature extraction using image transform\n\tFeature extraction with morphological operator\n\tSimilarity measures\n\tFusion based classifier\n\tArtificial neural network (ANN) classifier\n\tSupport vector machine (SVM) classifier\n\tDatasets used\n\tWang’s dataset\n\tOliva and torralba (OT-Scene) dataset\n\tCorel dataset\n\tCaltech dataset\n\n\tResults and discussions\n\tConclusions\n\tAuthor’s contributions\n\tReferences\n\n\n\n\n",
      "text": [
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "",
        "O O O O Large Margin O O O",
        "",
        "nan Zins Cavern",
        "",
        "RE-USE metani SITED SENTES MASa",
        "",
        "",
        "Published online: 01 December 2015"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"O O O O Large Margin O O O\",\"lines\":[{\"boundingBox\":[{\"x\":500,\"y\":22},{\"x\":531,\"y\":22},{\"x\":531,\"y\":50},{\"x\":499,\"y\":49}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":567,\"y\":102},{\"x\":602,\"y\":98},{\"x\":606,\"y\":135},{\"x\":571,\"y\":139}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":675,\"y\":104},{\"x\":707,\"y\":103},{\"x\":711,\"y\":134},{\"x\":675,\"y\":136}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":565,\"y\":194},{\"x\":596,\"y\":189},{\"x\":600,\"y\":216},{\"x\":570,\"y\":219}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":256,\"y\":316},{\"x\":357,\"y\":214},{\"x\":373,\"y\":229},{\"x\":270,\"y\":332}],\"text\":\"Large Margin\"},{\"boundingBox\":[{\"x\":678,\"y\":241},{\"x\":707,\"y\":237},{\"x\":711,\"y\":264},{\"x\":678,\"y\":266}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":137,\"y\":688},{\"x\":166,\"y\":686},{\"x\":169,\"y\":714},{\"x\":137,\"y\":717}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":410,\"y\":688},{\"x\":441,\"y\":686},{\"x\":442,\"y\":715},{\"x\":412,\"y\":716}],\"text\":\"O\"}],\"words\":[{\"boundingBox\":[{\"x\":499,\"y\":22},{\"x\":525,\"y\":22},{\"x\":525,\"y\":50},{\"x\":499,\"y\":49}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":567,\"y\":102},{\"x\":598,\"y\":98},{\"x\":603,\"y\":135},{\"x\":571,\"y\":139}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":675,\"y\":104},{\"x\":705,\"y\":103},{\"x\":706,\"y\":134},{\"x\":675,\"y\":135}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":565,\"y\":193},{\"x\":591,\"y\":190},{\"x\":595,\"y\":216},{\"x\":568,\"y\":220}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":256,\"y\":315},{\"x\":299,\"y\":273},{\"x\":312,\"y\":290},{\"x\":270,\"y\":332}],\"text\":\"Large\"},{\"boundingBox\":[{\"x\":302,\"y\":270},{\"x\":357,\"y\":215},{\"x\":373,\"y\":231},{\"x\":315,\"y\":287}],\"text\":\"Margin\"},{\"boundingBox\":[{\"x\":678,\"y\":239},{\"x\":701,\"y\":237},{\"x\":704,\"y\":265},{\"x\":678,\"y\":267}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":137,\"y\":688},{\"x\":162,\"y\":686},{\"x\":165,\"y\":714},{\"x\":137,\"y\":716}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":410,\"y\":687},{\"x\":437,\"y\":686},{\"x\":438,\"y\":715},{\"x\":411,\"y\":716}],\"text\":\"O\"}]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"nan Zins Cavern\",\"lines\":[{\"boundingBox\":[{\"x\":664,\"y\":172},{\"x\":715,\"y\":178},{\"x\":712,\"y\":195},{\"x\":662,\"y\":188}],\"text\":\"nan\"},{\"boundingBox\":[{\"x\":32,\"y\":276},{\"x\":115,\"y\":266},{\"x\":117,\"y\":278},{\"x\":33,\"y\":288}],\"text\":\"Zins Cavern\"}],\"words\":[{\"boundingBox\":[{\"x\":667,\"y\":173},{\"x\":714,\"y\":178},{\"x\":712,\"y\":195},{\"x\":666,\"y\":189}],\"text\":\"nan\"},{\"boundingBox\":[{\"x\":34,\"y\":276},{\"x\":64,\"y\":273},{\"x\":65,\"y\":285},{\"x\":34,\"y\":288}],\"text\":\"Zins\"},{\"boundingBox\":[{\"x\":68,\"y\":272},{\"x\":115,\"y\":267},{\"x\":116,\"y\":279},{\"x\":69,\"y\":284}],\"text\":\"Cavern\"}]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"RE-USE metani SITED SENTES MASa\",\"lines\":[{\"boundingBox\":[{\"x\":616,\"y\":21},{\"x\":665,\"y\":23},{\"x\":664,\"y\":39},{\"x\":616,\"y\":36}],\"text\":\"RE-USE\"},{\"boundingBox\":[{\"x\":775,\"y\":239},{\"x\":807,\"y\":236},{\"x\":808,\"y\":245},{\"x\":776,\"y\":249}],\"text\":\"metani\"},{\"boundingBox\":[{\"x\":767,\"y\":505},{\"x\":843,\"y\":513},{\"x\":842,\"y\":522},{\"x\":766,\"y\":514}],\"text\":\"SITED SENTES\"},{\"boundingBox\":[{\"x\":355,\"y\":515},{\"x\":399,\"y\":520},{\"x\":397,\"y\":534},{\"x\":353,\"y\":528}],\"text\":\"MASa\"}],\"words\":[{\"boundingBox\":[{\"x\":619,\"y\":21},{\"x\":664,\"y\":24},{\"x\":664,\"y\":39},{\"x\":619,\"y\":37}],\"text\":\"RE-USE\"},{\"boundingBox\":[{\"x\":776,\"y\":239},{\"x\":807,\"y\":236},{\"x\":808,\"y\":246},{\"x\":776,\"y\":249}],\"text\":\"metani\"},{\"boundingBox\":[{\"x\":771,\"y\":505},{\"x\":805,\"y\":509},{\"x\":804,\"y\":519},{\"x\":770,\"y\":514}],\"text\":\"SITED\"},{\"boundingBox\":[{\"x\":807,\"y\":509},{\"x\":843,\"y\":513},{\"x\":842,\"y\":523},{\"x\":806,\"y\":519}],\"text\":\"SENTES\"},{\"boundingBox\":[{\"x\":357,\"y\":515},{\"x\":399,\"y\":521},{\"x\":397,\"y\":535},{\"x\":355,\"y\":529}],\"text\":\"MASa\"}]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"Published online: 01 December 2015\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":16},{\"x\":1057,\"y\":16},{\"x\":1057,\"y\":69},{\"x\":0,\"y\":69}],\"text\":\"Published online: 01 December 2015\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":17},{\"x\":283,\"y\":17},{\"x\":283,\"y\":70},{\"x\":0,\"y\":69}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":293,\"y\":17},{\"x\":497,\"y\":17},{\"x\":497,\"y\":70},{\"x\":293,\"y\":70}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":507,\"y\":17},{\"x\":586,\"y\":17},{\"x\":586,\"y\":70},{\"x\":508,\"y\":70}],\"text\":\"01\"},{\"boundingBox\":[{\"x\":596,\"y\":17},{\"x\":898,\"y\":17},{\"x\":899,\"y\":70},{\"x\":596,\"y\":70}],\"text\":\"December\"},{\"boundingBox\":[{\"x\":909,\"y\":17},{\"x\":1055,\"y\":18},{\"x\":1056,\"y\":69},{\"x\":910,\"y\":70}],\"text\":\"2015\"}]}"
      ]
    },
    {
      "@search.score": 1.1207701,
      "content": "\nRESEARCH Open Access\n\nA classification method for social\ninformation of sellers on social network\nHaoliang Cui1, Shuai Shao2* , Shaozhang Niu1, Chengjie Shi3 and Lingyu Zhou1\n\n* Correspondence: shaoshuaib@163.\ncom\n2China Information Technology\nSecurity Evaluation Center, Beijing\n100085, China\nFull list of author information is\navailable at the end of the article\n\nAbstract\n\nSocial e-commerce has been a hot topic in recent years, with the number of users\nincreasing year by year and the transaction money exploding. Unlike traditional e-\ncommerce, the main activities of social e-commerce are on social network apps. To\nclassify sellers by the merchandise, this article designs and implements a social\nnetwork seller classification scheme. We develop an app, which runs on the mobile\nphones of the sellers and provides the operating environment and automated\nassistance capabilities of social network applications. The app can collect social\ninformation published by the sellers during the assistance process, uploads to the\nserver to perform model training on the data. We collect 38,970 sellers’ information,\nextract the text information in the picture with the help of OCR, and establish a\ndeep learning model based on BERT to classify the merchandise of sellers. In the\nfinal experiment, we achieve an accuracy of more than 90%, which shows that the\nmodel can accurately classify sellers on a social network.\n\nKeywords: User model, Machine learning, Social e-commerce\n\n1 Introduction\nWith the continuous improvement of social network and mobile payment technology,\n\none kind of commodity trading based on social relations called social e-commerce is in\n\nrapid development. According to the 2019 China social e-commerce industry develop-\n\nment report released by the Internet society of China, the number of employees of so-\n\ncial e-commerce in China is expected to reach 48.01 million in 2019, up by 58.3\n\npercent year on year, and the market size is expected to reach 2060.58 billion yuan, up\n\nby 63.2% year on year. Social e-commerce has become a large scale, and the high\n\ngrowth cannot be ignored. Different from e-commerce platforms such as Taobao, so-\n\ncial e-commerce is at the end of online retail. It carries out trading activities through\n\nsocial software and uses social interaction, user generated content and other means to\n\nassist the purchase and sale of goods. At the same time, sellers on social network use\n\ndifferent social software without uniform registration, have no systematic classification\n\nof products for sale, and there are no standardized terms for product description.\n\nThese bring great difficulty to the accurate classification of user portrait. This paper\n\nproposes a method based on the NLP classification model, which can realize accurate\n\n© The Author(s). 2021 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nEURASIP Journal on Image\nand Video Processing\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 \nhttps://doi.org/10.1186/s13640-020-00545-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-020-00545-z&domain=pdf\nhttp://orcid.org/0000-0001-9638-0201\nmailto:shaoshuaib@163.com\nmailto:shaoshuaib@163.com\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nbusiness classification of social e-commerce based on social information of social e-\n\ncommerce. This method analyzes 38,970 sellers on social networks and establishes a\n\ndeep learning model based on BERT to accurately classify the merchandise of sellers.\n\nIn addition, we introduced the OCR algorithm to extract the text information in the\n\npicture and superimposed it on the social content data, which effectively improved the\n\nclassification accuracy. The final experiment shows that the measured accuracy is more\n\nthan 90%.\n\n2 Related work\n2.1 Natural language processing\n\nIn order to analyze e-commerce business classification based on social data of sellers on a\n\nsocial network, the text needs to be analyzed based on the NLP correlation algorithm.\n\nThe rapid development of NLP at the present stage is due to the neural network language\n\nmodel (NNLM) Bengio et al. [1] proposed in 2003. Researchers have been trying to realize\n\nthe end-to-end classification recognition by using a neural network as a classifier in the\n\ntext classification research based on word embedding. Kim first introduces the convolu-\n\ntional neural network (CNN) into the study of text classification. The network structure is\n\na dropout full connection layer and a softmax layer connected after one convolution layer\n\n[2]. Although this algorithm achieves good results in various benchmark tests, it cannot\n\nobtain long-distance text dependency due to the limitation of network structure. There-\n\nfore, Tencent AI Lab proposed DPCNN, which further enhanced the extraction capacity\n\nof long-distance text dependency by deepening CNN [3].\n\nSocial content data includes multimedia text data and picture data. With the help of\n\nOCR, we extract the text in the picture and convert the picture data into text data. Text\n\nis a kind of sequential data, and the classification of it by recurrent neural network\n\n(RNN) has been the focus of long-term research in academia [4]. As a variation of\n\nRNN, long short-term memory (LSTM) adds control units such as forgetting gate, in-\n\nput gate, and output gate on the original basis, which solves the problem of gradient\n\nexplosion and gradient disappearance in the long sequence training of RNN and pro-\n\nmotes the use of RNN [5]. By introducing the sharing information mechanism, Liu\n\net al. further improved the accuracy of the RNN algorithm in the text multi-\n\nclassification task and achieved good results in four benchmark text classifications [6].\n\nHowever, Word vectors cannot be constructed in Word embedding to solve the\n\nproblem of polysemy. Even though different semantic environments are considered\n\nduring training, the result of training is still one word corresponding to one row vector.\n\nConsidering the widespread phenomenon of polysemy, Peters et al. propose embed-\n\ndings from language model (ELMO) to address the impact of polysemy on natural lan-\n\nguage modeling [7]. ELMO uses a feature-based form of pre-training. First, two-way\n\nLSTM is used to pre-train the corpus, and then word embedding resulting from train-\n\ning is adjusted by double-layer two-way LSTM when processing downstream tasks to\n\nadd more grammatical and semantic information according to the context words.\n\nThe ability of ELMO to extract features is limited for choosing LSTM as the feature\n\nextractor instead of Transformer [8], and ELMO’s bidirectional splicing method is also\n\nweak in feature fusion. Therefore, Devlin et al. propose the BERT model, taking Trans-\n\nformer as a feature extractor to pre-train large-scale text corpus [9].\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 2 of 12\n\n\n\n2.2 User analysis of social networks\n\nUser analysis is an important part of social network analysis. Most existing studies use\n\nuser-generated content or social links between users to simulate users. Wu et al. mod-\n\neled users on the content curation social network (CCSN) in the unified framework by\n\nmining user-generated content and social links [10]. They proposed a potential Bayes-\n\nian model, multilevel LDA (MLLDA), that could represent users of potential interest\n\nfound in social links formed by text descriptions contributed by users and information\n\nsharing. In 2017, Wu et al. proposed a latent model [11], trying to explain how the so-\n\ncial network structure and users’ historical preferences change over time affect each\n\nuser’s future behavior and predict each user’s consumption preferences and social con-\n\nnections in the near future. Malli et al. proposed a new online social network user pro-\n\nfile rating model [12], which solved the problem of large and complicated user data. In\n\nterms of data analysis platform, Chen et al. [13] developed a big data platform for the\n\nstudy of the garlic industry chain. Garlic planting management, price control, and pre-\n\ndiction were realized through data collection, storage, and pretreatment. Ning et al.\n\n[14] designed a ga-bp hybrid algorithm based on the fuzzy theory and constructed an\n\nair quality evaluation model by combining the knowledge of BP neural network, genetic\n\nalgorithm, and fuzzy theory. Yin et al. [15] studied two methods of extracting supervis-\n\nory relations and applied them to the field of English news. One is the combination of\n\nsupport vector machine and principal component analysis, and the other is the combin-\n\nation of support vector machine and CNN, which can extract high-quality feature vec-\n\ntors from sentences of support vector machine. In the social apps, the data we obtain is\n\nmostly image data, so we introduced the OCR technology to identify text information\n\nin images.\n\n3 Data collection\nIn order to analyze the behavior patterns of social e-commerce, we developed an auxil-\n\niary tool for social e-commerce. In this tool, sellers on a social network are provided\n\nwith the independent running environment of social software and the automatic auxil-\n\niary ability, and the information acquisition module of the auxiliary process is used to\n\ncollect the social information published by sellers on a social network, which is\n\nuploaded to the background server for model training. We provided this tool to nearly\n\n10,000 sellers on a social network who participated in the experiment to obtain their\n\nsocial information in their e-commerce activities.\n\n3.1 Overall structure\n\nThe whole data collection scheme is mainly composed of two parts: intelligent space\n\napp and background server. The overall architecture is shown in Fig. 1. Intelligent\n\nspace app is deployed in the mobile phones of sellers on a social network and imple-\n\nmented based on the application layer of the Android platform, providing sellers on a\n\nsocial network with a secure container for the independent operation of social software.\n\nThe app contains the automatic assistant module, which provides the automatic assist-\n\nant capability of various business processes for seller, and collects the social informa-\n\ntion in the auxiliary process through the information grasping module. The collected\n\ninformation is cached and uploaded locally through the information collection service.\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 3 of 12\n\n\n\nThe background server is responsible for receiving the collected data uploaded by the\n\nintelligent space, preprocessing the data first, and then classifying the social e-\n\ncommerce through the data based on the machine learning classification model, and fi-\n\nnally storing the classification results.\n\n3.1.1 Security container\n\nThe security container is designed to allow social software to run independently with-\n\nout modifying the OS or gaining root privileges. The basic principle of its realization is\n\nto create an independent container process; load APK file of social software dynamic-\n\nally; monitor and intercept process communication interface such as Binder IPC\n\nthrough Libc hook, Java reflection, dynamic proxy, and other technical means; and col-\n\nlect social information through an automatic assistant module. The main part of the\n\ncontainer is composed of an application layer module and a service layer module.\n\nThe application layer module is responsible for the process startup and execution of\n\nsocial software, and its main functions include three parts.\n\n3.1.1.1 Interactive interception The application layer module intercepts the inter-\n\naction between the application process and the underlying system in the container and\n\nmodifies the calling logic. By hook or dynamic proxy of system library API and Binder\n\ncommunication interface, the application layer module blocks all interfaces that interact\n\nwith the system during the execution of social software and controls the process\n\nboundary of interaction between social applications and system services.\n\n3.1.1.2 Social information collection The loading of the automatic auxiliary module\n\nby social software is realized when initializing the process of social application.\n\nThe application layer module injects the corresponding plugins in the automatic\n\nassistant module into the social application process. The automatic assistance mod-\n\nule provides a number of e-commerce auxiliary functions for sellers on a social\n\nnetwork, including customer acquisition, social customer relationship management\n\nLinux Kernel\n\nBinder Mode\n\nIntelligent Space\n\nService Layer Mode\nAMS Proxy PMS Proxy\n\nApplication Layer Mode\nSocial App\n\nInteractive \ninterception\n\nautomatic \nassistance  \nmodule\n\nInformation\nCollection \n\nBinder IPC\n\nBinder IPC\n\nBinder \nIPC Backgroud\n\n Server\n\nInternet\n\nFig. 1 The overall architecture diagram of the data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 4 of 12\n\n\n\n(SCRM), group management, sales assistance, and daily affairs. Sellers on social\n\nnetworks publish social information with commercial attributes through auxiliary\n\nfunctions, then the automatic auxiliary module will automatically collect the social\n\ninformation and send it to the information collection service for processing.\n\n3.1.1.3 Local processing of social information When the information collection ser-\n\nvice receives the social information collected by the automatic auxiliary module, the\n\ndata will be compressed and encrypted in the local cache. The service then uploads the\n\ncollected data to the background server periodically through the timer, and HTTPS is\n\nused to ensure data transmission security.\n\nThe main function of the service layer module is to take over the call logic modified\n\nby the application layer module by simulating the system service modify the parameters\n\nin the communication process and finally call the real system service. The service layer\n\nmodule exists in the container as an independent process. It focuses on the simulation\n\nof activity manager service (AMS) and package manager service (PMS) and realizes the\n\nsupport of system services in the process of launching and running social software.\n\n3.1.2 Background server\n\nThe background server mainly realizes the machine learning model processing of the\n\ncollected social data, including the functions of data preprocessing, data training, classi-\n\nfication, and result storage. The core processing logic will be described in chapter 5.\n\n3.2 Key processes\n\nThere are four key processes in the process of social information collection and pro-\n\ncessing. They are social software process initialization, social software process\n\nIntelligent \nSpace App \nlaunched\n\nProcess Boundaries\n\nUser Process\n\nSocial software process \ninitialization\n\nlaunching social \nsoftware\n\ninject automatic \nauxiliary\n\nSocial software process \nexecution\n\nRun the plug-in\n\nCollect social \ninformation\n\nProcess Boundaries\n\nUser Process\n\nLocal processing of \nsocial information\n\nBatch upload \nprocessed social \n\ninformation\n\nEncrypt, compress \nand store social \n\ninformation\n\nInternet\n\nInformation collecting \nservice process\n\nBackground processing \nof social information\n\nReceiving social \ninformation\n\nPreprocessing social \ninformation\n\nThe Server\n\nMachine learning \ncategorizes social \n\ninformation\n\nStore the \nclassification results\n\nFig. 2 Key flow chart of data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 5 of 12\n\n\n\nexecution, local processing of social information, and background processing of social\n\ninformation. The complete process is shown in Fig. 2.\n\n3.2.1 Social software process initialization\n\nWhen launching social software, the intelligent space will first intercept the callback\n\nfunction of the life cycle of all its components, then realize the process loading of the\n\nautomatic auxiliary module during the process initialization.\n\n3.2.2 Social software process execution\n\nThe process execution is completed by the application layer module and service layer\n\nmodule together. Sellers on a social network use automatic auxiliary modules to\n\ncomplete business activities, trigger information capture module to collect social infor-\n\nmation, and send it to the information collection service for subsequent processing.\n\n3.2.3 Local processing of social information\n\nThe local processing of social information is mainly completed by the information col-\n\nlection service. In order to ensure the safe storage and transmission of the collected so-\n\ncial information, the information collection service first adopts the encryption and\n\ncompression method to realize the local security cache and then adopts the HTTPS se-\n\ncure communication and transmission protocol to upload the data.\n\n3.2.4 Background processing of social information\n\nThe background processing of social information is completed by the background ser-\n\nver. The server first receives the social information uploaded by the intelligent space,\n\nnext decrypts and decompresses the social information, cleans the plaintext data, uses\n\nthird-party OCR technology to identify text information in images, and adds it to the\n\nuser’s social information after simple data processing. Then, the classification of sellers\n\non a social network is realized through the data based on machine learning modeling.\n\nFinally, the classification results are stored in the target database.\n\n4 Methods\nTo classify the business attributes of social e-commerce based on the information of\n\nsellers on a social network, traditional feature matching scheme and classification clus-\n\ntering scheme based on machine learning can be used to establish the model. In this\n\nchapter, we introduce the scheme based on term frequency-inverse document fre-\n\nquency (TF-IDF) clustering and the classification scheme based on BERT.\n\n4.1 Feature classification and TF-IDF clustering\n\n4.1.1 Feature classification\n\nWe randomly select 5000 sellers on a social network from the data collected by the\n\nbackground server and extracted the text data of their social information for analysis.\n\nEach social e-commerce user contains an average of 50 social text data. Based on the\n\ncontent, we manually classify social e-commerce into 11 categories. With the help of e-\n\ncommerce platforms like JD.COM, 50–100 keywords are sorted out for each category,\n\nand these keywords are screened and expanded according to the language habits of\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 6 of 12\n\n\n\nsellers on a social network. On this basis, we collect all the social information of each\n\nsocial network seller, cut and remove word segmentation, and match the results with\n\nthe keywords of the selected 11 categories. The number of keywords that are matched\n\nis counted as the matching degree. According to the situation of different classification,\n\nthe threshold of matching degree is determined by manual screening of some results,\n\nand then all social e-commerce is classified according to the threshold. After\n\noptimization and verification, the accuracy of the classical feature matching scheme fi-\n\nnally reached 40%. However, due to the simplicity of the rules of the feature matching\n\nscheme, the small optimization space, the high misjudgment rate of the scheme, and\n\nthe large human intervention in the basic word segmentation process, it is difficult to\n\ncover various situations of social e-commerce due to the limitation of these basic key-\n\nwords, thus making it insensitive to the dynamic changes of new hot words of social e-\n\ncommerce.\n\n4.1.2 TF-IDF clustering\n\nTo achieve the goal of accurate classification of social e-commerce, we designed a\n\nscheme based on TF-IDF clustering. Term frequency-inverse document frequency (TF-\n\nIDF) is a commonly used weighted technique for information retrieval and text mining\n\nto evaluate the importance of a single word to a document in a set of documents or a\n\ncorpus. In this scheme, the social information of each social e-commerce user is\n\nmapped as one file set of TF-IDF, and all texts of all sellers on a social network are\n\nmapped as the whole corpus. The words with the highest frequency used by each social\n\ne-commerce user are the most representative words in this document and become key-\n\nwords. Category labels can be generated to calculate the probability that a document\n\nbelongs to a certain category using the naive Bayes algorithm formula. The advantages\n\nof TF-IDF clustering to achieve the classification of sellers on the social network in-\n\nclude the following: (1) clear mapping; (2) emphasize the weight of keywords and lower\n\nthe weight of non-keywords; (3) compared with other machine learning algorithms, the\n\ncharacteristic dimension of the model is greatly reduced to avoid the dimension disas-\n\nter; and (4) while improving the efficiency of classification calculation, ensure that the\n\nclassification effect has a good accuracy and recall rate. The architecture of the entire\n\nsolution is shown in Fig. 3.\n\nIn the text preprocessing stage, the first thing to do is to format the social informa-\n\ntion, mainly including deleting the space, deleting the newline character, merging the\n\nsocial e-commerce text, and so on, and finally getting the text to be processed for word\n\nsegmentation. In this scheme, we choose Jieba’s simplified mode for word segmenta-\n\ntion, then filter out the noise by filtering the stop words (e.g., yes, ah, etc.).\n\nIn the stage of establishing the vector space model, the first step is to load the train-\n\ning set and take the pre-processed social information of each social e-commerce user\n\nas a document. The next step is to generate a dictionary, by adding every word that ap-\n\npears in the training set to it, using the complete dictionary to calculate the TF-IDF\n\nvalue of each document. In this scheme, CountVectorizer and TfidfTransformer in Py-\n\nthon’s Scikit-Learn library are used. CountVectorizer is used to convert words in the\n\ntext into word frequency matrix, TfidfTransformer is used to count the TF-IDF value\n\nof each word in each document, and the top20 words in each document are taken as\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 7 of 12\n\n\n\nkeywords of sellers on a social network. After this step, the keywords with a large TF-\n\nIDF value in each document are the most representative words in the document, which\n\nbecome the keyword set of the social e-commerce user. Finally, the naive Bayes method\n\nis used to generate the category label, and the document vectors belonging to the same\n\ncategory in the TF-IDF matrix are added to form a matrix of m*n, where m represents\n\nthe number of categories and n represents the number of documents. The weight of\n\neach word is divided by the total weight of all words of the class, to calculate the prob-\n\nability that a document belongs to a certain class.\n\nIn the model optimization stage, we optimize the whole scheme model by adjusting\n\nthe stop word set, adjusting parameters (including CountVectorizer, TfidfTransformer\n\nclass construction parameters), and adjusting the category label generation method.\n\nThe main idea of TFIDF is if a word or phrase appears in an article with a high fre-\n\nquency of TF, and rarely appears in other articles, it is considered that the word or\n\nphrase has a good classification ability and is suitable for classification. TFIDF is actu-\n\nally: TF * IDF, TF is term frequency and IDF is inverse document frequency.\n\nIn a given document, word frequency refers to the frequency of a given word in the\n\ndocument. This number is a normalization of the number of words to prevent it from\n\nbeing biased towards long documents. For the word ti in a particular document, its im-\n\nportance can be expressed as:\n\ntf i; j ¼\nj D j\n\nj j : ti∈d j\n� �\n\nj\n\namong them:\n\n|D|: The total number of files in the corpus\n\n∣{j : ti ∈ dj}∣: The number of documents containing the term ti (i.e., the number of\n\ndocuments in ni, j ≠ 0). If the term is not in the corpus, it will cause the dividend to be\n\nzero, so it is generally used 1 + ∣ {j : ti ∈ dj}∣.\n\nand then:\n\n Social e-\ncommerce data\n\nData preparation\n\nFormat processing\n\nFilter stop words\n\nText preprocessing\n\nGenerate directory\n\nBuild the vector space and \nTF-IDF\n\nGenerate category tags\n\nBayesian classifier\n\nText articiple\n\nLoad training set \n\nBuild tf matrix \n\nbuild vector\n\nbuild matrix \n\nConditional probability \nmatrix\n\nModel optimization\n\nFig. 3 TF-IDF scheme framework\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 8 of 12\n\n\n\ntfidf i; j ¼ tf i; j � idf i\n\nA high word frequency in a particular document and a low document frequency of\n\nthe word in the entire document collection can produce a high-weight TF-IDF. There-\n\nfore, TF-IDF tends to filter out common words and keep important words.\n\n4.2 Classification scheme based on BERT\n\n4.2.1 Data label\n\nWe manually classify and mark the data of sellers on a social network according to the\n\ncharacteristics of the products. Classified labels include 38,970 items and 17 categories\n\nof data, including 3c, dress, food, car, house, beauty, makeup, training, jewelry, promo-\n\ntion, medicine and health, phone charge recharge, finance, card category, cigarettes, es-\n\nsays, and others. The pre-processing phase removes emojis, numbers, and spaces from\n\nthe text through Unicode encoding.\n\n4.2.2 Classification scheme\n\nIn the BERT model, Transformer, as an encoder-decoder model based on attention\n\nmechanism, solves the problem that RNN cannot deal with long-distance dependence\n\nand the model cannot be parallel, improving the performance of the model without re-\n\nducing the accuracy. At the same time, BERT introduced the shading language model\n\n(MLM, masked language model) and context prediction method, further enhance the\n\ntwo-way training of the ability of feature extraction and text. MLM uses Transformer\n\nencoders and bilateral contexts to predict random masked tokens to pre-train two-way\n\ntransformers. This makes BERT different from the GPT model, which can only conduct\n\none-way training and can better extract context information through feature fusion.\n\nAnaphase prediction is more embodied in QA and NLI. Therefore, we choose the\n\nBERT model based on the bidirectional coding technology of pre-training and attention\n\nmechanism to classify sellers on a social network.\n\nWe chose the official Chinese pre-training model of Google as the pre-training model\n\nof the experiment: BERT-Base which is Chinese simplified and traditional, 12-layer,\n\n768-hidden, 12-head, 110M parameters [16]. This pre-training model is obtained by\n\nGoogle’s unsupervised pre-training on a large-scale Chinese corpus. On this basis, we\n\nwill carry out fine-tuning to realize the classification model of sellers on a social net-\n\nwork. When dividing the data set, we divided 38,970 pieces of data into training set\n\nand verification set according to the ratio of 6:4, that is, 23,382 pieces of training set\n\nand 15,588 pieces of verification set.\n\n5 Results and discussion\n5.1 TF-IDF clustering scheme\n\nThe computer used in the experiment is configured with AMD Ryzen R5-4600H CPU,\n\n16G memory, and windows10 64bit operating system. First, the default construction\n\nparameters are used, and the average accuracy of each classification is 45.7%. Next, the\n\nparameters are adjusted through a genetic algorithm, and 100 rounds of genetic algo-\n\nrithm optimization are performed, then the average accuracy reached the highest value\n\nof 52.5%. In the process of genetic algorithm, statistical estimation of algorithm time is\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 9 of 12\n\n\n\nalso carried out. On average, on this data set, the running time of each round of the\n\nTF-IDF model is about 28 s.\n\nExperiments show that the accuracy of the TF-IDF clustering scheme has been\n\nimproved after optimization, and it has a certain reference value for the classifica-\n\ntion of sellers on a social network, but there is still a big gap from the accurate\n\nclassification. We found three reasons after analyzing the experimental results. (1)\n\nCompared to the feature matching scheme, the TF-IDF-based model is improved\n\nto some extent. However, the input of the model is still the result of direct word\n\nsegmentation, and more information is lost in the word segmentation process, such\n\nas the semantic information of previous and later texts and the repetition fre-\n\nquency of corpus, which are relatively important in the process of natural language\n\nprocessing. (2) The classification problem of sellers on a social network is compli-\n\ncated. This model does not analyze the correlation between words and is essen-\n\ntially an upgraded version of word frequency statistics, which makes it difficult to\n\nimprove the accuracy after reaching a certain value. (3) For the optimization of the\n\nmodel, only the parameters of the intermediate function are adjusted, and the\n\nmethod is not upgraded. Therefore, the machine learning scheme based on TF-IDF\n\nclustering cannot solve the problem of accurate classification of sellers on a social\n\nnetwork. In the next chapter, we will introduce a scheme based on deep learning\n\nto achieve the goal of classifying sellers on a social network.\n\n5.2 Classification scheme based on BERT\n\nText classification fine-tuning is to serialize the preprocessed text information\n\ntoken and input BERT, and select the final hidden state of the first token [CLS] as\n\na sentence vector to output to the full connection layer, and then output the prob-\n\nability of obtaining various labels corresponding to the text through the softmax\n\nlayer. The experimental schematic diagram is shown in Figs. 4 and 5. The max-\n\nimum length of the sequence (ma_seq_length) is set to 256 according to the actual\n\ntext length of the social information data set of the sellers on a social network and\n\nFig. 4 Text message token serialization\n\nFig. 5 Text classification BERT fine-tuning model structure diagram\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 10 of 12\n\n\n\n\n\nthe batch_size and learning rate adopt the official recommended values of 32 and\n\n2e−5. In addition, we also adjust the super parameter num_train_epochs and in-\n\ncrease the number of training epochs (num_train_epochs) from 3 to 9 to improve\n\nthe recognition rate of the model (Table 1). The results are shown in Table 2.\n\nWe select an additional 9500 text data of sellers on social networks and test the\n\nmodel after the same preprocessing. The accuracy rate is 90.5%, which is lower than\n\nthat of the verification set (96.2%). The reason may be that the data of the test set con-\n\ntains a large number of commodity terms not included in the corpus and training set,\n\nand the text description of these commodities is too colloquial. Sellers on a social net-\n\nwork often use colloquial words in the industry to replace the standard product names\n\nwhen releasing product information, such as “Bobo” instead of “Botox,” which to some\n\nextent limits the accuracy of text-based classification in the social e-commerce market\n\nscene.\n\n6 Conclusion\nThe classification model proposes in this paper achieves an accuracy of 90.5% in the\n\ntest data. However, there are still some problems such as non-standard description text.\n\nA corpus with a high correlation with a social e-commerce environment will be estab-\n\nlished in order to further improve the accuracy of social e-commerce classification. At\n\nthe same time, we will use the knowledge distillation technology to compress and refine\n\nthe existing model, so as to improve the model recognition rate while simplifying the\n\nmodel and improving the operational performance [16]. In addition, in view of the high\n\nlabor cost and time cost of large-scale data marking, the next step will be trying to\n\nmake full use of semi-supervised learning to train unlabeled data and labeled completed\n\ndata [17]. The full use of large-scale unlabeled data is conducive to further improving\n\nthe accuracy and generalization ability of the model, as well as the analysis and process-\n\ning of emerging products, providing strong data support for the model landing. Since\n\nthe image data have also been studied to profiling the users in a social network [18]\n\nand perceptual image hashing schemes are proposed [19], we will improve our model\n\nso that the image and text data are combined for analysis.\n\nTable 1 Corresponding table of epoch and accuracy\n\nEpoch eval_accuracy (%)\n\n3 95.84\n\n6 96.05\n\n9 96.2\n\nThe training results are shown in Table 2, and the recognition rate is 96.2%\n\nTable 2 Text information classification results of sellers on social network\n\nResults Value\n\neval_accuracy 96.2%\n\neval_loss 0.25033528\n\nglobal_step 6024\n\nLoss 0.25023073\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 11 of 12\n\n\n\nAbbreviations\nBERT: Bidirectional Encoder Representations from Transformers; DPCNN: Deep pyramid convolutional neural networks;\nOCR: Optical character recognition\n\nAcknowledgements\nNot applicable\n\nAuthors’ contributions\nHaoliang Cui designed the scheme and carried out the experiments. Shuai Shao gave suggestions on the structure of\nthe manuscript and participated in modifying the manuscript. All authors read and approved the final manuscript.\n\nFunding\nNational Natural Science Foundation of China (Award Number 61370195, U1536121)\n\nAvailability of data and materials\nhttps://github.com/cuihaoliang/User-portraits-of-social-e-commerce\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAuthor details\n1Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia, Beijing University of Posts and\nTelecommunications, Beijing 100876, China. 2China Information Technology Security Evaluation Center, Beijing 100085,\nChina. 3Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100088, China.\n\nReceived: 16 March 2020 Accepted: 25 December 2020\n\nReferences\n1. Y. Bengio, R. Ducharme, P. Vincent, et al., A neural probabilistic language model. J. Mach. Learn. Res. 3, 1137–1155\n\n(2003)\n2. Kim Y. Convolutional neural networks for sentence classification arXiv preprint arXiv:1408.5882, 2014.\n3. R. Johnson, T. Zhang, Deep pyramid convolutional neural networks for text categorization [C]//Proceedings of the 55th\n\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (2017), pp. 562–570\n4. Otter D W, Medina J R, Kalita J K. A survey of the usages of deep learning in natural language processing arXiv preprint\n\narXiv:1807.10854, 2018.\n5. R. Jozefowicz, W. Zaremba, I. Sutskever, An empirical exploration of recurrent network architectures [C]//International\n\nconference on machine learning (2015), pp. 2342–2350\n6. Liu P, Qiu X, Huang X. Recurrent neural network for text classification with multi-task learning arXiv preprint arXiv:1605.\n\n05101, 2016.\n7. Peters M E, Neumann M, Iyyer M, et al. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.\n8. A. Vaswani, N. Shazeer, N. Parmar, et al., Attention is all you need [C]//Advances in neural information processing systems\n\n(2017), pp. 5998–6008\n9. Devlin J, Chang M W, Lee K, et al. BERT: pre-training of deep bidirectional transformers for language understanding\n\narXiv preprint arXiv:1810.04805, 2018.\n10. L. Wu et al., MLLDA: multi-level LDA for modelling users on content curation social networks. Neurocomputing 236, 73–\n\n81 (2017)\n11. L. Wu et al., Modeling the evolution of users’ preferences and social links in social networking services. IEEE Transact.\n\nKnowledge. Data. Eng. 29.6, 1240–1253 (2017)\n12. M. Malli, N. Said, A. Fadlallah, A new model for rating users’ profiles in online social networks. Comput. Information. Sci.\n\n10.2, 39–51 (2017)\n13. W. Chen et al., Development and application of big data platform for garlic industry chain. Comput. Mater. Continua 58.\n\n1, 229 (2019)\n14. M. Ning et al., GA-BP air quality evaluation method based on fuzzy theory. Comput. Mater. Continua 58.1, 215–227 (2019)\n15. Yin, Libo, et al. Relation extraction for massive news texts. Tech Science Press, CMC,60, no.1(2019), pp.275-285.\n16. Sun S, Cheng Y, Gan Z, et al. Patient knowledge distillation for BERT model compression arXiv preprint arXiv:1908.09355, 2019.\n17. Yalniz I Z, Jégou H, Chen K, et al. Billion-scale semi-supervised learning for image classification. arXiv preprint arXiv:1905.\n\n00546, 2019.\n18. Yaqiong Qiao, Xiangyang Luo, Chenliang Li, et al. Heterogeneous graph-based joint representation learning for users\n\nand POIs in location-based social network, Inf. Process. Manag., 2020, 57, 102151-1~102151-17\n19. Jinwei Wang, Hao Wang, Jian Li, Xiangyang Luo, Yun-Qing Shi, Sunil Kr. Jha, Detecting double JPEG compressed color\n\nimages with the same quantization matrix in spherical coordinates, IEEE Trans. on CSVT, doi: 10.1109/TCSVT.2019.\n2922309.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 12 of 12\n\n\n\nhttps://github.com/cuihaoliang/User-portraits-of-social-e-commerce\n\n\tAbstract\n\tIntroduction\n\tRelated work\n\tNatural language processing\n\tUser analysis of social networks\n\n\tData collection\n\tOverall structure\n\tSecurity container\n\tBackground server\n\n\tKey processes\n\tSocial software process initialization\n\tSocial software process execution\n\tLocal processing of social information\n\tBackground processing of social information\n\n\n\tMethods\n\tFeature classification and TF-IDF clustering\n\tFeature classification\n\tTF-IDF clustering\n\n\tClassification scheme based on BERT\n\tData label\n\tClassification scheme\n\n\n\tResults and discussion\n\tTF-IDF clustering scheme\n\tClassification scheme based on BERT\n\n\tConclusion\n\tAbbreviations\n\tAcknowledgements\n\tAuthors’ contributions\n\tFunding\n\tAvailability of data and materials\n\tCompeting interests\n\tAuthor details\n\tReferences\n\tPublisher’s Note\n\n",
      "metadata_storage_path": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9saWJyYXJ5L3MxMzY0MC0wMjAtMDA1NDUtei5wZGY1",
      "authors": [
        "Haoliang Cui1,",
        "Shuai Shao2",
        "Shaozhang Niu1,",
        "Chengjie Shi3",
        "Lingyu Zhou1",
        "Bengio",
        "Kim",
        "Liu",
        "Peters",
        "Devlin",
        "Wu",
        "Bayes",
        "Malli",
        "Chen",
        "Ning",
        "Yin",
        "Binder",
        "Jieba",
        "thon",
        "Tokels",
        "Haoliang Cui",
        "Shuai Shao",
        "Y. Bengio",
        "R. Ducharme",
        "P. Vincent",
        "J. Mach",
        "Kim Y.",
        "R. Johnson",
        "T. Zhang",
        "Otter D W",
        "Medina J R",
        "Kalita J K",
        "R. Jozefowicz",
        "W. Zaremba",
        "I. Sutskever",
        "Liu P",
        "Qiu X",
        "Huang X",
        "Peters M E",
        "Neumann M",
        "Iyyer M",
        "A. Vaswani",
        "N. Shazeer",
        "N. Parmar",
        "Devlin J",
        "Chang M W",
        "Lee K",
        "L. Wu",
        "M. Malli",
        "N. Said",
        "A. Fadlallah",
        "W. Chen",
        "M. Ning",
        "Libo",
        "Sun S",
        "Cheng Y",
        "Gan Z",
        "Yalniz I Z",
        "Jégou H",
        "Chen K",
        "Yaqiong Qiao",
        "Xiangyang Luo",
        "Chenliang Li",
        "Jinwei Wang",
        "Hao Wang",
        "Jian Li",
        "Yun-Qing Shi",
        "Sunil Kr. Jha"
      ],
      "institutions": [
        "Security Evaluation Center",
        "BERT",
        "Taobao",
        "EURASIP Journal",
        "EURASIP",
        "Tencent AI Lab",
        "CNN",
        "RNN",
        "ELMO",
        "curation social network",
        "CCSN",
        "ule",
        "Intelligent Space",
        "AMS",
        "PMS",
        "Binder IPC",
        "SCRM",
        "activity manager service",
        "process",
        "JD",
        "JD.COM",
        "TF",
        "IDF",
        "TF-IDF",
        "Scikit-Learn",
        "TFIDF",
        "Unicode",
        "MLM",
        "Google",
        "DPCNN",
        "National Natural Science Foundation of",
        "Key Laboratory of Intelligent Telecommunication Software and Multimedia",
        "Beijing University of Posts",
        "Information Technology Security Evaluation Center",
        "of Information Engineering",
        "Chinese Academy of Sciences",
        "Association for Computational Linguistics",
        "arXiv",
        "IEEE Transact",
        "Tech Science Press",
        "IEEE",
        "Springer Nature"
      ],
      "key_phrases": [
        "2China Information Technology Security Evaluation Center",
        "Creative Commons Attribution 4.0 International License",
        "social network seller classification scheme",
        "other third party material",
        "2019 China social e-commerce industry",
        "mobile payment technology",
        "Creative Commons licence",
        "automated assistance capabilities",
        "social network apps",
        "social network applications",
        "original author(s",
        "RESEARCH Open Access",
        "different social software",
        "NLP classification model",
        "deep learning model",
        "Video Processing Cui",
        "social network use",
        "social information",
        "author information",
        "other means",
        "2021 Open Access",
        "systematic classification",
        "text information",
        "Haoliang Cui",
        "mobile phones",
        "assistance process",
        "Machine learning",
        "social relations",
        "social interaction",
        "The Author",
        "classification method",
        "accurate classification",
        "model training",
        "Shuai Shao2",
        "Shaozhang Niu",
        "Chengjie Shi3",
        "Lingyu Zhou1",
        "Full list",
        "hot topic",
        "recent years",
        "transaction money",
        "main activities",
        "operating environment",
        "final experiment",
        "continuous improvement",
        "one kind",
        "commodity trading",
        "rapid development",
        "ment report",
        "Internet society",
        "market size",
        "large scale",
        "high growth",
        "online retail",
        "trading activities",
        "same time",
        "uniform registration",
        "standardized terms",
        "product description",
        "great difficulty",
        "appropriate credit",
        "credit line",
        "intended use",
        "statutory regulation",
        "permitted use",
        "copyright holder",
        "EURASIP Journal",
        "User model",
        "38,970 sellers’ information",
        "user portrait",
        "doi.org",
        "orcid.org",
        "commerce platforms",
        "Correspondence",
        "shaoshuaib",
        "Beijing",
        "article",
        "Abstract",
        "number",
        "users",
        "traditional",
        "merchandise",
        "server",
        "data",
        "picture",
        "help",
        "OCR",
        "BERT",
        "accuracy",
        "Keywords",
        "1 Introduction",
        "employees",
        "percent",
        "billion",
        "Taobao",
        "content",
        "purchase",
        "sale",
        "goods",
        "products",
        "paper",
        "sharing",
        "adaptation",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "images",
        "permission",
        "creativecommons",
        "licenses",
        "crossmark",
        "dropout full connection layer",
        "four benchmark text classifications",
        "content curation social network",
        "neural network language model",
        "various benchmark tests",
        "Tencent AI Lab",
        "long short-term memory",
        "Most existing studies",
        "tional neural network",
        "recurrent neural network",
        "one convolution layer",
        "one row vector",
        "Natural language processing",
        "different semantic environments",
        "end classification recognition",
        "multi- classification task",
        "sharing information mechanism",
        "long-distance text dependency",
        "bidirectional splicing method",
        "social content data",
        "e-commerce business classification",
        "social network analysis",
        "multimedia text data",
        "long sequence training",
        "large-scale text corpus",
        "text classification research",
        "NLP correlation algorithm",
        "double-layer two-way LSTM",
        "softmax layer",
        "user-generated content",
        "social data",
        "network structure",
        "semantic information",
        "social networks",
        "long-term research",
        "Video Processing",
        "social links",
        "one word",
        "sequential data",
        "2.2 User analysis",
        "BERT model",
        "Related work",
        "present stage",
        "good results",
        "extraction capacity",
        "control units",
        "original basis",
        "widespread phenomenon",
        "guage modeling",
        "feature-based form",
        "train- ing",
        "downstream tasks",
        "context words",
        "feature extractor",
        "feature fusion",
        "important part",
        "unified framework",
        "picture data",
        "word embedding",
        "Word vectors",
        "classification accuracy",
        "OCR algorithm",
        "gradient disappearance",
        "output gate",
        "RNN algorithm",
        "38,970 sellers",
        "addition",
        "order",
        "NNLM",
        "Bengio",
        "Researchers",
        "classifier",
        "Kim",
        "CNN",
        "study",
        "limitation",
        "fore",
        "kind",
        "focus",
        "academia",
        "variation",
        "problem",
        "explosion",
        "Liu",
        "al.",
        "polysemy",
        "Peters",
        "dings",
        "ELMO",
        "impact",
        "grammatical",
        "ability",
        "features",
        "Transformer",
        "Devlin",
        "Cui",
        "Image",
        "Page",
        "Wu",
        "CCSN",
        "new online social network user",
        "air quality evaluation model",
        "machine learning classification model",
        "support vector machine",
        "mining user-generated content",
        "garlic industry chain",
        "Garlic planting management",
        "principal component analysis",
        "automatic assistant module",
        "various business processes",
        "independent running environment",
        "BP neural network",
        "ga-bp hybrid algorithm",
        "process communication interface",
        "file rating model",
        "social con- nections",
        "social informa- tion",
        "information acquisition module",
        "information grasping module",
        "cial network structure",
        "information collection service",
        "data analysis platform",
        "big data platform",
        "complicated user data",
        "data collection scheme",
        "independent container process",
        "users’ historical preferences",
        "Intelligent space app",
        "classification results",
        "ian model",
        "latent model",
        "Android platform",
        "independent operation",
        "consumption preferences",
        "genetic algorithm",
        "3 Data collection",
        "auxiliary process",
        "Overall structure",
        "APK file",
        "social apps",
        "social e-commerce",
        "social software",
        "information sharing",
        "secure container",
        "Security container",
        "multilevel LDA",
        "potential interest",
        "text descriptions",
        "future behavior",
        "near future",
        "price control",
        "fuzzy theory",
        "two methods",
        "ory relations",
        "English news",
        "combin- ation",
        "OCR technology",
        "behavior patterns",
        "iary ability",
        "background server",
        "two parts",
        "overall architecture",
        "application layer",
        "ant capability",
        "root privileges",
        "basic principle",
        "Binder IPC",
        "image data",
        "commerce activities",
        "iary tool",
        "MLLDA",
        "time",
        "Malli",
        "large",
        "terms",
        "Chen",
        "diction",
        "storage",
        "pretreatment",
        "knowledge",
        "Yin",
        "field",
        "combination",
        "tors",
        "sentences",
        "sellers",
        "experiment",
        "Fig.",
        "OS",
        "realization",
        "load",
        "ally",
        "intercept",
        "1.1",
        "interception automatic assistance  module Information Collection Binder IPC Binder IPC Binder",
        "social customer relationship management Linux Kernel Binder Mode",
        "AMS Proxy PMS Proxy Application Layer Mode Social App Interactive",
        "Intelligent Space Service Layer Mode",
        "social information Process Boundaries User Process",
        "machine learning model processing",
        "Binder communication interface",
        "IPC Backgroud Server",
        "data acquisition scheme Cui",
        "social software process initialization",
        "Interactive interception",
        "application layer module",
        "Social software process execution",
        "Social information collection",
        "service layer module",
        "automatic auxiliary module",
        "Space App",
        "social application process",
        "other technical means",
        "overall architecture diagram",
        "activity manager service",
        "package manager service",
        "sales assistance",
        "data transmission security",
        "dynamic proxy",
        "system library API",
        "customer acquisition",
        "four key processes",
        "core processing logic",
        "group management",
        "communication process",
        "social applications",
        "social network",
        "3.2 Key processes",
        "process startup",
        "independent process",
        "system service",
        "calling logic",
        "Local processing",
        "call logic",
        "data preprocessing",
        "data training",
        "auxiliary functions",
        "Java reflection",
        "main part",
        "three parts",
        "inter- action",
        "underlying system",
        "corresponding plugins",
        "daily affairs",
        "commercial attributes",
        "local cache",
        "main function",
        "result storage",
        "Batch upload",
        "Libc hook",
        "container",
        "interfaces",
        "boundary",
        "interaction",
        "loading",
        "Internet",
        "SCRM",
        "timer",
        "HTTPS",
        "parameters",
        "real",
        "simulation",
        "support",
        "fication",
        "chapter",
        "Encrypt",
        "1.2",
        "Machine learning categorizes social information",
        "traditional feature matching scheme",
        "3.2.1 Social software process initialization",
        "machine learning modeling",
        "Key flow chart",
        "automatic auxiliary modules",
        "third-party OCR technology",
        "local security cache",
        "complete business activities",
        "information capture module",
        "social information Preprocessing",
        "social network seller",
        "simple data processing",
        "50 social text data",
        "social e-commerce user",
        "service process",
        "complete process",
        "service layer",
        "matching degree",
        "tering scheme",
        "process loading",
        "4.1 Feature classification",
        "4.1.1 Feature classification",
        "business attributes",
        "classification scheme",
        "local processing",
        "plaintext data",
        "Background processing",
        "subsequent processing",
        "intelligent space",
        "callback function",
        "life cycle",
        "safe storage",
        "compression method",
        "cure communication",
        "target database",
        "TF-IDF) clustering",
        "TF-IDF clustering",
        "JD.COM",
        "language habits",
        "word segmentation",
        "manual screening",
        "classification clus",
        "different classification",
        "The Server",
        "transmission protocol",
        "components",
        "Sellers",
        "encryption",
        "next",
        "4 Methods",
        "quency",
        "analysis",
        "average",
        "11 categories",
        "50–100 keywords",
        "category",
        "basis",
        "situation",
        "threshold",
        "3.2.2",
        "other machine learning algorithms",
        "naive Bayes algorithm formula",
        "classical feature matching scheme",
        "basic word segmentation process",
        "Term frequency-inverse document frequency",
        "naive Bayes method",
        "large human intervention",
        "high misjudgment rate",
        "basic key- words",
        "word segmenta- tion",
        "one file set",
        "new hot words",
        "small optimization space",
        "vector space model",
        "text preprocessing stage",
        "word frequency matrix",
        "model optimization stage",
        "social e-commerce text",
        "highest frequency",
        "recall rate",
        "single word",
        "various situations",
        "dynamic changes",
        "weighted technique",
        "information retrieval",
        "text mining",
        "clear mapping",
        "characteristic dimension",
        "entire solution",
        "first thing",
        "newline character",
        "simplified mode",
        "Scikit-Learn library",
        "keyword set",
        "m*n",
        "scheme model",
        "representative words",
        "stop words",
        "top20 words",
        "Category labels",
        "classification calculation",
        "classification effect",
        "same category",
        "4.1.2 TF-IDF clustering",
        "TF-IDF matrix",
        "first step",
        "next step",
        "good accuracy",
        "complete dictionary",
        "TF-IDF value",
        "document vectors",
        "total weight",
        "verification",
        "simplicity",
        "rules",
        "goal",
        "importance",
        "documents",
        "corpus",
        "texts",
        "probability",
        "advantages",
        "keywords",
        "lower",
        "efficiency",
        "architecture",
        "Jieba",
        "noise",
        "pears",
        "training",
        "CountVectorizer",
        "TfidfTransformer",
        "thon",
        "categories",
        "Conditional probability matrix Model optimization",
        "category label generation method",
        "3 TF-IDF scheme framework Cui",
        "official Chinese pre-training model",
        "phone charge recharge",
        "random masked tokens",
        "bidirectional coding technology",
        "vector build matrix",
        "context prediction method",
        "shading language model",
        "masked language model",
        "large-scale Chinese corpus",
        "entire document collection",
        "class construction parameters",
        "inverse document frequency",
        "low document frequency",
        "Load training set",
        "good classification ability",
        "high word frequency",
        "category tags",
        "4.2 Classification scheme",
        "card category",
        "4.2.2 Classification scheme",
        "Data label",
        "tf matrix",
        "vector space",
        "context information",
        "Anaphase prediction",
        "encoder-decoder model",
        "GPT model",
        "classification model",
        "particular document",
        "main idea",
        "other articles",
        "Format processing",
        "Bayesian classifier",
        "high-weight TF-IDF",
        "Classified labels",
        "promo- tion",
        "pre-processing phase",
        "Unicode encoding",
        "attention mechanism",
        "long-distance dependence",
        "feature extraction",
        "bilateral contexts",
        "two-way transformers",
        "traditional, 12-layer",
        "110M parameters",
        "data set",
        "two-way training",
        "one-way training",
        "term frequency",
        "commerce data",
        "Data preparation",
        "stop word",
        "Text preprocessing",
        "Text articiple",
        "common words",
        "important words",
        "long documents",
        "j � idf",
        "total number",
        "phrase",
        "normalization",
        "portance",
        "D|",
        "files",
        "dividend",
        "dj",
        "Filter",
        "directory",
        "characteristics",
        "38,970 items",
        "17 categories",
        "3c",
        "dress",
        "food",
        "house",
        "beauty",
        "makeup",
        "jewelry",
        "medicine",
        "health",
        "finance",
        "cigarettes",
        "others",
        "emojis",
        "numbers",
        "spaces",
        "RNN",
        "performance",
        "MLM",
        "encoders",
        "QA",
        "NLI",
        "Google",
        "BERT-Base",
        "fine-tuning",
        "38,970 pieces",
        "4.2.1",
        "Text classification BERT fine-tuning model structure diagram Cui",
        "AMD Ryzen R5-4600H CPU",
        "windows10 64bit operating system",
        "Text message token serialization",
        "discussion 5.1 TF-IDF clustering scheme",
        "Text classification fine-tuning",
        "experimental schematic diagram",
        "direct word segmentation",
        "word frequency statistics",
        "final hidden state",
        "Tokels Tok1 Tok10",
        "official recommended values",
        "feature matching scheme",
        "text information token",
        "full connection layer",
        "additional 9500 text data",
        "natural language processing",
        "machine learning scheme",
        "word segmentation process",
        "social information data",
        "5.2 Classification scheme",
        "text length",
        "text description",
        "input BERT",
        "first token",
        "TF-IDF model",
        "classification problem",
        "deep learning",
        "learning rate",
        "TF-IDF-based model",
        "training set",
        "16G memory",
        "default construction",
        "genetic algo",
        "statistical estimation",
        "classifica- tion",
        "big gap",
        "three reasons",
        "later texts",
        "upgraded version",
        "intermediate function",
        "next chapter",
        "sentence vector",
        "various labels",
        "imum length",
        "super parameter",
        "training epochs",
        "recognition rate",
        "same preprocessing",
        "commodity terms",
        "experimental results",
        "verification set",
        "highest value",
        "reference value",
        "average accuracy",
        "accuracy rate",
        "running time",
        "large number",
        "rithm optimization",
        "algorithm",
        "5 Results",
        "ratio",
        "23,382 pieces",
        "15,588 pieces",
        "computer",
        "100 rounds",
        "28 s",
        "Experiments",
        "extent",
        "previous",
        "correlation",
        "words",
        "method",
        "preprocessed",
        "Figs.",
        "sequence",
        "actual",
        "batch_size",
        "train_epochs",
        "Table",
        "test",
        "commodities",
        "Funding National Natural Science Foundation",
        "natural language processing arXiv preprint",
        "Deep pyramid convolutional neural networks",
        "social network Results Value eval_accuracy",
        "Table 2 Text information classification results",
        "sentence classification arXiv preprint",
        "neural probabilistic language model",
        "perceptual image hashing schemes",
        "knowledge distillation technology",
        "social e-commerce classification",
        "recurrent network architectures",
        "social net- work",
        "social e-commerce market",
        "social e-commerce environment",
        "standard description text",
        "Bidirectional Encoder Representations",
        "1Beijing Key Laboratory",
        "Intelligent Telecommunication Software",
        "Optical character recognition",
        "Kalita J K.",
        "standard product names",
        "Otter D W",
        "large-scale data marking",
        "strong data support",
        "Medina J R",
        "large-scale unlabeled data",
        "model recognition rate",
        "product information",
        "training results",
        "Information Engineering",
        "text-based classification",
        "text data",
        "Epoch eval_accuracy",
        "text categorization",
        "J. Mach.",
        "W. Zaremba",
        "Corresponding table",
        "R. Ducharme",
        "R. Johnson",
        "R. Jozefowicz",
        "test data",
        "colloquial words",
        "existing model",
        "operational performance",
        "labor cost",
        "time cost",
        "full use",
        "supervised learning",
        "generalization ability",
        "process- ing",
        "emerging products",
        "model landing",
        "Shuai Shao",
        "Award Number",
        "Competing interests",
        "Author details",
        "Chinese Academy",
        "Y. Bengio",
        "P. Vincent",
        "Kim Y",
        "T. Zhang",
        "Annual Meeting",
        "Computational Linguistics",
        "Long Papers",
        "I. Sutskever",
        "empirical exploration",
        "International conference",
        "machine learning",
        "high correlation",
        "Authors’ contributions",
        "final manuscript",
        "Beijing University",
        "industry",
        "Bobo",
        "Botox",
        "scene",
        "6 Conclusion",
        "problems",
        "view",
        "eval_loss",
        "Abbreviations",
        "Transformers",
        "DPCNN",
        "Acknowledgements",
        "experiments",
        "suggestions",
        "structure",
        "Availability",
        "materials",
        "github",
        "cuihaoliang",
        "User-portraits",
        "social-e-commerce",
        "Multimedia",
        "Posts",
        "Telecommunications",
        "3Institute",
        "Sciences",
        "References",
        "Res.",
        "Proceedings",
        "55th",
        "Association",
        "Volume",
        "survey",
        "usages",
        "16",
        "25",
        "GA-BP air quality evaluation method",
        "double JPEG compressed color images",
        "Heterogeneous graph-based joint representation learning",
        "BERT model compression arXiv preprint arXiv",
        "social information Methods Feature classification",
        "Overall structure Security container",
        "multi-task learning arXiv preprint",
        "Deep contextualized word representations",
        "language understanding arXiv preprint",
        "Social software process initialization",
        "content curation social networks",
        "neural information processing systems",
        "Data label Classification scheme",
        "Billion-scale semi-supervised learning",
        "Recurrent neural network",
        "deep bidirectional transformers",
        "social networking services",
        "online social networks",
        "location-based social network",
        "massive news texts",
        "Tech Science Press",
        "Sunil Kr. Jha",
        "same quantization matrix",
        "Authors’ contributions Funding",
        "Author details References",
        "Yalniz I Z",
        "Jégou H",
        "Peters M E",
        "Patient knowledge distillation",
        "Chang M W",
        "rating users’ profiles",
        "TF-IDF clustering scheme",
        "new model",
        "Inf. Process",
        "text classification",
        "Devlin J",
        "W. Chen",
        "Gan Z",
        "Neumann M",
        "Iyyer M",
        "M. Malli",
        "M. Ning",
        "image classification",
        "Data collection",
        "Liu P",
        "Qiu X",
        "Huang X",
        "A. Vaswani",
        "N. Shazeer",
        "N. Parmar",
        "Lee K",
        "L. Wu",
        "multi-level LDA",
        "IEEE Transact",
        "N. Said",
        "A. Fadlallah",
        "Relation extraction",
        "Sun S",
        "Cheng Y",
        "Chen K",
        "Yaqiong Qiao",
        "Xiangyang Luo",
        "Chenliang Li",
        "Jinwei Wang",
        "Hao Wang",
        "Jian Li",
        "Yun-Qing Shi",
        "spherical coordinates",
        "Springer Nature",
        "jurisdictional claims",
        "institutional affiliations",
        "User analysis",
        "Background server",
        "Key processes",
        "users’ preferences",
        "Attention",
        "Neurocomputing",
        "evolution",
        "Eng.",
        "Sci.",
        "Development",
        "application",
        "Mater",
        "Libo",
        "CMC",
        "POIs",
        "Manag.",
        "CSVT",
        "Publisher",
        "Note",
        "regard",
        "maps",
        "Introduction",
        "Results",
        "discussion",
        "Conclusion",
        "14"
      ],
      "merged_content": "\nRESEARCH Open Access\n\nA classification method for social\ninformation of sellers on social network\nHaoliang Cui1, Shuai Shao2* , Shaozhang Niu1, Chengjie Shi3 and Lingyu Zhou1\n\n* Correspondence: shaoshuaib@163.\ncom\n2China Information Technology\nSecurity Evaluation Center, Beijing\n100085, China\nFull list of author information is\navailable at the end of the article\n\nAbstract\n\nSocial e-commerce has been a hot topic in recent years, with the number of users\nincreasing year by year and the transaction money exploding. Unlike traditional e-\ncommerce, the main activities of social e-commerce are on social network apps. To\nclassify sellers by the merchandise, this article designs and implements a social\nnetwork seller classification scheme. We develop an app, which runs on the mobile\nphones of the sellers and provides the operating environment and automated\nassistance capabilities of social network applications. The app can collect social\ninformation published by the sellers during the assistance process, uploads to the\nserver to perform model training on the data. We collect 38,970 sellers’ information,\nextract the text information in the picture with the help of OCR, and establish a\ndeep learning model based on BERT to classify the merchandise of sellers. In the\nfinal experiment, we achieve an accuracy of more than 90%, which shows that the\nmodel can accurately classify sellers on a social network.\n\nKeywords: User model, Machine learning, Social e-commerce\n\n1 Introduction\nWith the continuous improvement of social network and mobile payment technology,\n\none kind of commodity trading based on social relations called social e-commerce is in\n\nrapid development. According to the 2019 China social e-commerce industry develop-\n\nment report released by the Internet society of China, the number of employees of so-\n\ncial e-commerce in China is expected to reach 48.01 million in 2019, up by 58.3\n\npercent year on year, and the market size is expected to reach 2060.58 billion yuan, up\n\nby 63.2% year on year. Social e-commerce has become a large scale, and the high\n\ngrowth cannot be ignored. Different from e-commerce platforms such as Taobao, so-\n\ncial e-commerce is at the end of online retail. It carries out trading activities through\n\nsocial software and uses social interaction, user generated content and other means to\n\nassist the purchase and sale of goods. At the same time, sellers on social network use\n\ndifferent social software without uniform registration, have no systematic classification\n\nof products for sale, and there are no standardized terms for product description.\n\nThese bring great difficulty to the accurate classification of user portrait. This paper\n\nproposes a method based on the NLP classification model, which can realize accurate\n\n© The Author(s). 2021 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nEURASIP Journal on Image\nand Video Processing\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 \nhttps://doi.org/10.1186/s13640-020-00545-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-020-00545-z&domain=pdf\nhttp://orcid.org/0000-0001-9638-0201\nmailto:shaoshuaib@163.com\nmailto:shaoshuaib@163.com\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nbusiness classification of social e-commerce based on social information of social e-\n\ncommerce. This method analyzes 38,970 sellers on social networks and establishes a\n\ndeep learning model based on BERT to accurately classify the merchandise of sellers.\n\nIn addition, we introduced the OCR algorithm to extract the text information in the\n\npicture and superimposed it on the social content data, which effectively improved the\n\nclassification accuracy. The final experiment shows that the measured accuracy is more\n\nthan 90%.\n\n2 Related work\n2.1 Natural language processing\n\nIn order to analyze e-commerce business classification based on social data of sellers on a\n\nsocial network, the text needs to be analyzed based on the NLP correlation algorithm.\n\nThe rapid development of NLP at the present stage is due to the neural network language\n\nmodel (NNLM) Bengio et al. [1] proposed in 2003. Researchers have been trying to realize\n\nthe end-to-end classification recognition by using a neural network as a classifier in the\n\ntext classification research based on word embedding. Kim first introduces the convolu-\n\ntional neural network (CNN) into the study of text classification. The network structure is\n\na dropout full connection layer and a softmax layer connected after one convolution layer\n\n[2]. Although this algorithm achieves good results in various benchmark tests, it cannot\n\nobtain long-distance text dependency due to the limitation of network structure. There-\n\nfore, Tencent AI Lab proposed DPCNN, which further enhanced the extraction capacity\n\nof long-distance text dependency by deepening CNN [3].\n\nSocial content data includes multimedia text data and picture data. With the help of\n\nOCR, we extract the text in the picture and convert the picture data into text data. Text\n\nis a kind of sequential data, and the classification of it by recurrent neural network\n\n(RNN) has been the focus of long-term research in academia [4]. As a variation of\n\nRNN, long short-term memory (LSTM) adds control units such as forgetting gate, in-\n\nput gate, and output gate on the original basis, which solves the problem of gradient\n\nexplosion and gradient disappearance in the long sequence training of RNN and pro-\n\nmotes the use of RNN [5]. By introducing the sharing information mechanism, Liu\n\net al. further improved the accuracy of the RNN algorithm in the text multi-\n\nclassification task and achieved good results in four benchmark text classifications [6].\n\nHowever, Word vectors cannot be constructed in Word embedding to solve the\n\nproblem of polysemy. Even though different semantic environments are considered\n\nduring training, the result of training is still one word corresponding to one row vector.\n\nConsidering the widespread phenomenon of polysemy, Peters et al. propose embed-\n\ndings from language model (ELMO) to address the impact of polysemy on natural lan-\n\nguage modeling [7]. ELMO uses a feature-based form of pre-training. First, two-way\n\nLSTM is used to pre-train the corpus, and then word embedding resulting from train-\n\ning is adjusted by double-layer two-way LSTM when processing downstream tasks to\n\nadd more grammatical and semantic information according to the context words.\n\nThe ability of ELMO to extract features is limited for choosing LSTM as the feature\n\nextractor instead of Transformer [8], and ELMO’s bidirectional splicing method is also\n\nweak in feature fusion. Therefore, Devlin et al. propose the BERT model, taking Trans-\n\nformer as a feature extractor to pre-train large-scale text corpus [9].\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 2 of 12\n\n\n\n2.2 User analysis of social networks\n\nUser analysis is an important part of social network analysis. Most existing studies use\n\nuser-generated content or social links between users to simulate users. Wu et al. mod-\n\neled users on the content curation social network (CCSN) in the unified framework by\n\nmining user-generated content and social links [10]. They proposed a potential Bayes-\n\nian model, multilevel LDA (MLLDA), that could represent users of potential interest\n\nfound in social links formed by text descriptions contributed by users and information\n\nsharing. In 2017, Wu et al. proposed a latent model [11], trying to explain how the so-\n\ncial network structure and users’ historical preferences change over time affect each\n\nuser’s future behavior and predict each user’s consumption preferences and social con-\n\nnections in the near future. Malli et al. proposed a new online social network user pro-\n\nfile rating model [12], which solved the problem of large and complicated user data. In\n\nterms of data analysis platform, Chen et al. [13] developed a big data platform for the\n\nstudy of the garlic industry chain. Garlic planting management, price control, and pre-\n\ndiction were realized through data collection, storage, and pretreatment. Ning et al.\n\n[14] designed a ga-bp hybrid algorithm based on the fuzzy theory and constructed an\n\nair quality evaluation model by combining the knowledge of BP neural network, genetic\n\nalgorithm, and fuzzy theory. Yin et al. [15] studied two methods of extracting supervis-\n\nory relations and applied them to the field of English news. One is the combination of\n\nsupport vector machine and principal component analysis, and the other is the combin-\n\nation of support vector machine and CNN, which can extract high-quality feature vec-\n\ntors from sentences of support vector machine. In the social apps, the data we obtain is\n\nmostly image data, so we introduced the OCR technology to identify text information\n\nin images.\n\n3 Data collection\nIn order to analyze the behavior patterns of social e-commerce, we developed an auxil-\n\niary tool for social e-commerce. In this tool, sellers on a social network are provided\n\nwith the independent running environment of social software and the automatic auxil-\n\niary ability, and the information acquisition module of the auxiliary process is used to\n\ncollect the social information published by sellers on a social network, which is\n\nuploaded to the background server for model training. We provided this tool to nearly\n\n10,000 sellers on a social network who participated in the experiment to obtain their\n\nsocial information in their e-commerce activities.\n\n3.1 Overall structure\n\nThe whole data collection scheme is mainly composed of two parts: intelligent space\n\napp and background server. The overall architecture is shown in Fig. 1. Intelligent\n\nspace app is deployed in the mobile phones of sellers on a social network and imple-\n\nmented based on the application layer of the Android platform, providing sellers on a\n\nsocial network with a secure container for the independent operation of social software.\n\nThe app contains the automatic assistant module, which provides the automatic assist-\n\nant capability of various business processes for seller, and collects the social informa-\n\ntion in the auxiliary process through the information grasping module. The collected\n\ninformation is cached and uploaded locally through the information collection service.\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 3 of 12\n\n\n\nThe background server is responsible for receiving the collected data uploaded by the\n\nintelligent space, preprocessing the data first, and then classifying the social e-\n\ncommerce through the data based on the machine learning classification model, and fi-\n\nnally storing the classification results.\n\n3.1.1 Security container\n\nThe security container is designed to allow social software to run independently with-\n\nout modifying the OS or gaining root privileges. The basic principle of its realization is\n\nto create an independent container process; load APK file of social software dynamic-\n\nally; monitor and intercept process communication interface such as Binder IPC\n\nthrough Libc hook, Java reflection, dynamic proxy, and other technical means; and col-\n\nlect social information through an automatic assistant module. The main part of the\n\ncontainer is composed of an application layer module and a service layer module.\n\nThe application layer module is responsible for the process startup and execution of\n\nsocial software, and its main functions include three parts.\n\n3.1.1.1 Interactive interception The application layer module intercepts the inter-\n\naction between the application process and the underlying system in the container and\n\nmodifies the calling logic. By hook or dynamic proxy of system library API and Binder\n\ncommunication interface, the application layer module blocks all interfaces that interact\n\nwith the system during the execution of social software and controls the process\n\nboundary of interaction between social applications and system services.\n\n3.1.1.2 Social information collection The loading of the automatic auxiliary module\n\nby social software is realized when initializing the process of social application.\n\nThe application layer module injects the corresponding plugins in the automatic\n\nassistant module into the social application process. The automatic assistance mod-\n\nule provides a number of e-commerce auxiliary functions for sellers on a social\n\nnetwork, including customer acquisition, social customer relationship management\n\nLinux Kernel\n\nBinder Mode\n\nIntelligent Space\n\nService Layer Mode\nAMS Proxy PMS Proxy\n\nApplication Layer Mode\nSocial App\n\nInteractive \ninterception\n\nautomatic \nassistance  \nmodule\n\nInformation\nCollection \n\nBinder IPC\n\nBinder IPC\n\nBinder \nIPC Backgroud\n\n Server\n\nInternet\n\nFig. 1 The overall architecture diagram of the data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 4 of 12\n\n\n\n(SCRM), group management, sales assistance, and daily affairs. Sellers on social\n\nnetworks publish social information with commercial attributes through auxiliary\n\nfunctions, then the automatic auxiliary module will automatically collect the social\n\ninformation and send it to the information collection service for processing.\n\n3.1.1.3 Local processing of social information When the information collection ser-\n\nvice receives the social information collected by the automatic auxiliary module, the\n\ndata will be compressed and encrypted in the local cache. The service then uploads the\n\ncollected data to the background server periodically through the timer, and HTTPS is\n\nused to ensure data transmission security.\n\nThe main function of the service layer module is to take over the call logic modified\n\nby the application layer module by simulating the system service modify the parameters\n\nin the communication process and finally call the real system service. The service layer\n\nmodule exists in the container as an independent process. It focuses on the simulation\n\nof activity manager service (AMS) and package manager service (PMS) and realizes the\n\nsupport of system services in the process of launching and running social software.\n\n3.1.2 Background server\n\nThe background server mainly realizes the machine learning model processing of the\n\ncollected social data, including the functions of data preprocessing, data training, classi-\n\nfication, and result storage. The core processing logic will be described in chapter 5.\n\n3.2 Key processes\n\nThere are four key processes in the process of social information collection and pro-\n\ncessing. They are social software process initialization, social software process\n\nIntelligent \nSpace App \nlaunched\n\nProcess Boundaries\n\nUser Process\n\nSocial software process \ninitialization\n\nlaunching social \nsoftware\n\ninject automatic \nauxiliary\n\nSocial software process \nexecution\n\nRun the plug-in\n\nCollect social \ninformation\n\nProcess Boundaries\n\nUser Process\n\nLocal processing of \nsocial information\n\nBatch upload \nprocessed social \n\ninformation\n\nEncrypt, compress \nand store social \n\ninformation\n\nInternet\n\nInformation collecting \nservice process\n\nBackground processing \nof social information\n\nReceiving social \ninformation\n\nPreprocessing social \ninformation\n\nThe Server\n\nMachine learning \ncategorizes social \n\ninformation\n\nStore the \nclassification results\n\nFig. 2 Key flow chart of data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 5 of 12\n\n\n\nexecution, local processing of social information, and background processing of social\n\ninformation. The complete process is shown in Fig. 2.\n\n3.2.1 Social software process initialization\n\nWhen launching social software, the intelligent space will first intercept the callback\n\nfunction of the life cycle of all its components, then realize the process loading of the\n\nautomatic auxiliary module during the process initialization.\n\n3.2.2 Social software process execution\n\nThe process execution is completed by the application layer module and service layer\n\nmodule together. Sellers on a social network use automatic auxiliary modules to\n\ncomplete business activities, trigger information capture module to collect social infor-\n\nmation, and send it to the information collection service for subsequent processing.\n\n3.2.3 Local processing of social information\n\nThe local processing of social information is mainly completed by the information col-\n\nlection service. In order to ensure the safe storage and transmission of the collected so-\n\ncial information, the information collection service first adopts the encryption and\n\ncompression method to realize the local security cache and then adopts the HTTPS se-\n\ncure communication and transmission protocol to upload the data.\n\n3.2.4 Background processing of social information\n\nThe background processing of social information is completed by the background ser-\n\nver. The server first receives the social information uploaded by the intelligent space,\n\nnext decrypts and decompresses the social information, cleans the plaintext data, uses\n\nthird-party OCR technology to identify text information in images, and adds it to the\n\nuser’s social information after simple data processing. Then, the classification of sellers\n\non a social network is realized through the data based on machine learning modeling.\n\nFinally, the classification results are stored in the target database.\n\n4 Methods\nTo classify the business attributes of social e-commerce based on the information of\n\nsellers on a social network, traditional feature matching scheme and classification clus-\n\ntering scheme based on machine learning can be used to establish the model. In this\n\nchapter, we introduce the scheme based on term frequency-inverse document fre-\n\nquency (TF-IDF) clustering and the classification scheme based on BERT.\n\n4.1 Feature classification and TF-IDF clustering\n\n4.1.1 Feature classification\n\nWe randomly select 5000 sellers on a social network from the data collected by the\n\nbackground server and extracted the text data of their social information for analysis.\n\nEach social e-commerce user contains an average of 50 social text data. Based on the\n\ncontent, we manually classify social e-commerce into 11 categories. With the help of e-\n\ncommerce platforms like JD.COM, 50–100 keywords are sorted out for each category,\n\nand these keywords are screened and expanded according to the language habits of\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 6 of 12\n\n\n\nsellers on a social network. On this basis, we collect all the social information of each\n\nsocial network seller, cut and remove word segmentation, and match the results with\n\nthe keywords of the selected 11 categories. The number of keywords that are matched\n\nis counted as the matching degree. According to the situation of different classification,\n\nthe threshold of matching degree is determined by manual screening of some results,\n\nand then all social e-commerce is classified according to the threshold. After\n\noptimization and verification, the accuracy of the classical feature matching scheme fi-\n\nnally reached 40%. However, due to the simplicity of the rules of the feature matching\n\nscheme, the small optimization space, the high misjudgment rate of the scheme, and\n\nthe large human intervention in the basic word segmentation process, it is difficult to\n\ncover various situations of social e-commerce due to the limitation of these basic key-\n\nwords, thus making it insensitive to the dynamic changes of new hot words of social e-\n\ncommerce.\n\n4.1.2 TF-IDF clustering\n\nTo achieve the goal of accurate classification of social e-commerce, we designed a\n\nscheme based on TF-IDF clustering. Term frequency-inverse document frequency (TF-\n\nIDF) is a commonly used weighted technique for information retrieval and text mining\n\nto evaluate the importance of a single word to a document in a set of documents or a\n\ncorpus. In this scheme, the social information of each social e-commerce user is\n\nmapped as one file set of TF-IDF, and all texts of all sellers on a social network are\n\nmapped as the whole corpus. The words with the highest frequency used by each social\n\ne-commerce user are the most representative words in this document and become key-\n\nwords. Category labels can be generated to calculate the probability that a document\n\nbelongs to a certain category using the naive Bayes algorithm formula. The advantages\n\nof TF-IDF clustering to achieve the classification of sellers on the social network in-\n\nclude the following: (1) clear mapping; (2) emphasize the weight of keywords and lower\n\nthe weight of non-keywords; (3) compared with other machine learning algorithms, the\n\ncharacteristic dimension of the model is greatly reduced to avoid the dimension disas-\n\nter; and (4) while improving the efficiency of classification calculation, ensure that the\n\nclassification effect has a good accuracy and recall rate. The architecture of the entire\n\nsolution is shown in Fig. 3.\n\nIn the text preprocessing stage, the first thing to do is to format the social informa-\n\ntion, mainly including deleting the space, deleting the newline character, merging the\n\nsocial e-commerce text, and so on, and finally getting the text to be processed for word\n\nsegmentation. In this scheme, we choose Jieba’s simplified mode for word segmenta-\n\ntion, then filter out the noise by filtering the stop words (e.g., yes, ah, etc.).\n\nIn the stage of establishing the vector space model, the first step is to load the train-\n\ning set and take the pre-processed social information of each social e-commerce user\n\nas a document. The next step is to generate a dictionary, by adding every word that ap-\n\npears in the training set to it, using the complete dictionary to calculate the TF-IDF\n\nvalue of each document. In this scheme, CountVectorizer and TfidfTransformer in Py-\n\nthon’s Scikit-Learn library are used. CountVectorizer is used to convert words in the\n\ntext into word frequency matrix, TfidfTransformer is used to count the TF-IDF value\n\nof each word in each document, and the top20 words in each document are taken as\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 7 of 12\n\n\n\nkeywords of sellers on a social network. After this step, the keywords with a large TF-\n\nIDF value in each document are the most representative words in the document, which\n\nbecome the keyword set of the social e-commerce user. Finally, the naive Bayes method\n\nis used to generate the category label, and the document vectors belonging to the same\n\ncategory in the TF-IDF matrix are added to form a matrix of m*n, where m represents\n\nthe number of categories and n represents the number of documents. The weight of\n\neach word is divided by the total weight of all words of the class, to calculate the prob-\n\nability that a document belongs to a certain class.\n\nIn the model optimization stage, we optimize the whole scheme model by adjusting\n\nthe stop word set, adjusting parameters (including CountVectorizer, TfidfTransformer\n\nclass construction parameters), and adjusting the category label generation method.\n\nThe main idea of TFIDF is if a word or phrase appears in an article with a high fre-\n\nquency of TF, and rarely appears in other articles, it is considered that the word or\n\nphrase has a good classification ability and is suitable for classification. TFIDF is actu-\n\nally: TF * IDF, TF is term frequency and IDF is inverse document frequency.\n\nIn a given document, word frequency refers to the frequency of a given word in the\n\ndocument. This number is a normalization of the number of words to prevent it from\n\nbeing biased towards long documents. For the word ti in a particular document, its im-\n\nportance can be expressed as:\n\ntf i; j ¼\nj D j\n\nj j : ti∈d j\n� �\n\nj\n\namong them:\n\n|D|: The total number of files in the corpus\n\n∣{j : ti ∈ dj}∣: The number of documents containing the term ti (i.e., the number of\n\ndocuments in ni, j ≠ 0). If the term is not in the corpus, it will cause the dividend to be\n\nzero, so it is generally used 1 + ∣ {j : ti ∈ dj}∣.\n\nand then:\n\n Social e-\ncommerce data\n\nData preparation\n\nFormat processing\n\nFilter stop words\n\nText preprocessing\n\nGenerate directory\n\nBuild the vector space and \nTF-IDF\n\nGenerate category tags\n\nBayesian classifier\n\nText articiple\n\nLoad training set \n\nBuild tf matrix \n\nbuild vector\n\nbuild matrix \n\nConditional probability \nmatrix\n\nModel optimization\n\nFig. 3 TF-IDF scheme framework\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 8 of 12\n\n\n\ntfidf i; j ¼ tf i; j � idf i\n\nA high word frequency in a particular document and a low document frequency of\n\nthe word in the entire document collection can produce a high-weight TF-IDF. There-\n\nfore, TF-IDF tends to filter out common words and keep important words.\n\n4.2 Classification scheme based on BERT\n\n4.2.1 Data label\n\nWe manually classify and mark the data of sellers on a social network according to the\n\ncharacteristics of the products. Classified labels include 38,970 items and 17 categories\n\nof data, including 3c, dress, food, car, house, beauty, makeup, training, jewelry, promo-\n\ntion, medicine and health, phone charge recharge, finance, card category, cigarettes, es-\n\nsays, and others. The pre-processing phase removes emojis, numbers, and spaces from\n\nthe text through Unicode encoding.\n\n4.2.2 Classification scheme\n\nIn the BERT model, Transformer, as an encoder-decoder model based on attention\n\nmechanism, solves the problem that RNN cannot deal with long-distance dependence\n\nand the model cannot be parallel, improving the performance of the model without re-\n\nducing the accuracy. At the same time, BERT introduced the shading language model\n\n(MLM, masked language model) and context prediction method, further enhance the\n\ntwo-way training of the ability of feature extraction and text. MLM uses Transformer\n\nencoders and bilateral contexts to predict random masked tokens to pre-train two-way\n\ntransformers. This makes BERT different from the GPT model, which can only conduct\n\none-way training and can better extract context information through feature fusion.\n\nAnaphase prediction is more embodied in QA and NLI. Therefore, we choose the\n\nBERT model based on the bidirectional coding technology of pre-training and attention\n\nmechanism to classify sellers on a social network.\n\nWe chose the official Chinese pre-training model of Google as the pre-training model\n\nof the experiment: BERT-Base which is Chinese simplified and traditional, 12-layer,\n\n768-hidden, 12-head, 110M parameters [16]. This pre-training model is obtained by\n\nGoogle’s unsupervised pre-training on a large-scale Chinese corpus. On this basis, we\n\nwill carry out fine-tuning to realize the classification model of sellers on a social net-\n\nwork. When dividing the data set, we divided 38,970 pieces of data into training set\n\nand verification set according to the ratio of 6:4, that is, 23,382 pieces of training set\n\nand 15,588 pieces of verification set.\n\n5 Results and discussion\n5.1 TF-IDF clustering scheme\n\nThe computer used in the experiment is configured with AMD Ryzen R5-4600H CPU,\n\n16G memory, and windows10 64bit operating system. First, the default construction\n\nparameters are used, and the average accuracy of each classification is 45.7%. Next, the\n\nparameters are adjusted through a genetic algorithm, and 100 rounds of genetic algo-\n\nrithm optimization are performed, then the average accuracy reached the highest value\n\nof 52.5%. In the process of genetic algorithm, statistical estimation of algorithm time is\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 9 of 12\n\n\n\nalso carried out. On average, on this data set, the running time of each round of the\n\nTF-IDF model is about 28 s.\n\nExperiments show that the accuracy of the TF-IDF clustering scheme has been\n\nimproved after optimization, and it has a certain reference value for the classifica-\n\ntion of sellers on a social network, but there is still a big gap from the accurate\n\nclassification. We found three reasons after analyzing the experimental results. (1)\n\nCompared to the feature matching scheme, the TF-IDF-based model is improved\n\nto some extent. However, the input of the model is still the result of direct word\n\nsegmentation, and more information is lost in the word segmentation process, such\n\nas the semantic information of previous and later texts and the repetition fre-\n\nquency of corpus, which are relatively important in the process of natural language\n\nprocessing. (2) The classification problem of sellers on a social network is compli-\n\ncated. This model does not analyze the correlation between words and is essen-\n\ntially an upgraded version of word frequency statistics, which makes it difficult to\n\nimprove the accuracy after reaching a certain value. (3) For the optimization of the\n\nmodel, only the parameters of the intermediate function are adjusted, and the\n\nmethod is not upgraded. Therefore, the machine learning scheme based on TF-IDF\n\nclustering cannot solve the problem of accurate classification of sellers on a social\n\nnetwork. In the next chapter, we will introduce a scheme based on deep learning\n\nto achieve the goal of classifying sellers on a social network.\n\n5.2 Classification scheme based on BERT\n\nText classification fine-tuning is to serialize the preprocessed text information\n\ntoken and input BERT, and select the final hidden state of the first token [CLS] as\n\na sentence vector to output to the full connection layer, and then output the prob-\n\nability of obtaining various labels corresponding to the text through the softmax\n\nlayer. The experimental schematic diagram is shown in Figs. 4 and 5. The max-\n\nimum length of the sequence (ma_seq_length) is set to 256 according to the actual\n\ntext length of the social information data set of the sellers on a social network and\n\nFig. 4 Text message token serialization\n\nFig. 5 Text classification BERT fine-tuning model structure diagram\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 10 of 12\n\n Tokels Tok1 Tok10 \n\n\n\nthe batch_size and learning rate adopt the official recommended values of 32 and\n\n2e−5. In addition, we also adjust the super parameter num_train_epochs and in-\n\ncrease the number of training epochs (num_train_epochs) from 3 to 9 to improve\n\nthe recognition rate of the model (Table 1). The results are shown in Table 2.\n\nWe select an additional 9500 text data of sellers on social networks and test the\n\nmodel after the same preprocessing. The accuracy rate is 90.5%, which is lower than\n\nthat of the verification set (96.2%). The reason may be that the data of the test set con-\n\ntains a large number of commodity terms not included in the corpus and training set,\n\nand the text description of these commodities is too colloquial. Sellers on a social net-\n\nwork often use colloquial words in the industry to replace the standard product names\n\nwhen releasing product information, such as “Bobo” instead of “Botox,” which to some\n\nextent limits the accuracy of text-based classification in the social e-commerce market\n\nscene.\n\n6 Conclusion\nThe classification model proposes in this paper achieves an accuracy of 90.5% in the\n\ntest data. However, there are still some problems such as non-standard description text.\n\nA corpus with a high correlation with a social e-commerce environment will be estab-\n\nlished in order to further improve the accuracy of social e-commerce classification. At\n\nthe same time, we will use the knowledge distillation technology to compress and refine\n\nthe existing model, so as to improve the model recognition rate while simplifying the\n\nmodel and improving the operational performance [16]. In addition, in view of the high\n\nlabor cost and time cost of large-scale data marking, the next step will be trying to\n\nmake full use of semi-supervised learning to train unlabeled data and labeled completed\n\ndata [17]. The full use of large-scale unlabeled data is conducive to further improving\n\nthe accuracy and generalization ability of the model, as well as the analysis and process-\n\ning of emerging products, providing strong data support for the model landing. Since\n\nthe image data have also been studied to profiling the users in a social network [18]\n\nand perceptual image hashing schemes are proposed [19], we will improve our model\n\nso that the image and text data are combined for analysis.\n\nTable 1 Corresponding table of epoch and accuracy\n\nEpoch eval_accuracy (%)\n\n3 95.84\n\n6 96.05\n\n9 96.2\n\nThe training results are shown in Table 2, and the recognition rate is 96.2%\n\nTable 2 Text information classification results of sellers on social network\n\nResults Value\n\neval_accuracy 96.2%\n\neval_loss 0.25033528\n\nglobal_step 6024\n\nLoss 0.25023073\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 11 of 12\n\n\n\nAbbreviations\nBERT: Bidirectional Encoder Representations from Transformers; DPCNN: Deep pyramid convolutional neural networks;\nOCR: Optical character recognition\n\nAcknowledgements\nNot applicable\n\nAuthors’ contributions\nHaoliang Cui designed the scheme and carried out the experiments. Shuai Shao gave suggestions on the structure of\nthe manuscript and participated in modifying the manuscript. All authors read and approved the final manuscript.\n\nFunding\nNational Natural Science Foundation of China (Award Number 61370195, U1536121)\n\nAvailability of data and materials\nhttps://github.com/cuihaoliang/User-portraits-of-social-e-commerce\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAuthor details\n1Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia, Beijing University of Posts and\nTelecommunications, Beijing 100876, China. 2China Information Technology Security Evaluation Center, Beijing 100085,\nChina. 3Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100088, China.\n\nReceived: 16 March 2020 Accepted: 25 December 2020\n\nReferences\n1. Y. Bengio, R. Ducharme, P. Vincent, et al., A neural probabilistic language model. J. Mach. Learn. Res. 3, 1137–1155\n\n(2003)\n2. Kim Y. Convolutional neural networks for sentence classification arXiv preprint arXiv:1408.5882, 2014.\n3. R. Johnson, T. Zhang, Deep pyramid convolutional neural networks for text categorization [C]//Proceedings of the 55th\n\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (2017), pp. 562–570\n4. Otter D W, Medina J R, Kalita J K. A survey of the usages of deep learning in natural language processing arXiv preprint\n\narXiv:1807.10854, 2018.\n5. R. Jozefowicz, W. Zaremba, I. Sutskever, An empirical exploration of recurrent network architectures [C]//International\n\nconference on machine learning (2015), pp. 2342–2350\n6. Liu P, Qiu X, Huang X. Recurrent neural network for text classification with multi-task learning arXiv preprint arXiv:1605.\n\n05101, 2016.\n7. Peters M E, Neumann M, Iyyer M, et al. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.\n8. A. Vaswani, N. Shazeer, N. Parmar, et al., Attention is all you need [C]//Advances in neural information processing systems\n\n(2017), pp. 5998–6008\n9. Devlin J, Chang M W, Lee K, et al. BERT: pre-training of deep bidirectional transformers for language understanding\n\narXiv preprint arXiv:1810.04805, 2018.\n10. L. Wu et al., MLLDA: multi-level LDA for modelling users on content curation social networks. Neurocomputing 236, 73–\n\n81 (2017)\n11. L. Wu et al., Modeling the evolution of users’ preferences and social links in social networking services. IEEE Transact.\n\nKnowledge. Data. Eng. 29.6, 1240–1253 (2017)\n12. M. Malli, N. Said, A. Fadlallah, A new model for rating users’ profiles in online social networks. Comput. Information. Sci.\n\n10.2, 39–51 (2017)\n13. W. Chen et al., Development and application of big data platform for garlic industry chain. Comput. Mater. Continua 58.\n\n1, 229 (2019)\n14. M. Ning et al., GA-BP air quality evaluation method based on fuzzy theory. Comput. Mater. Continua 58.1, 215–227 (2019)\n15. Yin, Libo, et al. Relation extraction for massive news texts. Tech Science Press, CMC,60, no.1(2019), pp.275-285.\n16. Sun S, Cheng Y, Gan Z, et al. Patient knowledge distillation for BERT model compression arXiv preprint arXiv:1908.09355, 2019.\n17. Yalniz I Z, Jégou H, Chen K, et al. Billion-scale semi-supervised learning for image classification. arXiv preprint arXiv:1905.\n\n00546, 2019.\n18. Yaqiong Qiao, Xiangyang Luo, Chenliang Li, et al. Heterogeneous graph-based joint representation learning for users\n\nand POIs in location-based social network, Inf. Process. Manag., 2020, 57, 102151-1~102151-17\n19. Jinwei Wang, Hao Wang, Jian Li, Xiangyang Luo, Yun-Qing Shi, Sunil Kr. Jha, Detecting double JPEG compressed color\n\nimages with the same quantization matrix in spherical coordinates, IEEE Trans. on CSVT, doi: 10.1109/TCSVT.2019.\n2922309.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nCui et al. EURASIP Journal on Image and Video Processing          (2021) 2021:4 Page 12 of 12\n\n Published online: 14 January 2021 \n\nhttps://github.com/cuihaoliang/User-portraits-of-social-e-commerce\n\n\tAbstract\n\tIntroduction\n\tRelated work\n\tNatural language processing\n\tUser analysis of social networks\n\n\tData collection\n\tOverall structure\n\tSecurity container\n\tBackground server\n\n\tKey processes\n\tSocial software process initialization\n\tSocial software process execution\n\tLocal processing of social information\n\tBackground processing of social information\n\n\n\tMethods\n\tFeature classification and TF-IDF clustering\n\tFeature classification\n\tTF-IDF clustering\n\n\tClassification scheme based on BERT\n\tData label\n\tClassification scheme\n\n\n\tResults and discussion\n\tTF-IDF clustering scheme\n\tClassification scheme based on BERT\n\n\tConclusion\n\tAbbreviations\n\tAcknowledgements\n\tAuthors’ contributions\n\tFunding\n\tAvailability of data and materials\n\tCompeting interests\n\tAuthor details\n\tReferences\n\tPublisher’s Note\n\n",
      "text": [
        "Tokels Tok1 Tok10",
        "Published online: 14 January 2021"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"Tokels Tok1 Tok10\",\"lines\":[{\"boundingBox\":[{\"x\":48,\"y\":114},{\"x\":170,\"y\":113},{\"x\":170,\"y\":141},{\"x\":48,\"y\":141}],\"text\":\"Tokels\"},{\"boundingBox\":[{\"x\":280,\"y\":113},{\"x\":371,\"y\":113},{\"x\":371,\"y\":141},{\"x\":280,\"y\":141}],\"text\":\"Tok1\"},{\"boundingBox\":[{\"x\":616,\"y\":113},{\"x\":732,\"y\":113},{\"x\":732,\"y\":141},{\"x\":616,\"y\":141}],\"text\":\"Tok10\"}],\"words\":[{\"boundingBox\":[{\"x\":53,\"y\":114},{\"x\":169,\"y\":115},{\"x\":168,\"y\":142},{\"x\":52,\"y\":142}],\"text\":\"Tokels\"},{\"boundingBox\":[{\"x\":285,\"y\":114},{\"x\":371,\"y\":113},{\"x\":371,\"y\":142},{\"x\":285,\"y\":142}],\"text\":\"Tok1\"},{\"boundingBox\":[{\"x\":619,\"y\":114},{\"x\":733,\"y\":114},{\"x\":733,\"y\":142},{\"x\":619,\"y\":142}],\"text\":\"Tok10\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 14 January 2021\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":16},{\"x\":982,\"y\":17},{\"x\":982,\"y\":72},{\"x\":0,\"y\":70}],\"text\":\"Published online: 14 January 2021\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":17},{\"x\":283,\"y\":17},{\"x\":283,\"y\":70},{\"x\":0,\"y\":69}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":293,\"y\":17},{\"x\":504,\"y\":17},{\"x\":505,\"y\":72},{\"x\":293,\"y\":70}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":514,\"y\":17},{\"x\":588,\"y\":17},{\"x\":589,\"y\":72},{\"x\":515,\"y\":72}],\"text\":\"14\"},{\"boundingBox\":[{\"x\":598,\"y\":17},{\"x\":827,\"y\":17},{\"x\":828,\"y\":73},{\"x\":599,\"y\":72}],\"text\":\"January\"},{\"boundingBox\":[{\"x\":837,\"y\":17},{\"x\":981,\"y\":18},{\"x\":982,\"y\":73},{\"x\":838,\"y\":73}],\"text\":\"2021\"}]}"
      ]
    },
    {
      "@search.score": 0.919536,
      "content": "\nMining aspects of customer’s review \non the social network\nTu Nguyen Thi Ngoc1*, Ha Nguyen Thi Thu1 and Viet Anh Nguyen2\n\nIntroduction\nIn recent years, a lot of people often express their opinions about things such as products \nand services on social networks and e-commerce web sites. These opinions or reviews \noften play significant role in improving the quality of products and services. However, \nthe huge amount of reviews poses a challenge of how to efficiently mine useful informa-\ntion about a product or a service. To deal with this problem, much work has been intro-\nduced including summarizing users’ opinions [1], extracting information from reviews \n[2–5], analyzing user sentiments [6–9], and so on. In this paper, we focus on the problem \nof extracting information from reviews. More specifically, this study aims at developing \nefficient methods for dealing with the three tasks: extracting aspects mentioned in the \nreviews of a product, inferring the user’s rating for each identified aspect, and estimating \nthe weight posed on each aspect by the users.\n\nA user review often mentions different aspects, which are attributes or components of \na product. An aspect is usually a concept in which the user’s opinion is expressed in dif-\nferent level of positivity or negativity. For example, in the review given in Fig. 1, the user \nlikes the coffee, manifested by a 5-star overall rating. However, positive opinions about \n\nAbstract \nThis study represents an efficient method for extracting product aspects from cus-\ntomer reviews and give solutions for inferring aspect ratings and aspect weights. \nAspect ratings often reflect the user’s satisfaction on aspects of a product and aspect \nweights reflect the degree of importance of the aspects posed by the user. These \ntasks therefore play a very important role for manufacturers to better understand their \ncustomers’ opinion on their products and services. The study addresses the problem \nof aspect extraction by using aspect words based on conditional probability com-\nbined with the bootstrap technique. To infer the user’s rating for aspects, a supervised \napproach called the Naïve Bayes classification method is proposed to learn the aspect \nratings in which sentiment words are considered as features. The weight of an aspect \nis estimated by leveraging the frequencies of aspect words within each review and \nthe aspect consistency across all reviews. Experimental results show that the proposed \nmethod obtains very good performance on real world datasets in comparison with \nother state-of-the-art methods.\n\nKeywords: Aspect extraction, Aspect rating, Aspect weight, Conditional probability, \nCore term, Naive Bayes\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nM E T H O D O LO G Y\n\nNguyen Thi Ngoc et al. J Big Data            (2019) 6:22  \nhttps://doi.org/10.1186/s40537-019-0184-5\n\n*Correspondence:   \ntunn.dhdl@gmail.com \n1 Department \nof E-Commerce, Vietnam \nElectric Power University, \n235 Hoang Quoc Viet, Hanoi, \nVietnam\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0184-5&domain=pdf\n\n\nPage 2 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nbody, taste, aroma and acidity aspects of the coffee are also given. The task of aspect \nextraction is to identify all such aspects from the review. A challenge here is that some \naspects are explicitly mentioned and some are not. For instance, in the review given in \nFig.  1, taste and acidity of the coffee are explicitly mentioned, but body and aroma are \nnot explicitly specified. Some previous work dealt with identifying explicit aspects only, \nfor example [10]. In our paper, both explicit and implicit aspects are identified. Another \ndifficulty of the aspect extraction task is that it may generate a lot of noise in terms of \nnon-aspect concepts. How to minimize noise while still be able to identify rare and \nimportant aspects is also one of our concerns in this paper.\n\nMost of the earliest work to identify aspects are unsupervised model-based [11], in \nwhich statistics of relevant words are used. These methods do not require the labeled \ntraining data and have low cost. For example, frequency-based methods [10, 12, 13] \nconsider high-frequent nouns or noun phrases as aspect candidates. However, fre-\nquency-based approaches may miss low-frequent aspects. Several complex filter-based \napproaches are applied to solve this problem; however, the results are not as good as \nexpected because some aspects are still missed [14, 15]. Moreover, these methods face \ndifficulty in identifying implicit aspects. To overcome these problems, some supervised \nlearning techniques, such as the Hidden Markov Model (HMM) and Conditional Ran-\ndom Field (CRF) have been proposed. These techniques, however, require a set of manu-\nally labeled data for training the model and thus could be costly.\n\nThe problem of aspect extraction is solved by using aspect words based on conditional \nprobability combined with the bootstrap technique. It is assumed that the universal set \nof all possible aspects for each product are readily available together with aspect words \ncalled core terms (terms that describe aspects). This assumption is practical because \nthe number of important aspects is often small and can be easily obtained by domain \nexperts. The aspect extraction task then becomes how to correctly assign existing \naspects to sentences in the review. The main challenge here is that in many reviews, sen-\ntences do not contains enough core terms or even do not have any core term at all, and \nthus may be assigned with wrong aspects. This problem is solved by repeatedly updating \n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish-style cardamon coffee, brewed in a flared \ncopper stove-top pot like you see in Istanbul! But wow! This stuff is \namazing. \n\nDark without being bitter. Never acid at all, no matter how strong \nyou make it. So soft, so lovely. There’s a chocolate-like note, all warm \nand clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened condensed \nmilk as they suggest but it seems superfluous. Just drink it hot and strait\nand you will be very happy! \n\nFig. 1 Comment of Trung Nguyen coffee\n\n\n\nPage 3 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nand enlarging the set of core terms to the set of aspect words by using the conditional \nprobability technique combined with the bootstrap technique. This method leads to bet-\nter results of aspect extraction as shown in “Results and discussion” section.\n\nAfter the aspects are identified, inferring the user’s rating for them may bring more \nthorough understanding of the user’s satisfaction. A user usually gives an overall rat-\ning which express a general impression about a product. The overall rating is not always \ninformative enough. However, it can be assumed that the overall rating on a product \nis weighted sum of the user’s specific rating on multiple aspects of the product, where \n\nThree tasks\n\nExtracting \nAspects\n\nInferring \nAspect Rate\n\nEstimating Aspect \nWeight\n\nDark without \nbeing bitter.\n\nNever acid at \nall, no matter \nhow strong you \nmake it..\n\nSo soft, so \nlovely.\n\nThere’s a \nchocolate-like \nnote, all warm \nand clean, but \nnothing \nchocolate about \ntaste.\n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish -style cardamon coffee, brewed \nin a flared copper stove -top pot like you see in Istanbul! But \nwow! This stuff is amazing.\n\nDark without being bitter. Never acid at all, no matter how \nstrong you make it. So soft, so lovely. There’s a chocolate-like\nnote, all warm and clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened \ncondensed milk as they suggest but it seems superfluous. \nJust drink it hot and strait and you will be very happy!\n\nBody:   5\n\nAroma: -\n\nTaste:   5\n\nAcidity: 4\n\nBody:   0.2\n\nAroma: 0\n\nTaste:   0.6\n\nAcidity: \n0.2\n\nDark , \nbitter\n\nAcid, \nstrong\n\nSoft, \nlovely\n\nChocol-\nate-like, \nnote, \nwarm, \nclean, \ntaste\n\nFig. 2 An example of aspect extracting, aspect inferring, and aspect weighting tasks\n\n\n\nPage 4 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nthe weights basically measure the degree of importance of the aspects. Some previous \nwork [16, 17] infer the user’s rating for aspects and estimate the weight of aspects at \nthe simultaneously based on regression methods and using only the review content and \nthe associated overall rating. Different approach is applied to infer rating and weight of \naspects. More specifically, the weight of an aspect is calculated by leveraging the aspect \nwords frequency within the review and the aspect consistency across all reviews. Then, \na supervised approach called the Naïve Bayes classification method is used to infer the \nuser’s rating for aspects. Despite the fact that the solution is relatively simple, its tested \naccuracy on different real-life datasets are comparable to much more sophisticated state \nof the art approaches as shown in “Results and discussion” section.\n\nThe Fig. 2 summaries the three tasks mentioned above. The methods for solving these \ntasks are discussed in details in “Method” section of this paper.\n\nThe rest of this paper is structured as follows. “Related work” section introduces \nrelated works. “Problem definition” and “Method” sections represent the proposed \nmethodology. “Results and discussion” section show experimental and evaluation of the \nproposed method. Finally, “Conclusion” section concludes the paper and gives some \nfuture research directions.\n\nRelated work\nDuring the last decade, many researches work has been proposed in the opinion mining \narea. Researchers are paying increasing attention to methods of extracting information \nfrom reviews that indicates users’ opinions of aspects about products. A survey on opin-\nion mining and sentiment analysis [18] shows that two important tasks of aspect-based \nopinion mining are aspect identification and aspect-based rating inference. The survey \nalso mentions some interesting methods for these tasks including frequency-based, lexi-\ncon-based, machine learning and topic modeling.\n\nMost of the earliest researches to identify aspects are frequency-based ones [11]. In \nthese approaches, nouns and noun phrases are considered as aspect candidates [10, \n12–15]. Hu and Liu [10] uses a data mining algorithm for nouns and noun phrases iden-\ntification and label assignment by the part-of-speech/POS [19]. Their occurrence fre-\nquencies are counted, and only the frequent ones are kept. A frequency threshold is used \nand can be decided via experimental. In spite of its simplicity, this method is actually \nquite effective. Some commercial companies are using this method with some improve-\nments to increase in their business [11]. However, producing “non-aspect” is the limita-\ntion of these methods because some nouns or noun phrases that have high-frequency \nare not really aspects.\n\nTo solve these problems, some improved methods of this filtering approach have been \nproposed. [15] augments the frequency-based approach with an additional pattern-\nbased filters to remove some non-aspect terms. A similar solution, [14] extracts aspects \n(nouns) based on frequency and information distance. Firstly, they find seed words for \neach aspect by using the frequency-based method. Secondly, they use the information \ndistance in [20] to find other related words to aspects, e.g., for aspect price, it may find \n“$” and “dollars”. However, the frequency-based and rule-based approaches require the \nmanual effort of tuning various parameters, which limits their generalization in practice.\n\n\n\nPage 5 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTo deal with the limitations of frequency-based methods, in recent years, topic mod-\neling has emerged as a principled method for discovering topics from a large collection \nof texts. These researches are primarily based on two main basic models, pLSA (Prob-\nabilistic Latent Semantic Analysis) [21] and LDA (Latent Dirichlet allocation) [22]. In \n[4, 15, 23–25], the authors apply topic modeling to learn latent topics that correlate \ndirectly with aspects. [23] proposes a topic modeling for mining aspects. Firstly, they \nidentify aspects using topic modeling and then identify aspect-specific sentiment words \nby considering adjectives only. Lin et  al. [4] proposes Joint Sentiment-Topic (JST) and \nReverse-JST. Both models were based on the modified Latent Dirichlet allocation (LDA). \nThese models can extract sentiment as well as positive and negative topic from the text. \nBoth JST and RJST yield an accuracy of 76.6% on Pang and Lee [7] dataset. While topic-\nmodeling approaches learn distributions of words used to describe each aspect, in [24], \nthey separate words that describe an aspect and words that describe sentiment about an \naspect. To perform, this study use two parameter vectors to encode these two proper-\nties, respectively. Then, a weighted bipartite graph is constructed for each review, which \nmatches sentences in review to aspects. Learning aspect labels and parameters are per-\nformed with no supervision (i.e., using only aspect ratings), weak supervision (using a \nsmall number of manually-labeled sentences in addition to unlabeled data), or with full \nsupervision (using only manually-labeled data). Moghaddam and Ester [15] devised fac-\ntorized LDA (FLDA) to extract aspects and estimate aspect rating. The FLDA method \nassumes that each user (and item) has a set of distributions over aspects and aspect-\nbased ratings. Their work on multi-domain reviews reaches to 74% for review rating on \nTripAdvisor data set. In [26], the authors propose a new method called Aspect Identi-\nfication and Rating model (AIR) for mining textual reviews and overall ratings. Within \nAIR model, they allow an aspect rating to influence the sampling of word distribution \nof the aspect for each review. This approach is based on the LDA model. However, dif-\nferent from traditional topic models, the extraction of aspects (topics) and the sampling \nof words for each aspect are affected by the sampled latent aspect ratings which are \ndependent on the overall ratings given by reviewers. Then, they further enhance AIR \nmodel to handle quite unbalance of aspects mentioned in short reviews.\n\nAlthough topic modeling is an approach based on probabilistic inference and it can be \nexpanded to many types of information models, it has some limitations that restrict their \nuse in real-life sentiment analysis applications. For example, it requires a huge amount of \ndata and a significant amount of tuning in order to achieve reasonable results. It is very \neasy to find those general and frequent topics or aspects from a large document collec-\ntion, but it is hard to find those locally frequent but globally that is not frequent aspects. \nSuch locally frequent aspects are often the most useful ones for applications because \nthey are likely to be most relevant to the specific entities that the user is interested in. In \nshort, the results from current topic modeling methods are usually not relevant or spe-\ncific enough for many practical sentiment analysis applications [11].\n\nBesides, some lexicon-based methods, which are also unsupervised approach, are pro-\nposed. Opinions are extracted with respect to each feature using the dictionary-based \napproach, which also yields polarity and strength. These methods use a dictionary \nof sentiment words and phrases with their associated orientations and strength. They \nare combined with intensification and negation to compute a sentiment score for each \n\n\n\nPage 6 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\ndocument [8]. Xiaowen Ding, Minqing Hu use sentence and aspect-level sentiment clas-\nsification [10, 27, 28]. Yan et al. [29] propose a method called EXPRS (An Extended Pag-\neRank algorithm enhanced by a Synonym lexicon) to extract product features. To do so, \nthey extract nouns/noun phrases first and then extract dependency relations between \nnouns/noun phrases and associated sentiment words. Dependency relations included \nsubject-predicate relations, adjectival modifying relations, relative clause modifying rela-\ntions, and verb-object relations. The list of product features was extended by using its \nsynonyms. Non-features nouns are removed on the basis of proper nouns, brand names, \nverbal nouns and personal nouns. Peñalver-Martinez et  al. [30] developed a methodol-\nogy to perform aspect-based sentiment analysis of movie reviews. To extract the movie \nfeatures from the reviews, they make a domain ontology (Movie Ontology). SentiWord-\nNet is utilized to calculate the sentiment score. However, the critical issue here is how \nto construct such a sentiment lexicon, due to the cost of time and money to build such \ndictionaries.\n\nSentiment classification can be performed using machine learning approaches which \noften yield higher accuracy. Machine learning methods can be further divided into \nsupervised and unsupervised ones. For supervised methods, two sets of annotated data, \none for training and the other for testing are needed. Some of the commonly applied \nclassifiers for supervised learning are Decision Tree (DT), SVM, Neural Network (NN), \nNaïve Bayes, and Maximum Entropy (ME). In paper Asha et  al. [31], propose a Gini \nIndex based feature selection method with Support Vector Machine (SVM) classifier \nfor sentiment classification for large movie review data set. The Gini Index method for \nfeature selection in sentiment analysis has improved the accuracy. Another research, \nDuc-Hong Pham and Anh-Cuong Le [32] design a multiple layer architecture of knowl-\nedge representation for representing the different sentiment levels for an input text. This \nrepresentation is then integrated into a neural network to form a model for prediction \nof product overall ratings. These techniques, however, require a set of manually labeled \ndata for training the model and thus could be costly.\n\nProblem definition\nA user review i on some product is assumed containing two parts: the review’s text \ndenoted by di, and the review’s overall rating denoted by yi. Each review’s text di can \ncontain multiple sentences. Furthermore, each sentence contains multiple words coming \nfrom the universal set of all possible worlds V = {wk| k = 1, P} , called a word dictionary.\n\nIt is assumed further that for a product, the set of all possible K aspects is already \nknown together with topic words, called core terms that describe each aspect of the \nproduct.\n\nDefinition 1. Aspect An aspect is a feature (an attribute or a component) of a product. \nFor example, taste, aroma, and body are some possible aspects of the product “coffee”. We \nassume that there are K aspects mentioned in all reviews, denoted by A = {aj|j = 1, K} . \nAn aspect is represented by a set of words and denoted by aj = {w|w ∈ V, A(w) = j} , \nwhere aj is the name of the aspect, w is a word from the set V , and A(.) is a operator that \nmaps a word to the aspect. For example, words such as “taste”, “aftertaste”, and “mouth \nfeel” can characterize the taste aspect of the product coffee.\n\n\n\nPage 7 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nDefinition 2. Aspect rating Given a review i, a K-dimensional vector ri ∈ RK is used to \nrepresent the rating of K aspects in the review’s text di, denoted by ri = (ri1, ri2, . . . , riK ) , \nwhere rij is a number indicating the user’s opinion assessment on aspect aj, and \nrij ∈ [rmin, rmax] (e.g., the range of rij can be from 1 to 5).\n\nDefinition 3. Aspect weight Given a review i, a K-dimensional vector αi ∈ RK is used. \nThe vector is denoted as αi =\n\n(\n\nαi1, αi2, . . . , αiK\n)\n\n ) where αij is a number measuring the \ndegree of importance of aspect aj posed by the user, αij ∊ [0, 1], and \n\n∑K\nj=1 αij = 1 . A \n\nhigher weight means more emphasis is put on the corresponding aspect.\n\nDefinition 4. Aspect core terms Given an aspect aj, the set of associated core terms \nfor aj is denoted by Cj =\n\n{\n\nwj1, wj2, . . . , wjN\n}\n\n where wjk is a word that describes the \naspect aj. The core terms can be provided by the user or by some field experts.\n\nMajor notations used throughout the paper are given in Table 1.\n\nExtracting aspect\n\nThe goal of this task is to extract aspects mentioned in a review. It is assumed that each \naspect is a probability distribution over words. It is also assumed that each sentence in \na review’s text can mention more than one aspect. Therefore, our method to extract \naspects is based on conditional probability of words such that each sentence can be \nassigned with multiple labels.\n\nInferring aspect rate\n\nThis task is to infer the vector ri of aspect ratings (defined in Definition 2) given a \nreview di. Rating of an aspect reflects the user’s sentiment on the aspect which is often \nexpressed in positive or negative words. The more positive words the user use, the higher \nrating he/she want to pose on the aspect. This research adopts a supervised learning \nmethod, the Naive Bayes method, to learn the aspect ratings in which sentiment words \nare considered as features.\n\nTable 1 Notations used in this paper\n\nNotation Description\n\nD =\n{\n\ndi|i = 1, Q\n}\n\nThe set of reviews’ text, where Q is the number of reviews\n\nY =\n{\n\nyi|i = 1, Q\n}\n\nThe set of overall rating, yi is overall rating corresponded with di\nA = {a1, a2, . . . , aK } The set of aspect, where K is the number of aspects\n\nCj =\n{\n\nwj1, wj2, . . . , wjN\n}\n\nThe set of associated core terms for aspect aj, where N is the number of words\n\nV =\n{\n\nwk| k = 1, P\n}\n\nThe corpus of words, where P is the number of words\n\nSj =\n{\n\nsj1, sj2, . . . , sjM\n}\n\nThe set of sentences are assigned aspect aj, where M is the number of sentences\n\nTj =\n{\n\nwj1, wj2, . . . , wjT\n}\n\nThe set of aspect words are aspect expressions, where Tj  is the expression for aspect \naj, and T is the number of words\n\nij ∈ R\nK The aspect rating inferred from review di over K aspect, ri = (ri1 , ri2 , . . . riK)\n\nαi ∈ R\nK The aspect weights user places on K aspect within reviews’ text di, \n\nαi =\n(\n\nαi1 , αi2 , . . . , αiK\n)\n\nyi ∈ R\n+ The overall rating of review di\n\nrij The aspect rating on j-th aspect of review i, rij ∈ [1,5]\nαij The aspect weight of j-th aspect of review i, αij ∈ [0,1]\n\n\n\nPage 8 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nEstimating aspect weight\n\nThis task is to estimate non-negative weights αi that a user places on aspect aij of \nreview i. Weight of an aspect essentially measures the degree of importance posed \nby the user on the aspect. It is observed that people often talk more on aspects that \nthey are interested in a same review. Besides, the idea that an aspect is important is \noften shared by many other people. Based on these observations, a formula is devised \nto calculate aspect weight. The formula takes into account the occurrences of words \ndiscussing the aspect within a review and the frequency of text sentences discussing \nthe same aspect across all reviews.\n\nMethod\nExtracting aspect\n\nThe goal of this task is to assign a subset of aspect labels from the universal set of all \naspect labels of a product to every sentence in a review. Aspect label is determined \nbased on the set of relevant words called aspect words or terms. Each aspect in the \nuniversal label set is provided with some initial core terms. The main challenge here \nis that many reviews contain very few core terms or even do not contain any term at \nall. This results in incorrect labels being assigned to sentences. Therefore, it is required \nto expand the core terms to a richer set of aspect words based on the given data (the \nreviews). In some existing methods, the set of aspect words is built based on Bayes or \nHidden Markov Model. Our method use conditional probabilistic model [33] combined \nwith the Bootstrap technique to generate aspect words. Figure 3 illustrates four aspects \nof a coffee product represented by their corresponding aspect words, in which the sym-\nbol O represents core terms, the symbol X represents words appearing in the corpus. \nFor this coffee product four aspects body, taste, aroma, and acidity are already known. \n\naroma\n\nsmell\n\nflavor\n\ntaste\n\naftertaste\n\nmouthfeel \nfinishing\n\nbody\n\nacidity\n\nacid\n\nFig. 3 Core terms with aspects\n\n\n\nPage 9 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe sets of core terms corresponding to these aspects are {body}, {taste, aftertaste, fin-\nishing, mouthfeel}, {aroma, smell, flavor} and {acid, acidity}, respectively. Core terms are \nthen enlarged by inserting words that have high probability to appear in the same sen-\ntences that they occur. Sets of aspect words are represented by the four circles. These \ncircles may overlap, indicating that some aspect words may belong to different aspects.\n\nSuppose that A = {a1, a2, . . . , aK } is the set of K aspects. For each aj , a set of words \nthat appear in sentences labeled with aspect aj such that their occurrences exceed a \ngiven threshold is obtained. The set of words of two aspects can overlap, such that \nsome terms may belong to multiple aspects. First, sentences that contain at least one \nword in the original core terms of the aspect are located. Then, all words including \nnouns, noun phrases, adjectives, and adverbs that appeared in these sentences are \nfound. Words that occur more than a given threshold θ are inserted to the set of \naspect words. Words with maximum number of occurrences in the set of new-found \naspect words are added to the set of core terms. The new set of aspect words with \ncore terms excluded is used to find new sentences. The above-mentioned process is \nrepeated until no more new words are found.\n\nThe procedure for updating aspect words for an aspect aj is given below.\n\n\n\nPage 10 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nA bootstrapping algorithm to assign labels to sentences in the reviews is given below.\n\n\n\nPage 11 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe proposed Aspect Extraction Algorithm works as follows. First all reviews’ texts are \nsplit into sentences (step 2). Then, aspect labels from the set A of all labels are assigned \nto every sentence of the set D of reviews’ text based on the initial aspect core terms \n(step 3). Based on this initial aspect labeling, the set of aspect core terms and the set \nof aspect words for every aspect are updated (step 4). The labels for all sentences are \nupdated using the new core terms and the aspect words sets (step 5). Step 4 and step 5 \nare repeated until no more new aspect word set are found or the number of iterations \nexceeds a given threshold.\n\nInferring aspect rating and estimating aspect weight\n\nAspect ratings often reflect the user’s satisfaction on aspects of a product. Meanwhile, \naspect weights measure the degree of importance of the aspects posed by the user. Given \nthe overall rating on a product, it is assumed that the overall rating is the weighted sum \nof rating on multiple aspects of the product. Following this assumption, some regres-\nsion-based methods [16, 17, 34] have been proposed to estimate the two parameters by \nsolving the following equation:\n\nwhere rij and αij are the rating and the weight of k-th aspect of the review i, respectively.\nThere are linear regression methods [35] which estimate only the aspect weight and \n\nrequire that the aspect ratings are available. Some other methods [17, 34] estimate both \naspect’s rating and weight at the same time. The key point of these methods is to use sen-\ntiment words, more specifically the polarity of sentiment words, to calculate ratings and \nweights. Even though sentiment words can usually correctly reflect the user’s rating for \neach aspect, they do not always reflect the user’s opinion about an aspect’s weight.\n\nAspect rating and aspect weight of an aspect are estimated separately. An important \npoint in our method is that aspect rating and aspect weight are calculated based on the \nreview content only, without the requirement of knowing the user’s overall rating. How-\never, in “Results and discussion” section, Eq.  (1) is still used to test our method. It is \nshown experimentally that our results conform well to the assumption that the overall \nrating is the weighted sum of rating on multiple aspects.\n\nThe aspect rating problem is treated as the problem of multi-label classification, in which \nratings (from 1 to 5) as considered as labels, and sentiment words are used as features. \nIn most sentiment analysis work, adjectives and adverbs are used as candidate sentiment \nwords. Adjectives and adverbs are detected based on the well-known Part of Speech tech-\nnique (POS). It is recognized that some phrases can also be used to express sentiments \ndepending on different contexts. For example, in the following two sentences “we have big \nproblem with staff ”, and “we have a big room”, the two noun phrases “big problem” and “big \nroom” convey opposite sentiments, negative vs. positive, while both phrases contain the \nsame adjective “big”. Some fixed syntactic patterns in [9] as phrases of sentiment word fea-\ntures are used. Only fixed patterns of two consecutive words in which one word is an adjec-\ntive or an adverb and the other provides a context are considered.\n\n(1)yi =\nK\n∑\n\nj=1\n\nrijαij\n\n\n\nPage 12 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTwo consecutive words are extracted if their POS tags conform to any of the rules in \nTable 2 in which JJ tags are adjectives, NN tags are nouns, RB tags are adverbs, and VB \ntags are verbs. For example, rule 2 in this table means that two consecutive words are \nextracted if the first word is an adverb, the second word is an adjective, and the third \nword (which is not extracted) is not a noun. As an example, in the sentence “Quite dry, \nwith a good grassy note”, two patterns “quite dry” and “good grassy” are extracted as they \nsatisfy the second and the third rules, respectively. Then, conditional probability of word \nfeatures in the corpus is determined. Label (scoring) for each aspect is predicted based \non Naïve Bayes method.\n\nGiven a review’s text di, the rating of an aspect aj with q extracted features is inferred \nbased on the probability rij that the rating label belongs to class c ∈ C = {1, 2, 3, 4, 5}. The \nprobability is as:\n\nIt is assumed that the features are independent, then (2) is transformed into:\n\nin which: P\n(\n\nfk|rij ∈ c\n)\n\n= naj\n(\n\nfk, c\n)\n\n/naj(c) is the probability that feature fk belongs to the \n\nclass c, naj( fk, c) is the number of sentences labeled as c of the aspect aj which contains \nthe feature fk, and naj(c) is the number of all sentences containing the aspect aj and has \nclass label c,\nP(rij ∈ c)= naj(c)/naj is the probability that the rating rij belongs to the class c, naj(c) is \n\nthe number of sentences labeled as c of aspect aj, and naj is the number of all sentences \ncontaining the aspect aj,\n\nP( fk) is the probability of feature fk.\nFor smoothing (3), Laplace transformation is used. We get:\n\nin which, |V| is number of word features regarding the aspect aj.\nThe rating rij is the label c that maximize P(rij ∈ c|f1, . . . , fq).\n\n(2)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\nP\n(\n\nf1, . . . , fq|rij ∈ c\n)\n\nP\n(\n\nrij ∈ c\n)\n\nP\n(\n\nF1, . . . , Fq\n)\n\n(3)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\n\n∏q\nk=1\n\nP(fk|rij ∈ c)P\n(\n\nrij ∈ c\n)\n\n∑q\nk=1\n\nP\n(\n\nfk\n)\n\n(4)P\n(\n\nfk|rij ∈ c\n)\n\n=\nnaj\n\n(\n\nfj, c\n)\n\n+ 1\n\nnaj(c) + |V | + 1\n\nTable 2 POS labeled rules [9]\n\nThe first word The second word The third word \n(non extracted)\n\n1. JJ NN or NNS Any word\n\n2. RB, RBR, or RBS JJ Not NN nor NNS\n\n3. JJ JJ Not NN nor NNS\n\n4. NN or NNS JJ Not NN nor NNS\n\n5. RB, RBR, or RBS VB, VBD, VBN, or VBG Any word\n\n\n\nPage 13 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nNow the method to estimate aspect weight is given. By doing research carefully through-\nout the reviews, it can be seen that if a user care more about an aspect (showing that the \naspect is important to the user), he/she will mention more about it in the review. Moreover, \nthe idea that an aspect is important is often shared by many other users. Following this \nobservation, we estimate aspect weights by calculating two components: the weight meas-\nure of aspect aj within the reviews’ text  di, denoted by EDij, and the weight measure of the \naspect across all reviews, denoted by ECj. Note that in this way, the polarity measures of \nsentiment words are not used as in some other approaches. Instead, probability measures of \nwords and sentences regarding an aspect in the review and the corpus are considered. This \nidea is similar to the idea of using tf/idf for measuring word importance to some extent.\n\nGiven a review i, the weight component of the aspect aj, EDij, is calculated as:\n\nIn which: wijk is the k-th word in the aspect words of aspect aj , and Ni is the number of \naspect words that occur in the review’s text di for all aspects.\n\nThe weight component ECj is calculated as:\n\nIn which: sjk is the k-th sentence in the corpus labeled by the aspect aj , and M is the \nnumber of all sentences in the corpus.\n\nFinally, the weight αij for an aspect aj of review i is calculated as:\n\nThe denominator \n∑K\n\nj=1 EDijECj is to normalize the value of αij to the range [0,1].\n\nResults and discussion\nIn this section, experiments to evaluate the proposed methods are conducted.\n\nData set\n\nThe experiments are carried out using three different data sets including a data set for \nhotel review collected from Tripadvisor.com [17], one data set for beer review used in \n[24], and a data set for Trung Nguyen coffee review collected by our self from the Ama-\nzon web site.\n\nThe Hotel data set contains seven different aspects that are room, location, cleanliness, \ncheck-in/front desk, service and business services. The beer data set has five distinct \naspects that are aroma (or smell), palate (or feel), taste, appearance (or look), and over-\nall. This data set is quite big with millions of reviews. A subset of 50,000 beer reviews is \n\nĉ = argmaxc∈C\n\nq\n∏\n\nk=1\n\nP(fk|rij ∈ c)P\n(\n\nrij ∈ c\n)\n\n.\n\n(5)EDij =\n∑Ni\n\nk=1\nwijk\n\nNi\n.\n\n(6)ECj =\n∑M\n\nk=1 sjk\n\nM\n\n(7)αij =\nEDij ∗ ECj\n\n∑K\nj=1 EDij ∗ ECj\n\n\n\nPage 14 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nused in the experiment. The coffee data set contains 1200 reviews belongs to 17 different \nkinds of coffee. Table 3 gives some statistics of the three data sets.\n\nInferring aspect rating task\n\nNote that each review may be assigned with different labels. This means that sentence \nlevel, not review level is considered. Testing sets of 2500, 2000, and 500 sentences are \nselected randomly from the hotel data set, beer data set, and coffee data set, respectively. \nThe rest of sentences are used as the training sets.\n\nTable 4 gives initial core terms for the three data sets.\nThe precision measure is used to evaluate the experimental results:\n\nTable  5 shows the performance of our method on three data sets for the aspect \nextraction task. Our method yields up to average precision of 0.786, 0.803 and 0.653 \nfor hotel data set, beer data set and coffee data set, respectively. Our method obtains \ngood performance on the hotel and beer data set. However, for the coffee data set, \nthe result is not as good as expected. This is because in the coffee data set, users often \ngive only general view about a product, and moreover, the data set contains mostly \n\n(8)P =\n∣\n\n∣extrating Aspect ∩ True Aspect\n∣\n\n∣\n\n∣\n\n∣extracting Aspect\n∣\n\n∣\n\nTable 3 Summary of the Data Set\n\nHotel dataset Beer dataset Coffee dataset\n\n#Reviews 193,661 50,000 1200\n\n#Sentences 1,790,880 509,320 5289\n\n#Avg. sentences per review 9.25 10.19 4.41\n\nTable 4 Seed word for main aspects\n\nCategory Aspect Seed words\n\nHotel Value Value, price, worth\n\nRoom Room, rooms\n\nLocation Location\n\nCleanliness Dirty, smelled, clean\n\nCheck in/front desk Staff\n\nService Service, breakfast, food\n\nBusiness service Internet, wifi\n\nBeer Appearance Appearance, color, colors, coloring, head, foam\n\nAroma Aroma, aromas, smell, smelling\n\nPalate Palate, mouth, feel, mouth feel\n\nTaste Taste, tastes, aftertaste, in the end, finish, finishing\n\nOverall Overall\n\nCoffee Aroma Aroma, aromas, smell, smelling, flavor, flavors\n\nTaste Taste, tastes, aftertaste, finish, finishing, mouth feel\n\nAcidity Acid, acidity\n\nBody Body, aged, vintage\n\n\n\nPage 15 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nvery short reviews, with average number of sentences of 4.5, compared to 10 and 9 of \nthe hotel data set and the beer data set.\n\nOur method is compared with other works. First, our method is compared with the \nfrequency-based method in [14] on the hotel dataset. Figure 4 shows that our method \noutperforms Long’s in room (R), service (S), and cleanliness (C) aspects. But Long’s \nmethod outperforms us in detecting the value (V) aspect.\n\nOur method is compared with two topic modeling-based methods in [22] and in \n[24] on the beer data set. The method in [22] is a semi-supervised method, called \n\nTable 5 Aspect Identification results\n\nCategory Aspect Precision\n\nHotel Value 0.747\n\nRoom 0.837\n\nLocation 0.814\n\nCleanliness 0.764\n\nCheck in/front desk 0.850\n\nService 0.754\n\nBusiness service 0.737\n\nAverage 0.786\n\nBeer Appearance 0.750\n\nAroma 0.857\n\nPalate 0.857\n\nTaste 0.848\n\nOverall 0.704\n\nAverage 0.803\n\nCoffee Aroma 0.667\n\nTaste 0.677\n\nAcidity 0.667\n\nBody 0.600\n\nAverage 0.653\n\nV R S C AVER\n\nLong,Zhang, and Zhu 0.759 0.776 0.746 0.750 0.758\n\nOur Method 0.747 0.837 0.754 0.764 0.776\n\n0.700\n\n0.720\n\n0.740\n\n0.760\n\n0.780\n\n0.800\n\n0.820\n\n0.840\n\n0.860\n\nPr\nec\n\nis\nio\n\nn \n\nAspect\n\nHotel \n\nFig. 4 The results of our method and Long et al. method\n\n\n\nPage 16 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nLDA. In [24], the authors give 3 different methods, namely, unsupervised, semi-super-\nvised, and fully supervised methods. As our method can be considered as a semi-\nsupervised method, it is compared with PALE LAGER, a semi-supervised method, \nand with PALE LAGER, a supervised method given in [24].\n\nThe results in Fig.  5 shows that our method outperforms LDA with a large mar-\ngin, and slightly outperforms PALE LAGER (a semi-supervised method) and PALE \nLAGER (a supervised method).\n\nWe then search for the best threshold θ at which our method performs the best. The \nresults are shown in Fig. 6, where the threshold θ of about 0.15 is the best one.\n\nAspect ranking prediction\n\nUnlike the evaluation of the aspect extraction task that is done based on the sentence level, \nin this task, the result based on the review level is evaluated.\n\nThe mean square error measure (named �2aspect ) is used for evaluating methods of min-\ning aspect rating.\n\nwhere K is the number of aspects, Q is the number of reviews, and r∗ij is the true ratings \n\nfor aspect aj within review’s text di.\nTo evaluate how well the predicted aspect ratings can preserve their relative order within \n\na review given the true ratings, the aspect correlation measure (named ρaspect ) is used:\n\n(9)�2aspect =\n\n∑Q\ni=1\n\n∑K\nj=1\n\n(\n\nrij − r\n∗\nij\n\n)2\n\nQ × K\n\n(10)ρaspect =\n∑Q\n\ni=1 ρri,r\n∗\ni\n\nQ\n\n0.3000.320\n0.240\n\n0.750\n0.800 0.803\n\n0.000\n\n0.500\n\n1.000\n\nP\nre\n\nci\nsi\n\non\n\nBeeradvocate\n\nLDA, K topics, semi-supervised\n\nLDA, 10 topics, semi-supervised\n\nLDA, 50 topics, semi-supervised\n\nPALE LAGER, semi-supervised\n\nPALE LAGER, fully-supervised\n\nOur Method\n\nFig. 5 The results of our method and LDA, PALE LAGER\n\n\n\nPage 17 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nwhere Q is the number of reviews, and ρri,r∗i  is the Pearson correlation between two vec-\ntors ri and r∗i  of the inferred and the true ratings, respectively.\n\nThe two measures above are for evaluating the results for each review. The results on the \nwhole set of reviews are evaluated by using the so called aspect correlation across reviews \nmeasure ( ρreview):\n\nwhere ρ\n(\n\n−→\nrj ,\n\n−→\nr∗j\n\n)\n\n is the Pearson correlation between two vectors −→rj  and \n−→\nr∗j  of the inferred \n\nand rating.\nOur method is also compared with Long’s [14] and Wang’s [17]. Long proposed \n\ntwo methods based on the SVM classifier and the Bayesian Network classifier. Wang’s \nmethod is called Latent Rating Regression (LRR) which infers aspect ratings and aspect \nweights simultaneously.\n\nThe performance results are shown in Table  6. Our method performs much better \nthan Long’s method and Wang’s method on all three measures.\n\nEstimating aspect weight\n\nFor evaluating the correctness of estimated weights by our method, the overall rating is \ncalculated and compared with the true overall rating given by the user. The estimated \noverall rating is given by the following formula:\n\nwhere rij is the rating of the j-th aspect of the review i and αij is the estimated weight.\n\n(11)ρreview =\n\n∑K\nj=1 ρ\n\n(\n\n−→\nrj ,\n\n−→\nr∗j\n\n)\n\nK\n\n(12)ŷi =\nK\n∑\n\nj=1\n\nrij αij\n\n0.000\n\n0.200\n\n0.400\n\n0.600\n\n0.800\n\n1.000\n\n0.05 0.08 0.1 0.15 0.2 0.3 0.4 0.5\n\nPr\nec\n\nis\nio\n\nn\n\nAspect Evaluation with θ\n\nAppearance Aroma taste\n\npalate overall\n\nFig. 6 Aspect evaluation with θ\n\n\n\nPage 18 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nOur method is compared with Wang’s method [17] based on the �2\noverallrating\n\n . Table 7 \npresents the mean square errors of overall rating for the three data sets. As can be seen \nin the table, our results are comparable to Wang’s.\n\nConclusion\nThis paper dealed with three important sub-tasks of the opinion mining problem, that \nare (1) extracting aspects mentioned in the reviews of a product by using conditional \nprobability of words, (2) inferring the user’s rating for each identified aspect based on \nNaïve Bayes classifier, (3) estimating the weight placed on each aspect by the users by \nusing the occurrences of word that discuss the aspect within a review and the frequency \nof text sentences that discuss the same aspect across all reviews.\n\nOur method does not require to know the overall ratings and is as not complicated as \nsome other previous methods. However, it still works very well on real world datasets in \ncomparison with other state of the art methods.\n\nIn the future, the problem of aspect mining from unlabeled data will be considered. \nIn addition, the proposed model will be applied to other domains such as movie, digital \ncamera businesses to validate its generalized effectiveness.\n\nAbbreviations\npLSA: Probabilistic Latent Semantic Analysis; LDA: Latent Dirichlet allocation; HMM: Hidden Markov Model; CRF: Condi-\ntional Random Field.\n\nAuthors’ contributions\nTNTN proposed method and performed experiments, HNTT supervised the programming and wrote draft manuscript. \nVAN wrote a part of the manuscript and corrected after received reviews. All authors read and approved the final \nmanuscript.\n\nAuthor details\n1 Department of E-Commerce, Vietnam Electric Power University, 235 Hoang Quoc Viet, Hanoi, Vietnam. 2 Institute \nof Information Technology, Vietnam Academy of Science and Technology, Hanoi, Vietnam. \n\nAcknowledgements\nThis research is funded by the project “Building a System for Prediction and Management of Information Spreading in \nSocial Networks in Vietnam” under Grant VAST01.01/17-18\n\nCompeting interests\nData mining, big data, machine learning and natural language processing.\n\nTable 6 Comparison with other models for referring aspect ratings\n\nMethod �2aspect ρaspect ρreview\n\nLong et al. with SVM 0.286 0.557 0.708\n\nLong et al. with BN 0.441 0.429 0.591\n\nLRR 0.896 0.464 0.618\n\nOur method 0.101 0.583 0.757\n\nTable 7 MSE of overall rating prediction\n\nMethod Product datasets\n\nHotel Beer Coffee\n\nLRR 0.905 0.856 1.234\n\nThe proposed method 0.1456 0.1423 0.1904\n\n\n\nPage 19 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nFunding\nNot applicable.\n\nAvailability of data and materials\nAll data used in this study are publicly available and accessible in the source Tripadvisor.com.\n\nAppendix\nSee Tables 8, 9, 10.\n\nTable 8 Aspect word set of Hotel data\n\nAspect Aspect words\n\nValue Hotel, charge, cost, discount, dollars\n\nRoom Bathroom, bathrooms, bed, beds, bath, floor, floors, chair, chairs, balcony, \nshower, lobby, noise, pool, queen, couple, Sheraton, coffee, desk, hotel, \nsuite, tv, view, water, window, carpet, closet, doors, furniture, king, pillows, \nsink, toilet, tub, toiletries, …\n\nLocation Airport, area, center, downtown, hotel, market, place, places, restaurant, \nshop, shops, shopping, show review, street, view, views, neighborhood, \nsquare, waterfront, …\n\nCleanliness Hotel, floor, shelf, desk, chair, bag, door, lobby, stairs, …\n\nCheck in/front desk Desk, clerk, lounge, luggage, reception, checkout, …\n\nService Bar, bars, coffee, concierge, food, park, parking, restaurant, wine, buffet, …\n\nBusiness service Tv, television, wireless, hotwire, cable, computer, connection, free, freeway, \n…\n\nTable 9 Aspect word set of Beer data\n\nAspect Aspect words\n\nAppearance Black, Body, brown, bubble, bud, copper, lace, lacing, dots, drip, dust, back, finger, fizzy, fluff, \ngolden, half, layer, orange, straw, surface, top, white, yellow…\n\nAroma Bacon, banana, basil, caramel, cheese, cream, dry citrus, fruitiness, honey, light, malt, malts, meat, \nmint, nose, pear, perfume, pill, pine, roast, sandalwood, smoke, smoky, spice, sweet, sweetness, \nyeast…\n\nPalate Alcohol, body, carbonation, cream, Drinkability, dry, hoppy, light, round, spring, summer, …\n\nTaste Alcohol, avalanche, balance, bitterness, body, bread, burn, caramel, carbonation, cheese, chocolate, \nclear, cocoa, coffee, complexity, flavors, flavors, fruit, fruitiness, ginger, grains, malt, matiness, \nmeat, Medium-dry, oats, pear, roast, smoke, smoothness, spring, subtleties, summer, sweet, \nthroat, toffee, tongue, vanilla, wood, …\n\nOverall Beer, beers, bottle, drink, beverage, style, glass, sense, quality, brewery, level, lace, brewpub …\n\nTable 10 Aspect word set of Coffee data\n\nAspect Aspect words\n\nAroma Bran, brew, butter, charr, chocolate, citrus, fruit, \nhoney, love, lover, organic, press, quality, \nsmooth, lemon, smoke, stuff, …\n\nTaste Bitter, bitterness, chocolate, honey, salt, fresh-\nness, brew, love, lover, mild, organic, press, \nquality, roaster, smooth, soft, sour, stuff, sweet, \nsweetness, syrup, …\n\nAcidity Acidic, acidness, sourness, …\n\nBody Love, press, smooth, richness, thick, thin, soft, …\n\n\n\nPage 20 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 12 December 2017   Accepted: 14 February 2019\n\nReferences\n 1. Park S, Lee K, Song J. Contrasting opposing views of news articles on contentious issues. In: Proceedings of the 49th \n\nannual meeting of the association for computational linguistics (ACL-2011). 2011.\n 2. van den Camp M, van den Bosch A. The socialist network. Decis Support Syst. 2012;53:761–9.\n 3. Li SK, Guan Z, Tang LY, et al. Exploiting consumer reviews for product feature ranking. J Comput Sci Technol. \n\n2012;27(3):635–49. https ://doi.org/10.1007/s1139 0-012-1250-z.\n 4. Lin C, He Y, Everson R, Ruger S. Weakly supervised joint sentiment-topic detection from text. IEEE Trans Knowl \n\nData Eng. 2012;24(6):1134–45.\n 5. Zhan J, Loh HT, Liu Y. Gather customer concerns from online product reviews—a text summarization approach. \n\nExpert Syst Appl. 2009;36:2107–15.\n 6. Dang Y, Zhang Y, Chen H. A lexicon-enhanced method for sentiment classification: an experiment on online \n\nproduct reviews. IEEE Intell Syst. 2010;25(4):46–53.\n 7. Pang B, Lee L. A sentiment education: sentiment analysis using subjectivity summarization based on minimum \n\ncuts. In: Proceedings of the 42nd annual meeting on association for Computational Linguistics. 2004. p. 271.\n 8. Taboada M, Brooke J, Tofiloski M, Voll K, Stede M. Lexicon-based methods for sentiment analysis. Comput Lin-\n\nguistics. 2011;37(2):267–307.\n 9. Turney PD. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. \n\nIn: ACL ‘02 Proceedings of the 40th annual meeting on association for computational linguistics. p. 417–24.\n 10. Hu M, Liu B. Mining and summarizing customer reviews. In: Proceedings of the Tenth ACM SIGKDD international \n\nconference on knowledge discovery and data mining, KDD’04, New York: ACM; 2004, p. 168–77.\n 11. Liu B. Sentiment analysis and opinion mining. Synth Lect Human Lang Technol. 2012;5(1):1–67.\n 12. Popescu AM, Etzioni O. Extracting product features and opinions from reviews. In: HLT ‘05 Proceedings of \n\nthe conference on Human Language Technology and Empirical Methods in Natural Language Processing. p. \n339–46.\n\n 13. Zhu J, Wang H, Tsou BK, Zhu M. Multi-aspect opinion polling from textual reviews. In: Proceedings of ACM interna-\ntional conference on information and knowledge management (CIKM-2009). 2009.\n\n 14. Long C, Zhang J, Zhut X. A review selection approach for accurate feature rating estimation. In: Proceedings of Col-\ning 2010: Poster volume. 2010.\n\n 15. Moghaddam S, Ester M. Opinion digger: an unsupervised opinion miner from unstructured product reviews. In: \nProceeding of the ACM conference on Information and knowledge management (CIKM-2010). 2010.\n\n 16. Chen Li, Wang Feng. Preference-based clustering reviews for augmenting e-commerce recommendation. Knowl \nBased Syst. 2013;50:44–59.\n\n 17. Wang H, Lu Y, Zhai C. Latent aspect rating analysis on review text data: a rating regression approach. In: Proceedings \nof the 16th ACM SIGKDD international conference on knowledge discovery and data mining, KDD’10, New York: \nACM; 2010. p. 783–92.\n\n 18. Ravi K, Ravi V. A survey on opinion mining and sentiment analysis: tasks, approaches and applications. Knowl Based \nSyst. 2015;89:14–46.\n\n 19. Santorini B. Part-of-speech tagging guidelines for the Penn Treebank Project, University of Pennsylvania, School of \nEngineering and Applied Science, Dept. of Computer and Information Science. 1990.\n\n 20. Cilibrasi RL, Vitanyi PMB. The google similarity distance on Knowledge and Data Engineering, IEEE transactions. 2007; \n370–83.\n\n 21. Hofmann T. Probabilistic latent semantic indexing. In: Proceedings of the 22nd annual international ACM SIGIR \nconference on Research and development in information SIGIR’99. New York: ACM; 1999. p. 50–7.\n\n 22. Blei DM, Ng AY, Jordan MI. Latent dirichlet allocation. J Mach Learn Res. 2003;3:993–1022.\n 23. Brody S, Elhadad N. An unsupervised aspect-sentiment model for online reviews. In: Human language technolo-\n\ngies: the annual conference of the north american chapter of the association for computational linguistics, HLT’10, \nStroudsburg; 2010. p. 804–12.\n\n 24. McAuley J, Leskovec J, Jurafsky D. Learning attitudes and attributes from multi-aspect review. In: International \nconference on data mining (ICDM). 2012.\n\n 25. Sauper C, Barzilay R. Auto-matic aggregation by joint modeling of aspects and values. J Artif Int Res. \n2013;46(1):89–127.\n\n 26. Li H, Lin R, Hong R, Ge Y. Generative models for mining latent aspects and their ratings from short reviews. In: 2015 \nIEEE international conference on data mining. p. 241–50.\n\n 27. Ding X, Liu B, Yu PS. A holistic lexicon-based approach to opinion mining. In: Proceedings of the conference on web \nsearch and web data mining (WSDM-2008). 2008.\n\n 28. Kim SM, Hovy E. Determining the sentiment of opinions. In: Proceedings of international conference on computa-\ntional linguistics (COLING-2004).\n\n 29. Yan Z, Xing M, Zhang D, Ma B. EXPRS: an extended pagerank method for product feature extraction from online \nconsumer reviews, Inf. Manage. 2015.\n\n 30. Penalver-Martinez I, Garcia-Sanchez F, Valencia-Garcia R, Rodriguez-Garcia MA, Moreno V, Fraga A, Sanchez-Cer-\nvantes JL. Feature-based opinion mining through ontologies. Expert Syst Appl. 2014;41(13):5995–6008.\n\n\n\nhttps://doi.org/10.1007/s11390-012-1250-z\n\n\nPage 21 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\n 31. Manek AS, Shenoy PD, Mohan MC, Venugopal KR. Aspect term extraction for sentiment analysis in large movie \nreviews using Gini Index feature selection method and SVM classifier. World Wide Web. 2017;20(2):135–54.\n\n 32. Pham DH, Le AC. Learning multiple layers of knowledge representation for aspect based sentiment analysis. Data \nKnowl Eng. 2017;114:26–39.\n\n 33. Dao TT, Thanh TD, Hai TN, Ngoc VH. Building Vietnamese topic modeling based on core terms and applying in text \nclassification. In: Proc. of the fifth IEEE international conference on communication systems and network technolo-\ngies. 2015. P. 1284–88.\n\n 34. Yu J, Zha ZJ, Wang M, Chua TS. Aspect ranking: identifying important product aspects from online consumer \nreviews. In: Proceedings of the 49th Annual meeting of the association for computational linguistics: human \nlanguage technologies. Volume 1, HLT’11, Stroudsburg, PA, USA, 2011. Association for Computational Linguistics. p. \n1496–505.\n\n 35. Archak N, Ghose A, Ipeirotis PG. Show me the money!: Deriving the pricing power of product features by mining \nconsumer reviews. In: Proceedings of the 13th ACM SIGKDD in-ternational conference on Knowledge discovery and \ndata min-ing, KDD’07, New York: ACM; 2007. p. 56–65.\n\n\n\tMining aspects of customer’s review on the social network\n\tAbstract \n\tIntroduction\n\tRelated work\n\tProblem definition\n\tExtracting aspect\n\tInferring aspect rate\n\tEstimating aspect weight\n\n\tMethod\n\tExtracting aspect\n\tInferring aspect rating and estimating aspect weight\n\n\tResults and discussion\n\tData set\n\tInferring aspect rating task\n\tAspect ranking prediction\n\tEstimating aspect weight\n\n\tConclusion\n\tAuthors’ contributions\n\tReferences\n\n\n\n\n",
      "metadata_storage_path": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9saWJyYXJ5L3M0MDUzNy0wMTktMDE4NC01LnBkZg2",
      "authors": [
        "Tu Nguyen Thi Ngoc1",
        "Ha Nguyen Thi Thu1",
        "Viet Anh Nguyen2",
        "Naive",
        "Nguyen Thi Ngoc",
        "Thi Ngoc",
        "Trung Nguyen",
        "Dark",
        "Soft",
        "Hu",
        "Liu",
        "eling",
        "Lin",
        "Pang",
        "Lee",
        "Moghaddam",
        "Ester",
        "Xiaowen Ding",
        "Minqing Hu",
        "Yan",
        "Peñalver-Martinez",
        "Naïve Bayes",
        "Asha",
        "Gini",
        "Duc-Hong Pham",
        "Anh-Cuong Le",
        "Bayes",
        "EDij",
        "wijk",
        "Long",
        "V R S C AVER",
        "Zhang",
        "Zhu",
        "Pearson",
        "Wang",
        "VAN",
        "Park S",
        "Lee K",
        "Song J.",
        "van den Camp M",
        "van den Bosch A",
        "Li SK",
        "Guan Z",
        "Tang LY",
        "Lin C",
        "He Y",
        "Everson R",
        "Ruger S.",
        "Zhan J",
        "Loh HT",
        "Liu Y.",
        "Dang Y",
        "Zhang Y",
        "Chen H",
        "Pang B",
        "Lee L",
        "Taboada M",
        "Brooke J",
        "Tofiloski M",
        "Voll K",
        "Stede M.",
        "Turney",
        "Hu M",
        "Liu B",
        "Liu B.",
        "Popescu AM",
        "Etzioni O",
        "Zhu J",
        "Wang H",
        "Tsou BK",
        "Zhu M",
        "Long C",
        "Zhang J",
        "Zhut X",
        "Moghaddam S",
        "Ester M.",
        "Chen Li",
        "Wang Feng",
        "Knowl",
        "Lu Y",
        "Zhai C."
      ],
      "institutions": [
        "Bayes",
        "Commons",
        "Electric Power University",
        "MYOB",
        "LDA",
        "FLDA",
        "TripAdvisor",
        "Support",
        "JJ",
        "NN",
        "RB",
        "fk",
        "ECj",
        "Tripadvisor.com",
        "zon",
        "EDij",
        "Beeradvocate",
        "PALE LAGER",
        "CRF",
        "TNTN",
        "HNTT",
        "1 Department of E-Commerce",
        "Vietnam Electric Power University",
        "of Information Technology",
        "Vietnam Academy of Science and Technology",
        "Value Hotel",
        "Springer Nature",
        "association for computational linguistics",
        "ACL-",
        "socialist network",
        "Decis",
        "J Comput Sci Technol",
        "IEEE",
        "PD",
        "ACM"
      ],
      "key_phrases": [
        "M E T H O D O",
        "Creative Commons Attribution 4.0 International License",
        "Naive Bayes Open Access",
        "Tu Nguyen Thi Ngoc1",
        "Ha Nguyen Thi Thu1",
        "Vietnam Electric Power University",
        "Creative Commons license",
        "Nguyen Thi Ngoc",
        "Viet Anh Nguyen2",
        "commerce web sites",
        "useful informa- tion",
        "real world datasets",
        "creat iveco mmons",
        "LO G Y",
        "235 Hoang Quoc Viet",
        "Bayes classification method",
        "original author(s",
        "5-star overall rating",
        "cus- tomer reviews",
        "social network",
        "recent years",
        "significant role",
        "huge amount",
        "efficient methods",
        "ferent level",
        "important role",
        "conditional probability",
        "bootstrap technique",
        "sentiment words",
        "Experimental results",
        "good performance",
        "other state",
        "art methods",
        "Core term",
        "unrestricted use",
        "appropriate credit",
        "Full list",
        "author information",
        "aspect ratings",
        "aspect weights",
        "aspect extraction",
        "aspect words",
        "aspect consistency",
        "three tasks",
        "positive opinions",
        "customers’ opinion",
        "doi.org",
        "Mining aspects",
        "different aspects",
        "acidity aspects",
        "user sentiments",
        "users’ opinions",
        "product aspects",
        "user review",
        "Introduction",
        "lot",
        "people",
        "things",
        "products",
        "services",
        "quality",
        "challenge",
        "problem",
        "paper",
        "study",
        "attributes",
        "components",
        "concept",
        "positivity",
        "negativity",
        "example",
        "Fig.",
        "coffee",
        "Abstract",
        "solutions",
        "satisfaction",
        "degree",
        "importance",
        "manufacturers",
        "approach",
        "features",
        "frequencies",
        "comparison",
        "Keywords",
        "article",
        "terms",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "Correspondence",
        "tunn",
        "dhdl",
        "1 Department",
        "E-Commerce",
        "Hanoi",
        "end",
        "creativecommons",
        "licenses",
        "crossmark",
        "Page",
        "body",
        "taste",
        "aroma",
        "instance",
        "Several complex filter-based approaches",
        "Turkish -style cardamon coffee",
        "Turkish-style cardamon coffee",
        "copper stove-top pot",
        "sweetened condensed milk",
        "21Nguyen Thi Ngoc",
        "Three tasks Extracting",
        "Trung Nguyen coffee",
        "overall rat- ing",
        "Hidden Markov Model",
        "aspect extraction task",
        "conditional probability technique",
        "enough core terms",
        "quency-based approaches",
        "aspect candidates",
        "Aspect Rate",
        "Estimating Aspect",
        "overall rating",
        "previous work",
        "non-aspect concepts",
        "earliest work",
        "relevant words",
        "low cost",
        "frequent nouns",
        "noun phrases",
        "dom Field",
        "manu- ally",
        "main challenge",
        "many reviews",
        "big fan",
        "thorough understanding",
        "general impression",
        "specific rating",
        "implicit aspects",
        "important aspects",
        "low-frequent aspects",
        "possible aspects",
        "wrong aspects",
        "multiple aspects",
        "training data",
        "learning techniques",
        "chocolate-like note",
        "frequency-based methods",
        "ter results",
        "universal set",
        "explicit aspects",
        "1, taste",
        "acidity",
        "difficulty",
        "noise",
        "rare",
        "concerns",
        "statistics",
        "high",
        "HMM",
        "CRF",
        "product",
        "assumption",
        "number",
        "domain",
        "experts",
        "existing",
        "sentences",
        "new",
        "MYOB",
        "January",
        "flared",
        "Istanbul",
        "stuff",
        "cream",
        "sugar",
        "1 Comment",
        "discussion",
        "section",
        "user",
        "Weight",
        "Dark",
        "flared copper stove",
        "J Big Data",
        "future research directions",
        "based, machine learning",
        "different real-life datasets",
        "data mining algorithm",
        "two important tasks",
        "aspect-based opinion mining",
        "associated overall rating",
        "aspect-based rating inference",
        "many researches work",
        "aspect weighting tasks",
        "Different approach",
        "earliest researches",
        "Related work",
        "top pot",
        "condensed milk",
        "supervised approach",
        "sophisticated state",
        "Problem definition",
        "last decade",
        "increasing attention",
        "sentiment analysis",
        "topic modeling",
        "label assignment",
        "commercial companies",
        "improve- ments",
        "filtering approach",
        "Method” sections",
        "frequency threshold",
        "frequency-based approach",
        "aspect extracting",
        "aspect identification",
        "aspect terms",
        "review content",
        "art approaches",
        "The Fig. 2",
        "similar solution",
        "information distance",
        "frequency-based method",
        "regression methods",
        "interesting methods",
        "words frequency",
        "chocolate",
        "note",
        "Body",
        "Aroma",
        "Acidity",
        "bitter",
        "weights",
        "aspects",
        "reviews",
        "fact",
        "accuracy",
        "Results",
        "details",
        "methodology",
        "experimental",
        "evaluation",
        "Conclusion",
        "area",
        "Researchers",
        "survey",
        "nouns",
        "Hu",
        "Liu",
        "part",
        "speech/POS",
        "occurrence",
        "quencies",
        "frequent",
        "spite",
        "simplicity",
        "business",
        "limita",
        "high-frequency",
        "problems",
        "filters",
        "seed",
        "0.",
        "many practical sentiment analysis applications",
        "abilistic Latent Semantic Analysis",
        "real-life sentiment analysis applications",
        "two main basic models",
        "current topic modeling methods",
        "two parameter vectors",
        "Latent Dirichlet allocation",
        "topic mod- eling",
        "large document collec",
        "topic- modeling approaches",
        "mining textual reviews",
        "Learning aspect labels",
        "traditional topic models",
        "The FLDA method",
        "aspect-specific sentiment words",
        "TripAdvisor data set",
        "latent aspect ratings",
        "many types",
        "lexicon-based methods",
        "rule-based approaches",
        "large collection",
        "latent topics",
        "negative topic",
        "principled method",
        "multi-domain reviews",
        "new method",
        "short reviews",
        "manual effort",
        "Joint Sentiment-Topic",
        "Lee [7] dataset",
        "bipartite graph",
        "small number",
        "Rating model",
        "overall ratings",
        "word distribution",
        "probabilistic inference",
        "significant amount",
        "specific entities",
        "mining aspects",
        "unlabeled data",
        "aspect price",
        "information models",
        "weak supervision",
        "full supervision",
        "AIR model",
        "various parameters",
        "reasonable results",
        "frequent topics",
        "unsupervised approach",
        "review rating",
        "LDA model",
        "frequent aspects",
        "labeled sentences",
        "distance",
        "other",
        "dollars",
        "generalization",
        "practice",
        "limitations",
        "texts",
        "researches",
        "pLSA",
        "authors",
        "adjectives",
        "JST",
        "Both",
        "positive",
        "Pang",
        "distributions",
        "addition",
        "Moghaddam",
        "Ester",
        "fac",
        "item",
        "work",
        "fication",
        "sampling",
        "extraction",
        "reviewers",
        "unbalance",
        "tuning",
        "order",
        "Such",
        "Gini Index based feature selection method",
        "The Gini Index method",
        "large movie review data set",
        "Naïve Bayes",
        "Support Vector Machine",
        "machine learning approaches",
        "multiple layer architecture",
        "different sentiment levels",
        "adjectival modifying relations",
        "knowl- edge representation",
        "aspect-based sentiment analysis",
        "Machine learning methods",
        "product overall ratings",
        "possible K aspects",
        "associated sentiment words",
        "annotated data",
        "supervised learning",
        "associated orientations",
        "Movie Ontology",
        "multiple sentences",
        "possible worlds",
        "wk| k",
        "sentiment score",
        "sentiment lexicon",
        "Sentiment classification",
        "dependency relations",
        "predicate relations",
        "verb-object relations",
        "multiple words",
        "dictionary-based approach",
        "Xiaowen Ding",
        "Minqing Hu",
        "eRank algorithm",
        "Synonym lexicon",
        "relative clause",
        "rela- tions",
        "Non-features nouns",
        "proper nouns",
        "brand names",
        "verbal nouns",
        "personal nouns",
        "Peñalver-Martinez",
        "domain ontology",
        "critical issue",
        "supervised methods",
        "two sets",
        "Decision Tree",
        "Neural Network",
        "Maximum Entropy",
        "Duc-Hong Pham",
        "Anh-Cuong Le",
        "two parts",
        "core terms",
        "movie reviews",
        "topic words",
        "nouns/noun phrases",
        "higher accuracy",
        "SVM) classifier",
        "input text",
        "product features",
        "product coffee",
        "word dictionary",
        "taste aspect",
        "Opinions",
        "respect",
        "polarity",
        "strength",
        "intensification",
        "negation",
        "document",
        "Yan",
        "subject",
        "list",
        "synonyms",
        "basis",
        "cost",
        "money",
        "dictionaries",
        "training",
        "testing",
        "classifiers",
        "DT",
        "Asha",
        "research",
        "model",
        "prediction",
        "techniques",
        "attribute",
        "component",
        "aj",
        "A(.",
        "operator",
        "aftertaste",
        "mouth",
        "supervised learning method",
        "Naive Bayes method",
        "many other people",
        "K-dimensional vector ri",
        "Aspect core terms",
        "field experts",
        "probability distribution",
        "multiple labels",
        "Notation Description",
        "non-negative weights",
        "corresponding aspect",
        "Extracting aspect",
        "one aspect",
        "aspect rate",
        "aspect expressions",
        "j-th aspect",
        "same aspect",
        "aspect labels",
        "higher weight",
        "Major notations",
        "K aspect",
        "negative words",
        "aspect aj",
        "same review",
        "reviews’ text",
        "positive words",
        "review i",
        "∑K",
        "αi",
        "RK",
        "riK",
        "rij",
        "opinion",
        "assessment",
        "range",
        "Definition",
        "emphasis",
        "associated",
        "Cj",
        "wjN",
        "wjk",
        "Table",
        "goal",
        "task",
        "sentence",
        "1, Q",
        "yi",
        "wk",
        "corpus",
        "Sj",
        "Tj",
        "wjT",
        "aij",
        "idea",
        "observations",
        "formula",
        "account",
        "occurrences",
        "frequency",
        "subset",
        "initial aspect core terms",
        "flavor taste aftertaste mouthfeel",
        "conditional probabilistic model",
        "initial aspect labeling",
        "initial core terms",
        "original core terms",
        "Aspect Extraction Algorithm",
        "universal label set",
        "new core terms",
        "corresponding aspect words",
        "new-found aspect words",
        "new aspect word",
        "body acidity acid",
        "bootstrapping algorithm",
        "aspect weight",
        "Aspect ratings",
        "new words",
        "existing methods",
        "Bootstrap technique",
        "bol O",
        "symbol X",
        "high probability",
        "sion-based methods",
        "two parameters",
        "following equation",
        "new set",
        "four aspects",
        "K aspects",
        "two aspects",
        "reviews’ texts",
        "new sentences",
        "four circles",
        "maximum number",
        "incorrect labels",
        "coffee product",
        "richer set",
        "Bayes",
        "Figure",
        "sets",
        "ishing",
        "smell",
        "threshold",
        "one",
        "adverbs",
        "process",
        "procedure",
        "step",
        "iterations",
        "sum",
        "θ",
        "most sentiment analysis work",
        "Naïve Bayes method",
        "Speech tech- nique",
        "two consecutive words",
        "linear regression methods",
        "good grassy note",
        "following two sentences",
        "two noun phrases",
        "aspect rating problem",
        "two patterns",
        "candidate sentiment",
        "other methods",
        "same time",
        "key point",
        "important point",
        "weighted sum",
        "multi-label classification",
        "different contexts",
        "big problem",
        "big room",
        "syntactic patterns",
        "fixed patterns",
        "JJ tags",
        "NN tags",
        "RB tags",
        "VB tags",
        "Laplace transformation",
        "class c",
        "one word",
        "first word",
        "P(rij",
        "k-th aspect",
        "opposite sentiments",
        "same adjective",
        "POS tags",
        "class label",
        "rating rij",
        "second word",
        "feature fk",
        "rijαij",
        "fk|rij",
        "rating label",
        "third rules",
        "probability rij",
        "word features",
        "review",
        "content",
        "requirement",
        "Eq.",
        "labels",
        "known",
        "Part",
        "staff",
        "naj",
        "smoothing",
        "∑",
        "Trung Nguyen coffee review",
        "three different data sets",
        "The Hotel data set",
        "POS labeled rules",
        "zon web site",
        "three data sets",
        "one data set",
        "many other users",
        "coffee data set",
        "beer data set",
        "aspect rating task",
        "seven different aspects",
        "weight component ECj",
        "Testing sets",
        "training sets",
        "17 different kinds",
        "different labels",
        "hotel review",
        "other approaches",
        "The denominator",
        "beer review",
        "RBS JJ",
        "JJ JJ",
        "RBS VB",
        "two components",
        "weight meas",
        "polarity measures",
        "probability measures",
        "k-th sentence",
        "Tripadvisor.com",
        "front desk",
        "business services",
        "five distinct",
        "sentence level",
        "precision measure",
        "average precision",
        "third word",
        "word importance",
        "k-th word",
        "review level",
        "experimental results",
        "JJ NN",
        "NNS JJ",
        "ECj.",
        "1200 reviews",
        "fq",
        "∏q",
        "∑q",
        "fj",
        "Table 2",
        "RBR",
        "VBD",
        "VBN",
        "VBG",
        "method",
        "observation",
        "EDij",
        "way",
        "tf/idf",
        "extent",
        "wijk",
        "sjk",
        "αij",
        "value",
        "experiments",
        "self",
        "room",
        "location",
        "cleanliness",
        "palate",
        "feel",
        "appearance",
        "look",
        "millions",
        "ĉ",
        "∑Ni",
        "∑M",
        "rest",
        "performance",
        "Data Set Hotel dataset Beer dataset Coffee dataset",
        "V R S C AVER",
        "front desk Staff Service Service",
        "mean square error measure",
        "two topic modeling-based methods",
        "Category Aspect Seed words",
        "Long et al. method",
        "Table 5 Aspect Identification results",
        "hotel data set",
        "Table 4 Seed word",
        "Beer Appearance Appearance",
        "Category Aspect Precision",
        "aspect correlation measure",
        "Aspect ranking prediction",
        "ing aspect rating",
        "Business service Internet",
        "Coffee Aroma Aroma",
        "rooms Location Location",
        "Hotel Value Value",
        "acidity Body Body",
        "V) aspect",
        "Aspect Hotel",
        "C) aspects",
        "r∗ij",
        "True Aspect",
        "extracting Aspect",
        "general view",
        "Acidity Acid",
        "other works",
        "3 different methods",
        "The results",
        "best one",
        "true ratings",
        "relative order",
        "main aspects",
        "supervised method",
        "Our Method",
        "PALE LAGER",
        "best threshold",
        "Palate Palate",
        "Taste Taste",
        "Cleanliness Dirty",
        "Room Room",
        "Table 3",
        "average number",
        "ρaspect",
        "users",
        "Summary",
        "Avg",
        "price",
        "Check",
        "breakfast",
        "food",
        "wifi",
        "color",
        "head",
        "foam",
        "aromas",
        "tastes",
        "finish",
        "flavor",
        "Zhang",
        "Zhu",
        "LDA",
        "gin",
        "�2aspect",
        "evaluating",
        "text",
        "∑Q",
        "720",
        "Probabilistic Latent Semantic Analysis",
        "Naïve Bayes classifier",
        "Bayesian Network classifier",
        "Appearance Aroma taste",
        "mean square errors",
        "tional Random Field",
        "three important sub-tasks",
        "other previous methods",
        "opinion mining problem",
        "true overall rating",
        "SVM classifier",
        "three measures",
        "two methods",
        "other domains",
        "aspect mining",
        "Pearson correlation",
        "two measures",
        "two vectors",
        "r∗j",
        "following formula",
        "ŷi",
        "Pr ec",
        "text sentences",
        "camera businesses",
        "generalized effectiveness",
        "Author details",
        "Social Networks",
        "Competing interests",
        "Vietnam Academy",
        "aspect correlation",
        "Aspect Evaluation",
        "draft manuscript",
        "final manuscript",
        "tors ri",
        "Authors’ contributions",
        "performance results",
        "reviews measure",
        "Information Technology",
        "K topics",
        "ρri",
        "10 topics",
        "50 topics",
        "Beeradvocate",
        "ρreview",
        "rj",
        "Long",
        "Wang",
        "LRR",
        "correctness",
        "�2 overallrating",
        "conditional",
        "probability",
        "words",
        "future",
        "movie",
        "digital",
        "Abbreviations",
        "TNTN",
        "HNTT",
        "programming",
        "VAN",
        "2 Institute",
        "Science",
        "Acknowledgements",
        "project",
        "System",
        "Prediction",
        "Management",
        "Grant",
        "0.600",
        "overall rating prediction Method Product datasets Hotel Beer Coffee",
        "van den Camp M",
        "van den Bosch A",
        "J Comput Sci Technol",
        "product feature ranking",
        "Overall Beer",
        "natural language processing",
        "dollars Room Bathroom",
        "49th annual meeting",
        "Decis Support Syst",
        "joint sentiment-topic detection",
        "IEEE Trans Knowl",
        "Aspect word set",
        "Ruger S. Weakly",
        "Aspect Aspect words",
        "Business service Tv",
        "front desk Desk",
        "Beer data",
        "Method �2aspect",
        "Song J.",
        "Value Hotel",
        "Cleanliness Hotel",
        "Hotel data",
        "Coffee data",
        "Service Bar",
        "machine learning",
        "other models",
        "source Tripadvisor",
        "Location Airport",
        "show review",
        "Appearance Black",
        "Aroma Bacon",
        "Aroma Bran",
        "Acidity Acidic",
        "Springer Nature",
        "jurisdictional claims",
        "institutional affiliations",
        "Park S",
        "Lee K",
        "news articles",
        "contentious issues",
        "computational linguistics",
        "socialist network",
        "Li SK",
        "Guan Z",
        "Tang LY",
        "consumer reviews",
        "Lin C",
        "He Y",
        "Everson R",
        "Data mining",
        "big data",
        "ρreview Long",
        "dry citrus",
        "Palate Alcohol",
        "Taste Bitter",
        "opposing views",
        "Table 7 MSE",
        "Body Love",
        "Table 6",
        "Table 8",
        "Comparison",
        "SVM",
        "BN",
        "proposed",
        "Funding",
        "Availability",
        "materials",
        "Appendix",
        "Tables",
        "charge",
        "discount",
        "bathrooms",
        "bed",
        "floor",
        "chair",
        "balcony",
        "shower",
        "lobby",
        "pool",
        "queen",
        "couple",
        "Sheraton",
        "suite",
        "water",
        "window",
        "carpet",
        "closet",
        "doors",
        "furniture",
        "pillows",
        "sink",
        "toilet",
        "tub",
        "center",
        "downtown",
        "market",
        "place",
        "restaurant",
        "shop",
        "street",
        "neighborhood",
        "square",
        "shelf",
        "bag",
        "stairs",
        "clerk",
        "lounge",
        "luggage",
        "reception",
        "bars",
        "concierge",
        "parking",
        "wine",
        "buffet",
        "television",
        "hotwire",
        "cable",
        "computer",
        "connection",
        "freeway",
        "brown",
        "bubble",
        "bud",
        "copper",
        "lacing",
        "dots",
        "drip",
        "dust",
        "finger",
        "fluff",
        "golden",
        "half",
        "layer",
        "orange",
        "straw",
        "surface",
        "banana",
        "basil",
        "caramel",
        "cheese",
        "fruitiness",
        "honey",
        "light",
        "malt",
        "meat",
        "mint",
        "nose",
        "perfume",
        "pine",
        "roast",
        "sandalwood",
        "smoke",
        "smoky",
        "spice",
        "sweet",
        "yeast",
        "carbonation",
        "Drinkability",
        "hoppy",
        "round",
        "summer",
        "avalanche",
        "balance",
        "bitterness",
        "bread",
        "burn",
        "cocoa",
        "complexity",
        "flavors",
        "ginger",
        "grains",
        "matiness",
        "Medium-dry",
        "oats",
        "smoothness",
        "subtleties",
        "throat",
        "toffee",
        "tongue",
        "vanilla",
        "beers",
        "bottle",
        "beverage",
        "style",
        "glass",
        "sense",
        "brewery",
        "level",
        "brewpub",
        "butter",
        "charr",
        "lover",
        "organic",
        "press",
        "lemon",
        "salt",
        "syrup",
        "acidness",
        "sourness",
        "richness",
        "Publisher",
        "Note",
        "regard",
        "maps",
        "14 February",
        "References",
        "Proceedings",
        "association",
        "ACL",
        "12",
        "Synth Lect Human Lang Technol",
        "Tenth ACM SIGKDD international conference",
        "accurate feature rating estimation",
        "Latent aspect rating analysi",
        "Stede M. Lexicon-based methods",
        "Ester M. Opinion digger",
        "Human Language Technology",
        "42nd annual meeting",
        "40th annual meeting",
        "Natural Language Processing",
        "Multi-aspect opinion polling",
        "review selection approach",
        "text summarization approach",
        "unsupervised opinion miner",
        "Expert Syst Appl",
        "IEEE Intell Syst",
        "A sentiment education",
        "Preference-based clustering reviews",
        "unstructured product reviews",
        "Liu B. Mining",
        "online product reviews",
        "opinion mining",
        "Empirical Methods",
        "ACM conference",
        "Taboada M",
        "Tofiloski M",
        "Hu M",
        "subjectivity summarization",
        "unsupervised classification",
        "Liu Y.",
        "Pang B",
        "data mining",
        "sentiment classification",
        "customer reviews",
        "textual reviews",
        "Data Eng",
        "Zhan J",
        "Loh HT",
        "customer concerns",
        "Dang Y",
        "Zhang Y",
        "Chen H.",
        "lexicon-enhanced method",
        "Lee L.",
        "minimum cuts",
        "Computational Linguistics",
        "Brooke J",
        "Voll K",
        "Turney PD.",
        "Semantic orientation",
        "knowledge discovery",
        "New York",
        "Popescu AM",
        "Etzioni O.",
        "Zhu J",
        "Wang H",
        "Tsou BK",
        "knowledge management",
        "Long C",
        "Zhang J",
        "Zhut X.",
        "Col- ing",
        "Poster volume",
        "Moghaddam S",
        "Chen Li",
        "Wang Feng",
        "e-commerce recommendation",
        "Lu Y",
        "Zhai C.",
        "experiment",
        "opinions",
        "HLT",
        "information",
        "CIKM"
      ],
      "merged_content": "\nMining aspects of customer’s review \non the social network\nTu Nguyen Thi Ngoc1*, Ha Nguyen Thi Thu1 and Viet Anh Nguyen2\n\nIntroduction\nIn recent years, a lot of people often express their opinions about things such as products \nand services on social networks and e-commerce web sites. These opinions or reviews \noften play significant role in improving the quality of products and services. However, \nthe huge amount of reviews poses a challenge of how to efficiently mine useful informa-\ntion about a product or a service. To deal with this problem, much work has been intro-\nduced including summarizing users’ opinions [1], extracting information from reviews \n[2–5], analyzing user sentiments [6–9], and so on. In this paper, we focus on the problem \nof extracting information from reviews. More specifically, this study aims at developing \nefficient methods for dealing with the three tasks: extracting aspects mentioned in the \nreviews of a product, inferring the user’s rating for each identified aspect, and estimating \nthe weight posed on each aspect by the users.\n\nA user review often mentions different aspects, which are attributes or components of \na product. An aspect is usually a concept in which the user’s opinion is expressed in dif-\nferent level of positivity or negativity. For example, in the review given in Fig. 1, the user \nlikes the coffee, manifested by a 5-star overall rating. However, positive opinions about \n\nAbstract \nThis study represents an efficient method for extracting product aspects from cus-\ntomer reviews and give solutions for inferring aspect ratings and aspect weights. \nAspect ratings often reflect the user’s satisfaction on aspects of a product and aspect \nweights reflect the degree of importance of the aspects posed by the user. These \ntasks therefore play a very important role for manufacturers to better understand their \ncustomers’ opinion on their products and services. The study addresses the problem \nof aspect extraction by using aspect words based on conditional probability com-\nbined with the bootstrap technique. To infer the user’s rating for aspects, a supervised \napproach called the Naïve Bayes classification method is proposed to learn the aspect \nratings in which sentiment words are considered as features. The weight of an aspect \nis estimated by leveraging the frequencies of aspect words within each review and \nthe aspect consistency across all reviews. Experimental results show that the proposed \nmethod obtains very good performance on real world datasets in comparison with \nother state-of-the-art methods.\n\nKeywords: Aspect extraction, Aspect rating, Aspect weight, Conditional probability, \nCore term, Naive Bayes\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nM E T H O D O LO G Y\n\nNguyen Thi Ngoc et al. J Big Data            (2019) 6:22  \nhttps://doi.org/10.1186/s40537-019-0184-5\n\n*Correspondence:   \ntunn.dhdl@gmail.com \n1 Department \nof E-Commerce, Vietnam \nElectric Power University, \n235 Hoang Quoc Viet, Hanoi, \nVietnam\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0184-5&domain=pdf\n\n\nPage 2 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nbody, taste, aroma and acidity aspects of the coffee are also given. The task of aspect \nextraction is to identify all such aspects from the review. A challenge here is that some \naspects are explicitly mentioned and some are not. For instance, in the review given in \nFig.  1, taste and acidity of the coffee are explicitly mentioned, but body and aroma are \nnot explicitly specified. Some previous work dealt with identifying explicit aspects only, \nfor example [10]. In our paper, both explicit and implicit aspects are identified. Another \ndifficulty of the aspect extraction task is that it may generate a lot of noise in terms of \nnon-aspect concepts. How to minimize noise while still be able to identify rare and \nimportant aspects is also one of our concerns in this paper.\n\nMost of the earliest work to identify aspects are unsupervised model-based [11], in \nwhich statistics of relevant words are used. These methods do not require the labeled \ntraining data and have low cost. For example, frequency-based methods [10, 12, 13] \nconsider high-frequent nouns or noun phrases as aspect candidates. However, fre-\nquency-based approaches may miss low-frequent aspects. Several complex filter-based \napproaches are applied to solve this problem; however, the results are not as good as \nexpected because some aspects are still missed [14, 15]. Moreover, these methods face \ndifficulty in identifying implicit aspects. To overcome these problems, some supervised \nlearning techniques, such as the Hidden Markov Model (HMM) and Conditional Ran-\ndom Field (CRF) have been proposed. These techniques, however, require a set of manu-\nally labeled data for training the model and thus could be costly.\n\nThe problem of aspect extraction is solved by using aspect words based on conditional \nprobability combined with the bootstrap technique. It is assumed that the universal set \nof all possible aspects for each product are readily available together with aspect words \ncalled core terms (terms that describe aspects). This assumption is practical because \nthe number of important aspects is often small and can be easily obtained by domain \nexperts. The aspect extraction task then becomes how to correctly assign existing \naspects to sentences in the review. The main challenge here is that in many reviews, sen-\ntences do not contains enough core terms or even do not have any core term at all, and \nthus may be assigned with wrong aspects. This problem is solved by repeatedly updating \n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish-style cardamon coffee, brewed in a flared \ncopper stove-top pot like you see in Istanbul! But wow! This stuff is \namazing. \n\nDark without being bitter. Never acid at all, no matter how strong \nyou make it. So soft, so lovely. There’s a chocolate-like note, all warm \nand clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened condensed \nmilk as they suggest but it seems superfluous. Just drink it hot and strait\nand you will be very happy! \n\nFig. 1 Comment of Trung Nguyen coffee\n\n\n\nPage 3 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nand enlarging the set of core terms to the set of aspect words by using the conditional \nprobability technique combined with the bootstrap technique. This method leads to bet-\nter results of aspect extraction as shown in “Results and discussion” section.\n\nAfter the aspects are identified, inferring the user’s rating for them may bring more \nthorough understanding of the user’s satisfaction. A user usually gives an overall rat-\ning which express a general impression about a product. The overall rating is not always \ninformative enough. However, it can be assumed that the overall rating on a product \nis weighted sum of the user’s specific rating on multiple aspects of the product, where \n\nThree tasks\n\nExtracting \nAspects\n\nInferring \nAspect Rate\n\nEstimating Aspect \nWeight\n\nDark without \nbeing bitter.\n\nNever acid at \nall, no matter \nhow strong you \nmake it..\n\nSo soft, so \nlovely.\n\nThere’s a \nchocolate-like \nnote, all warm \nand clean, but \nnothing \nchocolate about \ntaste.\n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish -style cardamon coffee, brewed \nin a flared copper stove -top pot like you see in Istanbul! But \nwow! This stuff is amazing.\n\nDark without being bitter. Never acid at all, no matter how \nstrong you make it. So soft, so lovely. There’s a chocolate-like\nnote, all warm and clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened \ncondensed milk as they suggest but it seems superfluous. \nJust drink it hot and strait and you will be very happy!\n\nBody:   5\n\nAroma: -\n\nTaste:   5\n\nAcidity: 4\n\nBody:   0.2\n\nAroma: 0\n\nTaste:   0.6\n\nAcidity: \n0.2\n\nDark , \nbitter\n\nAcid, \nstrong\n\nSoft, \nlovely\n\nChocol-\nate-like, \nnote, \nwarm, \nclean, \ntaste\n\nFig. 2 An example of aspect extracting, aspect inferring, and aspect weighting tasks\n\n\n\nPage 4 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nthe weights basically measure the degree of importance of the aspects. Some previous \nwork [16, 17] infer the user’s rating for aspects and estimate the weight of aspects at \nthe simultaneously based on regression methods and using only the review content and \nthe associated overall rating. Different approach is applied to infer rating and weight of \naspects. More specifically, the weight of an aspect is calculated by leveraging the aspect \nwords frequency within the review and the aspect consistency across all reviews. Then, \na supervised approach called the Naïve Bayes classification method is used to infer the \nuser’s rating for aspects. Despite the fact that the solution is relatively simple, its tested \naccuracy on different real-life datasets are comparable to much more sophisticated state \nof the art approaches as shown in “Results and discussion” section.\n\nThe Fig. 2 summaries the three tasks mentioned above. The methods for solving these \ntasks are discussed in details in “Method” section of this paper.\n\nThe rest of this paper is structured as follows. “Related work” section introduces \nrelated works. “Problem definition” and “Method” sections represent the proposed \nmethodology. “Results and discussion” section show experimental and evaluation of the \nproposed method. Finally, “Conclusion” section concludes the paper and gives some \nfuture research directions.\n\nRelated work\nDuring the last decade, many researches work has been proposed in the opinion mining \narea. Researchers are paying increasing attention to methods of extracting information \nfrom reviews that indicates users’ opinions of aspects about products. A survey on opin-\nion mining and sentiment analysis [18] shows that two important tasks of aspect-based \nopinion mining are aspect identification and aspect-based rating inference. The survey \nalso mentions some interesting methods for these tasks including frequency-based, lexi-\ncon-based, machine learning and topic modeling.\n\nMost of the earliest researches to identify aspects are frequency-based ones [11]. In \nthese approaches, nouns and noun phrases are considered as aspect candidates [10, \n12–15]. Hu and Liu [10] uses a data mining algorithm for nouns and noun phrases iden-\ntification and label assignment by the part-of-speech/POS [19]. Their occurrence fre-\nquencies are counted, and only the frequent ones are kept. A frequency threshold is used \nand can be decided via experimental. In spite of its simplicity, this method is actually \nquite effective. Some commercial companies are using this method with some improve-\nments to increase in their business [11]. However, producing “non-aspect” is the limita-\ntion of these methods because some nouns or noun phrases that have high-frequency \nare not really aspects.\n\nTo solve these problems, some improved methods of this filtering approach have been \nproposed. [15] augments the frequency-based approach with an additional pattern-\nbased filters to remove some non-aspect terms. A similar solution, [14] extracts aspects \n(nouns) based on frequency and information distance. Firstly, they find seed words for \neach aspect by using the frequency-based method. Secondly, they use the information \ndistance in [20] to find other related words to aspects, e.g., for aspect price, it may find \n“$” and “dollars”. However, the frequency-based and rule-based approaches require the \nmanual effort of tuning various parameters, which limits their generalization in practice.\n\n\n\nPage 5 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTo deal with the limitations of frequency-based methods, in recent years, topic mod-\neling has emerged as a principled method for discovering topics from a large collection \nof texts. These researches are primarily based on two main basic models, pLSA (Prob-\nabilistic Latent Semantic Analysis) [21] and LDA (Latent Dirichlet allocation) [22]. In \n[4, 15, 23–25], the authors apply topic modeling to learn latent topics that correlate \ndirectly with aspects. [23] proposes a topic modeling for mining aspects. Firstly, they \nidentify aspects using topic modeling and then identify aspect-specific sentiment words \nby considering adjectives only. Lin et  al. [4] proposes Joint Sentiment-Topic (JST) and \nReverse-JST. Both models were based on the modified Latent Dirichlet allocation (LDA). \nThese models can extract sentiment as well as positive and negative topic from the text. \nBoth JST and RJST yield an accuracy of 76.6% on Pang and Lee [7] dataset. While topic-\nmodeling approaches learn distributions of words used to describe each aspect, in [24], \nthey separate words that describe an aspect and words that describe sentiment about an \naspect. To perform, this study use two parameter vectors to encode these two proper-\nties, respectively. Then, a weighted bipartite graph is constructed for each review, which \nmatches sentences in review to aspects. Learning aspect labels and parameters are per-\nformed with no supervision (i.e., using only aspect ratings), weak supervision (using a \nsmall number of manually-labeled sentences in addition to unlabeled data), or with full \nsupervision (using only manually-labeled data). Moghaddam and Ester [15] devised fac-\ntorized LDA (FLDA) to extract aspects and estimate aspect rating. The FLDA method \nassumes that each user (and item) has a set of distributions over aspects and aspect-\nbased ratings. Their work on multi-domain reviews reaches to 74% for review rating on \nTripAdvisor data set. In [26], the authors propose a new method called Aspect Identi-\nfication and Rating model (AIR) for mining textual reviews and overall ratings. Within \nAIR model, they allow an aspect rating to influence the sampling of word distribution \nof the aspect for each review. This approach is based on the LDA model. However, dif-\nferent from traditional topic models, the extraction of aspects (topics) and the sampling \nof words for each aspect are affected by the sampled latent aspect ratings which are \ndependent on the overall ratings given by reviewers. Then, they further enhance AIR \nmodel to handle quite unbalance of aspects mentioned in short reviews.\n\nAlthough topic modeling is an approach based on probabilistic inference and it can be \nexpanded to many types of information models, it has some limitations that restrict their \nuse in real-life sentiment analysis applications. For example, it requires a huge amount of \ndata and a significant amount of tuning in order to achieve reasonable results. It is very \neasy to find those general and frequent topics or aspects from a large document collec-\ntion, but it is hard to find those locally frequent but globally that is not frequent aspects. \nSuch locally frequent aspects are often the most useful ones for applications because \nthey are likely to be most relevant to the specific entities that the user is interested in. In \nshort, the results from current topic modeling methods are usually not relevant or spe-\ncific enough for many practical sentiment analysis applications [11].\n\nBesides, some lexicon-based methods, which are also unsupervised approach, are pro-\nposed. Opinions are extracted with respect to each feature using the dictionary-based \napproach, which also yields polarity and strength. These methods use a dictionary \nof sentiment words and phrases with their associated orientations and strength. They \nare combined with intensification and negation to compute a sentiment score for each \n\n\n\nPage 6 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\ndocument [8]. Xiaowen Ding, Minqing Hu use sentence and aspect-level sentiment clas-\nsification [10, 27, 28]. Yan et al. [29] propose a method called EXPRS (An Extended Pag-\neRank algorithm enhanced by a Synonym lexicon) to extract product features. To do so, \nthey extract nouns/noun phrases first and then extract dependency relations between \nnouns/noun phrases and associated sentiment words. Dependency relations included \nsubject-predicate relations, adjectival modifying relations, relative clause modifying rela-\ntions, and verb-object relations. The list of product features was extended by using its \nsynonyms. Non-features nouns are removed on the basis of proper nouns, brand names, \nverbal nouns and personal nouns. Peñalver-Martinez et  al. [30] developed a methodol-\nogy to perform aspect-based sentiment analysis of movie reviews. To extract the movie \nfeatures from the reviews, they make a domain ontology (Movie Ontology). SentiWord-\nNet is utilized to calculate the sentiment score. However, the critical issue here is how \nto construct such a sentiment lexicon, due to the cost of time and money to build such \ndictionaries.\n\nSentiment classification can be performed using machine learning approaches which \noften yield higher accuracy. Machine learning methods can be further divided into \nsupervised and unsupervised ones. For supervised methods, two sets of annotated data, \none for training and the other for testing are needed. Some of the commonly applied \nclassifiers for supervised learning are Decision Tree (DT), SVM, Neural Network (NN), \nNaïve Bayes, and Maximum Entropy (ME). In paper Asha et  al. [31], propose a Gini \nIndex based feature selection method with Support Vector Machine (SVM) classifier \nfor sentiment classification for large movie review data set. The Gini Index method for \nfeature selection in sentiment analysis has improved the accuracy. Another research, \nDuc-Hong Pham and Anh-Cuong Le [32] design a multiple layer architecture of knowl-\nedge representation for representing the different sentiment levels for an input text. This \nrepresentation is then integrated into a neural network to form a model for prediction \nof product overall ratings. These techniques, however, require a set of manually labeled \ndata for training the model and thus could be costly.\n\nProblem definition\nA user review i on some product is assumed containing two parts: the review’s text \ndenoted by di, and the review’s overall rating denoted by yi. Each review’s text di can \ncontain multiple sentences. Furthermore, each sentence contains multiple words coming \nfrom the universal set of all possible worlds V = {wk| k = 1, P} , called a word dictionary.\n\nIt is assumed further that for a product, the set of all possible K aspects is already \nknown together with topic words, called core terms that describe each aspect of the \nproduct.\n\nDefinition 1. Aspect An aspect is a feature (an attribute or a component) of a product. \nFor example, taste, aroma, and body are some possible aspects of the product “coffee”. We \nassume that there are K aspects mentioned in all reviews, denoted by A = {aj|j = 1, K} . \nAn aspect is represented by a set of words and denoted by aj = {w|w ∈ V, A(w) = j} , \nwhere aj is the name of the aspect, w is a word from the set V , and A(.) is a operator that \nmaps a word to the aspect. For example, words such as “taste”, “aftertaste”, and “mouth \nfeel” can characterize the taste aspect of the product coffee.\n\n\n\nPage 7 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nDefinition 2. Aspect rating Given a review i, a K-dimensional vector ri ∈ RK is used to \nrepresent the rating of K aspects in the review’s text di, denoted by ri = (ri1, ri2, . . . , riK ) , \nwhere rij is a number indicating the user’s opinion assessment on aspect aj, and \nrij ∈ [rmin, rmax] (e.g., the range of rij can be from 1 to 5).\n\nDefinition 3. Aspect weight Given a review i, a K-dimensional vector αi ∈ RK is used. \nThe vector is denoted as αi =\n\n(\n\nαi1, αi2, . . . , αiK\n)\n\n ) where αij is a number measuring the \ndegree of importance of aspect aj posed by the user, αij ∊ [0, 1], and \n\n∑K\nj=1 αij = 1 . A \n\nhigher weight means more emphasis is put on the corresponding aspect.\n\nDefinition 4. Aspect core terms Given an aspect aj, the set of associated core terms \nfor aj is denoted by Cj =\n\n{\n\nwj1, wj2, . . . , wjN\n}\n\n where wjk is a word that describes the \naspect aj. The core terms can be provided by the user or by some field experts.\n\nMajor notations used throughout the paper are given in Table 1.\n\nExtracting aspect\n\nThe goal of this task is to extract aspects mentioned in a review. It is assumed that each \naspect is a probability distribution over words. It is also assumed that each sentence in \na review’s text can mention more than one aspect. Therefore, our method to extract \naspects is based on conditional probability of words such that each sentence can be \nassigned with multiple labels.\n\nInferring aspect rate\n\nThis task is to infer the vector ri of aspect ratings (defined in Definition 2) given a \nreview di. Rating of an aspect reflects the user’s sentiment on the aspect which is often \nexpressed in positive or negative words. The more positive words the user use, the higher \nrating he/she want to pose on the aspect. This research adopts a supervised learning \nmethod, the Naive Bayes method, to learn the aspect ratings in which sentiment words \nare considered as features.\n\nTable 1 Notations used in this paper\n\nNotation Description\n\nD =\n{\n\ndi|i = 1, Q\n}\n\nThe set of reviews’ text, where Q is the number of reviews\n\nY =\n{\n\nyi|i = 1, Q\n}\n\nThe set of overall rating, yi is overall rating corresponded with di\nA = {a1, a2, . . . , aK } The set of aspect, where K is the number of aspects\n\nCj =\n{\n\nwj1, wj2, . . . , wjN\n}\n\nThe set of associated core terms for aspect aj, where N is the number of words\n\nV =\n{\n\nwk| k = 1, P\n}\n\nThe corpus of words, where P is the number of words\n\nSj =\n{\n\nsj1, sj2, . . . , sjM\n}\n\nThe set of sentences are assigned aspect aj, where M is the number of sentences\n\nTj =\n{\n\nwj1, wj2, . . . , wjT\n}\n\nThe set of aspect words are aspect expressions, where Tj  is the expression for aspect \naj, and T is the number of words\n\nij ∈ R\nK The aspect rating inferred from review di over K aspect, ri = (ri1 , ri2 , . . . riK)\n\nαi ∈ R\nK The aspect weights user places on K aspect within reviews’ text di, \n\nαi =\n(\n\nαi1 , αi2 , . . . , αiK\n)\n\nyi ∈ R\n+ The overall rating of review di\n\nrij The aspect rating on j-th aspect of review i, rij ∈ [1,5]\nαij The aspect weight of j-th aspect of review i, αij ∈ [0,1]\n\n\n\nPage 8 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nEstimating aspect weight\n\nThis task is to estimate non-negative weights αi that a user places on aspect aij of \nreview i. Weight of an aspect essentially measures the degree of importance posed \nby the user on the aspect. It is observed that people often talk more on aspects that \nthey are interested in a same review. Besides, the idea that an aspect is important is \noften shared by many other people. Based on these observations, a formula is devised \nto calculate aspect weight. The formula takes into account the occurrences of words \ndiscussing the aspect within a review and the frequency of text sentences discussing \nthe same aspect across all reviews.\n\nMethod\nExtracting aspect\n\nThe goal of this task is to assign a subset of aspect labels from the universal set of all \naspect labels of a product to every sentence in a review. Aspect label is determined \nbased on the set of relevant words called aspect words or terms. Each aspect in the \nuniversal label set is provided with some initial core terms. The main challenge here \nis that many reviews contain very few core terms or even do not contain any term at \nall. This results in incorrect labels being assigned to sentences. Therefore, it is required \nto expand the core terms to a richer set of aspect words based on the given data (the \nreviews). In some existing methods, the set of aspect words is built based on Bayes or \nHidden Markov Model. Our method use conditional probabilistic model [33] combined \nwith the Bootstrap technique to generate aspect words. Figure 3 illustrates four aspects \nof a coffee product represented by their corresponding aspect words, in which the sym-\nbol O represents core terms, the symbol X represents words appearing in the corpus. \nFor this coffee product four aspects body, taste, aroma, and acidity are already known. \n\naroma\n\nsmell\n\nflavor\n\ntaste\n\naftertaste\n\nmouthfeel \nfinishing\n\nbody\n\nacidity\n\nacid\n\nFig. 3 Core terms with aspects\n\n\n\nPage 9 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe sets of core terms corresponding to these aspects are {body}, {taste, aftertaste, fin-\nishing, mouthfeel}, {aroma, smell, flavor} and {acid, acidity}, respectively. Core terms are \nthen enlarged by inserting words that have high probability to appear in the same sen-\ntences that they occur. Sets of aspect words are represented by the four circles. These \ncircles may overlap, indicating that some aspect words may belong to different aspects.\n\nSuppose that A = {a1, a2, . . . , aK } is the set of K aspects. For each aj , a set of words \nthat appear in sentences labeled with aspect aj such that their occurrences exceed a \ngiven threshold is obtained. The set of words of two aspects can overlap, such that \nsome terms may belong to multiple aspects. First, sentences that contain at least one \nword in the original core terms of the aspect are located. Then, all words including \nnouns, noun phrases, adjectives, and adverbs that appeared in these sentences are \nfound. Words that occur more than a given threshold θ are inserted to the set of \naspect words. Words with maximum number of occurrences in the set of new-found \naspect words are added to the set of core terms. The new set of aspect words with \ncore terms excluded is used to find new sentences. The above-mentioned process is \nrepeated until no more new words are found.\n\nThe procedure for updating aspect words for an aspect aj is given below.\n\n\n\nPage 10 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nA bootstrapping algorithm to assign labels to sentences in the reviews is given below.\n\n\n\nPage 11 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nThe proposed Aspect Extraction Algorithm works as follows. First all reviews’ texts are \nsplit into sentences (step 2). Then, aspect labels from the set A of all labels are assigned \nto every sentence of the set D of reviews’ text based on the initial aspect core terms \n(step 3). Based on this initial aspect labeling, the set of aspect core terms and the set \nof aspect words for every aspect are updated (step 4). The labels for all sentences are \nupdated using the new core terms and the aspect words sets (step 5). Step 4 and step 5 \nare repeated until no more new aspect word set are found or the number of iterations \nexceeds a given threshold.\n\nInferring aspect rating and estimating aspect weight\n\nAspect ratings often reflect the user’s satisfaction on aspects of a product. Meanwhile, \naspect weights measure the degree of importance of the aspects posed by the user. Given \nthe overall rating on a product, it is assumed that the overall rating is the weighted sum \nof rating on multiple aspects of the product. Following this assumption, some regres-\nsion-based methods [16, 17, 34] have been proposed to estimate the two parameters by \nsolving the following equation:\n\nwhere rij and αij are the rating and the weight of k-th aspect of the review i, respectively.\nThere are linear regression methods [35] which estimate only the aspect weight and \n\nrequire that the aspect ratings are available. Some other methods [17, 34] estimate both \naspect’s rating and weight at the same time. The key point of these methods is to use sen-\ntiment words, more specifically the polarity of sentiment words, to calculate ratings and \nweights. Even though sentiment words can usually correctly reflect the user’s rating for \neach aspect, they do not always reflect the user’s opinion about an aspect’s weight.\n\nAspect rating and aspect weight of an aspect are estimated separately. An important \npoint in our method is that aspect rating and aspect weight are calculated based on the \nreview content only, without the requirement of knowing the user’s overall rating. How-\never, in “Results and discussion” section, Eq.  (1) is still used to test our method. It is \nshown experimentally that our results conform well to the assumption that the overall \nrating is the weighted sum of rating on multiple aspects.\n\nThe aspect rating problem is treated as the problem of multi-label classification, in which \nratings (from 1 to 5) as considered as labels, and sentiment words are used as features. \nIn most sentiment analysis work, adjectives and adverbs are used as candidate sentiment \nwords. Adjectives and adverbs are detected based on the well-known Part of Speech tech-\nnique (POS). It is recognized that some phrases can also be used to express sentiments \ndepending on different contexts. For example, in the following two sentences “we have big \nproblem with staff ”, and “we have a big room”, the two noun phrases “big problem” and “big \nroom” convey opposite sentiments, negative vs. positive, while both phrases contain the \nsame adjective “big”. Some fixed syntactic patterns in [9] as phrases of sentiment word fea-\ntures are used. Only fixed patterns of two consecutive words in which one word is an adjec-\ntive or an adverb and the other provides a context are considered.\n\n(1)yi =\nK\n∑\n\nj=1\n\nrijαij\n\n\n\nPage 12 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nTwo consecutive words are extracted if their POS tags conform to any of the rules in \nTable 2 in which JJ tags are adjectives, NN tags are nouns, RB tags are adverbs, and VB \ntags are verbs. For example, rule 2 in this table means that two consecutive words are \nextracted if the first word is an adverb, the second word is an adjective, and the third \nword (which is not extracted) is not a noun. As an example, in the sentence “Quite dry, \nwith a good grassy note”, two patterns “quite dry” and “good grassy” are extracted as they \nsatisfy the second and the third rules, respectively. Then, conditional probability of word \nfeatures in the corpus is determined. Label (scoring) for each aspect is predicted based \non Naïve Bayes method.\n\nGiven a review’s text di, the rating of an aspect aj with q extracted features is inferred \nbased on the probability rij that the rating label belongs to class c ∈ C = {1, 2, 3, 4, 5}. The \nprobability is as:\n\nIt is assumed that the features are independent, then (2) is transformed into:\n\nin which: P\n(\n\nfk|rij ∈ c\n)\n\n= naj\n(\n\nfk, c\n)\n\n/naj(c) is the probability that feature fk belongs to the \n\nclass c, naj( fk, c) is the number of sentences labeled as c of the aspect aj which contains \nthe feature fk, and naj(c) is the number of all sentences containing the aspect aj and has \nclass label c,\nP(rij ∈ c)= naj(c)/naj is the probability that the rating rij belongs to the class c, naj(c) is \n\nthe number of sentences labeled as c of aspect aj, and naj is the number of all sentences \ncontaining the aspect aj,\n\nP( fk) is the probability of feature fk.\nFor smoothing (3), Laplace transformation is used. We get:\n\nin which, |V| is number of word features regarding the aspect aj.\nThe rating rij is the label c that maximize P(rij ∈ c|f1, . . . , fq).\n\n(2)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\nP\n(\n\nf1, . . . , fq|rij ∈ c\n)\n\nP\n(\n\nrij ∈ c\n)\n\nP\n(\n\nF1, . . . , Fq\n)\n\n(3)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\n\n∏q\nk=1\n\nP(fk|rij ∈ c)P\n(\n\nrij ∈ c\n)\n\n∑q\nk=1\n\nP\n(\n\nfk\n)\n\n(4)P\n(\n\nfk|rij ∈ c\n)\n\n=\nnaj\n\n(\n\nfj, c\n)\n\n+ 1\n\nnaj(c) + |V | + 1\n\nTable 2 POS labeled rules [9]\n\nThe first word The second word The third word \n(non extracted)\n\n1. JJ NN or NNS Any word\n\n2. RB, RBR, or RBS JJ Not NN nor NNS\n\n3. JJ JJ Not NN nor NNS\n\n4. NN or NNS JJ Not NN nor NNS\n\n5. RB, RBR, or RBS VB, VBD, VBN, or VBG Any word\n\n\n\nPage 13 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nNow the method to estimate aspect weight is given. By doing research carefully through-\nout the reviews, it can be seen that if a user care more about an aspect (showing that the \naspect is important to the user), he/she will mention more about it in the review. Moreover, \nthe idea that an aspect is important is often shared by many other users. Following this \nobservation, we estimate aspect weights by calculating two components: the weight meas-\nure of aspect aj within the reviews’ text  di, denoted by EDij, and the weight measure of the \naspect across all reviews, denoted by ECj. Note that in this way, the polarity measures of \nsentiment words are not used as in some other approaches. Instead, probability measures of \nwords and sentences regarding an aspect in the review and the corpus are considered. This \nidea is similar to the idea of using tf/idf for measuring word importance to some extent.\n\nGiven a review i, the weight component of the aspect aj, EDij, is calculated as:\n\nIn which: wijk is the k-th word in the aspect words of aspect aj , and Ni is the number of \naspect words that occur in the review’s text di for all aspects.\n\nThe weight component ECj is calculated as:\n\nIn which: sjk is the k-th sentence in the corpus labeled by the aspect aj , and M is the \nnumber of all sentences in the corpus.\n\nFinally, the weight αij for an aspect aj of review i is calculated as:\n\nThe denominator \n∑K\n\nj=1 EDijECj is to normalize the value of αij to the range [0,1].\n\nResults and discussion\nIn this section, experiments to evaluate the proposed methods are conducted.\n\nData set\n\nThe experiments are carried out using three different data sets including a data set for \nhotel review collected from Tripadvisor.com [17], one data set for beer review used in \n[24], and a data set for Trung Nguyen coffee review collected by our self from the Ama-\nzon web site.\n\nThe Hotel data set contains seven different aspects that are room, location, cleanliness, \ncheck-in/front desk, service and business services. The beer data set has five distinct \naspects that are aroma (or smell), palate (or feel), taste, appearance (or look), and over-\nall. This data set is quite big with millions of reviews. A subset of 50,000 beer reviews is \n\nĉ = argmaxc∈C\n\nq\n∏\n\nk=1\n\nP(fk|rij ∈ c)P\n(\n\nrij ∈ c\n)\n\n.\n\n(5)EDij =\n∑Ni\n\nk=1\nwijk\n\nNi\n.\n\n(6)ECj =\n∑M\n\nk=1 sjk\n\nM\n\n(7)αij =\nEDij ∗ ECj\n\n∑K\nj=1 EDij ∗ ECj\n\n\n\nPage 14 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nused in the experiment. The coffee data set contains 1200 reviews belongs to 17 different \nkinds of coffee. Table 3 gives some statistics of the three data sets.\n\nInferring aspect rating task\n\nNote that each review may be assigned with different labels. This means that sentence \nlevel, not review level is considered. Testing sets of 2500, 2000, and 500 sentences are \nselected randomly from the hotel data set, beer data set, and coffee data set, respectively. \nThe rest of sentences are used as the training sets.\n\nTable 4 gives initial core terms for the three data sets.\nThe precision measure is used to evaluate the experimental results:\n\nTable  5 shows the performance of our method on three data sets for the aspect \nextraction task. Our method yields up to average precision of 0.786, 0.803 and 0.653 \nfor hotel data set, beer data set and coffee data set, respectively. Our method obtains \ngood performance on the hotel and beer data set. However, for the coffee data set, \nthe result is not as good as expected. This is because in the coffee data set, users often \ngive only general view about a product, and moreover, the data set contains mostly \n\n(8)P =\n∣\n\n∣extrating Aspect ∩ True Aspect\n∣\n\n∣\n\n∣\n\n∣extracting Aspect\n∣\n\n∣\n\nTable 3 Summary of the Data Set\n\nHotel dataset Beer dataset Coffee dataset\n\n#Reviews 193,661 50,000 1200\n\n#Sentences 1,790,880 509,320 5289\n\n#Avg. sentences per review 9.25 10.19 4.41\n\nTable 4 Seed word for main aspects\n\nCategory Aspect Seed words\n\nHotel Value Value, price, worth\n\nRoom Room, rooms\n\nLocation Location\n\nCleanliness Dirty, smelled, clean\n\nCheck in/front desk Staff\n\nService Service, breakfast, food\n\nBusiness service Internet, wifi\n\nBeer Appearance Appearance, color, colors, coloring, head, foam\n\nAroma Aroma, aromas, smell, smelling\n\nPalate Palate, mouth, feel, mouth feel\n\nTaste Taste, tastes, aftertaste, in the end, finish, finishing\n\nOverall Overall\n\nCoffee Aroma Aroma, aromas, smell, smelling, flavor, flavors\n\nTaste Taste, tastes, aftertaste, finish, finishing, mouth feel\n\nAcidity Acid, acidity\n\nBody Body, aged, vintage\n\n\n\nPage 15 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nvery short reviews, with average number of sentences of 4.5, compared to 10 and 9 of \nthe hotel data set and the beer data set.\n\nOur method is compared with other works. First, our method is compared with the \nfrequency-based method in [14] on the hotel dataset. Figure 4 shows that our method \noutperforms Long’s in room (R), service (S), and cleanliness (C) aspects. But Long’s \nmethod outperforms us in detecting the value (V) aspect.\n\nOur method is compared with two topic modeling-based methods in [22] and in \n[24] on the beer data set. The method in [22] is a semi-supervised method, called \n\nTable 5 Aspect Identification results\n\nCategory Aspect Precision\n\nHotel Value 0.747\n\nRoom 0.837\n\nLocation 0.814\n\nCleanliness 0.764\n\nCheck in/front desk 0.850\n\nService 0.754\n\nBusiness service 0.737\n\nAverage 0.786\n\nBeer Appearance 0.750\n\nAroma 0.857\n\nPalate 0.857\n\nTaste 0.848\n\nOverall 0.704\n\nAverage 0.803\n\nCoffee Aroma 0.667\n\nTaste 0.677\n\nAcidity 0.667\n\nBody 0.600\n\nAverage 0.653\n\nV R S C AVER\n\nLong,Zhang, and Zhu 0.759 0.776 0.746 0.750 0.758\n\nOur Method 0.747 0.837 0.754 0.764 0.776\n\n0.700\n\n0.720\n\n0.740\n\n0.760\n\n0.780\n\n0.800\n\n0.820\n\n0.840\n\n0.860\n\nPr\nec\n\nis\nio\n\nn \n\nAspect\n\nHotel \n\nFig. 4 The results of our method and Long et al. method\n\n\n\nPage 16 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nLDA. In [24], the authors give 3 different methods, namely, unsupervised, semi-super-\nvised, and fully supervised methods. As our method can be considered as a semi-\nsupervised method, it is compared with PALE LAGER, a semi-supervised method, \nand with PALE LAGER, a supervised method given in [24].\n\nThe results in Fig.  5 shows that our method outperforms LDA with a large mar-\ngin, and slightly outperforms PALE LAGER (a semi-supervised method) and PALE \nLAGER (a supervised method).\n\nWe then search for the best threshold θ at which our method performs the best. The \nresults are shown in Fig. 6, where the threshold θ of about 0.15 is the best one.\n\nAspect ranking prediction\n\nUnlike the evaluation of the aspect extraction task that is done based on the sentence level, \nin this task, the result based on the review level is evaluated.\n\nThe mean square error measure (named �2aspect ) is used for evaluating methods of min-\ning aspect rating.\n\nwhere K is the number of aspects, Q is the number of reviews, and r∗ij is the true ratings \n\nfor aspect aj within review’s text di.\nTo evaluate how well the predicted aspect ratings can preserve their relative order within \n\na review given the true ratings, the aspect correlation measure (named ρaspect ) is used:\n\n(9)�2aspect =\n\n∑Q\ni=1\n\n∑K\nj=1\n\n(\n\nrij − r\n∗\nij\n\n)2\n\nQ × K\n\n(10)ρaspect =\n∑Q\n\ni=1 ρri,r\n∗\ni\n\nQ\n\n0.3000.320\n0.240\n\n0.750\n0.800 0.803\n\n0.000\n\n0.500\n\n1.000\n\nP\nre\n\nci\nsi\n\non\n\nBeeradvocate\n\nLDA, K topics, semi-supervised\n\nLDA, 10 topics, semi-supervised\n\nLDA, 50 topics, semi-supervised\n\nPALE LAGER, semi-supervised\n\nPALE LAGER, fully-supervised\n\nOur Method\n\nFig. 5 The results of our method and LDA, PALE LAGER\n\n\n\nPage 17 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nwhere Q is the number of reviews, and ρri,r∗i  is the Pearson correlation between two vec-\ntors ri and r∗i  of the inferred and the true ratings, respectively.\n\nThe two measures above are for evaluating the results for each review. The results on the \nwhole set of reviews are evaluated by using the so called aspect correlation across reviews \nmeasure ( ρreview):\n\nwhere ρ\n(\n\n−→\nrj ,\n\n−→\nr∗j\n\n)\n\n is the Pearson correlation between two vectors −→rj  and \n−→\nr∗j  of the inferred \n\nand rating.\nOur method is also compared with Long’s [14] and Wang’s [17]. Long proposed \n\ntwo methods based on the SVM classifier and the Bayesian Network classifier. Wang’s \nmethod is called Latent Rating Regression (LRR) which infers aspect ratings and aspect \nweights simultaneously.\n\nThe performance results are shown in Table  6. Our method performs much better \nthan Long’s method and Wang’s method on all three measures.\n\nEstimating aspect weight\n\nFor evaluating the correctness of estimated weights by our method, the overall rating is \ncalculated and compared with the true overall rating given by the user. The estimated \noverall rating is given by the following formula:\n\nwhere rij is the rating of the j-th aspect of the review i and αij is the estimated weight.\n\n(11)ρreview =\n\n∑K\nj=1 ρ\n\n(\n\n−→\nrj ,\n\n−→\nr∗j\n\n)\n\nK\n\n(12)ŷi =\nK\n∑\n\nj=1\n\nrij αij\n\n0.000\n\n0.200\n\n0.400\n\n0.600\n\n0.800\n\n1.000\n\n0.05 0.08 0.1 0.15 0.2 0.3 0.4 0.5\n\nPr\nec\n\nis\nio\n\nn\n\nAspect Evaluation with θ\n\nAppearance Aroma taste\n\npalate overall\n\nFig. 6 Aspect evaluation with θ\n\n\n\nPage 18 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nOur method is compared with Wang’s method [17] based on the �2\noverallrating\n\n . Table 7 \npresents the mean square errors of overall rating for the three data sets. As can be seen \nin the table, our results are comparable to Wang’s.\n\nConclusion\nThis paper dealed with three important sub-tasks of the opinion mining problem, that \nare (1) extracting aspects mentioned in the reviews of a product by using conditional \nprobability of words, (2) inferring the user’s rating for each identified aspect based on \nNaïve Bayes classifier, (3) estimating the weight placed on each aspect by the users by \nusing the occurrences of word that discuss the aspect within a review and the frequency \nof text sentences that discuss the same aspect across all reviews.\n\nOur method does not require to know the overall ratings and is as not complicated as \nsome other previous methods. However, it still works very well on real world datasets in \ncomparison with other state of the art methods.\n\nIn the future, the problem of aspect mining from unlabeled data will be considered. \nIn addition, the proposed model will be applied to other domains such as movie, digital \ncamera businesses to validate its generalized effectiveness.\n\nAbbreviations\npLSA: Probabilistic Latent Semantic Analysis; LDA: Latent Dirichlet allocation; HMM: Hidden Markov Model; CRF: Condi-\ntional Random Field.\n\nAuthors’ contributions\nTNTN proposed method and performed experiments, HNTT supervised the programming and wrote draft manuscript. \nVAN wrote a part of the manuscript and corrected after received reviews. All authors read and approved the final \nmanuscript.\n\nAuthor details\n1 Department of E-Commerce, Vietnam Electric Power University, 235 Hoang Quoc Viet, Hanoi, Vietnam. 2 Institute \nof Information Technology, Vietnam Academy of Science and Technology, Hanoi, Vietnam. \n\nAcknowledgements\nThis research is funded by the project “Building a System for Prediction and Management of Information Spreading in \nSocial Networks in Vietnam” under Grant VAST01.01/17-18\n\nCompeting interests\nData mining, big data, machine learning and natural language processing.\n\nTable 6 Comparison with other models for referring aspect ratings\n\nMethod �2aspect ρaspect ρreview\n\nLong et al. with SVM 0.286 0.557 0.708\n\nLong et al. with BN 0.441 0.429 0.591\n\nLRR 0.896 0.464 0.618\n\nOur method 0.101 0.583 0.757\n\nTable 7 MSE of overall rating prediction\n\nMethod Product datasets\n\nHotel Beer Coffee\n\nLRR 0.905 0.856 1.234\n\nThe proposed method 0.1456 0.1423 0.1904\n\n\n\nPage 19 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nFunding\nNot applicable.\n\nAvailability of data and materials\nAll data used in this study are publicly available and accessible in the source Tripadvisor.com.\n\nAppendix\nSee Tables 8, 9, 10.\n\nTable 8 Aspect word set of Hotel data\n\nAspect Aspect words\n\nValue Hotel, charge, cost, discount, dollars\n\nRoom Bathroom, bathrooms, bed, beds, bath, floor, floors, chair, chairs, balcony, \nshower, lobby, noise, pool, queen, couple, Sheraton, coffee, desk, hotel, \nsuite, tv, view, water, window, carpet, closet, doors, furniture, king, pillows, \nsink, toilet, tub, toiletries, …\n\nLocation Airport, area, center, downtown, hotel, market, place, places, restaurant, \nshop, shops, shopping, show review, street, view, views, neighborhood, \nsquare, waterfront, …\n\nCleanliness Hotel, floor, shelf, desk, chair, bag, door, lobby, stairs, …\n\nCheck in/front desk Desk, clerk, lounge, luggage, reception, checkout, …\n\nService Bar, bars, coffee, concierge, food, park, parking, restaurant, wine, buffet, …\n\nBusiness service Tv, television, wireless, hotwire, cable, computer, connection, free, freeway, \n…\n\nTable 9 Aspect word set of Beer data\n\nAspect Aspect words\n\nAppearance Black, Body, brown, bubble, bud, copper, lace, lacing, dots, drip, dust, back, finger, fizzy, fluff, \ngolden, half, layer, orange, straw, surface, top, white, yellow…\n\nAroma Bacon, banana, basil, caramel, cheese, cream, dry citrus, fruitiness, honey, light, malt, malts, meat, \nmint, nose, pear, perfume, pill, pine, roast, sandalwood, smoke, smoky, spice, sweet, sweetness, \nyeast…\n\nPalate Alcohol, body, carbonation, cream, Drinkability, dry, hoppy, light, round, spring, summer, …\n\nTaste Alcohol, avalanche, balance, bitterness, body, bread, burn, caramel, carbonation, cheese, chocolate, \nclear, cocoa, coffee, complexity, flavors, flavors, fruit, fruitiness, ginger, grains, malt, matiness, \nmeat, Medium-dry, oats, pear, roast, smoke, smoothness, spring, subtleties, summer, sweet, \nthroat, toffee, tongue, vanilla, wood, …\n\nOverall Beer, beers, bottle, drink, beverage, style, glass, sense, quality, brewery, level, lace, brewpub …\n\nTable 10 Aspect word set of Coffee data\n\nAspect Aspect words\n\nAroma Bran, brew, butter, charr, chocolate, citrus, fruit, \nhoney, love, lover, organic, press, quality, \nsmooth, lemon, smoke, stuff, …\n\nTaste Bitter, bitterness, chocolate, honey, salt, fresh-\nness, brew, love, lover, mild, organic, press, \nquality, roaster, smooth, soft, sour, stuff, sweet, \nsweetness, syrup, …\n\nAcidity Acidic, acidness, sourness, …\n\nBody Love, press, smooth, richness, thick, thin, soft, …\n\n\n\nPage 20 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 12 December 2017   Accepted: 14 February 2019\n\nReferences\n 1. Park S, Lee K, Song J. Contrasting opposing views of news articles on contentious issues. In: Proceedings of the 49th \n\nannual meeting of the association for computational linguistics (ACL-2011). 2011.\n 2. van den Camp M, van den Bosch A. The socialist network. Decis Support Syst. 2012;53:761–9.\n 3. Li SK, Guan Z, Tang LY, et al. Exploiting consumer reviews for product feature ranking. J Comput Sci Technol. \n\n2012;27(3):635–49. https ://doi.org/10.1007/s1139 0-012-1250-z.\n 4. Lin C, He Y, Everson R, Ruger S. Weakly supervised joint sentiment-topic detection from text. IEEE Trans Knowl \n\nData Eng. 2012;24(6):1134–45.\n 5. Zhan J, Loh HT, Liu Y. Gather customer concerns from online product reviews—a text summarization approach. \n\nExpert Syst Appl. 2009;36:2107–15.\n 6. Dang Y, Zhang Y, Chen H. A lexicon-enhanced method for sentiment classification: an experiment on online \n\nproduct reviews. IEEE Intell Syst. 2010;25(4):46–53.\n 7. Pang B, Lee L. A sentiment education: sentiment analysis using subjectivity summarization based on minimum \n\ncuts. In: Proceedings of the 42nd annual meeting on association for Computational Linguistics. 2004. p. 271.\n 8. Taboada M, Brooke J, Tofiloski M, Voll K, Stede M. Lexicon-based methods for sentiment analysis. Comput Lin-\n\nguistics. 2011;37(2):267–307.\n 9. Turney PD. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. \n\nIn: ACL ‘02 Proceedings of the 40th annual meeting on association for computational linguistics. p. 417–24.\n 10. Hu M, Liu B. Mining and summarizing customer reviews. In: Proceedings of the Tenth ACM SIGKDD international \n\nconference on knowledge discovery and data mining, KDD’04, New York: ACM; 2004, p. 168–77.\n 11. Liu B. Sentiment analysis and opinion mining. Synth Lect Human Lang Technol. 2012;5(1):1–67.\n 12. Popescu AM, Etzioni O. Extracting product features and opinions from reviews. In: HLT ‘05 Proceedings of \n\nthe conference on Human Language Technology and Empirical Methods in Natural Language Processing. p. \n339–46.\n\n 13. Zhu J, Wang H, Tsou BK, Zhu M. Multi-aspect opinion polling from textual reviews. In: Proceedings of ACM interna-\ntional conference on information and knowledge management (CIKM-2009). 2009.\n\n 14. Long C, Zhang J, Zhut X. A review selection approach for accurate feature rating estimation. In: Proceedings of Col-\ning 2010: Poster volume. 2010.\n\n 15. Moghaddam S, Ester M. Opinion digger: an unsupervised opinion miner from unstructured product reviews. In: \nProceeding of the ACM conference on Information and knowledge management (CIKM-2010). 2010.\n\n 16. Chen Li, Wang Feng. Preference-based clustering reviews for augmenting e-commerce recommendation. Knowl \nBased Syst. 2013;50:44–59.\n\n 17. Wang H, Lu Y, Zhai C. Latent aspect rating analysis on review text data: a rating regression approach. In: Proceedings \nof the 16th ACM SIGKDD international conference on knowledge discovery and data mining, KDD’10, New York: \nACM; 2010. p. 783–92.\n\n 18. Ravi K, Ravi V. A survey on opinion mining and sentiment analysis: tasks, approaches and applications. Knowl Based \nSyst. 2015;89:14–46.\n\n 19. Santorini B. Part-of-speech tagging guidelines for the Penn Treebank Project, University of Pennsylvania, School of \nEngineering and Applied Science, Dept. of Computer and Information Science. 1990.\n\n 20. Cilibrasi RL, Vitanyi PMB. The google similarity distance on Knowledge and Data Engineering, IEEE transactions. 2007; \n370–83.\n\n 21. Hofmann T. Probabilistic latent semantic indexing. In: Proceedings of the 22nd annual international ACM SIGIR \nconference on Research and development in information SIGIR’99. New York: ACM; 1999. p. 50–7.\n\n 22. Blei DM, Ng AY, Jordan MI. Latent dirichlet allocation. J Mach Learn Res. 2003;3:993–1022.\n 23. Brody S, Elhadad N. An unsupervised aspect-sentiment model for online reviews. In: Human language technolo-\n\ngies: the annual conference of the north american chapter of the association for computational linguistics, HLT’10, \nStroudsburg; 2010. p. 804–12.\n\n 24. McAuley J, Leskovec J, Jurafsky D. Learning attitudes and attributes from multi-aspect review. In: International \nconference on data mining (ICDM). 2012.\n\n 25. Sauper C, Barzilay R. Auto-matic aggregation by joint modeling of aspects and values. J Artif Int Res. \n2013;46(1):89–127.\n\n 26. Li H, Lin R, Hong R, Ge Y. Generative models for mining latent aspects and their ratings from short reviews. In: 2015 \nIEEE international conference on data mining. p. 241–50.\n\n 27. Ding X, Liu B, Yu PS. A holistic lexicon-based approach to opinion mining. In: Proceedings of the conference on web \nsearch and web data mining (WSDM-2008). 2008.\n\n 28. Kim SM, Hovy E. Determining the sentiment of opinions. In: Proceedings of international conference on computa-\ntional linguistics (COLING-2004).\n\n 29. Yan Z, Xing M, Zhang D, Ma B. EXPRS: an extended pagerank method for product feature extraction from online \nconsumer reviews, Inf. Manage. 2015.\n\n 30. Penalver-Martinez I, Garcia-Sanchez F, Valencia-Garcia R, Rodriguez-Garcia MA, Moreno V, Fraga A, Sanchez-Cer-\nvantes JL. Feature-based opinion mining through ontologies. Expert Syst Appl. 2014;41(13):5995–6008.\n\n Published online: 28 February 2019 \n\nhttps://doi.org/10.1007/s11390-012-1250-z\n\n\nPage 21 of 21Nguyen Thi Ngoc et al. J Big Data            (2019) 6:22 \n\n 31. Manek AS, Shenoy PD, Mohan MC, Venugopal KR. Aspect term extraction for sentiment analysis in large movie \nreviews using Gini Index feature selection method and SVM classifier. World Wide Web. 2017;20(2):135–54.\n\n 32. Pham DH, Le AC. Learning multiple layers of knowledge representation for aspect based sentiment analysis. Data \nKnowl Eng. 2017;114:26–39.\n\n 33. Dao TT, Thanh TD, Hai TN, Ngoc VH. Building Vietnamese topic modeling based on core terms and applying in text \nclassification. In: Proc. of the fifth IEEE international conference on communication systems and network technolo-\ngies. 2015. P. 1284–88.\n\n 34. Yu J, Zha ZJ, Wang M, Chua TS. Aspect ranking: identifying important product aspects from online consumer \nreviews. In: Proceedings of the 49th Annual meeting of the association for computational linguistics: human \nlanguage technologies. Volume 1, HLT’11, Stroudsburg, PA, USA, 2011. Association for Computational Linguistics. p. \n1496–505.\n\n 35. Archak N, Ghose A, Ipeirotis PG. Show me the money!: Deriving the pricing power of product features by mining \nconsumer reviews. In: Proceedings of the 13th ACM SIGKDD in-ternational conference on Knowledge discovery and \ndata min-ing, KDD’07, New York: ACM; 2007. p. 56–65.\n\n\n\tMining aspects of customer’s review on the social network\n\tAbstract \n\tIntroduction\n\tRelated work\n\tProblem definition\n\tExtracting aspect\n\tInferring aspect rate\n\tEstimating aspect weight\n\n\tMethod\n\tExtracting aspect\n\tInferring aspect rating and estimating aspect weight\n\n\tResults and discussion\n\tData set\n\tInferring aspect rating task\n\tAspect ranking prediction\n\tEstimating aspect weight\n\n\tConclusion\n\tAuthors’ contributions\n\tReferences\n\n\n\n\n",
      "text": [
        "Published online: 28 February 2019"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"Published online: 28 February 2019\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":16},{\"x\":1020,\"y\":17},{\"x\":1019,\"y\":73},{\"x\":0,\"y\":70}],\"text\":\"Published online: 28 February 2019\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":18},{\"x\":282,\"y\":17},{\"x\":282,\"y\":70},{\"x\":0,\"y\":69}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":292,\"y\":17},{\"x\":500,\"y\":16},{\"x\":501,\"y\":72},{\"x\":292,\"y\":70}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":510,\"y\":16},{\"x\":587,\"y\":16},{\"x\":588,\"y\":72},{\"x\":511,\"y\":72}],\"text\":\"28\"},{\"boundingBox\":[{\"x\":598,\"y\":16},{\"x\":862,\"y\":17},{\"x\":864,\"y\":74},{\"x\":598,\"y\":72}],\"text\":\"February\"},{\"boundingBox\":[{\"x\":873,\"y\":17},{\"x\":1017,\"y\":18},{\"x\":1019,\"y\":74},{\"x\":874,\"y\":74}],\"text\":\"2019\"}]}"
      ]
    },
    {
      "@search.score": 0.35833597,
      "content": "\nSänger et al. Journal of Trust Management  (2015) 2:5 \nDOI 10.1186/s40493-015-0015-3\n\nRESEARCH Open Access\n\nReusable components for online reputation\nsystems\nJohannes Sänger*, Christian Richthammer and Günther Pernul\n\n*Correspondence:\njohannes.saenger@wiwi.\nuni-regensburg.de\nUniversity of Regensburg,\nUniversitätsstraße 31, 93053\nRegensburg, Germany\n\nAbstract\n\nReputation systems have been extensively explored in various disciplines and\napplication areas. A problem in this context is that the computation engines applied by\nmost reputation systems available are designed from scratch and rarely consider well\nestablished concepts and achievements made by others. Thus, approved models and\npromising approaches may get lost in the shuffle. In this work, we aim to foster reuse in\nrespect of trust and reputation systems by providing a hierarchical component\ntaxonomy of computation engines which serves as a natural framework for the design\nof new reputation systems. In order to assist the design process we, furthermore,\nprovide a component repository that contains design knowledge on both a\nconceptual and an implementation level. To evaluate our approach we conduct a\ndescriptive scenario-based analysis which shows that it has an obvious utility from a\npractical point of view. Matching the identified components and the properties of trust\nintroduced in literature, we finally show which properties of trust are widely covered by\ncommon models and which aspects have only rarely been considered so far.\n\nKeywords: Trust; Reputation; Reusability; Trust pattern\n\nIntroduction\nIn the last decade, trust and reputation have been extensively explored in various disci-\nplines and application areas. Thereby, a wide range of metrics and computation methods\nfor reputation-based trust has been proposed. While most common systems have been\nintroduced in e-commerce, such as eBay’s reputation system [1] that allows to rate sell-\ners and buyers, considerable research has also been done in the context of peer-to-peer\nnetworks, mobile ad hoc networks, social networks or ensuring data accuracy, relevance\nand quality in several environments [2]. Computation methods applied range from sim-\nple arithmetic over statistical approaches up to graph-based models involving multiple\nfactors such as context information, propagation or personal preferences. A general prob-\nlem is that most of the newly introduced trust and reputation models use computation\nmethods that are designed from scratch and rely on one novel idea which could lead to\nbetter solutions [3]. Only a few authors build on proposals of others. Therefore, approved\nmodels and promising approaches may get lost in the shuffle.\nIn this work, we aim to encourage reuse in the development of reputation systems by\n\nproviding a framework for creating reputation systems based on reusable components.\nDesign approaches for reuse have been given much attention in the software engineering\n\n© 2015 Sänger et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nhttp://creativecommons.org/licenses/by/4.0\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 2 of 21\n\ncommunity. The research in trust and reputation systems could also profit from ben-\nefits like effective use of specialists, accelerated development and increased reliability.\nToward this goal, we propose a hierarchical taxonomy for components of computation\nengines used in reputation systems. Thereto, we decompose the computation phase of\ncommon reputation models to derive single building blocks. The classification based on\ntheir functions serves as a natural framework for the design of new reputation systems.\nMoreover, we set up a component repository containing artifacts on both a conceptual\nand an implementation level to facilitate the reuse of the identified components. On the\nconceptual level, we describe each building block as a design pattern-like solution. On\nthe implementation level, we provide already implemented components by means of web\nservices.\nThe rest of this paper is based on the design science research paradigm involving the\n\nguidelines for conducting design science research by Hevner et al. [4] and organized as\nfollows: Firstly, we give an overview of the general problem context as well as the relevance\nand motivation of our work. Thereby, we identify the research gap and define the objec-\ntives of our research. In the following section, we introduce our hierarchical component\ntaxonomy of computation engines used in reputation systems. After that, we point out\nhow our component repository is conceptually designed and implemented. Subsequently,\nwe carry out a descriptive scenario-based analysis of our approach. At the same time, we\nmatch all components identified with the properties of trust introduced in literature. We\nshow which properties of trust are widely covered by common models and which aspects\nhave only rarely been considered so far. Finally, we summarize the contribution and name\nour plans for future work.\n\nProblem context and motivation\nWith the success of the Internet and the increasing distribution and connectivity, trust\nand reputation systems have become important artifacts to support decision making in\nnetwork environments. To impart a common understanding, we firstly provide a defi-\nnition of the notion of trust. At the same time, we explain the properties of trust that\nare important with regard to this work. Then, we point out how trust can be established\napplying computational trust models. Focusing on reputation-based trust, we explain how\nand why the research in reputation models could profit from reuse. Thereby, we identify\nthe research gap and define the objectives of this work.\n\nThe notion of trust and its properties\n\nThe notion of trust is a topic that has been discussed in research for decades. Although\nit has been intensively examined in various fields, it still lacks a uniform and generally\naccepted definition. Reasons for this circumstance are the multifaceted terms trust is\nassociated with like credibility, reliability or confidence as well as the multidimension-\nality of trust as an abstract concept that has a cognitive, an emotional and a behavioral\ndimension. As pointed out by [5], trust has been described as being structural in\nnature by sociologists while psychologists viewed trust as an interpersonal phenomenon.\nEconomists, however, interpreted trust as a rational choice mechanism. The definition\noften cited in literature regarding trust and reputation online that is referred to as relia-\nbility trust was proposed by Gambetta in 1988 [6]: “Trust (or, symmetrically, distrust) is\na particular level of the subjective probability with which an agent assesses that another\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 3 of 21\n\nagent or group of agents will perform a particular action, both before he can monitor such\naction (or independently of his capacity ever to be able to monitor it) and in a context in\nwhich it affects his own action.”\nMultiple authors furthermore include security and risk which can lead to more com-\n\nplex definitions. Anyway, it is generally agreed that trust is multifaceted and dependent\non a variety of factors. Moreover, there are several properties of trust described in lit-\nerature (see Table 1). These properties are important with respect to this work because\nthey form the basis for many applied computation techniques in trust and reputation\nsystems described in Section ‘Hierarchical component taxonomy’. Reusable components\ncould extend current models by the ability to gradually include these properties.\n\nReputation-based trust\n\nIn recent years, several trust models have been developed to establish trust. Thereby,\ntwo common ways can be distinguished, namely policy-based and reputation-based trust\n\nTable 1 Overview of properties of trust described in literature [14,41-46]\n\nDynamic Trust can increase or decrease through gathering new experiences. Moreover,\ntrust is said to decay with time (time-based aging [45]). Because of these char-\nacteristics, trust values strongly depend on the time they are determined. The\ngreater importance of new experiences compared to old experiences has been\nwidely studied and considered in many trust models such as [32,47] or [30].\n\nContext-dependent Trust is bound to a specific context. For example, Alice trusts Bob as her doctor.\nHowever, she might not trust him as a cook to prepare a delicious meal for her.\n\nMulti-faceted Even in the same context, a trust value may not reflect all aspects of this context\n[43]. For example, a customer may trust a particular restaurant for its quality of\nfood but not for its quality of service. The overall trust on this restaurant depends\non the combination of the amount of trust in the specific aspects.\n\nPropagative One property of trust made use of in several models is its propagativity. If Alice\ntrusts Bob, who in turn trusts Claire, Alice can derive trust on Claire from the rela-\ntionships between her and Bob as well as between Bob and Claire. Because of\nthis propagative nature, it is possible to create trust chains passing trust from\none agent to another agent. As clarified by Christianson and Harbison [48], trust\nis not automatically transitive although trust transitivity was assumed proven for\na long time. If Alice trusts Bob, who in turn trusts Claire, it does not inherently\nmean that Alice trusts Claire. It follows from the foregoing that transitivity implies\npropagation. The reverse, though, is not the case.\n\nComposable When trust is propagated, a particular agent may be connected to multiple\ntrust chains. To come up with a final decision whether to trust or distrust this\nagent, the trust information received from the different chains need to be com-\nposed in order to build one aggregated picture. In this context, trust statements\npropagated from nodes close to oneself should have greater influence on the\naggregated value than the ones from distant nodes (distance-based aging [45]).\nComposition is potentially difficult if the trust statements are contradictory [14].\n\nSubjective The subjective nature of trust becomes clear if one thinks about a review on Ama-\nzon [26]. A book review that totally reflects Alice’s opinion will probably resolve\nin a high level of trust against the reviewer Rachel. Bob, however, who disagrees\nwith the review, will have a lower trust in Rachel although it bases on the same\nevidence.\n\nFine-grained Although trust is sometimes modeled in a binary manner (i.e. either trust or dis-\ntrust), it is possible that Alice trusts both Bob and Claire but that she trusts Bob\nmore than Claire. Hence, there may be multiple discrete levels of trust such as\nhigh, medium and low [41]. Mapped to numbers, trust may also be a continuous\nvariable taking values within a certain interval (e.g. between 0 and 1).\n\nEvent-sensitive It can take a long time to build trust. One negative experience, though, can\ndestroy it [23].\n\nReflexive Trust in oneself is always at the maximum value.\n\nSelf-reinforcing It is human nature to preferentially interact with other agents that are trusted.\nAnalogously, agents will avoid interacting with untrustworthy agents. Thus, the\ntrustworthiness of other agents is inherently taken into consideration.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 4 of 21\n\nestablishment [7]. Policy-based trust is often referred to as a hard security mechanism due\nto the exchange of hard evidence (e.g. credentials). Reputation-based trust, in contrast, is\nderived from the history of interactions. Hence, it can be seen as an estimation of trust-\nworthiness (soft security). In this work, we focus on reputation-based trust. Reputation\nis defined as follows: “Reputation is what is generally said or believed about a person’s or\nthing’s character or standing.” [8].\nIt is based on referrals, ratings or reviews from members of a community. Therefore,\n\nit can be considered as a collective measure of trustworthiness [8]. Trustworthiness as a\nglobal value is objective. However, the trust an agent puts in someone or something as a\ncombination of personal experience and referrals is subjective.\n\nResearch gap: design of reputation systems with reuse\n\nIt has been argued (e.g. by [3]) that most reputation-based trust models proposed in the\nacademic community are built from scratch and do not rely on existing approaches. Only\na few authors continue their research on the ideas of others. Thus, many approved models\nand promising thoughts go unregarded. The benefits of reuse, though, have been rec-\nognized in software engineering for years. However, there are only very few works that\nproposed single components to enhance existing approaches. Rehak et al. [9], for instance,\nintroduced a generic mechanism that can be combined with existing trust models to\nextend their capabilities by efficiently modeling context. The benefits of such a compo-\nnent that can easily be combined with existing systems are obvious. Nonetheless, research\nin trust and reputation still lacks in sound and accepted principles to foster reuse.\nTo gradually close this gap, we aim to provide a framework for the design of new\n\nreputation systems with reuse. As described above, we thereto propose a hierarchical\ncomponent taxonomy of computation engines used in reputation systems. Based on this\ntaxonomy, we set up a repository containing design knowledge on both a conceptual\nand an implementation level. On the one hand, the uniform and well-structured artifacts\ncollected in this repository can be used by developers to select, understand and apply\nexisting concepts. On the other hand, they may encourage researchers to provide novel\ncomponents on a conceptual and an implementation level. In this way, the reuse of ideas,\nconcepts and implemented components as well as the communication of reuse knowledge\nshould be achieved. Furthermore, we argue that the reusable components we identify in\nthis work could extend current reputation models by the ability to gradually include the\nproperties of trust described above. To evaluate whether our taxonomy/framework can\ncover all aspects of trust, we finally provide a table matching our component classes with\ntrust properties.\n\nA hierarchical component taxonomy for computation methods in reputation\nsystems\nTo derive a taxonomy from existing models, our research includes two steps: (1) the\nanalysis of the generic process of reputation systems and (2) the identification of logical\ncomponents of the computation methods used in common trust and reputation models.\nA critical question is how to determine and classify single components. Thereto, we follow\nan approach to function-based component classification, which means that the taxonomy\nis derived from the functions the identified components fulfill.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 5 of 21\n\nThe generic process of reputation systems\n\nThe generic process of reputation systems, as depicted in Figure 1, can be divided into\nthree steps: (1) collection & preparation, (2) computation and (3) storage & communica-\ntion. These steps are adapted from the three fundamental phases of reputation systems\nidentified by [10] and [11]: feedback generation/collection, feedback aggregation and\nfeedback distribution. Feedback aggregation as the central part of every trust and repu-\ntation system is furthermore divided into the three process steps filtering, weighting and\naggregation taken together as computation. The context setting consists of a trustor who\nwants to build a trust relation toward a trustee by providing context and personalization\nparameters and receiving a trustee’s reputation value.\n\nCollection and preparation\n\nIn the collection and preparation phase, the reputation system gleans information about\nthe past behavior of a trustee and prepares it for subsequent computing. Although per-\nsonal experience is the most reliable, it is often not sufficiently available or nonexistent.\nTherefore, data from other sources needs to be collected. These can be various, ranging\nfrom public or personal collections of data centrally stored to data requested from dif-\nferent peers in a distributed network. After all available data is gathered, it is prepared\nfor further use. Preparation techniques include normalization, for instance, which brings\nthe input data from different sources into a uniform format. Once the preparation is\ncompleted, the reputation data serves as input for the computation phase.\n\nComputation\n\nThe computation phase is the central part of every reputation system and takes the rep-\nutation information collected as input and generates a trust/reputation value as output.\nThis phase can be divided into the three generic process steps filtering, weighting and\naggregation. Depending on the computation engine, not all steps have to be implemented.\nThe first two steps (filtering and weighting) preprocess the data for the subsequent aggre-\ngation. The need for these steps is obvious: The first question to be answered is which\ninformation is useful for further processing (filtering). The second process step concerns\nthe question of how relevant the information is for the specific situation (weighting). In\nline with this, Zhang et al. [12] pointed out that current trust models can be classified into\nthe two broad categories filtering-based and discounting-based. The difference between\nfiltering and weighting is that the filtering process reduces the information amount while\n\nFigure 1 Generic process of a reputation system, inspired by [10].\n\n\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 6 of 21\n\nit is enriched by weight factors in the second case. Therefore, filtering can be seen as\nhard selection while weighting is more like a soft selection. Finally, the reputation values\nare aggregated to calculate one or several reputation scores. Depending on the algo-\nrithm, the whole computation process or single process steps can be run through for\nmultiple times.\n\nStorage and communication\n\nAfter reputation scores are calculated, they are either stored locally, in a public storage\nor both depending on the structure (centralized/decentralized/hybrid) of the reputation\nsystem. Common reputation systems not only provide the reputation scores but also offer\nextra information to help the end-users understand the meaning of a score. They should\nfurthermore reveal the computation process to accomplish transparency.\nIn this work, we focus on the computation phase, since the first phase (collection &\n\npreparation) and the last phase (storage & communication) strongly depend on the struc-\nture of the reputation system (centralized or decentralized). The computation phase,\nhowever, is independent of the structure and can look alike for systems implemented in\nboth centralized and decentralized environments. Therefore, it works well for design with\nreuse.\n\nHierarchical component taxonomy\n\nIn this section, the computation process is examined in detail. We introduce a novel\nhierarchical component taxonomy that is based on the functional blocks of common rep-\nutation systems identified in this work. Thereto, we clarify the objectives of the identified\nclasses (functions) and name common examples. Our analysis and selection of reputa-\ntion systems is based on different surveys [2,3,8,13,14]. Figure 2 gives an overview of the\nprimary and secondary classes identified.\n\nFigure 2 Classes of filtering-, weighting- and aggregation-techniques.\n\n\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 7 of 21\n\nBeginning with the filtering phase, the three broad classes attribute-based, statistic-\nbased and clustering-based filtering can be identified:\n\n1. Attribute-based filtering: In several trust models, input data is filtered based on a\nconstraint-factor defined for the value of single attributes. Attribute-based filters\nmostly implement a very simple logic, in which an attribute is usually compared to\na reference value. Due to their lightweight, they are proper for reducing huge\namounts of input data to the part necessary for the reputation calculation. Besides\nthe initial filtering of input data, it is often applied after the weighting phase in\norder to filter referrals that have been strongly discounted. Time is an example of\nan attribute that is often constrained because it is desirable to disregard very old\nratings. eBay’s reputation system, for instance, only considers transactions having\noccurred in the last 12 months for their overview of positive, neutral and negative\nratings. Other models such as Sporas [15] ignore every referral but the latest, if one\nparty rated another party more than once. In this way, simple ballot stuffing attacks\ncan be prevented. In ballot stuffing attacks, parties improve their reputation by\nmeans of positive ratings after fake transactions.\n\n2. Statistic-based filtering: Further techniques that are used to enhance the\nrobustness of trust models against the spread of false rumors apply statistical\npatterns. Whitby et al. [16], for example, proposed a statistical filter technique to\nfilter out unfair ratings in Bayesian reputation systems applying the majority rule.\nThe majority rule considers feedback that is far away from the majority’s referrals as\ndishonest. In this way, dishonest or false feedback can easily be detected and filtered.\n\n3. Clustering-based filtering: Clustering-based filter use cluster analysis approaches\nto identify unfair ratings. These approaches are comparatively expensive and\ntherefore rarely used as filtering techniques. An exemplary procedure is to analyze\nan advisor’s history. Since a rater never lies to himself, an obvious way to detect\nfalse ratings is to compare own experience with the advisor’s referrals. Thus, both\nfair and unfair ratings can be identified. iCLUB [17], for example, calculates\nclusters of advisors whose evaluations against other parties are alike. Then, the\ncluster being most similar to the own opinion is chosen as fair ratings. If there is no\ncommon experience (e.g. bootstrapping), the majority rule will be applied. Another\nexample for an approach using cluster filtering was proposed by Dellarocas [18].\n\nOnce all available information is reduced to those suitable for measuring trust and\nreputation in the current situation, it becomes clear that various data differ in their\ncharacteristics (e.g. context, reliability). Hence, the referrals are weighted in the second\nprocess step based on different factors. In contrast to the filtering step, applied techniques\ndiffer strongly. For that reason, our classification of weighting techniques is based on the\nproperties of referrals that are analyzed for the discounting. We distinguish between the\nfollowing classes:\n\n1. Context comparability: Reputation data is always bound to the specific context in\nwhich it is created. Ratings that are generated in one application area might not be\nautomatically applicable in another application area. In e-commerce, for instance,\ntransactions are accomplished involving different prices, product types, payment\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 8 of 21\n\nmethods, quality or time. The non-consideration of this context leads to the value\nimbalance problem where a malicious seller can build a high reputation by selling\ncheap products while cheating on expensive ones. To increase comparability and\navoid such situations, context has become a crucial attribute for many current\napproaches like [19] or [9].\n\n2. Criteria comparability: Besides the context in which feedback is created, the\ncriteria that underlie the evaluation are important. Particularly, if referrals from\ndifferent application areas or communities are integrated, criteria comparability\ncan be crucial. In file-sharing networks, for instance, a positive rating is often\ngranted with a successful transaction independent of the quality of service. On\ne-commerce platforms, in contrast, quality may be a critical factor for customer\nsatisfaction. Other distinctions could be the costs of reviews, the level of\nanonymity or the number of peers in different communities or application\nareas. Weighting based on criteria comparability can compensate these\ndifferences.\n\n3. Credibility/propagation: In network structures such as in the web-of-trust, trust\ncan be established along a recommendation or trust chain. Obviously, referrals that\nhave first-hand information about the trustworthiness of an agent are more\ncredible than referrals received at second-hand (with propagation degree of two) or\nhigher. Therefore, several models apply a propagation (transitivity) rate to discount\nreferrals based on their distance. The biometric identity trust model [20], for\ninstance, derives the reputation-factor from the distance of nodes in a web-of-trust.\n\n4. Reliability: Reliability or honesty of referrals can strongly affect the weight of\nreviews. The concept of feedback reputation that measures the agents’ reliability in\nterms of providing honest feedback is often applied. As a consequence, referrals\ncreated by agents having a low feedback reputation have a low impact on the\naggregated reputation. The bases for this calculation can be various. Google’s\nPageRank [21], for instance, involves the position of every website connected to the\ntrustee in the web graph in their recursive algorithm. Epinions [22], on the other\nhand, allows users to directly rate reviews and reviewers. In this way, the effects of\nunfair ratings are diminished.\n\n5. Rating value: Trust is event sensitive. For stronger punishment of bad behavior,\nthe weight of positive ratings compared to negative ratings can be calculated\nasymmetrically. An example for a model using an “adaptive forgetting scheme” was\nproposed by Sun et al. [23], in which good reputation can be built slowly through\ngood behavior but easily be ruined through bad behavior.\n\n6. Time: Due to the dynamic nature of trust, it has been widely recognized that time\nis one important factor for the weighting of referrals. Old feedback might not be as\nrelevant for reputation scoring as new referrals. An example measure for\ntime-based weighting is the “forgetting factor” proposed by Jøsang [24].\n\n7. Personal preferences: Reputation systems are used by various end-users (e.g.\nhuman decision makers, services). Therefore, a reputation system must allow the\nadaptation of its techniques to subjective personal preferences. Different actors\nmight have different perceptions regarding the importance of direct experience\nand referrals, the significance of distinct information sources or the rating of\nnewcomers.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 9 of 21\n\nThe tuple of reputation data and weight-factor(s) serve as input for the third step of\nthe computation process - the aggregation. In this phase, one or several trust/reputa-\ntion values are calculated by composing the available information. In some cases, the\nweighting and the aggregation process are run through repetitively in an iterative manner.\nHowever, the single steps can still be logically separated. The list of proposed algorithms\nto aggregate trust and reputation values has become very long during the last decade.\nHere, we summarize the most common aggregation techniques and classify them into the\nfour blocks simple arithmetic, statistic, fuzzy and graph-based models:\n\n1. Simple arithmetic: The first class includes simple aggregation techniques like\nranking, summation or average. Ranking is a very basic way to measure\ntrustworthiness. In ranking algorithms, ratings are counted and organized in a\ndescending order based on that value. This measure has no exact reputation score.\nInstead, it is frequently used as a proxy for the relative importance/trustworthiness.\nExamples for systems using ranking algorithms are message boards like Slashdot\n[25] or citation counts used to calculate the impact factor in academic literature.\nOther aggregation techniques that are well known due to the implementation on\neBay or Amazon [26] are the summation (adding up positive and negative ratings)\nor the average of ratings. Summation, though, can easily be misleading, since a\nvalue of 90 does not reveal the composition of positive and negative ratings (e.g.\n+100,-10 or +90,0). The average, on the other hand, is a very intuitive and easily\nunderstandable algorithm.\n\n2. Statistic: Many of the prominent trust models proposed in the last years use a\nstatistical approach to provide a solid mathematical basis for trust management.\nApplied techniques range from Bayesian probability over belief models to Hidden\nMarkov Models. All models based on the beta probability density function (beta\nPDF) are examples for models simply using Bayesian probability. The beta PDF\nrepresents the probability distributions of binary events. The a priori reputation\nscore is thereby gradually updated by new ratings. The result is a reputation score\nthat is described in a beta PDF function parameter tuple (α, β), whereby α\nrepresents positive and β represents negative ratings. A well known model using\nthe beta PDF is the Beta Reputation system [24]. A weakness of Bayesian\nprobabilistic models, however, is that they cannot handle uncertainty. Therefore,\nbelief models extend the probabilistic approach by Dempster-Shafer theory (DST)\nor subjective logic to include the notion of uncertainty. Trust and reputation\nmodels involving a belief model were proposed by Jøsang [27] or Yu and Singh [28].\nMore complex solutions that are based on machine learning, use the Hidden\nMarkov Model, a generalization of the beta model, to better cope with the dynamic\nbehavior. An example was introduced by Malik et al. [29].\n\n3. Fuzzy: Aggregation techniques classified as fuzzy models use fuzzy logic to\ncalculate a reputation value. In contrast to classical logic, fuzzy logic allows to\nmodel truth or falsity within an interval of [0,1]. Thus, it can describe the degree to\nwhich an agent/resource is trustworthy or not trustworthy. Fuzzy logic has been\nproven to deal well with uncertainty and mimic the human decision making\nprocess [30]. Thereby, a linguistic approach is often applied. REGRET [31] is one\nprominent example of a trust model making use of fuzzy logic.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 10 of 21\n\n4. Graph-based: A variety of trust models employ a graph-based approach. They rely\non different measures describing the position of nodes in a network involving the\nflow of transitive trust along trust chains in network structures. As online social\nnetworks have become popular as a medium for disseminating information and\nconnecting people, many models regarding trust in social networks have lately\nbeen proposed. Graph-based approaches use measures from the field of graph\ntheory such as centrality (e.g. Eigenvector, betweenness), distance or node-degree.\nReputation values, for instance, grow with the number of incoming edges (in-\ndegree) and increase or decrease with the number of outgoing edges (out-degree).\nThe impact of one edge on the overall reputation can depend on several factors like\nthe reputation of the node an edge comes from or the distance of two nodes.\nPopular algorithms using graph-based flow model are Google’s PageRank [21] as\nwell as the Eigentrust Algorithm [32]. Other examples are the web-of-trust or trust\nmodels particularly designed for social networks as described in [14]. As mentioned\nabove, the weighting and aggregation phases are incrementally run through for\nseveral times due to the incremental nature of these algorithms.\n\nThe classification of the computation engine’s components used in different trust mod-\nels in this taxonomy is not limited to one component of each primary class. Depending\non the computation process, several filtering, weighting and aggregation techniques can\nbe combined and run through more than once. Malik et al. [29], for instance, introduced\na hybrid model combining heuristic and statistical approaches. However, our taxonomy\ncan reveal the single logical components a computation engine is built on. Moreover,\nit serves as an overview of existing approaches. Since every currently known reputa-\ntion system can find its position, to the best of our knowledge, this taxonomy can be\nseen as complete. Though, an extension by new classes driven by novel models and\nideas is possible. Our hierarchical component taxonomy currently contains 3 primary\ncomponent classes, 14 secondary component classes, 23 component terms and 29 sub-\nsets. Table 2 shows an excerpt of the hierarchical component taxonomy with building\nblocks of the primary class “weighting”. The full taxonomy is provided in Additional file 1:\nTable S1.\n\nThe component taxonomy as a framework for design with reuse\nThe hierarchical component taxonomy introduced in the former section serves as a nat-\nural framework for the design of reputation systems with reuse. To support this process,\nwe set up a component repository combining a knowledge and a service repository.\nThus, it does not only contain information about software components on implementa-\ntion level but also provides extensive descriptions of the ideas applied on a conceptual\nlevel. This comprehensive set of fundamental component concepts and ideas combined\nwith the related implementation allows the reuse of both ideas and already implemented\ncomponents.\nIn this section, we firstly describe the conceptual design of our component reposi-\n\ntory in detail. Then, we elaborate on the implementation of a web application employing\nour thorough repository to provide design knowledge for reuse on a conceptual and an\nimplementation level.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 11 of 21\n\nTable 2 Excerpt of the hierarchical component taxonomy with descriptions\n\nPrimary\ncomponent\nclass\n\nSecondary\ncomponent\nclass\n\nComponent term Subset Description\n\ncredibility/\npropagation\n\npropagation discount Discount referrals along\ntrust chains\n\nsubjective reliability property similarity Discount based on\nsimilarity of personal\nproperties\n\nrating similarity Discount referrals based\non similarity of ratings\ntoward other agents\n\nweighting reliability Explicit Discount based on explicit\nreputation information like\nreferrals or certificates\n\nobjective reliability Implicit Discount based on implicit\nreputation information like\nprofile age, number of\nreferrals or position\n\nrating value asymmetric rating Strongly discount positive\nratings compared to\nnegative ratings (event\nsensitive)\n\n. . . . . . . . . . . . . . .\n\nConceptual design of the component repository\n\nReuse-based software engineering can be implemented on different levels of abstraction,\nranging from the reuse of ideas to the reuse of already implemented software components\nfor a very specific application area. In this work, we want to apply our taxonomy for reuse\non two levels – a conceptual level and an implementation level. Therefore, the developed\nrepository provides design knowledge for reuse on two logical layers (see Figure 3).\n\nFigure 3 Logical layers of the component repository for design with reuse.\n\n\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 12 of 21\n\nReuse on conceptual level\n\nWhen reusing an implemented component, one is unavoidably constrained by design\ndecisions that have been made by the developer. A way to prevent this is to conceive more\nabstract designs that do not specify the implementation. Thus, we provide an abstract\nsolution to a problem by means of design pattern-like concepts. Design patterns are\ndescriptions of commonly occurring problems and a generic solution to the problems that\ncan be used in different settings [33]. Our design pattern-like concepts consist of essential\nelements that are exemplary depicted in Table 3.\n\nReuse on implementation level\n\nOn implementation level, we provide fully implemented reusable components by means\nof web services in a service-orientated architecture. These services encapsulate the con-\ncepts’ logic and functionality in independent and interchangeable modules to achieve the\nseparation of concerns. The web services are incorporated via well-defined interfaces. All\nservices provided are registered as artifacts in the service repository. An artifact contains\nessential information about one live reachable service such as ID, type (REST or ws), URL,\ndescription, parameters, example calls, example output, the design pattern that is imple-\nmented by the service, and tags describing the functionality. Table 4 shows an example\nartifact for the design pattern described above.\n\nTable 3 Design pattern on the conceptual level (example)\n\nComponent term Context similarity\n\nSubset Absolute congruence\n\nDescription This component uses an absolute congruence metric as similarity measure to\nidentify context similarity.\n\nProblem description Reputation data is always bound to the specific context in which it was created.\nRatings that were generated in one application area might not be automatically\napplicable in another application area which can result in the value imbalance\nproblem.\n\nSolution description Apply similarity measurement between context ci (reference context) and con-\ntext cj of referrals in the referral set to deliver a weight-factor for each item of the\nreferral set using the following formula:\n\nw(c1, c2) :=\nk(ci) ∩ k(cj)\nk(ci) ∪ k(cj)\n\nk(ci) denotes the total number of keywords describing context ci.\n\nApplicability Set of nominal context attributes.\n\nCode example (php)\n\nfunction calculate_values($reference, $context_sets) {\n$reference_context = $reference[’context_attributes’];\n$return_values = array();\nwhile(!empty($context_sets)) {\n\n...shortened...\n}\nreturn $return_values;\n\n}\n\nImplementation Context similarity-based weighting service (absolute congruence)\n\nLiterature\n• Mohammad Gias Uddin, Mohammad Zulkernine, and Sheikh Iqbal\n\nAhamed. 2008. CAT: a context-aware trust model for open and dynamic\nsystems. In Proceedings of the 2008 ACM symposium on Applied\ncomputing (SAC ’08). ACM, New York, NY, USA, 2024–2029.\n\nTags weighting, context, similarity, congruence\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 13 of 21\n\nTable 4 Web service description on implementation level (example)\n\nComponent term Context similarity\n\nSubset Absolute congruence\n\nType REST\n\nDemo http://trust.bayforsec.de/ngot/webservice/Client/?=Weighting-congruence-absolute-\ncall.php\n\nDescription This service provides an absolute similarity measurement between a reference context\nand a context-set of referrals.\nExample: The sets ’registered’,’charged’, ’verified’ and ’registered’, ’costless’, ’unverified’\nhave a similarity of 1/3.\n\nParameters\n\n// define words that describe the quality of a referall\n$reference_context : array(\"words\" => array (TEXT));\n\n$referral_sets = array( $context_set );\n\n$referral_set = array( \"id\" => NUMBER, \"words\" => array (TEXT));\n\nExample call\n\nrequire_once(’WebserviceCallHelper.php’);\n\n$arguments = array(\"words\" => array (\"registered\",\"charged\",\"verified\"));\n$referral_set = array();\n$referral_set[0] = array( \"id\" => \"10000\", \"words\" =>\n\narray (\"registered\",\"costless\", \"unverified\"));\n$referral_set[1] = array( \"id\" => \"10001\", \"words\" =>\n\narray (\"registered\",\"charged\", \"verified\"));\n$referral_set[2] = array( \"id\" => \"10002\", \"words\" =>\n\narray (\"registered\",\"costless\", \"verified\"));\n\n$webservice_call = new WebserviceCallHelper(array(\n’base_url’ => WEBSERVICE_URL,\n’format’ => \"html\",\n’component’ => \"Weighting\\CongruenceAbsolute\"\n\n));\n$webservice_call->get_result($arguments, $referral_set);\n\nExample output\n\nArray\n(\n\n[status] => 200\n[data] => Array\n\n(\n[0] => Array\n\n(\n[0] => 10000\n[1] => 0.2\n\n)\n[1] => Array\n\n(\n[0] => 10001\n[1] => 1\n\n)\n[2] => Array\n\n(\n[0] => 10002\n[1] => 0.5\n\n)\n)\n\n)\n\nPattern Implemented Context similarity (Absolute congruence)\n\nTags weighting, context, similarity, congruence\n\nImplementation of the repository\n\nTo demonstrate the feasibility of our approach, we have prototypically implemented the\nrepository as a web-based application in a three-tier client-server-architecture [34]. To\ngive an overview of the chosen architecture, we distinguish between server-side and\nclient-side implementation.\n\nServer-side\n\nOn server-side, the logic is implemented in PHP on an Apache server (logic layer) con-\nnecting to a MySQL database (persistent layer). The MySQL database contains all data\nregarding the design patterns as described in Table 2. Each of these design patterns is also\nimplemented in a web service. To enable a standardized realization of new web services\nand a flawless call via standardized interfaces, we employ an abstract class Component. All\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 14 of 21\n\ncomponents (implemented as web services) must inherit from this class, which particu-\nlarly requires overwriting the function calculate_values. To make the generic component\nindependent of the input data, developers are advised to make use of the PHP function\nfunc_get_args(). In this way, distinct components can receive a variety of arguments. To\nconsistently handle client calls, our architecture is extended by a WebserviceCallHandler.\nFigure 4 depicts the schematic layout.\nAll web services implementing the trust pattern currently described in our knowledge\n\nrepository have been created and registered as artifacts in our service repository [34].\nFurthermore, these artifacts are described in detail including a definition of input, output\nand example calls as defined in Table 4.\n\nClient-side\n\nOn client-side (presentation layer), we employ the current web standards HTML5,\nJavaScript and CSS (Bootstrap). The front end is divided into three main pages –\n“overview”, “knowledge repository” and “service repository” – which provide information\non the general concept, the trust patterns and the web services. To enable a standard-\nized call of a web service from client-side, a WebserviceCallHelper allows a simple call of\neach component by configuration and provides all functions necessary to establish a con-\nnection to the repository. The configuration details are passed to the constructor, which\nrequires a base_url, an output format (HTML, XML or JSON) and a unique component\nname as illustrated below.\n\nExample call of a filtering component via the WebserviceCallHelper\n\n$webservice_call = new WebserviceCallHelper(array(\n’base_url’ => \"http://trust.bayforsec.de/ngot/webservice/\",\n’format’ => \"html\",\n’component’ => \"Filtering\\AgeBasedAbsolute\"\n\n));\n\n$webservice_call->get_result($arguments, $referral_set);\n\nFigure 4 Schematic view on the service architecture.\n\n\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 15 of 21\n\nEvaluation\nTo rigorously demonstrate the proper functioning and quality of our approach, we carry\nout a two-part evaluation of our artifact in this section. As there is currently no compara-\nble framework, we firstly perform a descriptive scenario-based evaluation. According to\nHevner et al. [4], this is a standard approach for innovative artifacts like ours. To demon-\nstrate the completeness of our taxonomy, we secondly conduct a static analysis [4], in\nwhich we match all components to the trust properties described in Section ‘The notion\nof trust and its properties’.\n\nScenario analysis: Reputation system development\n\nThe fictitious web developer John Gray runs an electronic marketplace platform for\nphilatelists and numismatics. The platform has been launched with his friends as the first\nusers but has been growing fast. Meanwhile, most users do not know each other in person\nanymore. As a result, many of the initial users have stopped interacting with the newcom-\ners as they do not trust them. After realizing this problem, John decides to introduce an\nonline reputation system in order to establish trust among the strangers. In the following,\nwe describe how our knowledge and service repository can help him to build a reputation\nsystem that perfectly meets his requirements.\nHaving read the basics on our component model, John concludes that he wants to build\n\na computation engine that makes use of components of all three phases – filtering, weight-\ning and aggregation. Thinking about the experiences made with sellers on the platform,\nhe recognizes that most of them do not deliver the same quality all the time. Thus, an\nage-based filter should be employed to make old referrals less important than new ones.\nFurthermore, there are sellers that usually deliver high quality stamps while offering poor\nquality coins. Therefore, a weighting component based on context similarity (absolute\ncongruence) should be selected. Regarding the aggregation alternatives, John decides to\nmake use of the average component as the simple average is probably the most intuitive\nand most transparent aggregation technique for the users. Finally, the single components\nare combined in sequence to a fully functional computation engine as depicted in Figure 5.\nThe code listed below shows an example for the implementation of John’s computation\n\nengine. Here, the WebserviceCallHelpers for each of the selected components have to be\ninstantiated first as described in Section ‘Client-side’. Secondly, the referral set needs to\nbe loaded and prepared according to input parameter descriptions provided for each web\nservice in the component repository. In its current form, the framework does not pro-\nvide any classes to automatically plug single components together (glue class). Thus, the\ndeveloper has to ensure that the output of one component is correctly provided as an\ninput for the following component. This lose coupling, however, allows for more flexibil-\nity. All details on the input and output format can be found in the artifact description of\neach component. While the input data varies from component to component, the output\nis alike for components of each primary class.\n\nFigure 5 Sequence of components for John’s computation engine.\n\n\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 16 of 21\n\nExample code for the computation engine described above\n\n//create helper for filtering component\n$webservice_filter = new WebserviceCallHelper(array(\n\n’base_url’ => WEBSERVICE_URL,\n’format’ => \"html\",\n’component’ => \"Filtering\\AgeBasedAbsolute\"\n\n));\n\n//create helper for weighting component\n$webservice_weight = ...as above...\n\n//create helper for aggregation component\n$webservice_aggregate = ...as above...\n\n//define referral_set\n$referral_set = array(\"id\" => NUMBER, \"time\" => DATE, \"context\" => array (TEXT), \"\n\nrating\" => NUMBER);\n\n//call computation\n$reputation_value = $webservice_aggregate(\n\n$glue->prepare_for_aggregation($webservice_weight(\n$glue->prepare_for_weighting($webservice_filter->get_result(\n\n$referral_set))\n))\n\n);\n\nThis scenario elucidates that our knowledge and service repository has an obvious util-\nity from a practical point of view since developers can easily access it and gain knowledge\nabout online reputation systems. Thereby, we may help to better spread innovative ideas\nand allow developers to experiment with different computation techniques. However,\nour approach requires specific knowledge on the structure of the repository, the func-\ntioning of each component and details on how to plug components together. Developers\nneed to manually combine components and take care, whether they use valid input data\nand a feasible combination of reputation system components. In its current form, our\nframework is not very “developer friendly”. Therefore, further research will be necessary\nin order to improve the practical usability of our component repository. In Section ‘Con-\ntribution and future work’, we discuss open issues in more detail.\n\nStatic analysis: Matching components and trust properties\n\nTo guarantee the proper generation of computational trust, trust-enforcing mechanisms\nsuch as reputation systems should be able to consider and address all properties of trust.\nAs our component taxonomy serves as a framework for the design of new reputation sys-\ntems with reuse, it should enable developers to extend current reputation models by the\nability to gradually include the various properties of trust. Therefore, a way to evaluate\nthe completeness of our solution is to review whether a trust system that is built accord-\ning to our framework could meet this standard. In Table 5, we match the computation\ncomponents identified above to the properties of trust introduced in Section ‘The notion\nof trust and its properties’. Since our taxonomy is based on reputation systems analyzed\nin various surveys, this approach also enables us to identify aspects of trust that have only\nrarely been considered in research so far.\nExamining Table 5, we find that all trust properties listed are widely covered. There is at\n\nleast one component addressing each single characteristic. Going into more detail, we find\nthat there are many proposals that have developed components to personalize reputation\nsystems, thus covering the subjective property of trust. This reflects a general trend to an\nenhanced personalization of reputation systems. Furthermore, it becomes clear that all\nof our weighting and aggregation components follow the fine-grained property of trust,\ni.e. that trust can be modeled as a continuous variable. For the filtering components, the\nfine-grained property is not entirely applicable in the same meaning as filtering has no\ndirect influence on the trust value of referrals. Since the effect of filtering is that a referral\n\n\n\nSän\ng\ner\n\neta\nl.Jo\n\nu\nrn\na\nlo\nfTru\n\nstM\na\nn\na\ng\nem\n\nen\nt\n\n (2\n0\n\n1\n5\n\n) 2\n:5\n\n \nP\nag\n\ne\n1\n7\no\nf2\n\n1\n\nTable 5 Matching reputation system components and trust properties\n\nTrust properties\n\nPrimary Secondary Dynamic Context- Multi- Propagative Composable Subjective Fine- Event- Reflexive Self-\n\ncomponent component dependent faceted grained sensitive reinforcing\n\nclass class\n\nComputation\ncomponents\n\nFiltering\n\nattribute-based � � � � �� � � � � �\nstatistic-based � � � � � � � � � ��\nclustering � � � � � � � � � ��\n\nWeighting\n\ncontext comparability �� � �� � � � � � � �\ncriteria comparability � �� � � � � � � � �\ncredibility � � � � � � � � �� �\nreliability � � � �� �� � � � �� �\nrating value � � � � � � � � � ��\ntime � � � � � � � � � �\npersonal preferences � � � � � � � � �� �\n\nAggregation\n\nsimple arithmetic � � � � � � � � � ��\nstatistic � � � � � � � � � ��\nfuzzy � � � � � � � � � ��\ngraph-based � � � � � �� � � � ��\n\n� not addressed.�� partly addressed.� completely addressed.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 18 of 21\n\neither is further considered or not, it shows a binary character rather than being fine-\ngrained. In contrast to the subjective and fine-grained properties of trust, other properties\nsuch as context-dependent, multi-faceted and event-sensitive are particularly addressed\nby only one or two components. Note that this does not automatically mean that there\nis an increased necessity for future research concerning these properties. It may also be\npossible that one component is enough to cover one trust property. More detailed studies\non this could be part of future work.\nOverall, we can say that computational trust can be represented quite accurately when\n\nusing our taxonomy and the provided components as a basis for the development of new\nreputation systems or the extension of existing models. Note, however, that this is only\none view on our taxonomy. Conducting a comparable analysis from the viewpoint of\nattacks and defense mechanisms, for instance, the outcomes may vary greatly.\n\nContribution and future work\nMany surveys of trust and reputation systems give an overview of existing trust and repu-\ntation systems by means of a classification of existing models and approaches. In contrast\nto this, we provide a collection of ideas and concepts classified by their functions. Further-\nmore, these ideas are not only named but also clearly described in well-structured design\npattern-like artifacts which can easily be adapted to a specific situation. Therewith, we\nreorganized the design knowledge for computation techniques in reputation systems and\ntranslated the most common ideas into a uniform format. To directly make use of novel\ncomponents, the web services created on implementation level can instantly be reused\nand integrated in existing reputation systems to extend their capabilities. This approach\n(i.e. publicly providing implemented computation components as web services) may help\nto better spread innovative ideas in trust and reputation systems and give system builders\na better choice allowing to experiment with different computation techniques. Moreover,\nwe encourage researchers to focus on the design of single components by providing a\nplatform on which concepts and their prototypical implementation can be made publicly\navailable.\nNonetheless, there are still some unexplored areas regarding the design with reuse in\n\ntrust and reputation systems. Firstly, reusability could play a role in process steps other\nthan the computation phase. To clarify the opportunities, further research is necessary in\nthis area. Secondly, our hierarchical taxonomy is currently limited to a functional view on\nthe identified components but developers may also benefit from additional views. Because\nof the importance of the robustness of trust and reputation systems [35], we are particu-\nlarly interested in an attack view. In [36], we present first ideas on this issue. We propose\na taxonomy of attacks on reputation systems and then refer to the single components of\nour repository as solutions to the specific attack classes. In this way, we not only support\nreputation system designers in the development of more reliable and more robust rep-\nutation systems with already existing components but also help to identify weaknesses\nthat have not been addressed so far. Thirdly, the selection and interpretation of adequate\ncomponents for new reputation systems in a particular application area requires time,\neffort and – to some extent – knowledge of this research area. To increase usability, a\nsoftware application is needed to support a user in this development process. Ultimately,\nthe application may even be able to automatically find the most qualified composition\nfor specific requirements and input data. This, in turn, demands for generic testbeds that\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 19 of 21\n\nenable objective evaluations of reputation systems because so far, researchers have mainly\nbeen developing their own testing scenarios favoring their own work [37]. The most well-\nknown proposals regarding independent testbeds are ART [38] and TREET [37]. Recently,\nIrissappane and Zhang [39,40] made another important step forward by introducing a\npublicly available testbed that is able to reflect real environmental settings. We plan to\nuse their tool in future studies. Finally, we need to observe the usage of our repository in\npractice to learn from how users deal with it. This can either be done through conducting\nexperimental user studies or by interviewing developers who use our repository in a real\nenvironment. In this way, we can run through a continuous improvement process.\n\nConclusion\nThe research in trust and reputation systems is still growing. In this paper, we presented\nconcepts to foster reuse of existing approaches. We provided a hierarchical taxonomy of\ncomputation components from a functional view and described the implementation of\na component repository that serves as both a knowledge base and a service repository.\nIn this way, we communicate design knowledge for reuse, support the development of\nnew reputation systems and encourage researchers to focus on the development of single\ncomponents that can be integrated in various reputation systems to easily extend their\ncapabilities by new features. Matching the identified components and the properties of\ntrust, we found that integrating existing ideas and concepts can lead to a reputation sys-\ntem that widely reflects computational trust by addressing all properties of trust described\nin literature.\n\nAdditional file\n\nAdditional file 1: Table S1.\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAuthors’ contributions\nJS proposed the initial idea of this paper. He developed the hierarchical component taxonomy and implemented the\ncomponent repository. CR and JS conducted the evaluation of the proposed ideas. CR and JS furthermore revised this\npaper according to reviewers’ comments. GP supervised the research, contributed to the paper writing and made\nsuggestions. All authors read and approved the final manuscript.\n\nAcknowledgment\nThe research leading to these results was supported by the Bavarian State Ministry of Education, Science and the Arts\" as\npart of the FORSEC research association.\n\nReceived: 9 December 2014 Accepted: 1 April 2015\n\nReferences\n1. Electronics, Cars, Fashion, Collectibles, Coupons and More | eBay. http://www.ebay.com\n2. Yao Y, Ruohomaa S, Xu F (2012) Addressing common vulnerabilities of reputation systems for electronic commerce.\n\nJ Theor Appl Electron Commerce Res 7(1):1–20\n3. Tavakolifard M, Almeroth KC (2012) A taxonomy to express open challenges in trust and reputation systems. J\n\nCommun 7(7):538–551\n4. Hevner AR, March ST, Park J, Ram S (2004) Design science in information systems research. MIS Quarterly 28(1):75–105\n5. McKnight DH, Chervany NL (1996) The Meanings of Trust. Technical report. University of Minnesota, Management\n\nInformation Systems Research Center\n6. Gambetta D (1988) Can we trust trust? In: Gambetta D (ed). Trust: making and breaking cooperative relations. Basil\n\nBlackwell, Oxford. pp 213–237\n7. Artz D, Gil Y (2007) A survey of trust in computer science and the semantic web. Web Semantics 5(2):58–71\n8. Jøsang A, Ismail R, Boyd C (2007) A survey of trust and reputation systems for online service provision. Decis Support\n\nSyst 43(2):618–644\n\n\n\nhttp://www.journaloftrustmanagement.com/content/supplementary/s40493-015-0015-3-s1.pdf\nhttp://www.journaloftrustmanagement.com/content/supplementary/s40493-015-0015-3-s1.pdf\nhttp://www.ebay.com\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 20 of 21\n\n9. Rehak M, Gregor M, Pechoucek M, Bradshaw J (2006) Representing context for multiagent trust modeling. In:\nSkowron A, Barthès JP, Jain LC, Sun R, Morizet-Mahoudeaux P, Liu J, Zhong N (eds). Proceedings of the 2006\nIEEE/WIC/ACM International Conference on Intelligent Agent Technology, Hong Kong, China. IEEE Computer\nSociety, Washington, DC. pp 737–746\n\n10. Swamynathan G, Almeroth KC, Zhao BY (2010) The design of a reliable reputation system. Electron Commerce Res\n10(3–4):239–270\n\n11. Resnick P, Kuwabara K, Zeckhauser R, Friedman E (2000) Reputation systems. Commun ACM 43(12):45–48\n12. Zhang L, Jiang S, Zhang J, Ng WK (2012) Robustness of trust models and combinations for handling unfair ratings. In:\n\nDimitrakos T, Moona R, Patel D, McKnight DH (eds). Trust Management VI: Proceedings of the 6th IFIP WG 11.11\ninternational conference (IFIPTM). Springer, Berlin, Heidelberg, Surat, India. pp 36–51\n\n13. Noorian Z, Ulieru M (2010) The state of the art in trust and reputation systems: a framework for comparison. J Theor\nAppl Electron Commerce Res 5(2):97–117\n\n14. Sherchan W, Nepal S, Paris C (2013) A survey of trust in social networks. ACM Comput Surv 45(4):1–33\n15. Zacharia G, Moukas A, Maes P (2000) Collaborative reputation mechanisms for electronic marketplaces. Decis\n\nSupport Syst 29(4):371–388\n16. Whitby A, Jøsang A, Indulska J (2004) Filtering out unfair ratings in Bayesian reputation systems. In: Falcone R, Barber\n\nS, Sabater J, Singh M (eds). Proceedings of the third international joint conference on autonomous agents and multi\nagent systems, New, York, USA. IEEE Computer Society, Washington, DC. pp 106–117\n\n17. Liu S, Zhang J, Miao C, Theng Y-L, Kot AC (2011) iCLUB: an integrated clustering-based approach to improve the\nrobustness of reputation systems. In: Sonenberg L, Stone P, Tumer K, Yolum P (eds). Proceedings of the 10th\ninternational conference on Autonomous Agents and Multiagent Systems (AAMAS), Taipei, Taiwan. IFAAMAS,\nRichland, SC. pp 1151–1152\n\n18. Dellarocas C (2000) Immunizing online reputation reporting systems against unfair ratings and discriminatory\nbehavior. In: Jhingran A, MacKie J, Tygar D (eds). Proceedings of the 2nd ACM conference on electronic commerce,\nMinneapolis, MN. ACM, New York. pp 150–157\n\n19. Zhang H, Wang Y, Zhang X (2012) A trust vector approach to transaction context-aware trust evaluation in\ne-commerce and e-service environments. In: Shih C, Son S, Kuo T, Huemer C (eds). Proceedings of the 5th IEEE\ninternational conference on Service-Oriented Computing and Applications (SOCA). IEEE Computer Society\nWashington, DC, Taipei, Taiwan. pp 1–8\n\n20. Obergrusberger F, Baloglu B, Sänger J, Senk C (2013) Biometric identity trust: toward secure biometric enrollment in\nweb environments. In: Yousif M, Schubert L (eds). Proceedings of the 3rd international conference on Cloud\nComputing (CloudComp), Vienna, Austria. Springer, Berlin, Heidelberg. pp 124–133\n\n21. Brin S, Page L (1998) The anatomy of a large-scale hypertextual web search engine. Comput Networks\n30(1-7):107–177\n\n22. Epinions.com: Read expert reviews on Electronics, Cars, Books, Movies, Music and More. http://www.epinions.com/\n23. Sun Y, Han Z, Yu W, Ray Liu K (2006) Attacks on trust evaluation in distributed networks. In: Proceedings of Th 40th\n\nannual Conference on Information Sciences and Systems (CISS), Princeton, NJ, USA, IEEE Computer Society\nWashington, DC. pp 1461–1466\n\n24. Jøsang A, Ismail R (2002) The beta reputation system. In: Proceedings of the 15th bled conference on electronic\ncommerce, Bled, Slovenia. pp 41–55\n\n25. Slashdot: News for nerds, stuff that matters. http://www.slashdot.org/\n26. Amazon.com: Online Shopping for Electronics, Apparel, Computers, Books, DVDs & more. http://www.amazon.com\n27. Jøsang A (2001) A logic for uncertain probabilities. Int J Uncertainty Fuzziness Knowledge-Based Syst 9(3):279–311\n28. Yu B, Singh MP (2002) An evidential model of distributed reputation management. In: Proceedings of the first\n\nInternational Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), Bologna, Italy. ACM, New\nYork, NY. pp 294–301\n\n29. Malik Z, Akbar I, Bouguettaya A (2009) Web services reputation assessment using a Hidden Markov Model. In: Baresi\nL, Chi CH, Suzuki J (eds). Service-oriented computing: Proceedings of the 7th International Joint Conference on\nService-Oriented Computing (ICSOC-ServiceWave), Stockholm, Sweden. Springer Berlin, Heidelberg. pp 576–591\n\n30. Song S, Hwang K, Zhou R, Yu-Kwong K (2005) Trusted P2P transactions with fuzzy reputation aggregation, Vol. 9\n31. Sabater J, Sierra C (2002) Reputation and social network analysis in multi-agent systems. In: Proceedings of the first\n\nInternational joint conference on Autonomous Agents and Multiagent Systems (AAMAS), Bologna, Italy. ACM, New\nYork, NY. pp 475–482\n\n32. Kamvar SD, Schlosser MT, Garcia-Molina H (2003) The Eigentrust algorithm for reputation management in P2P\nnetworks. In: Hencsey G, White B, Chen YF, Kovács L, Lawrence S (eds). Proceedings of the 12th International\nConference on World Wide Web (WWW), Budapest, Hungary. ACM, New York, NY. pp 640–651\n\n33. Gamma E (1995) Design patterns: elements of reusable object-oriented software. Addison-Wesley, Reading\n34. Next Generation Online Trust. http://trust.bayforsec.de\n35. Jøsang A (2012) Robustness of trust and reputation systems: does it matter? In: Dimitrakos T, Moona R, Patel D,\n\nMcKnight DH (eds). Trust management VI: Proceedings of the 6th IFIP WG 11.11 International Conference (IFIPTM),\nSurat, India. Springer, Berlin, Heidelberg. pp 253–262\n\n36. Sänger J, Pernul G (2015) Reusable defense components for online reputation systems. In: Marsh S, Jensen CD,\nMurayma Y, Dimitrakos T (eds). Trust management IX: Proceedings of the 9th IFIP WG 11.11 International\nConference (IFIPTM), Hamburg, Germany. Springer, Berlin, Heidelberg\n\n37. Kerr R, Cohen R (2010) TREET: The Trust and Reputation Experimentation and Evaluation Testbed. Electron\nCommerce Res 10(3–4):271–290\n\n38. Fullam KK, Voss M, Klos TB, Muller G, Sabater J, Schlosser A, Topol Z, Barber KS, Rosenschein JS, Vercouter L (2005) A\nspecification of the Agent Reputation and Trust (ART) Testbed. In: Dignum F, Dignum V, Koenig S, Kraus S, Singh MP,\nWooldridge M (eds). Proceedings of the 4th international joint conference on Autonomous Agents and Multiagent\nSystems (AAMAS), Utrecht, Netherlands. ACM, New York, NY, USA. pp 512–518\n\nhttp://www.epinions.com/\nhttp://www.slashdot.org/\nhttp://www.amazon.com\nhttp://trust.bayforsec.de\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 21 of 21\n\n39. Irissappane AA, Jiang S, Zhang J (2012) Towards a comprehensive Testbed to evaluate the robustness of reputation\nsystems against unfair rating attacks. In: Herder E, Yacef K, Chen L, Weibelzahl S (eds). Workshop and Poster\nProceedings of the 20th conference on User Modeling, Adaptation, and Personalization (UMAP),",
      "metadata_storage_path": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9saWJyYXJ5L3M0MDQ5My0wMTUtMDAxNS0zLnBkZg2",
      "authors": [
        "Sänger",
        "Johannes Sänger",
        "Christian Richthammer",
        "Günther Pernul",
        "johannes.saenger",
        "Hevner",
        "Gambetta",
        "Alice",
        "Bob",
        "Claire",
        "Christianson",
        "Harbison",
        "Rachel",
        "Rehak",
        "Zhang",
        "Sporas",
        "Whitby",
        "Dellarocas",
        "Epinions",
        "Sun",
        "Jøsang",
        "Dempster",
        "Shafer",
        "Yu",
        "Singh",
        "Malik",
        "tory",
        "Mohammad Gias Uddin",
        "Mohammad Zulkernine",
        "Sheikh Iqbal",
        "Ahamed",
        "John Gray",
        "John"
      ],
      "institutions": [
        "University of Regensburg",
        "eBay",
        "Springer",
        "Google",
        "Slashdot",
        "Amazon",
        "ACM",
        "MySQL",
        "Bootstrap",
        "abov"
      ],
      "key_phrases": [
        "Creative Commons Attribution License",
        "mobile ad hoc networks",
        "Günther Pernul",
        "Universitätsstraße",
        "descriptive scenario-based analysis",
        "general prob- lem",
        "one novel idea",
        "Open Access article",
        "single building blocks",
        "RESEARCH Open Access",
        "online reputation systems",
        "new reputation systems",
        "hierarchical component taxonomy",
        "most reputation systems",
        "common reputation models",
        "Johannes Sänger",
        "hierarchical taxonomy",
        "common systems",
        "peer networks",
        "social networks",
        "component repository",
        "common models",
        "Christian Richthammer",
        "various disciplines",
        "application areas",
        "promising approaches",
        "implementation level",
        "obvious utility",
        "practical point",
        "last decade",
        "wide range",
        "considerable research",
        "data accuracy",
        "several environments",
        "statistical approaches",
        "personal preferences",
        "software engineering",
        "unrestricted use",
        "effective use",
        "computation engines",
        "computation phase",
        "graph-based models",
        "natural framework",
        "design process",
        "design knowledge",
        "Design approaches",
        "computation methods",
        "Reusable components",
        "Trust Management",
        "Trust pattern",
        "reputation-based trust",
        "context information",
        "original work",
        "Journal",
        "DOI",
        "Correspondence",
        "saenger",
        "wiwi",
        "regensburg",
        "University",
        "Germany",
        "Abstract",
        "problem",
        "scratch",
        "concepts",
        "achievements",
        "others",
        "shuffle",
        "reuse",
        "respect",
        "order",
        "conceptual",
        "view",
        "properties",
        "literature",
        "aspects",
        "Keywords",
        "Reusability",
        "Introduction",
        "metrics",
        "commerce",
        "eBay",
        "buyers",
        "relevance",
        "quality",
        "arithmetic",
        "multiple",
        "factors",
        "propagation",
        "solutions",
        "authors",
        "proposals",
        "development",
        "attention",
        "licensee",
        "Springer",
        "terms",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "mailto",
        "Page",
        "community",
        "efits",
        "specialists",
        "reliability",
        "goal",
        "classification",
        "functions",
        "many applied computation techniques",
        "design science research paradigm",
        "design pattern-like solution",
        "rational choice mechanism",
        "general problem context",
        "multifaceted terms trust",
        "computational trust models",
        "current models",
        "reputation models",
        "building block",
        "web services",
        "research gap",
        "objec- tives",
        "same time",
        "increasing distribution",
        "decision making",
        "network environments",
        "common understanding",
        "various fields",
        "multidimension- ality",
        "abstract concept",
        "behavioral dimension",
        "interpersonal phenomenon",
        "particular level",
        "subjective probability",
        "Sänger",
        "Multiple authors",
        "plex definitions",
        "reputation systems",
        "conceptual level",
        "following section",
        "important artifacts",
        "bility trust",
        "particular action",
        "future work",
        "several properties",
        "means",
        "rest",
        "paper",
        "guidelines",
        "Hevner",
        "overview",
        "motivation",
        "approach",
        "contribution",
        "name",
        "plans",
        "success",
        "Internet",
        "connectivity",
        "notion",
        "regard",
        "objectives",
        "topic",
        "decades",
        "uniform",
        "Reasons",
        "circumstance",
        "credibility",
        "confidence",
        "emotional",
        "nature",
        "sociologists",
        "psychologists",
        "Economists",
        "online",
        "Gambetta",
        "distrust",
        "agent",
        "group",
        "capacity",
        "security",
        "risk",
        "variety",
        "Table",
        "basis",
        "two common ways",
        "multiple discrete levels",
        "hard security mechanism",
        "One negative experience",
        "Propagative One property",
        "one aggregated picture",
        "many trust models",
        "multiple trust chains",
        "several trust models",
        "several models",
        "propagative nature",
        "aggregated value",
        "different chains",
        "recent years",
        "new experiences",
        "time-based aging",
        "greater importance",
        "old experiences",
        "delicious meal",
        "rela- tionships",
        "final decision",
        "greater influence",
        "distance-based aging",
        "subjective nature",
        "high level",
        "binary manner",
        "maximum value",
        "human nature",
        "hard evidence",
        "one agent",
        "other agents",
        "untrustworthy agents",
        "Dynamic Trust",
        "Context-dependent Trust",
        "trust value",
        "overall trust",
        "trust information",
        "trust statements",
        "lower trust",
        "Reflexive Trust",
        "specific context",
        "particular restaurant",
        "specific aspects",
        "long time",
        "particular agent",
        "distant nodes",
        "reviewer Rachel",
        "book review",
        "trust transitivity",
        "Policy-based trust",
        "same context",
        "values",
        "Table 1",
        "Overview",
        "acteristics",
        "example",
        "Alice",
        "Bob",
        "doctor",
        "cook",
        "customer",
        "food",
        "service",
        "combination",
        "amount",
        "use",
        "propagativity",
        "turn",
        "Claire",
        "Christianson",
        "Harbison",
        "transitive",
        "reverse",
        "case",
        "Composable",
        "Composition",
        "zon",
        "opinion",
        "numbers",
        "continuous",
        "variable",
        "interval",
        "reinforcing",
        "trustworthiness",
        "consideration",
        "establishment",
        "exchange",
        "credentials",
        "contrast",
        "history",
        "interactions",
        "estimation",
        "three process steps filtering",
        "most reputation-based trust models",
        "three fundamental phases",
        "repu- tation system",
        "function-based component classification",
        "current reputation models",
        "existing trust models",
        "three steps",
        "existing models",
        "component classes",
        "generic process",
        "two steps",
        "existing approaches",
        "existing systems",
        "soft security",
        "collective measure",
        "global value",
        "personal experience",
        "promising thoughts",
        "generic mechanism",
        "compo- nent",
        "accepted principles",
        "one hand",
        "other hand",
        "critical question",
        "communica- tion",
        "feedback generation/collection",
        "feedback distribution",
        "central part",
        "common trust",
        "feedback aggregation",
        "existing concepts",
        "academic community",
        "context setting",
        "single components",
        "reusable components",
        "trust properties",
        "Research gap",
        "worthiness",
        "work",
        "thing",
        "character",
        "referrals",
        "ratings",
        "reviews",
        "members",
        "someone",
        "ideas",
        "many",
        "benefits",
        "years",
        "Rehak",
        "instance",
        "capabilities",
        "sound",
        "new",
        "repository",
        "artifacts",
        "developers",
        "researchers",
        "novel",
        "way",
        "implemented",
        "communication",
        "ability",
        "table",
        "analysis",
        "identification",
        "logical",
        "Figure",
        "preparation",
        "storage",
        "weighting",
        "trustor",
        "personalisation trust relation reputation value",
        "filtering weighting aggregation communication input context",
        "novel hierarchical component taxonomy",
        "common rep- utation systems",
        "output trustor trustee transaction",
        "three generic process steps",
        "current trust models",
        "two broad categories",
        "subsequent aggre- gation",
        "situation Sänger",
        "reputation system computation collection",
        "Common reputation systems",
        "second process step",
        "single process steps",
        "several reputation scores",
        "first two steps",
        "trust/reputation value",
        "common examples",
        "filtering process",
        "computation process",
        "subsequent computing",
        "utation information",
        "specific situation",
        "second case",
        "reputation values",
        "first phase",
        "computation engine",
        "personalization parameters",
        "past behavior",
        "sonal experience",
        "other sources",
        "personal collections",
        "ferent peers",
        "distributed network",
        "different sources",
        "uniform format",
        "weight factors",
        "struc- ture",
        "decentralized environments",
        "functional blocks",
        "input data",
        "last phase",
        "reputation data",
        "first question",
        "hard selection",
        "soft selection",
        "information amount",
        "extra information",
        "available data",
        "Preparation techniques",
        "preparation phase",
        "public storage",
        "normalization",
        "need",
        "processing",
        "line",
        "Zhang",
        "difference",
        "algo",
        "rithm",
        "structure",
        "decentralized/hybrid",
        "meaning",
        "transparency",
        "design",
        "section",
        "detail",
        "classes",
        "propagation statistic statistic-based reliability rating value fuzzy time clustering-based graph-based personal preferences Sänger",
        "simple arithmetic attribute-based criteria compatibility credibility",
        "three broad classes attribute-based, statistic- based",
        "simple ballot stuffing attacks",
        "weighting aggregation context comparability",
        "Statistic-based filtering",
        "statistical filter technique",
        "one application area",
        "reference value",
        "Clustering-based filter",
        "Bayesian reputation systems",
        "cluster analysis approaches",
        "Attribute-based filtering",
        "Attribute-based filters",
        "simple logic",
        "secondary classes",
        "following classes",
        "weighting phase",
        "Figure 2 Classes",
        "statistical patterns",
        "filtering step",
        "weighting techniques",
        "Other models",
        "different surveys",
        "filtering phase",
        "single attributes",
        "huge amounts",
        "initial filtering",
        "last 12 months",
        "positive, neutral",
        "false rumors",
        "exemplary procedure",
        "cluster filtering",
        "available information",
        "current situation",
        "various data",
        "different factors",
        "reputation calculation",
        "Reputation data",
        "negative ratings",
        "positive ratings",
        "unfair ratings",
        "false ratings",
        "Further techniques",
        "majority rule",
        "filtering techniques",
        "fake transactions",
        "false feedback",
        "other parties",
        "common experience",
        "obvious way",
        "primary",
        "aggregation-techniques",
        "constraint-factor",
        "lightweight",
        "Sporas",
        "party",
        "robustness",
        "spread",
        "Whitby",
        "dishonest",
        "advisor",
        "rater",
        "iCLUB",
        "clusters",
        "evaluations",
        "bootstrapping",
        "Dellarocas",
        "characteristics",
        "reason",
        "discounting",
        "1.",
        "several trust/reputa- tion values",
        "biometric identity trust model",
        "human decision makers",
        "adaptive forgetting scheme",
        "distinct information sources",
        "payment Sänger",
        "one important factor",
        "different application areas",
        "low feedback reputation",
        "forgetting factor",
        "low impact",
        "critical factor",
        "first-hand information",
        "weight-factor(s",
        "different prices",
        "Different actors",
        "different perceptions",
        "high reputation",
        "aggregated reputation",
        "good reputation",
        "reputation scoring",
        "Reputation systems",
        "product types",
        "imbalance problem",
        "malicious seller",
        "cheap products",
        "crucial attribute",
        "many current",
        "file-sharing networks",
        "successful transaction",
        "customer satisfaction",
        "network structures",
        "transitivity) rate",
        "honest feedback",
        "recursive algorithm",
        "stronger punishment",
        "bad behavior",
        "good behavior",
        "dynamic nature",
        "Old feedback",
        "Jøsang",
        "Personal preferences",
        "various end-users",
        "direct experience",
        "third step",
        "different communities",
        "trust chain",
        "2. Criteria comparability",
        "Other distinctions",
        "propagation degree",
        "example measure",
        "web graph",
        "time-based weighting",
        "Rating value",
        "new referrals",
        "agents’ reliability",
        "transactions",
        "methods",
        "non",
        "context",
        "expensive",
        "situations",
        "approaches",
        "evaluation",
        "costs",
        "level",
        "number",
        "peers",
        "differences",
        "recommendation",
        "second",
        "distance",
        "reputation-factor",
        "nodes",
        "honesty",
        "concept",
        "consequence",
        "bases",
        "calculation",
        "Google",
        "position",
        "website",
        "trustee",
        "Epinions",
        "reviewers",
        "effects",
        "Sun",
        "adaptation",
        "techniques",
        "importance",
        "significance",
        "newcomers",
        "tuple",
        "input",
        "aggregation",
        "phase",
        "cases",
        "4.",
        "5.",
        "beta PDF function parameter tuple",
        "human decision making process",
        "beta probability density function",
        "solid mathematical basis",
        "More complex solutions",
        "The beta PDF",
        "common aggregation techniques",
        "Beta Reputation system",
        "online social networks",
        "exact reputation score",
        "priori reputation score",
        "simple aggregation techniques",
        "Other aggregation techniques",
        "Bayesian probabilistic models",
        "Hidden Markov Model",
        "prominent trust models",
        "aggregation process",
        "beta model",
        "Bayesian probability",
        "Applied techniques",
        "Simple arithmetic",
        "probability distributions",
        "probabilistic approach",
        "Markov Models",
        "iterative manner",
        "single steps",
        "first class",
        "basic way",
        "descending order",
        "message boards",
        "citation counts",
        "impact factor",
        "academic literature",
        "understandable algorithm",
        "last years",
        "statistical approach",
        "binary events",
        "Dempster-Shafer theory",
        "subjective logic",
        "belief model",
        "machine learning",
        "dynamic behavior",
        "classical logic",
        "model truth",
        "linguistic approach",
        "prominent example",
        "graph-based approach",
        "graph theory",
        "many models",
        "trust management",
        "transitive trust",
        "trust chains",
        "fuzzy logic",
        "new ratings",
        "fuzzy models",
        "different measures",
        "ranking algorithms",
        "summation",
        "proxy",
        "Examples",
        "systems",
        "Slashdot",
        "implementation",
        "Amazon",
        "positive",
        "average",
        "composition",
        "result",
        "weakness",
        "uncertainty",
        "DST",
        "Yu",
        "Singh",
        "generalization",
        "Malik",
        "falsity",
        "degree",
        "agent/resource",
        "REGRET",
        "flow",
        "information",
        "people",
        "field",
        "centrality",
        "Eigenvector",
        "betweenness",
        "α",
        "β",
        "personal properties rating similarity Discount referrals",
        "position rating value asymmetric rating",
        "propagation propagation discount Discount referrals",
        "subjective reliability property similarity Discount",
        "Component term Subset Description",
        "different trust mod- els",
        "other agents weighting reliability",
        "descriptions Primary component class",
        "certificates objective reliability",
        "reputa- tion system",
        "fundamental component concepts",
        "graph-based flow model",
        "specific application area",
        "Secondary component class",
        "implementa- tion level",
        "Reuse-based software engineering",
        "3 primary component classes",
        "nat- ural framework",
        "single logical components",
        "Explicit Discount",
        "Implicit Discount",
        "primary class",
        "Other examples",
        "extensive descriptions",
        "different levels",
        "one component",
        "23 component terms",
        "new classes",
        "hybrid model",
        "web application",
        "trust models",
        "software components",
        "incoming edges",
        "outgoing edges",
        "several factors",
        "two nodes",
        "Eigentrust Algorithm",
        "aggregation phases",
        "incremental nature",
        "aggregation techniques",
        "novel models",
        "29 sub- sets",
        "building blocks",
        "full taxonomy",
        "Additional file",
        "service repository",
        "comprehensive set",
        "thorough repository",
        "profile age",
        "Reputation values",
        "overall reputation",
        "related implementation",
        "one edge",
        "Popular algorithms",
        "former section",
        "reputation information",
        "conceptual design",
        "Table S",
        "Table 2",
        "impact",
        "PageRank",
        "heuristic",
        "extension",
        "excerpt",
        "event",
        "abstraction",
        "context comparability simple arithmetic attribute-based criteria compatibility credibility",
        "propagation statistic statistic-based reliability rating value",
        "graph-based personal preferences knowledge repository",
        "fuzzy time clustering-based service repository",
        "Absolute congruence Type REST Demo",
        "Implementation Context similarity-based weighting service",
        "Component term Context similarity Subset",
        "one live reachable service",
        "weighting aggregation Sänger",
        "congruence Sänger",
        "Table 4 Web service description",
        "value imbalance problem",
        "absolute congruence metric",
        "nominal context attributes",
        "two logical layers",
        "Mohammad Gias Uddin",
        "context-aware trust model",
        "absolute similarity measurement",
        "Figure 3 Logical layers",
        "design pattern-like concepts",
        "Table 3 Design pattern",
        "context ci",
        "reference context",
        "two levels",
        "Mohammad Zulkernine",
        "different settings",
        "service-orientated architecture",
        "cepts’ logic",
        "interchangeable modules",
        "An artifact",
        "text cj",
        "following formula",
        "total number",
        "Applicability Set",
        "Sheikh Iqbal",
        "dynamic systems",
        "Applied computing",
        "New York",
        "Design patterns",
        "Problem description",
        "generic solution",
        "example calls",
        "example output",
        "example artifact",
        "Code example",
        "abstract designs",
        "occurring problems",
        "essential information",
        "Solution description",
        "2008 ACM symposium",
        "decisions",
        "developer",
        "descriptions",
        "elements",
        "functionality",
        "independent",
        "separation",
        "concerns",
        "interfaces",
        "ID",
        "ws",
        "URL",
        "parameters",
        "tags",
        "Ratings",
        "weight-factor",
        "item",
        "keywords",
        "reference_context",
        "context_attributes",
        "array",
        "Literature",
        "open",
        "Proceedings",
        "SAC",
        "NY",
        "bayforsec",
        "ngot",
        "webservice",
        "Client",
        "Apache server (logic layer",
        "three main pages",
        "current web standards",
        "The MySQL database",
        "new web services",
        "abstract class Component",
        "Example output Array",
        "persistent layer",
        "presentation layer",
        "Example call",
        "web-based application",
        "three-tier client-server-architecture",
        "design patterns",
        "standardized realization",
        "flawless call",
        "standardized interfaces",
        "client calls",
        "schematic layout",
        "trust pattern",
        "front end",
        "general concept",
        "ized call",
        "simple call",
        "con- nection",
        "output format",
        "new WebserviceCallHelper",
        "generic component",
        "unique component",
        "filtering component",
        "knowledge repository",
        "Absolute congruence",
        "distinct components",
        "configuration details",
        "PHP function",
        "client-side implementation",
        "Context similarity",
        "context-set",
        "sets",
        "Parameters",
        "words",
        "referall",
        "NUMBER",
        "$arguments",
        "WEBSERVICE_URL",
        "Weighting",
        "CongruenceAbsolute",
        "Tags",
        "feasibility",
        "server-side",
        "calculate_values",
        "args",
        "WebserviceCallHandler",
        "definition",
        "HTML",
        "JavaScript",
        "CSS",
        "Bootstrap",
        "constructor",
        "base_url",
        "XML",
        "JSON",
        "1",
        "CongruenceAbsolute Webservice Webservice abstract CallHelper CallHandler Component TimeDiscounting Relative aggregation",
        "CongruenceAbsolute TimeDiscounting Relative Sänger",
        "fictitious web developer John Gray",
        "set average value congruence",
        "transparent aggregation technique",
        "Reputation system development",
        "online reputation system",
        "compara- ble framework",
        "descriptive scenario-based evaluation",
        "high quality stamps",
        "input parameter descriptions",
        "electronic marketplace platform",
        "functional computation engine",
        "age-based context similarity",
        "referral reputation filter",
        "aggregation alternatives",
        "web service",
        "referral Aggregation",
        "average component",
        "simple average",
        "component model",
        "age-based filter",
        "following component",
        "Schematic view",
        "service architecture",
        "Client Server",
        "AgeBasedAbsolute Clustering",
        "proper functioning",
        "two-part evaluation",
        "innovative artifacts",
        "static analysis",
        "Scenario analysis",
        "newcom- ers",
        "three phases",
        "weight- ing",
        "same quality",
        "old referrals",
        "quality coins",
        "weighting component",
        "current form",
        "glue class",
        "lose coupling",
        "flexibil- ity",
        "standard approach",
        "most users",
        "initial users",
        "artifact description",
        "arguments",
        "5 Page",
        "completeness",
        "taxonomy",
        "philatelists",
        "numismatics",
        "friends",
        "person",
        "strangers",
        "knowledge",
        "requirements",
        "basics",
        "filtering",
        "experiences",
        "sellers",
        "poor",
        "sequence",
        "code",
        "WebserviceCallHelpers",
        "Client-side",
        "details",
        "obvious util- ity",
        "valid input data",
        "different computation techniques",
        "reputation system components",
        "call computation",
        "trust system",
        "Example code",
        "innovative ideas",
        "func- tioning",
        "feasible combination",
        "practical usability",
        "open issues",
        "Static analysis",
        "proper generation",
        "trust-enforcing mechanisms",
        "computation components",
        "component taxonomy",
        "various properties",
        "computational trust",
        "specific knowledge",
        "aggregation component",
        "referral",
        "time",
        "DATE",
        "rating",
        "reputation_value",
        "webservice_filter",
        "scenario",
        "care",
        "framework",
        "research",
        "Section",
        "tribution",
        "solution",
        "standard",
        "abov"
      ],
      "merged_content": "\nSänger et al. Journal of Trust Management  (2015) 2:5 \nDOI 10.1186/s40493-015-0015-3\n\nRESEARCH Open Access\n\nReusable components for online reputation\nsystems\nJohannes Sänger*, Christian Richthammer and Günther Pernul\n\n*Correspondence:\njohannes.saenger@wiwi.\nuni-regensburg.de\nUniversity of Regensburg,\nUniversitätsstraße 31, 93053\nRegensburg, Germany\n\nAbstract\n\nReputation systems have been extensively explored in various disciplines and\napplication areas. A problem in this context is that the computation engines applied by\nmost reputation systems available are designed from scratch and rarely consider well\nestablished concepts and achievements made by others. Thus, approved models and\npromising approaches may get lost in the shuffle. In this work, we aim to foster reuse in\nrespect of trust and reputation systems by providing a hierarchical component\ntaxonomy of computation engines which serves as a natural framework for the design\nof new reputation systems. In order to assist the design process we, furthermore,\nprovide a component repository that contains design knowledge on both a\nconceptual and an implementation level. To evaluate our approach we conduct a\ndescriptive scenario-based analysis which shows that it has an obvious utility from a\npractical point of view. Matching the identified components and the properties of trust\nintroduced in literature, we finally show which properties of trust are widely covered by\ncommon models and which aspects have only rarely been considered so far.\n\nKeywords: Trust; Reputation; Reusability; Trust pattern\n\nIntroduction\nIn the last decade, trust and reputation have been extensively explored in various disci-\nplines and application areas. Thereby, a wide range of metrics and computation methods\nfor reputation-based trust has been proposed. While most common systems have been\nintroduced in e-commerce, such as eBay’s reputation system [1] that allows to rate sell-\ners and buyers, considerable research has also been done in the context of peer-to-peer\nnetworks, mobile ad hoc networks, social networks or ensuring data accuracy, relevance\nand quality in several environments [2]. Computation methods applied range from sim-\nple arithmetic over statistical approaches up to graph-based models involving multiple\nfactors such as context information, propagation or personal preferences. A general prob-\nlem is that most of the newly introduced trust and reputation models use computation\nmethods that are designed from scratch and rely on one novel idea which could lead to\nbetter solutions [3]. Only a few authors build on proposals of others. Therefore, approved\nmodels and promising approaches may get lost in the shuffle.\nIn this work, we aim to encourage reuse in the development of reputation systems by\n\nproviding a framework for creating reputation systems based on reusable components.\nDesign approaches for reuse have been given much attention in the software engineering\n\n© 2015 Sänger et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nhttp://creativecommons.org/licenses/by/4.0\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 2 of 21\n\ncommunity. The research in trust and reputation systems could also profit from ben-\nefits like effective use of specialists, accelerated development and increased reliability.\nToward this goal, we propose a hierarchical taxonomy for components of computation\nengines used in reputation systems. Thereto, we decompose the computation phase of\ncommon reputation models to derive single building blocks. The classification based on\ntheir functions serves as a natural framework for the design of new reputation systems.\nMoreover, we set up a component repository containing artifacts on both a conceptual\nand an implementation level to facilitate the reuse of the identified components. On the\nconceptual level, we describe each building block as a design pattern-like solution. On\nthe implementation level, we provide already implemented components by means of web\nservices.\nThe rest of this paper is based on the design science research paradigm involving the\n\nguidelines for conducting design science research by Hevner et al. [4] and organized as\nfollows: Firstly, we give an overview of the general problem context as well as the relevance\nand motivation of our work. Thereby, we identify the research gap and define the objec-\ntives of our research. In the following section, we introduce our hierarchical component\ntaxonomy of computation engines used in reputation systems. After that, we point out\nhow our component repository is conceptually designed and implemented. Subsequently,\nwe carry out a descriptive scenario-based analysis of our approach. At the same time, we\nmatch all components identified with the properties of trust introduced in literature. We\nshow which properties of trust are widely covered by common models and which aspects\nhave only rarely been considered so far. Finally, we summarize the contribution and name\nour plans for future work.\n\nProblem context and motivation\nWith the success of the Internet and the increasing distribution and connectivity, trust\nand reputation systems have become important artifacts to support decision making in\nnetwork environments. To impart a common understanding, we firstly provide a defi-\nnition of the notion of trust. At the same time, we explain the properties of trust that\nare important with regard to this work. Then, we point out how trust can be established\napplying computational trust models. Focusing on reputation-based trust, we explain how\nand why the research in reputation models could profit from reuse. Thereby, we identify\nthe research gap and define the objectives of this work.\n\nThe notion of trust and its properties\n\nThe notion of trust is a topic that has been discussed in research for decades. Although\nit has been intensively examined in various fields, it still lacks a uniform and generally\naccepted definition. Reasons for this circumstance are the multifaceted terms trust is\nassociated with like credibility, reliability or confidence as well as the multidimension-\nality of trust as an abstract concept that has a cognitive, an emotional and a behavioral\ndimension. As pointed out by [5], trust has been described as being structural in\nnature by sociologists while psychologists viewed trust as an interpersonal phenomenon.\nEconomists, however, interpreted trust as a rational choice mechanism. The definition\noften cited in literature regarding trust and reputation online that is referred to as relia-\nbility trust was proposed by Gambetta in 1988 [6]: “Trust (or, symmetrically, distrust) is\na particular level of the subjective probability with which an agent assesses that another\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 3 of 21\n\nagent or group of agents will perform a particular action, both before he can monitor such\naction (or independently of his capacity ever to be able to monitor it) and in a context in\nwhich it affects his own action.”\nMultiple authors furthermore include security and risk which can lead to more com-\n\nplex definitions. Anyway, it is generally agreed that trust is multifaceted and dependent\non a variety of factors. Moreover, there are several properties of trust described in lit-\nerature (see Table 1). These properties are important with respect to this work because\nthey form the basis for many applied computation techniques in trust and reputation\nsystems described in Section ‘Hierarchical component taxonomy’. Reusable components\ncould extend current models by the ability to gradually include these properties.\n\nReputation-based trust\n\nIn recent years, several trust models have been developed to establish trust. Thereby,\ntwo common ways can be distinguished, namely policy-based and reputation-based trust\n\nTable 1 Overview of properties of trust described in literature [14,41-46]\n\nDynamic Trust can increase or decrease through gathering new experiences. Moreover,\ntrust is said to decay with time (time-based aging [45]). Because of these char-\nacteristics, trust values strongly depend on the time they are determined. The\ngreater importance of new experiences compared to old experiences has been\nwidely studied and considered in many trust models such as [32,47] or [30].\n\nContext-dependent Trust is bound to a specific context. For example, Alice trusts Bob as her doctor.\nHowever, she might not trust him as a cook to prepare a delicious meal for her.\n\nMulti-faceted Even in the same context, a trust value may not reflect all aspects of this context\n[43]. For example, a customer may trust a particular restaurant for its quality of\nfood but not for its quality of service. The overall trust on this restaurant depends\non the combination of the amount of trust in the specific aspects.\n\nPropagative One property of trust made use of in several models is its propagativity. If Alice\ntrusts Bob, who in turn trusts Claire, Alice can derive trust on Claire from the rela-\ntionships between her and Bob as well as between Bob and Claire. Because of\nthis propagative nature, it is possible to create trust chains passing trust from\none agent to another agent. As clarified by Christianson and Harbison [48], trust\nis not automatically transitive although trust transitivity was assumed proven for\na long time. If Alice trusts Bob, who in turn trusts Claire, it does not inherently\nmean that Alice trusts Claire. It follows from the foregoing that transitivity implies\npropagation. The reverse, though, is not the case.\n\nComposable When trust is propagated, a particular agent may be connected to multiple\ntrust chains. To come up with a final decision whether to trust or distrust this\nagent, the trust information received from the different chains need to be com-\nposed in order to build one aggregated picture. In this context, trust statements\npropagated from nodes close to oneself should have greater influence on the\naggregated value than the ones from distant nodes (distance-based aging [45]).\nComposition is potentially difficult if the trust statements are contradictory [14].\n\nSubjective The subjective nature of trust becomes clear if one thinks about a review on Ama-\nzon [26]. A book review that totally reflects Alice’s opinion will probably resolve\nin a high level of trust against the reviewer Rachel. Bob, however, who disagrees\nwith the review, will have a lower trust in Rachel although it bases on the same\nevidence.\n\nFine-grained Although trust is sometimes modeled in a binary manner (i.e. either trust or dis-\ntrust), it is possible that Alice trusts both Bob and Claire but that she trusts Bob\nmore than Claire. Hence, there may be multiple discrete levels of trust such as\nhigh, medium and low [41]. Mapped to numbers, trust may also be a continuous\nvariable taking values within a certain interval (e.g. between 0 and 1).\n\nEvent-sensitive It can take a long time to build trust. One negative experience, though, can\ndestroy it [23].\n\nReflexive Trust in oneself is always at the maximum value.\n\nSelf-reinforcing It is human nature to preferentially interact with other agents that are trusted.\nAnalogously, agents will avoid interacting with untrustworthy agents. Thus, the\ntrustworthiness of other agents is inherently taken into consideration.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 4 of 21\n\nestablishment [7]. Policy-based trust is often referred to as a hard security mechanism due\nto the exchange of hard evidence (e.g. credentials). Reputation-based trust, in contrast, is\nderived from the history of interactions. Hence, it can be seen as an estimation of trust-\nworthiness (soft security). In this work, we focus on reputation-based trust. Reputation\nis defined as follows: “Reputation is what is generally said or believed about a person’s or\nthing’s character or standing.” [8].\nIt is based on referrals, ratings or reviews from members of a community. Therefore,\n\nit can be considered as a collective measure of trustworthiness [8]. Trustworthiness as a\nglobal value is objective. However, the trust an agent puts in someone or something as a\ncombination of personal experience and referrals is subjective.\n\nResearch gap: design of reputation systems with reuse\n\nIt has been argued (e.g. by [3]) that most reputation-based trust models proposed in the\nacademic community are built from scratch and do not rely on existing approaches. Only\na few authors continue their research on the ideas of others. Thus, many approved models\nand promising thoughts go unregarded. The benefits of reuse, though, have been rec-\nognized in software engineering for years. However, there are only very few works that\nproposed single components to enhance existing approaches. Rehak et al. [9], for instance,\nintroduced a generic mechanism that can be combined with existing trust models to\nextend their capabilities by efficiently modeling context. The benefits of such a compo-\nnent that can easily be combined with existing systems are obvious. Nonetheless, research\nin trust and reputation still lacks in sound and accepted principles to foster reuse.\nTo gradually close this gap, we aim to provide a framework for the design of new\n\nreputation systems with reuse. As described above, we thereto propose a hierarchical\ncomponent taxonomy of computation engines used in reputation systems. Based on this\ntaxonomy, we set up a repository containing design knowledge on both a conceptual\nand an implementation level. On the one hand, the uniform and well-structured artifacts\ncollected in this repository can be used by developers to select, understand and apply\nexisting concepts. On the other hand, they may encourage researchers to provide novel\ncomponents on a conceptual and an implementation level. In this way, the reuse of ideas,\nconcepts and implemented components as well as the communication of reuse knowledge\nshould be achieved. Furthermore, we argue that the reusable components we identify in\nthis work could extend current reputation models by the ability to gradually include the\nproperties of trust described above. To evaluate whether our taxonomy/framework can\ncover all aspects of trust, we finally provide a table matching our component classes with\ntrust properties.\n\nA hierarchical component taxonomy for computation methods in reputation\nsystems\nTo derive a taxonomy from existing models, our research includes two steps: (1) the\nanalysis of the generic process of reputation systems and (2) the identification of logical\ncomponents of the computation methods used in common trust and reputation models.\nA critical question is how to determine and classify single components. Thereto, we follow\nan approach to function-based component classification, which means that the taxonomy\nis derived from the functions the identified components fulfill.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 5 of 21\n\nThe generic process of reputation systems\n\nThe generic process of reputation systems, as depicted in Figure 1, can be divided into\nthree steps: (1) collection & preparation, (2) computation and (3) storage & communica-\ntion. These steps are adapted from the three fundamental phases of reputation systems\nidentified by [10] and [11]: feedback generation/collection, feedback aggregation and\nfeedback distribution. Feedback aggregation as the central part of every trust and repu-\ntation system is furthermore divided into the three process steps filtering, weighting and\naggregation taken together as computation. The context setting consists of a trustor who\nwants to build a trust relation toward a trustee by providing context and personalization\nparameters and receiving a trustee’s reputation value.\n\nCollection and preparation\n\nIn the collection and preparation phase, the reputation system gleans information about\nthe past behavior of a trustee and prepares it for subsequent computing. Although per-\nsonal experience is the most reliable, it is often not sufficiently available or nonexistent.\nTherefore, data from other sources needs to be collected. These can be various, ranging\nfrom public or personal collections of data centrally stored to data requested from dif-\nferent peers in a distributed network. After all available data is gathered, it is prepared\nfor further use. Preparation techniques include normalization, for instance, which brings\nthe input data from different sources into a uniform format. Once the preparation is\ncompleted, the reputation data serves as input for the computation phase.\n\nComputation\n\nThe computation phase is the central part of every reputation system and takes the rep-\nutation information collected as input and generates a trust/reputation value as output.\nThis phase can be divided into the three generic process steps filtering, weighting and\naggregation. Depending on the computation engine, not all steps have to be implemented.\nThe first two steps (filtering and weighting) preprocess the data for the subsequent aggre-\ngation. The need for these steps is obvious: The first question to be answered is which\ninformation is useful for further processing (filtering). The second process step concerns\nthe question of how relevant the information is for the specific situation (weighting). In\nline with this, Zhang et al. [12] pointed out that current trust models can be classified into\nthe two broad categories filtering-based and discounting-based. The difference between\nfiltering and weighting is that the filtering process reduces the information amount while\n\nFigure 1 Generic process of a reputation system, inspired by [10].\n\n reputation system computation collection & storage & preparation filtering weighting aggregation communication input context, personalisation trust relation reputation value(s) output trustor trustee transaction/situation \n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 6 of 21\n\nit is enriched by weight factors in the second case. Therefore, filtering can be seen as\nhard selection while weighting is more like a soft selection. Finally, the reputation values\nare aggregated to calculate one or several reputation scores. Depending on the algo-\nrithm, the whole computation process or single process steps can be run through for\nmultiple times.\n\nStorage and communication\n\nAfter reputation scores are calculated, they are either stored locally, in a public storage\nor both depending on the structure (centralized/decentralized/hybrid) of the reputation\nsystem. Common reputation systems not only provide the reputation scores but also offer\nextra information to help the end-users understand the meaning of a score. They should\nfurthermore reveal the computation process to accomplish transparency.\nIn this work, we focus on the computation phase, since the first phase (collection &\n\npreparation) and the last phase (storage & communication) strongly depend on the struc-\nture of the reputation system (centralized or decentralized). The computation phase,\nhowever, is independent of the structure and can look alike for systems implemented in\nboth centralized and decentralized environments. Therefore, it works well for design with\nreuse.\n\nHierarchical component taxonomy\n\nIn this section, the computation process is examined in detail. We introduce a novel\nhierarchical component taxonomy that is based on the functional blocks of common rep-\nutation systems identified in this work. Thereto, we clarify the objectives of the identified\nclasses (functions) and name common examples. Our analysis and selection of reputa-\ntion systems is based on different surveys [2,3,8,13,14]. Figure 2 gives an overview of the\nprimary and secondary classes identified.\n\nFigure 2 Classes of filtering-, weighting- and aggregation-techniques.\n\n filtering weighting aggregation context comparability simple arithmetic attribute-based criteria compatibility credibility/propagation statistic statistic-based reliability rating value fuzzy time clustering-based graph-based personal preferences \n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 7 of 21\n\nBeginning with the filtering phase, the three broad classes attribute-based, statistic-\nbased and clustering-based filtering can be identified:\n\n1. Attribute-based filtering: In several trust models, input data is filtered based on a\nconstraint-factor defined for the value of single attributes. Attribute-based filters\nmostly implement a very simple logic, in which an attribute is usually compared to\na reference value. Due to their lightweight, they are proper for reducing huge\namounts of input data to the part necessary for the reputation calculation. Besides\nthe initial filtering of input data, it is often applied after the weighting phase in\norder to filter referrals that have been strongly discounted. Time is an example of\nan attribute that is often constrained because it is desirable to disregard very old\nratings. eBay’s reputation system, for instance, only considers transactions having\noccurred in the last 12 months for their overview of positive, neutral and negative\nratings. Other models such as Sporas [15] ignore every referral but the latest, if one\nparty rated another party more than once. In this way, simple ballot stuffing attacks\ncan be prevented. In ballot stuffing attacks, parties improve their reputation by\nmeans of positive ratings after fake transactions.\n\n2. Statistic-based filtering: Further techniques that are used to enhance the\nrobustness of trust models against the spread of false rumors apply statistical\npatterns. Whitby et al. [16], for example, proposed a statistical filter technique to\nfilter out unfair ratings in Bayesian reputation systems applying the majority rule.\nThe majority rule considers feedback that is far away from the majority’s referrals as\ndishonest. In this way, dishonest or false feedback can easily be detected and filtered.\n\n3. Clustering-based filtering: Clustering-based filter use cluster analysis approaches\nto identify unfair ratings. These approaches are comparatively expensive and\ntherefore rarely used as filtering techniques. An exemplary procedure is to analyze\nan advisor’s history. Since a rater never lies to himself, an obvious way to detect\nfalse ratings is to compare own experience with the advisor’s referrals. Thus, both\nfair and unfair ratings can be identified. iCLUB [17], for example, calculates\nclusters of advisors whose evaluations against other parties are alike. Then, the\ncluster being most similar to the own opinion is chosen as fair ratings. If there is no\ncommon experience (e.g. bootstrapping), the majority rule will be applied. Another\nexample for an approach using cluster filtering was proposed by Dellarocas [18].\n\nOnce all available information is reduced to those suitable for measuring trust and\nreputation in the current situation, it becomes clear that various data differ in their\ncharacteristics (e.g. context, reliability). Hence, the referrals are weighted in the second\nprocess step based on different factors. In contrast to the filtering step, applied techniques\ndiffer strongly. For that reason, our classification of weighting techniques is based on the\nproperties of referrals that are analyzed for the discounting. We distinguish between the\nfollowing classes:\n\n1. Context comparability: Reputation data is always bound to the specific context in\nwhich it is created. Ratings that are generated in one application area might not be\nautomatically applicable in another application area. In e-commerce, for instance,\ntransactions are accomplished involving different prices, product types, payment\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 8 of 21\n\nmethods, quality or time. The non-consideration of this context leads to the value\nimbalance problem where a malicious seller can build a high reputation by selling\ncheap products while cheating on expensive ones. To increase comparability and\navoid such situations, context has become a crucial attribute for many current\napproaches like [19] or [9].\n\n2. Criteria comparability: Besides the context in which feedback is created, the\ncriteria that underlie the evaluation are important. Particularly, if referrals from\ndifferent application areas or communities are integrated, criteria comparability\ncan be crucial. In file-sharing networks, for instance, a positive rating is often\ngranted with a successful transaction independent of the quality of service. On\ne-commerce platforms, in contrast, quality may be a critical factor for customer\nsatisfaction. Other distinctions could be the costs of reviews, the level of\nanonymity or the number of peers in different communities or application\nareas. Weighting based on criteria comparability can compensate these\ndifferences.\n\n3. Credibility/propagation: In network structures such as in the web-of-trust, trust\ncan be established along a recommendation or trust chain. Obviously, referrals that\nhave first-hand information about the trustworthiness of an agent are more\ncredible than referrals received at second-hand (with propagation degree of two) or\nhigher. Therefore, several models apply a propagation (transitivity) rate to discount\nreferrals based on their distance. The biometric identity trust model [20], for\ninstance, derives the reputation-factor from the distance of nodes in a web-of-trust.\n\n4. Reliability: Reliability or honesty of referrals can strongly affect the weight of\nreviews. The concept of feedback reputation that measures the agents’ reliability in\nterms of providing honest feedback is often applied. As a consequence, referrals\ncreated by agents having a low feedback reputation have a low impact on the\naggregated reputation. The bases for this calculation can be various. Google’s\nPageRank [21], for instance, involves the position of every website connected to the\ntrustee in the web graph in their recursive algorithm. Epinions [22], on the other\nhand, allows users to directly rate reviews and reviewers. In this way, the effects of\nunfair ratings are diminished.\n\n5. Rating value: Trust is event sensitive. For stronger punishment of bad behavior,\nthe weight of positive ratings compared to negative ratings can be calculated\nasymmetrically. An example for a model using an “adaptive forgetting scheme” was\nproposed by Sun et al. [23], in which good reputation can be built slowly through\ngood behavior but easily be ruined through bad behavior.\n\n6. Time: Due to the dynamic nature of trust, it has been widely recognized that time\nis one important factor for the weighting of referrals. Old feedback might not be as\nrelevant for reputation scoring as new referrals. An example measure for\ntime-based weighting is the “forgetting factor” proposed by Jøsang [24].\n\n7. Personal preferences: Reputation systems are used by various end-users (e.g.\nhuman decision makers, services). Therefore, a reputation system must allow the\nadaptation of its techniques to subjective personal preferences. Different actors\nmight have different perceptions regarding the importance of direct experience\nand referrals, the significance of distinct information sources or the rating of\nnewcomers.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 9 of 21\n\nThe tuple of reputation data and weight-factor(s) serve as input for the third step of\nthe computation process - the aggregation. In this phase, one or several trust/reputa-\ntion values are calculated by composing the available information. In some cases, the\nweighting and the aggregation process are run through repetitively in an iterative manner.\nHowever, the single steps can still be logically separated. The list of proposed algorithms\nto aggregate trust and reputation values has become very long during the last decade.\nHere, we summarize the most common aggregation techniques and classify them into the\nfour blocks simple arithmetic, statistic, fuzzy and graph-based models:\n\n1. Simple arithmetic: The first class includes simple aggregation techniques like\nranking, summation or average. Ranking is a very basic way to measure\ntrustworthiness. In ranking algorithms, ratings are counted and organized in a\ndescending order based on that value. This measure has no exact reputation score.\nInstead, it is frequently used as a proxy for the relative importance/trustworthiness.\nExamples for systems using ranking algorithms are message boards like Slashdot\n[25] or citation counts used to calculate the impact factor in academic literature.\nOther aggregation techniques that are well known due to the implementation on\neBay or Amazon [26] are the summation (adding up positive and negative ratings)\nor the average of ratings. Summation, though, can easily be misleading, since a\nvalue of 90 does not reveal the composition of positive and negative ratings (e.g.\n+100,-10 or +90,0). The average, on the other hand, is a very intuitive and easily\nunderstandable algorithm.\n\n2. Statistic: Many of the prominent trust models proposed in the last years use a\nstatistical approach to provide a solid mathematical basis for trust management.\nApplied techniques range from Bayesian probability over belief models to Hidden\nMarkov Models. All models based on the beta probability density function (beta\nPDF) are examples for models simply using Bayesian probability. The beta PDF\nrepresents the probability distributions of binary events. The a priori reputation\nscore is thereby gradually updated by new ratings. The result is a reputation score\nthat is described in a beta PDF function parameter tuple (α, β), whereby α\nrepresents positive and β represents negative ratings. A well known model using\nthe beta PDF is the Beta Reputation system [24]. A weakness of Bayesian\nprobabilistic models, however, is that they cannot handle uncertainty. Therefore,\nbelief models extend the probabilistic approach by Dempster-Shafer theory (DST)\nor subjective logic to include the notion of uncertainty. Trust and reputation\nmodels involving a belief model were proposed by Jøsang [27] or Yu and Singh [28].\nMore complex solutions that are based on machine learning, use the Hidden\nMarkov Model, a generalization of the beta model, to better cope with the dynamic\nbehavior. An example was introduced by Malik et al. [29].\n\n3. Fuzzy: Aggregation techniques classified as fuzzy models use fuzzy logic to\ncalculate a reputation value. In contrast to classical logic, fuzzy logic allows to\nmodel truth or falsity within an interval of [0,1]. Thus, it can describe the degree to\nwhich an agent/resource is trustworthy or not trustworthy. Fuzzy logic has been\nproven to deal well with uncertainty and mimic the human decision making\nprocess [30]. Thereby, a linguistic approach is often applied. REGRET [31] is one\nprominent example of a trust model making use of fuzzy logic.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 10 of 21\n\n4. Graph-based: A variety of trust models employ a graph-based approach. They rely\non different measures describing the position of nodes in a network involving the\nflow of transitive trust along trust chains in network structures. As online social\nnetworks have become popular as a medium for disseminating information and\nconnecting people, many models regarding trust in social networks have lately\nbeen proposed. Graph-based approaches use measures from the field of graph\ntheory such as centrality (e.g. Eigenvector, betweenness), distance or node-degree.\nReputation values, for instance, grow with the number of incoming edges (in-\ndegree) and increase or decrease with the number of outgoing edges (out-degree).\nThe impact of one edge on the overall reputation can depend on several factors like\nthe reputation of the node an edge comes from or the distance of two nodes.\nPopular algorithms using graph-based flow model are Google’s PageRank [21] as\nwell as the Eigentrust Algorithm [32]. Other examples are the web-of-trust or trust\nmodels particularly designed for social networks as described in [14]. As mentioned\nabove, the weighting and aggregation phases are incrementally run through for\nseveral times due to the incremental nature of these algorithms.\n\nThe classification of the computation engine’s components used in different trust mod-\nels in this taxonomy is not limited to one component of each primary class. Depending\non the computation process, several filtering, weighting and aggregation techniques can\nbe combined and run through more than once. Malik et al. [29], for instance, introduced\na hybrid model combining heuristic and statistical approaches. However, our taxonomy\ncan reveal the single logical components a computation engine is built on. Moreover,\nit serves as an overview of existing approaches. Since every currently known reputa-\ntion system can find its position, to the best of our knowledge, this taxonomy can be\nseen as complete. Though, an extension by new classes driven by novel models and\nideas is possible. Our hierarchical component taxonomy currently contains 3 primary\ncomponent classes, 14 secondary component classes, 23 component terms and 29 sub-\nsets. Table 2 shows an excerpt of the hierarchical component taxonomy with building\nblocks of the primary class “weighting”. The full taxonomy is provided in Additional file 1:\nTable S1.\n\nThe component taxonomy as a framework for design with reuse\nThe hierarchical component taxonomy introduced in the former section serves as a nat-\nural framework for the design of reputation systems with reuse. To support this process,\nwe set up a component repository combining a knowledge and a service repository.\nThus, it does not only contain information about software components on implementa-\ntion level but also provides extensive descriptions of the ideas applied on a conceptual\nlevel. This comprehensive set of fundamental component concepts and ideas combined\nwith the related implementation allows the reuse of both ideas and already implemented\ncomponents.\nIn this section, we firstly describe the conceptual design of our component reposi-\n\ntory in detail. Then, we elaborate on the implementation of a web application employing\nour thorough repository to provide design knowledge for reuse on a conceptual and an\nimplementation level.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 11 of 21\n\nTable 2 Excerpt of the hierarchical component taxonomy with descriptions\n\nPrimary\ncomponent\nclass\n\nSecondary\ncomponent\nclass\n\nComponent term Subset Description\n\ncredibility/\npropagation\n\npropagation discount Discount referrals along\ntrust chains\n\nsubjective reliability property similarity Discount based on\nsimilarity of personal\nproperties\n\nrating similarity Discount referrals based\non similarity of ratings\ntoward other agents\n\nweighting reliability Explicit Discount based on explicit\nreputation information like\nreferrals or certificates\n\nobjective reliability Implicit Discount based on implicit\nreputation information like\nprofile age, number of\nreferrals or position\n\nrating value asymmetric rating Strongly discount positive\nratings compared to\nnegative ratings (event\nsensitive)\n\n. . . . . . . . . . . . . . .\n\nConceptual design of the component repository\n\nReuse-based software engineering can be implemented on different levels of abstraction,\nranging from the reuse of ideas to the reuse of already implemented software components\nfor a very specific application area. In this work, we want to apply our taxonomy for reuse\non two levels – a conceptual level and an implementation level. Therefore, the developed\nrepository provides design knowledge for reuse on two logical layers (see Figure 3).\n\nFigure 3 Logical layers of the component repository for design with reuse.\n\n context comparability simple arithmetic attribute-based criteria compatibility credibility/propagation statistic statistic-based reliability rating value fuzzy time clustering-based service repository (implementation level)- graph-based personal preferences knowledge repository (conceptual level) filtering weighting aggregation \n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 12 of 21\n\nReuse on conceptual level\n\nWhen reusing an implemented component, one is unavoidably constrained by design\ndecisions that have been made by the developer. A way to prevent this is to conceive more\nabstract designs that do not specify the implementation. Thus, we provide an abstract\nsolution to a problem by means of design pattern-like concepts. Design patterns are\ndescriptions of commonly occurring problems and a generic solution to the problems that\ncan be used in different settings [33]. Our design pattern-like concepts consist of essential\nelements that are exemplary depicted in Table 3.\n\nReuse on implementation level\n\nOn implementation level, we provide fully implemented reusable components by means\nof web services in a service-orientated architecture. These services encapsulate the con-\ncepts’ logic and functionality in independent and interchangeable modules to achieve the\nseparation of concerns. The web services are incorporated via well-defined interfaces. All\nservices provided are registered as artifacts in the service repository. An artifact contains\nessential information about one live reachable service such as ID, type (REST or ws), URL,\ndescription, parameters, example calls, example output, the design pattern that is imple-\nmented by the service, and tags describing the functionality. Table 4 shows an example\nartifact for the design pattern described above.\n\nTable 3 Design pattern on the conceptual level (example)\n\nComponent term Context similarity\n\nSubset Absolute congruence\n\nDescription This component uses an absolute congruence metric as similarity measure to\nidentify context similarity.\n\nProblem description Reputation data is always bound to the specific context in which it was created.\nRatings that were generated in one application area might not be automatically\napplicable in another application area which can result in the value imbalance\nproblem.\n\nSolution description Apply similarity measurement between context ci (reference context) and con-\ntext cj of referrals in the referral set to deliver a weight-factor for each item of the\nreferral set using the following formula:\n\nw(c1, c2) :=\nk(ci) ∩ k(cj)\nk(ci) ∪ k(cj)\n\nk(ci) denotes the total number of keywords describing context ci.\n\nApplicability Set of nominal context attributes.\n\nCode example (php)\n\nfunction calculate_values($reference, $context_sets) {\n$reference_context = $reference[’context_attributes’];\n$return_values = array();\nwhile(!empty($context_sets)) {\n\n...shortened...\n}\nreturn $return_values;\n\n}\n\nImplementation Context similarity-based weighting service (absolute congruence)\n\nLiterature\n• Mohammad Gias Uddin, Mohammad Zulkernine, and Sheikh Iqbal\n\nAhamed. 2008. CAT: a context-aware trust model for open and dynamic\nsystems. In Proceedings of the 2008 ACM symposium on Applied\ncomputing (SAC ’08). ACM, New York, NY, USA, 2024–2029.\n\nTags weighting, context, similarity, congruence\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 13 of 21\n\nTable 4 Web service description on implementation level (example)\n\nComponent term Context similarity\n\nSubset Absolute congruence\n\nType REST\n\nDemo http://trust.bayforsec.de/ngot/webservice/Client/?=Weighting-congruence-absolute-\ncall.php\n\nDescription This service provides an absolute similarity measurement between a reference context\nand a context-set of referrals.\nExample: The sets ’registered’,’charged’, ’verified’ and ’registered’, ’costless’, ’unverified’\nhave a similarity of 1/3.\n\nParameters\n\n// define words that describe the quality of a referall\n$reference_context : array(\"words\" => array (TEXT));\n\n$referral_sets = array( $context_set );\n\n$referral_set = array( \"id\" => NUMBER, \"words\" => array (TEXT));\n\nExample call\n\nrequire_once(’WebserviceCallHelper.php’);\n\n$arguments = array(\"words\" => array (\"registered\",\"charged\",\"verified\"));\n$referral_set = array();\n$referral_set[0] = array( \"id\" => \"10000\", \"words\" =>\n\narray (\"registered\",\"costless\", \"unverified\"));\n$referral_set[1] = array( \"id\" => \"10001\", \"words\" =>\n\narray (\"registered\",\"charged\", \"verified\"));\n$referral_set[2] = array( \"id\" => \"10002\", \"words\" =>\n\narray (\"registered\",\"costless\", \"verified\"));\n\n$webservice_call = new WebserviceCallHelper(array(\n’base_url’ => WEBSERVICE_URL,\n’format’ => \"html\",\n’component’ => \"Weighting\\CongruenceAbsolute\"\n\n));\n$webservice_call->get_result($arguments, $referral_set);\n\nExample output\n\nArray\n(\n\n[status] => 200\n[data] => Array\n\n(\n[0] => Array\n\n(\n[0] => 10000\n[1] => 0.2\n\n)\n[1] => Array\n\n(\n[0] => 10001\n[1] => 1\n\n)\n[2] => Array\n\n(\n[0] => 10002\n[1] => 0.5\n\n)\n)\n\n)\n\nPattern Implemented Context similarity (Absolute congruence)\n\nTags weighting, context, similarity, congruence\n\nImplementation of the repository\n\nTo demonstrate the feasibility of our approach, we have prototypically implemented the\nrepository as a web-based application in a three-tier client-server-architecture [34]. To\ngive an overview of the chosen architecture, we distinguish between server-side and\nclient-side implementation.\n\nServer-side\n\nOn server-side, the logic is implemented in PHP on an Apache server (logic layer) con-\nnecting to a MySQL database (persistent layer). The MySQL database contains all data\nregarding the design patterns as described in Table 2. Each of these design patterns is also\nimplemented in a web service. To enable a standardized realization of new web services\nand a flawless call via standardized interfaces, we employ an abstract class Component. All\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 14 of 21\n\ncomponents (implemented as web services) must inherit from this class, which particu-\nlarly requires overwriting the function calculate_values. To make the generic component\nindependent of the input data, developers are advised to make use of the PHP function\nfunc_get_args(). In this way, distinct components can receive a variety of arguments. To\nconsistently handle client calls, our architecture is extended by a WebserviceCallHandler.\nFigure 4 depicts the schematic layout.\nAll web services implementing the trust pattern currently described in our knowledge\n\nrepository have been created and registered as artifacts in our service repository [34].\nFurthermore, these artifacts are described in detail including a definition of input, output\nand example calls as defined in Table 4.\n\nClient-side\n\nOn client-side (presentation layer), we employ the current web standards HTML5,\nJavaScript and CSS (Bootstrap). The front end is divided into three main pages –\n“overview”, “knowledge repository” and “service repository” – which provide information\non the general concept, the trust patterns and the web services. To enable a standard-\nized call of a web service from client-side, a WebserviceCallHelper allows a simple call of\neach component by configuration and provides all functions necessary to establish a con-\nnection to the repository. The configuration details are passed to the constructor, which\nrequires a base_url, an output format (HTML, XML or JSON) and a unique component\nname as illustrated below.\n\nExample call of a filtering component via the WebserviceCallHelper\n\n$webservice_call = new WebserviceCallHelper(array(\n’base_url’ => \"http://trust.bayforsec.de/ngot/webservice/\",\n’format’ => \"html\",\n’component’ => \"Filtering\\AgeBasedAbsolute\"\n\n));\n\n$webservice_call->get_result($arguments, $referral_set);\n\nFigure 4 Schematic view on the service architecture.\n\n Client Server filtering AgeBasedAbsolute Clustering implement/ inherit weighting CongruenceAbsolute Webservice Webservice abstract CallHelper CallHandler Component TimeDiscounting Relative aggregation CongruenceAbsolute TimeDiscounting Relative \n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 15 of 21\n\nEvaluation\nTo rigorously demonstrate the proper functioning and quality of our approach, we carry\nout a two-part evaluation of our artifact in this section. As there is currently no compara-\nble framework, we firstly perform a descriptive scenario-based evaluation. According to\nHevner et al. [4], this is a standard approach for innovative artifacts like ours. To demon-\nstrate the completeness of our taxonomy, we secondly conduct a static analysis [4], in\nwhich we match all components to the trust properties described in Section ‘The notion\nof trust and its properties’.\n\nScenario analysis: Reputation system development\n\nThe fictitious web developer John Gray runs an electronic marketplace platform for\nphilatelists and numismatics. The platform has been launched with his friends as the first\nusers but has been growing fast. Meanwhile, most users do not know each other in person\nanymore. As a result, many of the initial users have stopped interacting with the newcom-\ners as they do not trust them. After realizing this problem, John decides to introduce an\nonline reputation system in order to establish trust among the strangers. In the following,\nwe describe how our knowledge and service repository can help him to build a reputation\nsystem that perfectly meets his requirements.\nHaving read the basics on our component model, John concludes that he wants to build\n\na computation engine that makes use of components of all three phases – filtering, weight-\ning and aggregation. Thinking about the experiences made with sellers on the platform,\nhe recognizes that most of them do not deliver the same quality all the time. Thus, an\nage-based filter should be employed to make old referrals less important than new ones.\nFurthermore, there are sellers that usually deliver high quality stamps while offering poor\nquality coins. Therefore, a weighting component based on context similarity (absolute\ncongruence) should be selected. Regarding the aggregation alternatives, John decides to\nmake use of the average component as the simple average is probably the most intuitive\nand most transparent aggregation technique for the users. Finally, the single components\nare combined in sequence to a fully functional computation engine as depicted in Figure 5.\nThe code listed below shows an example for the implementation of John’s computation\n\nengine. Here, the WebserviceCallHelpers for each of the selected components have to be\ninstantiated first as described in Section ‘Client-side’. Secondly, the referral set needs to\nbe loaded and prepared according to input parameter descriptions provided for each web\nservice in the component repository. In its current form, the framework does not pro-\nvide any classes to automatically plug single components together (glue class). Thus, the\ndeveloper has to ensure that the output of one component is correctly provided as an\ninput for the following component. This lose coupling, however, allows for more flexibil-\nity. All details on the input and output format can be found in the artifact description of\neach component. While the input data varies from component to component, the output\nis alike for components of each primary class.\n\nFigure 5 Sequence of components for John’s computation engine.\n\n Filter: Weighting: referral reduced set age-based context similarity weighted referral Aggregation: referral reputation filter (absolut) (absolute set average value congruence) set \n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 16 of 21\n\nExample code for the computation engine described above\n\n//create helper for filtering component\n$webservice_filter = new WebserviceCallHelper(array(\n\n’base_url’ => WEBSERVICE_URL,\n’format’ => \"html\",\n’component’ => \"Filtering\\AgeBasedAbsolute\"\n\n));\n\n//create helper for weighting component\n$webservice_weight = ...as above...\n\n//create helper for aggregation component\n$webservice_aggregate = ...as above...\n\n//define referral_set\n$referral_set = array(\"id\" => NUMBER, \"time\" => DATE, \"context\" => array (TEXT), \"\n\nrating\" => NUMBER);\n\n//call computation\n$reputation_value = $webservice_aggregate(\n\n$glue->prepare_for_aggregation($webservice_weight(\n$glue->prepare_for_weighting($webservice_filter->get_result(\n\n$referral_set))\n))\n\n);\n\nThis scenario elucidates that our knowledge and service repository has an obvious util-\nity from a practical point of view since developers can easily access it and gain knowledge\nabout online reputation systems. Thereby, we may help to better spread innovative ideas\nand allow developers to experiment with different computation techniques. However,\nour approach requires specific knowledge on the structure of the repository, the func-\ntioning of each component and details on how to plug components together. Developers\nneed to manually combine components and take care, whether they use valid input data\nand a feasible combination of reputation system components. In its current form, our\nframework is not very “developer friendly”. Therefore, further research will be necessary\nin order to improve the practical usability of our component repository. In Section ‘Con-\ntribution and future work’, we discuss open issues in more detail.\n\nStatic analysis: Matching components and trust properties\n\nTo guarantee the proper generation of computational trust, trust-enforcing mechanisms\nsuch as reputation systems should be able to consider and address all properties of trust.\nAs our component taxonomy serves as a framework for the design of new reputation sys-\ntems with reuse, it should enable developers to extend current reputation models by the\nability to gradually include the various properties of trust. Therefore, a way to evaluate\nthe completeness of our solution is to review whether a trust system that is built accord-\ning to our framework could meet this standard. In Table 5, we match the computation\ncomponents identified above to the properties of trust introduced in Section ‘The notion\nof trust and its properties’. Since our taxonomy is based on reputation systems analyzed\nin various surveys, this approach also enables us to identify aspects of trust that have only\nrarely been considered in research so far.\nExamining Table 5, we find that all trust properties listed are widely covered. There is at\n\nleast one component addressing each single characteristic. Going into more detail, we find\nthat there are many proposals that have developed components to personalize reputation\nsystems, thus covering the subjective property of trust. This reflects a general trend to an\nenhanced personalization of reputation systems. Furthermore, it becomes clear that all\nof our weighting and aggregation components follow the fine-grained property of trust,\ni.e. that trust can be modeled as a continuous variable. For the filtering components, the\nfine-grained property is not entirely applicable in the same meaning as filtering has no\ndirect influence on the trust value of referrals. Since the effect of filtering is that a referral\n\n\n\nSän\ng\ner\n\neta\nl.Jo\n\nu\nrn\na\nlo\nfTru\n\nstM\na\nn\na\ng\nem\n\nen\nt\n\n (2\n0\n\n1\n5\n\n) 2\n:5\n\n \nP\nag\n\ne\n1\n7\no\nf2\n\n1\n\nTable 5 Matching reputation system components and trust properties\n\nTrust properties\n\nPrimary Secondary Dynamic Context- Multi- Propagative Composable Subjective Fine- Event- Reflexive Self-\n\ncomponent component dependent faceted grained sensitive reinforcing\n\nclass class\n\nComputation\ncomponents\n\nFiltering\n\nattribute-based � � � � �� � � � � �\nstatistic-based � � � � � � � � � ��\nclustering � � � � � � � � � ��\n\nWeighting\n\ncontext comparability �� � �� � � � � � � �\ncriteria comparability � �� � � � � � � � �\ncredibility � � � � � � � � �� �\nreliability � � � �� �� � � � �� �\nrating value � � � � � � � � � ��\ntime � � � � � � � � � �\npersonal preferences � � � � � � � � �� �\n\nAggregation\n\nsimple arithmetic � � � � � � � � � ��\nstatistic � � � � � � � � � ��\nfuzzy � � � � � � � � � ��\ngraph-based � � � � � �� � � � ��\n\n� not addressed.�� partly addressed.� completely addressed.\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 18 of 21\n\neither is further considered or not, it shows a binary character rather than being fine-\ngrained. In contrast to the subjective and fine-grained properties of trust, other properties\nsuch as context-dependent, multi-faceted and event-sensitive are particularly addressed\nby only one or two components. Note that this does not automatically mean that there\nis an increased necessity for future research concerning these properties. It may also be\npossible that one component is enough to cover one trust property. More detailed studies\non this could be part of future work.\nOverall, we can say that computational trust can be represented quite accurately when\n\nusing our taxonomy and the provided components as a basis for the development of new\nreputation systems or the extension of existing models. Note, however, that this is only\none view on our taxonomy. Conducting a comparable analysis from the viewpoint of\nattacks and defense mechanisms, for instance, the outcomes may vary greatly.\n\nContribution and future work\nMany surveys of trust and reputation systems give an overview of existing trust and repu-\ntation systems by means of a classification of existing models and approaches. In contrast\nto this, we provide a collection of ideas and concepts classified by their functions. Further-\nmore, these ideas are not only named but also clearly described in well-structured design\npattern-like artifacts which can easily be adapted to a specific situation. Therewith, we\nreorganized the design knowledge for computation techniques in reputation systems and\ntranslated the most common ideas into a uniform format. To directly make use of novel\ncomponents, the web services created on implementation level can instantly be reused\nand integrated in existing reputation systems to extend their capabilities. This approach\n(i.e. publicly providing implemented computation components as web services) may help\nto better spread innovative ideas in trust and reputation systems and give system builders\na better choice allowing to experiment with different computation techniques. Moreover,\nwe encourage researchers to focus on the design of single components by providing a\nplatform on which concepts and their prototypical implementation can be made publicly\navailable.\nNonetheless, there are still some unexplored areas regarding the design with reuse in\n\ntrust and reputation systems. Firstly, reusability could play a role in process steps other\nthan the computation phase. To clarify the opportunities, further research is necessary in\nthis area. Secondly, our hierarchical taxonomy is currently limited to a functional view on\nthe identified components but developers may also benefit from additional views. Because\nof the importance of the robustness of trust and reputation systems [35], we are particu-\nlarly interested in an attack view. In [36], we present first ideas on this issue. We propose\na taxonomy of attacks on reputation systems and then refer to the single components of\nour repository as solutions to the specific attack classes. In this way, we not only support\nreputation system designers in the development of more reliable and more robust rep-\nutation systems with already existing components but also help to identify weaknesses\nthat have not been addressed so far. Thirdly, the selection and interpretation of adequate\ncomponents for new reputation systems in a particular application area requires time,\neffort and – to some extent – knowledge of this research area. To increase usability, a\nsoftware application is needed to support a user in this development process. Ultimately,\nthe application may even be able to automatically find the most qualified composition\nfor specific requirements and input data. This, in turn, demands for generic testbeds that\n\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 19 of 21\n\nenable objective evaluations of reputation systems because so far, researchers have mainly\nbeen developing their own testing scenarios favoring their own work [37]. The most well-\nknown proposals regarding independent testbeds are ART [38] and TREET [37]. Recently,\nIrissappane and Zhang [39,40] made another important step forward by introducing a\npublicly available testbed that is able to reflect real environmental settings. We plan to\nuse their tool in future studies. Finally, we need to observe the usage of our repository in\npractice to learn from how users deal with it. This can either be done through conducting\nexperimental user studies or by interviewing developers who use our repository in a real\nenvironment. In this way, we can run through a continuous improvement process.\n\nConclusion\nThe research in trust and reputation systems is still growing. In this paper, we presented\nconcepts to foster reuse of existing approaches. We provided a hierarchical taxonomy of\ncomputation components from a functional view and described the implementation of\na component repository that serves as both a knowledge base and a service repository.\nIn this way, we communicate design knowledge for reuse, support the development of\nnew reputation systems and encourage researchers to focus on the development of single\ncomponents that can be integrated in various reputation systems to easily extend their\ncapabilities by new features. Matching the identified components and the properties of\ntrust, we found that integrating existing ideas and concepts can lead to a reputation sys-\ntem that widely reflects computational trust by addressing all properties of trust described\nin literature.\n\nAdditional file\n\nAdditional file 1: Table S1.\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAuthors’ contributions\nJS proposed the initial idea of this paper. He developed the hierarchical component taxonomy and implemented the\ncomponent repository. CR and JS conducted the evaluation of the proposed ideas. CR and JS furthermore revised this\npaper according to reviewers’ comments. GP supervised the research, contributed to the paper writing and made\nsuggestions. All authors read and approved the final manuscript.\n\nAcknowledgment\nThe research leading to these results was supported by the Bavarian State Ministry of Education, Science and the Arts\" as\npart of the FORSEC research association.\n\nReceived: 9 December 2014 Accepted: 1 April 2015\n\nReferences\n1. Electronics, Cars, Fashion, Collectibles, Coupons and More | eBay. http://www.ebay.com\n2. Yao Y, Ruohomaa S, Xu F (2012) Addressing common vulnerabilities of reputation systems for electronic commerce.\n\nJ Theor Appl Electron Commerce Res 7(1):1–20\n3. Tavakolifard M, Almeroth KC (2012) A taxonomy to express open challenges in trust and reputation systems. J\n\nCommun 7(7):538–551\n4. Hevner AR, March ST, Park J, Ram S (2004) Design science in information systems research. MIS Quarterly 28(1):75–105\n5. McKnight DH, Chervany NL (1996) The Meanings of Trust. Technical report. University of Minnesota, Management\n\nInformation Systems Research Center\n6. Gambetta D (1988) Can we trust trust? In: Gambetta D (ed). Trust: making and breaking cooperative relations. Basil\n\nBlackwell, Oxford. pp 213–237\n7. Artz D, Gil Y (2007) A survey of trust in computer science and the semantic web. Web Semantics 5(2):58–71\n8. Jøsang A, Ismail R, Boyd C (2007) A survey of trust and reputation systems for online service provision. Decis Support\n\nSyst 43(2):618–644\n\n Published online: 13 May 2015 \n\nhttp://www.journaloftrustmanagement.com/content/supplementary/s40493-015-0015-3-s1.pdf\nhttp://www.journaloftrustmanagement.com/content/supplementary/s40493-015-0015-3-s1.pdf\nhttp://www.ebay.com\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 20 of 21\n\n9. Rehak M, Gregor M, Pechoucek M, Bradshaw J (2006) Representing context for multiagent trust modeling. In:\nSkowron A, Barthès JP, Jain LC, Sun R, Morizet-Mahoudeaux P, Liu J, Zhong N (eds). Proceedings of the 2006\nIEEE/WIC/ACM International Conference on Intelligent Agent Technology, Hong Kong, China. IEEE Computer\nSociety, Washington, DC. pp 737–746\n\n10. Swamynathan G, Almeroth KC, Zhao BY (2010) The design of a reliable reputation system. Electron Commerce Res\n10(3–4):239–270\n\n11. Resnick P, Kuwabara K, Zeckhauser R, Friedman E (2000) Reputation systems. Commun ACM 43(12):45–48\n12. Zhang L, Jiang S, Zhang J, Ng WK (2012) Robustness of trust models and combinations for handling unfair ratings. In:\n\nDimitrakos T, Moona R, Patel D, McKnight DH (eds). Trust Management VI: Proceedings of the 6th IFIP WG 11.11\ninternational conference (IFIPTM). Springer, Berlin, Heidelberg, Surat, India. pp 36–51\n\n13. Noorian Z, Ulieru M (2010) The state of the art in trust and reputation systems: a framework for comparison. J Theor\nAppl Electron Commerce Res 5(2):97–117\n\n14. Sherchan W, Nepal S, Paris C (2013) A survey of trust in social networks. ACM Comput Surv 45(4):1–33\n15. Zacharia G, Moukas A, Maes P (2000) Collaborative reputation mechanisms for electronic marketplaces. Decis\n\nSupport Syst 29(4):371–388\n16. Whitby A, Jøsang A, Indulska J (2004) Filtering out unfair ratings in Bayesian reputation systems. In: Falcone R, Barber\n\nS, Sabater J, Singh M (eds). Proceedings of the third international joint conference on autonomous agents and multi\nagent systems, New, York, USA. IEEE Computer Society, Washington, DC. pp 106–117\n\n17. Liu S, Zhang J, Miao C, Theng Y-L, Kot AC (2011) iCLUB: an integrated clustering-based approach to improve the\nrobustness of reputation systems. In: Sonenberg L, Stone P, Tumer K, Yolum P (eds). Proceedings of the 10th\ninternational conference on Autonomous Agents and Multiagent Systems (AAMAS), Taipei, Taiwan. IFAAMAS,\nRichland, SC. pp 1151–1152\n\n18. Dellarocas C (2000) Immunizing online reputation reporting systems against unfair ratings and discriminatory\nbehavior. In: Jhingran A, MacKie J, Tygar D (eds). Proceedings of the 2nd ACM conference on electronic commerce,\nMinneapolis, MN. ACM, New York. pp 150–157\n\n19. Zhang H, Wang Y, Zhang X (2012) A trust vector approach to transaction context-aware trust evaluation in\ne-commerce and e-service environments. In: Shih C, Son S, Kuo T, Huemer C (eds). Proceedings of the 5th IEEE\ninternational conference on Service-Oriented Computing and Applications (SOCA). IEEE Computer Society\nWashington, DC, Taipei, Taiwan. pp 1–8\n\n20. Obergrusberger F, Baloglu B, Sänger J, Senk C (2013) Biometric identity trust: toward secure biometric enrollment in\nweb environments. In: Yousif M, Schubert L (eds). Proceedings of the 3rd international conference on Cloud\nComputing (CloudComp), Vienna, Austria. Springer, Berlin, Heidelberg. pp 124–133\n\n21. Brin S, Page L (1998) The anatomy of a large-scale hypertextual web search engine. Comput Networks\n30(1-7):107–177\n\n22. Epinions.com: Read expert reviews on Electronics, Cars, Books, Movies, Music and More. http://www.epinions.com/\n23. Sun Y, Han Z, Yu W, Ray Liu K (2006) Attacks on trust evaluation in distributed networks. In: Proceedings of Th 40th\n\nannual Conference on Information Sciences and Systems (CISS), Princeton, NJ, USA, IEEE Computer Society\nWashington, DC. pp 1461–1466\n\n24. Jøsang A, Ismail R (2002) The beta reputation system. In: Proceedings of the 15th bled conference on electronic\ncommerce, Bled, Slovenia. pp 41–55\n\n25. Slashdot: News for nerds, stuff that matters. http://www.slashdot.org/\n26. Amazon.com: Online Shopping for Electronics, Apparel, Computers, Books, DVDs & more. http://www.amazon.com\n27. Jøsang A (2001) A logic for uncertain probabilities. Int J Uncertainty Fuzziness Knowledge-Based Syst 9(3):279–311\n28. Yu B, Singh MP (2002) An evidential model of distributed reputation management. In: Proceedings of the first\n\nInternational Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), Bologna, Italy. ACM, New\nYork, NY. pp 294–301\n\n29. Malik Z, Akbar I, Bouguettaya A (2009) Web services reputation assessment using a Hidden Markov Model. In: Baresi\nL, Chi CH, Suzuki J (eds). Service-oriented computing: Proceedings of the 7th International Joint Conference on\nService-Oriented Computing (ICSOC-ServiceWave), Stockholm, Sweden. Springer Berlin, Heidelberg. pp 576–591\n\n30. Song S, Hwang K, Zhou R, Yu-Kwong K (2005) Trusted P2P transactions with fuzzy reputation aggregation, Vol. 9\n31. Sabater J, Sierra C (2002) Reputation and social network analysis in multi-agent systems. In: Proceedings of the first\n\nInternational joint conference on Autonomous Agents and Multiagent Systems (AAMAS), Bologna, Italy. ACM, New\nYork, NY. pp 475–482\n\n32. Kamvar SD, Schlosser MT, Garcia-Molina H (2003) The Eigentrust algorithm for reputation management in P2P\nnetworks. In: Hencsey G, White B, Chen YF, Kovács L, Lawrence S (eds). Proceedings of the 12th International\nConference on World Wide Web (WWW), Budapest, Hungary. ACM, New York, NY. pp 640–651\n\n33. Gamma E (1995) Design patterns: elements of reusable object-oriented software. Addison-Wesley, Reading\n34. Next Generation Online Trust. http://trust.bayforsec.de\n35. Jøsang A (2012) Robustness of trust and reputation systems: does it matter? In: Dimitrakos T, Moona R, Patel D,\n\nMcKnight DH (eds). Trust management VI: Proceedings of the 6th IFIP WG 11.11 International Conference (IFIPTM),\nSurat, India. Springer, Berlin, Heidelberg. pp 253–262\n\n36. Sänger J, Pernul G (2015) Reusable defense components for online reputation systems. In: Marsh S, Jensen CD,\nMurayma Y, Dimitrakos T (eds). Trust management IX: Proceedings of the 9th IFIP WG 11.11 International\nConference (IFIPTM), Hamburg, Germany. Springer, Berlin, Heidelberg\n\n37. Kerr R, Cohen R (2010) TREET: The Trust and Reputation Experimentation and Evaluation Testbed. Electron\nCommerce Res 10(3–4):271–290\n\n38. Fullam KK, Voss M, Klos TB, Muller G, Sabater J, Schlosser A, Topol Z, Barber KS, Rosenschein JS, Vercouter L (2005) A\nspecification of the Agent Reputation and Trust (ART) Testbed. In: Dignum F, Dignum V, Koenig S, Kraus S, Singh MP,\nWooldridge M (eds). Proceedings of the 4th international joint conference on Autonomous Agents and Multiagent\nSystems (AAMAS), Utrecht, Netherlands. ACM, New York, NY, USA. pp 512–518\n\nhttp://www.epinions.com/\nhttp://www.slashdot.org/\nhttp://www.amazon.com\nhttp://trust.bayforsec.de\n\n\nSänger et al. Journal of Trust Management  (2015) 2:5 Page 21 of 21\n\n39. Irissappane AA, Jiang S, Zhang J (2012) Towards a comprehensive Testbed to evaluate the robustness of reputation\nsystems against unfair rating attacks. In: Herder E, Yacef K, Chen L, Weibelzahl S (eds). Workshop and Poster\nProceedings of the 20th conference on User Modeling, Adaptation, and Personalization (UMAP),",
      "text": [
        "reputation system computation collection & storage & preparation filtering weighting aggregation communication input context, personalisation trust relation reputation value(s) output trustor trustee transaction/situation",
        "filtering weighting aggregation context comparability simple arithmetic attribute-based criteria compatibility credibility/propagation statistic statistic-based reliability rating value fuzzy time clustering-based graph-based personal preferences",
        "context comparability simple arithmetic attribute-based criteria compatibility credibility/propagation statistic statistic-based reliability rating value fuzzy time clustering-based service repository (implementation level)- graph-based personal preferences knowledge repository (conceptual level) filtering weighting aggregation",
        "Client Server filtering AgeBasedAbsolute Clustering implement/ inherit weighting CongruenceAbsolute Webservice Webservice abstract CallHelper CallHandler Component TimeDiscounting Relative aggregation CongruenceAbsolute TimeDiscounting Relative",
        "Filter: Weighting: referral reduced set age-based context similarity weighted referral Aggregation: referral reputation filter (absolut) (absolute set average value congruence) set",
        "Published online: 13 May 2015"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"reputation system computation collection & storage & preparation filtering weighting aggregation communication input context, personalisation trust relation reputation value(s) output trustor trustee transaction/situation\",\"lines\":[{\"boundingBox\":[{\"x\":1075,\"y\":9},{\"x\":1242,\"y\":8},{\"x\":1242,\"y\":32},{\"x\":1075,\"y\":33}],\"text\":\"reputation system\"},{\"boundingBox\":[{\"x\":612,\"y\":98},{\"x\":746,\"y\":97},{\"x\":746,\"y\":121},{\"x\":612,\"y\":122}],\"text\":\"computation\"},{\"boundingBox\":[{\"x\":182,\"y\":148},{\"x\":308,\"y\":146},{\"x\":308,\"y\":169},{\"x\":182,\"y\":171}],\"text\":\"collection &\"},{\"boundingBox\":[{\"x\":1065,\"y\":148},{\"x\":1174,\"y\":147},{\"x\":1174,\"y\":172},{\"x\":1066,\"y\":173}],\"text\":\"storage &\"},{\"boundingBox\":[{\"x\":181,\"y\":179},{\"x\":305,\"y\":177},{\"x\":305,\"y\":200},{\"x\":181,\"y\":202}],\"text\":\"preparation\"},{\"boundingBox\":[{\"x\":442,\"y\":170},{\"x\":521,\"y\":173},{\"x\":520,\"y\":197},{\"x\":441,\"y\":194}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":636,\"y\":172},{\"x\":742,\"y\":172},{\"x\":741,\"y\":198},{\"x\":636,\"y\":197}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":817,\"y\":174},{\"x\":945,\"y\":172},{\"x\":945,\"y\":195},{\"x\":818,\"y\":197}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":1036,\"y\":177},{\"x\":1200,\"y\":177},{\"x\":1200,\"y\":199},{\"x\":1036,\"y\":200}],\"text\":\"communication\"},{\"boundingBox\":[{\"x\":6,\"y\":425},{\"x\":54,\"y\":424},{\"x\":54,\"y\":445},{\"x\":6,\"y\":446}],\"text\":\"input\"},{\"boundingBox\":[{\"x\":261,\"y\":417},{\"x\":479,\"y\":416},{\"x\":479,\"y\":438},{\"x\":261,\"y\":439}],\"text\":\"context, personalisation\"},{\"boundingBox\":[{\"x\":619,\"y\":417},{\"x\":735,\"y\":417},{\"x\":735,\"y\":436},{\"x\":619,\"y\":436}],\"text\":\"trust relation\"},{\"boundingBox\":[{\"x\":918,\"y\":416},{\"x\":1094,\"y\":415},{\"x\":1094,\"y\":439},{\"x\":918,\"y\":439}],\"text\":\"reputation value(s)\"},{\"boundingBox\":[{\"x\":1220,\"y\":423},{\"x\":1284,\"y\":422},{\"x\":1284,\"y\":443},{\"x\":1220,\"y\":443}],\"text\":\"output\"},{\"boundingBox\":[{\"x\":490,\"y\":473},{\"x\":552,\"y\":473},{\"x\":552,\"y\":492},{\"x\":490,\"y\":492}],\"text\":\"trustor\"},{\"boundingBox\":[{\"x\":802,\"y\":475},{\"x\":869,\"y\":475},{\"x\":869,\"y\":494},{\"x\":802,\"y\":494}],\"text\":\"trustee\"},{\"boundingBox\":[{\"x\":1058,\"y\":467},{\"x\":1244,\"y\":467},{\"x\":1244,\"y\":487},{\"x\":1058,\"y\":487}],\"text\":\"transaction/situation\"}],\"words\":[{\"boundingBox\":[{\"x\":1076,\"y\":11},{\"x\":1168,\"y\":10},{\"x\":1169,\"y\":33},{\"x\":1077,\"y\":32}],\"text\":\"reputation\"},{\"boundingBox\":[{\"x\":1172,\"y\":10},{\"x\":1240,\"y\":9},{\"x\":1241,\"y\":32},{\"x\":1173,\"y\":33}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":613,\"y\":99},{\"x\":746,\"y\":97},{\"x\":746,\"y\":121},{\"x\":613,\"y\":121}],\"text\":\"computation\"},{\"boundingBox\":[{\"x\":182,\"y\":149},{\"x\":283,\"y\":147},{\"x\":284,\"y\":171},{\"x\":183,\"y\":171}],\"text\":\"collection\"},{\"boundingBox\":[{\"x\":288,\"y\":147},{\"x\":308,\"y\":147},{\"x\":308,\"y\":170},{\"x\":288,\"y\":171}],\"text\":\"&\"},{\"boundingBox\":[{\"x\":1066,\"y\":149},{\"x\":1149,\"y\":148},{\"x\":1149,\"y\":173},{\"x\":1066,\"y\":173}],\"text\":\"storage\"},{\"boundingBox\":[{\"x\":1154,\"y\":148},{\"x\":1174,\"y\":148},{\"x\":1174,\"y\":173},{\"x\":1154,\"y\":173}],\"text\":\"&\"},{\"boundingBox\":[{\"x\":181,\"y\":180},{\"x\":305,\"y\":177},{\"x\":305,\"y\":200},{\"x\":183,\"y\":203}],\"text\":\"preparation\"},{\"boundingBox\":[{\"x\":442,\"y\":171},{\"x\":521,\"y\":173},{\"x\":520,\"y\":197},{\"x\":441,\"y\":194}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":637,\"y\":174},{\"x\":742,\"y\":173},{\"x\":742,\"y\":199},{\"x\":637,\"y\":196}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":818,\"y\":176},{\"x\":945,\"y\":172},{\"x\":945,\"y\":195},{\"x\":819,\"y\":197}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":1037,\"y\":179},{\"x\":1200,\"y\":177},{\"x\":1200,\"y\":200},{\"x\":1038,\"y\":200}],\"text\":\"communication\"},{\"boundingBox\":[{\"x\":6,\"y\":425},{\"x\":53,\"y\":424},{\"x\":54,\"y\":445},{\"x\":6,\"y\":446}],\"text\":\"input\"},{\"boundingBox\":[{\"x\":261,\"y\":418},{\"x\":332,\"y\":418},{\"x\":333,\"y\":439},{\"x\":262,\"y\":438}],\"text\":\"context,\"},{\"boundingBox\":[{\"x\":336,\"y\":418},{\"x\":479,\"y\":417},{\"x\":479,\"y\":438},{\"x\":337,\"y\":439}],\"text\":\"personalisation\"},{\"boundingBox\":[{\"x\":619,\"y\":417},{\"x\":659,\"y\":417},{\"x\":659,\"y\":436},{\"x\":619,\"y\":436}],\"text\":\"trust\"},{\"boundingBox\":[{\"x\":663,\"y\":417},{\"x\":735,\"y\":417},{\"x\":734,\"y\":436},{\"x\":663,\"y\":436}],\"text\":\"relation\"},{\"boundingBox\":[{\"x\":919,\"y\":418},{\"x\":1014,\"y\":417},{\"x\":1014,\"y\":440},{\"x\":920,\"y\":439}],\"text\":\"reputation\"},{\"boundingBox\":[{\"x\":1018,\"y\":417},{\"x\":1094,\"y\":416},{\"x\":1093,\"y\":439},{\"x\":1018,\"y\":440}],\"text\":\"value(s)\"},{\"boundingBox\":[{\"x\":1220,\"y\":425},{\"x\":1284,\"y\":423},{\"x\":1284,\"y\":444},{\"x\":1222,\"y\":443}],\"text\":\"output\"},{\"boundingBox\":[{\"x\":491,\"y\":474},{\"x\":552,\"y\":474},{\"x\":552,\"y\":492},{\"x\":491,\"y\":492}],\"text\":\"trustor\"},{\"boundingBox\":[{\"x\":803,\"y\":476},{\"x\":869,\"y\":477},{\"x\":869,\"y\":494},{\"x\":802,\"y\":495}],\"text\":\"trustee\"},{\"boundingBox\":[{\"x\":1059,\"y\":468},{\"x\":1244,\"y\":467},{\"x\":1244,\"y\":488},{\"x\":1059,\"y\":488}],\"text\":\"transaction/situation\"}]}",
        "{\"language\":\"en\",\"text\":\"filtering weighting aggregation context comparability simple arithmetic attribute-based criteria compatibility credibility/propagation statistic statistic-based reliability rating value fuzzy time clustering-based graph-based personal preferences\",\"lines\":[{\"boundingBox\":[{\"x\":175,\"y\":16},{\"x\":318,\"y\":17},{\"x\":317,\"y\":62},{\"x\":175,\"y\":60}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":640,\"y\":16},{\"x\":824,\"y\":16},{\"x\":824,\"y\":62},{\"x\":640,\"y\":61}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":1072,\"y\":21},{\"x\":1292,\"y\":16},{\"x\":1293,\"y\":60},{\"x\":1073,\"y\":64}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":559,\"y\":130},{\"x\":869,\"y\":130},{\"x\":869,\"y\":167},{\"x\":559,\"y\":167}],\"text\":\"context comparability\"},{\"boundingBox\":[{\"x\":1055,\"y\":181},{\"x\":1307,\"y\":179},{\"x\":1307,\"y\":211},{\"x\":1055,\"y\":213}],\"text\":\"simple arithmetic\"},{\"boundingBox\":[{\"x\":137,\"y\":209},{\"x\":360,\"y\":208},{\"x\":360,\"y\":238},{\"x\":137,\"y\":239}],\"text\":\"attribute-based\"},{\"boundingBox\":[{\"x\":568,\"y\":250},{\"x\":859,\"y\":250},{\"x\":859,\"y\":287},{\"x\":568,\"y\":286}],\"text\":\"criteria compatibility\"},{\"boundingBox\":[{\"x\":554,\"y\":371},{\"x\":876,\"y\":370},{\"x\":876,\"y\":406},{\"x\":554,\"y\":407}],\"text\":\"credibility/propagation\"},{\"boundingBox\":[{\"x\":1127,\"y\":396},{\"x\":1240,\"y\":396},{\"x\":1240,\"y\":425},{\"x\":1128,\"y\":425}],\"text\":\"statistic\"},{\"boundingBox\":[{\"x\":141,\"y\":506},{\"x\":352,\"y\":506},{\"x\":352,\"y\":536},{\"x\":141,\"y\":537}],\"text\":\"statistic-based\"},{\"boundingBox\":[{\"x\":650,\"y\":495},{\"x\":780,\"y\":496},{\"x\":780,\"y\":530},{\"x\":650,\"y\":529}],\"text\":\"reliability\"},{\"boundingBox\":[{\"x\":628,\"y\":624},{\"x\":800,\"y\":623},{\"x\":801,\"y\":656},{\"x\":628,\"y\":656}],\"text\":\"rating value\"},{\"boundingBox\":[{\"x\":1141,\"y\":606},{\"x\":1220,\"y\":610},{\"x\":1219,\"y\":638},{\"x\":1140,\"y\":635}],\"text\":\"fuzzy\"},{\"boundingBox\":[{\"x\":681,\"y\":750},{\"x\":749,\"y\":751},{\"x\":748,\"y\":779},{\"x\":680,\"y\":779}],\"text\":\"time\"},{\"boundingBox\":[{\"x\":123,\"y\":799},{\"x\":370,\"y\":798},{\"x\":370,\"y\":831},{\"x\":123,\"y\":832}],\"text\":\"clustering-based\"},{\"boundingBox\":[{\"x\":1085,\"y\":826},{\"x\":1276,\"y\":820},{\"x\":1277,\"y\":851},{\"x\":1086,\"y\":857}],\"text\":\"graph-based\"},{\"boundingBox\":[{\"x\":557,\"y\":873},{\"x\":872,\"y\":872},{\"x\":872,\"y\":903},{\"x\":557,\"y\":905}],\"text\":\"personal preferences\"}],\"words\":[{\"boundingBox\":[{\"x\":175,\"y\":17},{\"x\":316,\"y\":18},{\"x\":316,\"y\":63},{\"x\":176,\"y\":61}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":641,\"y\":17},{\"x\":823,\"y\":17},{\"x\":823,\"y\":63},{\"x\":641,\"y\":62}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":1073,\"y\":22},{\"x\":1292,\"y\":17},{\"x\":1294,\"y\":61},{\"x\":1073,\"y\":65}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":560,\"y\":133},{\"x\":665,\"y\":132},{\"x\":665,\"y\":165},{\"x\":561,\"y\":164}],\"text\":\"context\"},{\"boundingBox\":[{\"x\":671,\"y\":132},{\"x\":869,\"y\":130},{\"x\":868,\"y\":168},{\"x\":671,\"y\":165}],\"text\":\"comparability\"},{\"boundingBox\":[{\"x\":1055,\"y\":181},{\"x\":1150,\"y\":181},{\"x\":1150,\"y\":213},{\"x\":1056,\"y\":214}],\"text\":\"simple\"},{\"boundingBox\":[{\"x\":1157,\"y\":181},{\"x\":1306,\"y\":180},{\"x\":1305,\"y\":212},{\"x\":1156,\"y\":213}],\"text\":\"arithmetic\"},{\"boundingBox\":[{\"x\":137,\"y\":210},{\"x\":359,\"y\":208},{\"x\":359,\"y\":239},{\"x\":138,\"y\":240}],\"text\":\"attribute-based\"},{\"boundingBox\":[{\"x\":569,\"y\":253},{\"x\":666,\"y\":252},{\"x\":667,\"y\":286},{\"x\":569,\"y\":284}],\"text\":\"criteria\"},{\"boundingBox\":[{\"x\":672,\"y\":252},{\"x\":859,\"y\":250},{\"x\":859,\"y\":288},{\"x\":673,\"y\":286}],\"text\":\"compatibility\"},{\"boundingBox\":[{\"x\":555,\"y\":372},{\"x\":875,\"y\":370},{\"x\":875,\"y\":406},{\"x\":554,\"y\":405}],\"text\":\"credibility/propagation\"},{\"boundingBox\":[{\"x\":1128,\"y\":397},{\"x\":1240,\"y\":396},{\"x\":1240,\"y\":425},{\"x\":1129,\"y\":426}],\"text\":\"statistic\"},{\"boundingBox\":[{\"x\":142,\"y\":507},{\"x\":351,\"y\":507},{\"x\":350,\"y\":536},{\"x\":142,\"y\":537}],\"text\":\"statistic-based\"},{\"boundingBox\":[{\"x\":651,\"y\":497},{\"x\":781,\"y\":497},{\"x\":779,\"y\":531},{\"x\":651,\"y\":528}],\"text\":\"reliability\"},{\"boundingBox\":[{\"x\":628,\"y\":625},{\"x\":713,\"y\":624},{\"x\":713,\"y\":657},{\"x\":628,\"y\":656}],\"text\":\"rating\"},{\"boundingBox\":[{\"x\":719,\"y\":624},{\"x\":801,\"y\":624},{\"x\":801,\"y\":656},{\"x\":719,\"y\":657}],\"text\":\"value\"},{\"boundingBox\":[{\"x\":1141,\"y\":606},{\"x\":1218,\"y\":609},{\"x\":1217,\"y\":638},{\"x\":1140,\"y\":634}],\"text\":\"fuzzy\"},{\"boundingBox\":[{\"x\":680,\"y\":750},{\"x\":748,\"y\":750},{\"x\":747,\"y\":779},{\"x\":680,\"y\":778}],\"text\":\"time\"},{\"boundingBox\":[{\"x\":124,\"y\":799},{\"x\":371,\"y\":799},{\"x\":370,\"y\":831},{\"x\":125,\"y\":832}],\"text\":\"clustering-based\"},{\"boundingBox\":[{\"x\":1086,\"y\":826},{\"x\":1277,\"y\":821},{\"x\":1276,\"y\":852},{\"x\":1087,\"y\":858}],\"text\":\"graph-based\"},{\"boundingBox\":[{\"x\":558,\"y\":875},{\"x\":684,\"y\":873},{\"x\":684,\"y\":905},{\"x\":559,\"y\":905}],\"text\":\"personal\"},{\"boundingBox\":[{\"x\":690,\"y\":873},{\"x\":872,\"y\":873},{\"x\":871,\"y\":903},{\"x\":690,\"y\":905}],\"text\":\"preferences\"}]}",
        "{\"language\":\"en\",\"text\":\"context comparability simple arithmetic attribute-based criteria compatibility credibility/propagation statistic statistic-based reliability rating value fuzzy time clustering-based service repository (implementation level)- graph-based personal preferences knowledge repository (conceptual level) filtering weighting aggregation\",\"lines\":[{\"boundingBox\":[{\"x\":805,\"y\":101},{\"x\":1022,\"y\":100},{\"x\":1022,\"y\":126},{\"x\":805,\"y\":126}],\"text\":\"context comparability\"},{\"boundingBox\":[{\"x\":1097,\"y\":142},{\"x\":1272,\"y\":141},{\"x\":1272,\"y\":163},{\"x\":1097,\"y\":164}],\"text\":\"simple arithmetic\"},{\"boundingBox\":[{\"x\":565,\"y\":160},{\"x\":719,\"y\":160},{\"x\":719,\"y\":182},{\"x\":565,\"y\":182}],\"text\":\"attribute-based\"},{\"boundingBox\":[{\"x\":812,\"y\":192},{\"x\":1014,\"y\":192},{\"x\":1014,\"y\":220},{\"x\":812,\"y\":220}],\"text\":\"criteria compatibility\"},{\"boundingBox\":[{\"x\":801,\"y\":287},{\"x\":1026,\"y\":287},{\"x\":1026,\"y\":313},{\"x\":801,\"y\":312}],\"text\":\"credibility/propagation\"},{\"boundingBox\":[{\"x\":1145,\"y\":308},{\"x\":1223,\"y\":308},{\"x\":1223,\"y\":329},{\"x\":1145,\"y\":330}],\"text\":\"statistic\"},{\"boundingBox\":[{\"x\":569,\"y\":379},{\"x\":715,\"y\":379},{\"x\":715,\"y\":401},{\"x\":570,\"y\":401}],\"text\":\"statistic-based\"},{\"boundingBox\":[{\"x\":867,\"y\":378},{\"x\":959,\"y\":378},{\"x\":959,\"y\":403},{\"x\":867,\"y\":402}],\"text\":\"reliability\"},{\"boundingBox\":[{\"x\":853,\"y\":473},{\"x\":973,\"y\":473},{\"x\":973,\"y\":497},{\"x\":854,\"y\":497}],\"text\":\"rating value\"},{\"boundingBox\":[{\"x\":1156,\"y\":470},{\"x\":1212,\"y\":474},{\"x\":1211,\"y\":495},{\"x\":1155,\"y\":492}],\"text\":\"fuzzy\"},{\"boundingBox\":[{\"x\":890,\"y\":566},{\"x\":936,\"y\":567},{\"x\":935,\"y\":587},{\"x\":890,\"y\":586}],\"text\":\"time\"},{\"boundingBox\":[{\"x\":558,\"y\":603},{\"x\":728,\"y\":602},{\"x\":728,\"y\":626},{\"x\":558,\"y\":626}],\"text\":\"clustering-based\"},{\"boundingBox\":[{\"x\":0,\"y\":646},{\"x\":409,\"y\":645},{\"x\":409,\"y\":670},{\"x\":0,\"y\":672}],\"text\":\"service repository (implementation level)-\"},{\"boundingBox\":[{\"x\":1119,\"y\":630},{\"x\":1249,\"y\":627},{\"x\":1250,\"y\":650},{\"x\":1119,\"y\":654}],\"text\":\"graph-based\"},{\"boundingBox\":[{\"x\":806,\"y\":663},{\"x\":1021,\"y\":661},{\"x\":1021,\"y\":685},{\"x\":806,\"y\":687}],\"text\":\"personal preferences\"},{\"boundingBox\":[{\"x\":40,\"y\":689},{\"x\":438,\"y\":688},{\"x\":438,\"y\":715},{\"x\":40,\"y\":716}],\"text\":\"knowledge repository (conceptual level)\"},{\"boundingBox\":[{\"x\":611,\"y\":765},{\"x\":689,\"y\":766},{\"x\":689,\"y\":788},{\"x\":611,\"y\":787}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":863,\"y\":767},{\"x\":961,\"y\":767},{\"x\":961,\"y\":787},{\"x\":863,\"y\":788}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":1114,\"y\":768},{\"x\":1238,\"y\":767},{\"x\":1238,\"y\":786},{\"x\":1114,\"y\":788}],\"text\":\"aggregation\"}],\"words\":[{\"boundingBox\":[{\"x\":806,\"y\":103},{\"x\":879,\"y\":102},{\"x\":879,\"y\":126},{\"x\":807,\"y\":125}],\"text\":\"context\"},{\"boundingBox\":[{\"x\":883,\"y\":102},{\"x\":1022,\"y\":101},{\"x\":1020,\"y\":127},{\"x\":883,\"y\":126}],\"text\":\"comparability\"},{\"boundingBox\":[{\"x\":1098,\"y\":142},{\"x\":1162,\"y\":142},{\"x\":1163,\"y\":165},{\"x\":1098,\"y\":165}],\"text\":\"simple\"},{\"boundingBox\":[{\"x\":1167,\"y\":142},{\"x\":1271,\"y\":141},{\"x\":1272,\"y\":164},{\"x\":1167,\"y\":165}],\"text\":\"arithmetic\"},{\"boundingBox\":[{\"x\":566,\"y\":161},{\"x\":720,\"y\":161},{\"x\":719,\"y\":183},{\"x\":566,\"y\":183}],\"text\":\"attribute-based\"},{\"boundingBox\":[{\"x\":813,\"y\":194},{\"x\":879,\"y\":194},{\"x\":880,\"y\":219},{\"x\":814,\"y\":218}],\"text\":\"criteria\"},{\"boundingBox\":[{\"x\":884,\"y\":194},{\"x\":1015,\"y\":192},{\"x\":1014,\"y\":221},{\"x\":884,\"y\":219}],\"text\":\"compatibility\"},{\"boundingBox\":[{\"x\":802,\"y\":288},{\"x\":1026,\"y\":288},{\"x\":1026,\"y\":313},{\"x\":803,\"y\":311}],\"text\":\"credibility/propagation\"},{\"boundingBox\":[{\"x\":1145,\"y\":308},{\"x\":1222,\"y\":308},{\"x\":1222,\"y\":330},{\"x\":1145,\"y\":331}],\"text\":\"statistic\"},{\"boundingBox\":[{\"x\":570,\"y\":380},{\"x\":715,\"y\":380},{\"x\":714,\"y\":401},{\"x\":570,\"y\":401}],\"text\":\"statistic-based\"},{\"boundingBox\":[{\"x\":867,\"y\":379},{\"x\":960,\"y\":379},{\"x\":959,\"y\":403},{\"x\":867,\"y\":403}],\"text\":\"reliability\"},{\"boundingBox\":[{\"x\":854,\"y\":475},{\"x\":911,\"y\":473},{\"x\":911,\"y\":498},{\"x\":854,\"y\":497}],\"text\":\"rating\"},{\"boundingBox\":[{\"x\":915,\"y\":473},{\"x\":973,\"y\":473},{\"x\":973,\"y\":498},{\"x\":915,\"y\":498}],\"text\":\"value\"},{\"boundingBox\":[{\"x\":1156,\"y\":470},{\"x\":1212,\"y\":473},{\"x\":1210,\"y\":495},{\"x\":1155,\"y\":491}],\"text\":\"fuzzy\"},{\"boundingBox\":[{\"x\":890,\"y\":566},{\"x\":935,\"y\":567},{\"x\":935,\"y\":587},{\"x\":890,\"y\":586}],\"text\":\"time\"},{\"boundingBox\":[{\"x\":559,\"y\":603},{\"x\":729,\"y\":603},{\"x\":728,\"y\":626},{\"x\":560,\"y\":626}],\"text\":\"clustering-based\"},{\"boundingBox\":[{\"x\":0,\"y\":649},{\"x\":69,\"y\":648},{\"x\":69,\"y\":671},{\"x\":0,\"y\":670}],\"text\":\"service\"},{\"boundingBox\":[{\"x\":73,\"y\":647},{\"x\":176,\"y\":646},{\"x\":176,\"y\":672},{\"x\":73,\"y\":671}],\"text\":\"repository\"},{\"boundingBox\":[{\"x\":180,\"y\":646},{\"x\":341,\"y\":645},{\"x\":342,\"y\":671},{\"x\":180,\"y\":672}],\"text\":\"(implementation\"},{\"boundingBox\":[{\"x\":345,\"y\":645},{\"x\":409,\"y\":646},{\"x\":409,\"y\":670},{\"x\":346,\"y\":671}],\"text\":\"level)-\"},{\"boundingBox\":[{\"x\":1119,\"y\":631},{\"x\":1249,\"y\":628},{\"x\":1249,\"y\":651},{\"x\":1121,\"y\":654}],\"text\":\"graph-based\"},{\"boundingBox\":[{\"x\":806,\"y\":665},{\"x\":893,\"y\":662},{\"x\":893,\"y\":687},{\"x\":807,\"y\":687}],\"text\":\"personal\"},{\"boundingBox\":[{\"x\":897,\"y\":662},{\"x\":1021,\"y\":663},{\"x\":1019,\"y\":686},{\"x\":897,\"y\":687}],\"text\":\"preferences\"},{\"boundingBox\":[{\"x\":41,\"y\":690},{\"x\":150,\"y\":690},{\"x\":149,\"y\":716},{\"x\":41,\"y\":714}],\"text\":\"knowledge\"},{\"boundingBox\":[{\"x\":154,\"y\":690},{\"x\":255,\"y\":689},{\"x\":255,\"y\":716},{\"x\":154,\"y\":716}],\"text\":\"repository\"},{\"boundingBox\":[{\"x\":260,\"y\":689},{\"x\":377,\"y\":689},{\"x\":377,\"y\":716},{\"x\":259,\"y\":716}],\"text\":\"(conceptual\"},{\"boundingBox\":[{\"x\":382,\"y\":689},{\"x\":438,\"y\":689},{\"x\":438,\"y\":714},{\"x\":382,\"y\":715}],\"text\":\"level)\"},{\"boundingBox\":[{\"x\":611,\"y\":766},{\"x\":688,\"y\":767},{\"x\":689,\"y\":788},{\"x\":611,\"y\":788}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":864,\"y\":768},{\"x\":962,\"y\":768},{\"x\":962,\"y\":787},{\"x\":864,\"y\":788}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":1115,\"y\":769},{\"x\":1237,\"y\":767},{\"x\":1237,\"y\":787},{\"x\":1115,\"y\":788}],\"text\":\"aggregation\"}]}",
        "{\"language\":\"en\",\"text\":\"Client Server filtering AgeBasedAbsolute Clustering implement/ inherit weighting CongruenceAbsolute Webservice Webservice abstract CallHelper CallHandler Component TimeDiscounting Relative aggregation CongruenceAbsolute TimeDiscounting Relative\",\"lines\":[{\"boundingBox\":[{\"x\":204,\"y\":10},{\"x\":274,\"y\":11},{\"x\":273,\"y\":36},{\"x\":204,\"y\":34}],\"text\":\"Client\"},{\"boundingBox\":[{\"x\":289,\"y\":11},{\"x\":369,\"y\":12},{\"x\":369,\"y\":35},{\"x\":289,\"y\":34}],\"text\":\"Server\"},{\"boundingBox\":[{\"x\":693,\"y\":78},{\"x\":762,\"y\":81},{\"x\":762,\"y\":102},{\"x\":692,\"y\":99}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":822,\"y\":116},{\"x\":1002,\"y\":114},{\"x\":1002,\"y\":136},{\"x\":822,\"y\":138}],\"text\":\"AgeBasedAbsolute\"},{\"boundingBox\":[{\"x\":865,\"y\":216},{\"x\":960,\"y\":218},{\"x\":959,\"y\":240},{\"x\":865,\"y\":238}],\"text\":\"Clustering\"},{\"boundingBox\":[{\"x\":1114,\"y\":220},{\"x\":1221,\"y\":218},{\"x\":1222,\"y\":240},{\"x\":1114,\"y\":242}],\"text\":\"implement/\"},{\"boundingBox\":[{\"x\":1137,\"y\":244},{\"x\":1197,\"y\":244},{\"x\":1197,\"y\":265},{\"x\":1137,\"y\":265}],\"text\":\"inherit\"},{\"boundingBox\":[{\"x\":691,\"y\":315},{\"x\":783,\"y\":316},{\"x\":782,\"y\":338},{\"x\":691,\"y\":337}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":813,\"y\":351},{\"x\":1011,\"y\":351},{\"x\":1011,\"y\":374},{\"x\":813,\"y\":373}],\"text\":\"CongruenceAbsolute\"},{\"boundingBox\":[{\"x\":42,\"y\":380},{\"x\":182,\"y\":381},{\"x\":182,\"y\":406},{\"x\":42,\"y\":404}],\"text\":\"Webservice\"},{\"boundingBox\":[{\"x\":388,\"y\":380},{\"x\":525,\"y\":381},{\"x\":524,\"y\":406},{\"x\":388,\"y\":405}],\"text\":\"Webservice\"},{\"boundingBox\":[{\"x\":1251,\"y\":385},{\"x\":1346,\"y\":385},{\"x\":1346,\"y\":408},{\"x\":1251,\"y\":408}],\"text\":\"abstract\"},{\"boundingBox\":[{\"x\":50,\"y\":411},{\"x\":173,\"y\":413},{\"x\":172,\"y\":440},{\"x\":49,\"y\":437}],\"text\":\"CallHelper\"},{\"boundingBox\":[{\"x\":388,\"y\":411},{\"x\":526,\"y\":411},{\"x\":526,\"y\":436},{\"x\":388,\"y\":436}],\"text\":\"CallHandler\"},{\"boundingBox\":[{\"x\":1230,\"y\":416},{\"x\":1367,\"y\":416},{\"x\":1367,\"y\":442},{\"x\":1230,\"y\":442}],\"text\":\"Component\"},{\"boundingBox\":[{\"x\":833,\"y\":441},{\"x\":990,\"y\":441},{\"x\":990,\"y\":464},{\"x\":833,\"y\":463}],\"text\":\"TimeDiscounting\"},{\"boundingBox\":[{\"x\":874,\"y\":467},{\"x\":951,\"y\":466},{\"x\":951,\"y\":485},{\"x\":874,\"y\":486}],\"text\":\"Relative\"},{\"boundingBox\":[{\"x\":690,\"y\":554},{\"x\":802,\"y\":551},{\"x\":803,\"y\":572},{\"x\":690,\"y\":575}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":814,\"y\":588},{\"x\":1010,\"y\":587},{\"x\":1010,\"y\":611},{\"x\":814,\"y\":611}],\"text\":\"CongruenceAbsolute\"},{\"boundingBox\":[{\"x\":834,\"y\":676},{\"x\":992,\"y\":677},{\"x\":991,\"y\":700},{\"x\":834,\"y\":698}],\"text\":\"TimeDiscounting\"},{\"boundingBox\":[{\"x\":874,\"y\":702},{\"x\":951,\"y\":702},{\"x\":951,\"y\":721},{\"x\":874,\"y\":722}],\"text\":\"Relative\"}],\"words\":[{\"boundingBox\":[{\"x\":204,\"y\":10},{\"x\":274,\"y\":11},{\"x\":273,\"y\":36},{\"x\":204,\"y\":34}],\"text\":\"Client\"},{\"boundingBox\":[{\"x\":290,\"y\":11},{\"x\":370,\"y\":12},{\"x\":369,\"y\":36},{\"x\":290,\"y\":35}],\"text\":\"Server\"},{\"boundingBox\":[{\"x\":694,\"y\":79},{\"x\":762,\"y\":82},{\"x\":761,\"y\":102},{\"x\":693,\"y\":100}],\"text\":\"filtering\"},{\"boundingBox\":[{\"x\":822,\"y\":116},{\"x\":1002,\"y\":115},{\"x\":1001,\"y\":137},{\"x\":822,\"y\":139}],\"text\":\"AgeBasedAbsolute\"},{\"boundingBox\":[{\"x\":866,\"y\":217},{\"x\":960,\"y\":219},{\"x\":958,\"y\":240},{\"x\":866,\"y\":238}],\"text\":\"Clustering\"},{\"boundingBox\":[{\"x\":1114,\"y\":220},{\"x\":1221,\"y\":218},{\"x\":1220,\"y\":240},{\"x\":1115,\"y\":242}],\"text\":\"implement/\"},{\"boundingBox\":[{\"x\":1138,\"y\":245},{\"x\":1198,\"y\":245},{\"x\":1198,\"y\":265},{\"x\":1137,\"y\":265}],\"text\":\"inherit\"},{\"boundingBox\":[{\"x\":692,\"y\":316},{\"x\":782,\"y\":317},{\"x\":781,\"y\":339},{\"x\":692,\"y\":337}],\"text\":\"weighting\"},{\"boundingBox\":[{\"x\":814,\"y\":352},{\"x\":1011,\"y\":352},{\"x\":1011,\"y\":374},{\"x\":816,\"y\":374}],\"text\":\"CongruenceAbsolute\"},{\"boundingBox\":[{\"x\":43,\"y\":380},{\"x\":182,\"y\":382},{\"x\":181,\"y\":407},{\"x\":43,\"y\":405}],\"text\":\"Webservice\"},{\"boundingBox\":[{\"x\":389,\"y\":380},{\"x\":524,\"y\":381},{\"x\":524,\"y\":405},{\"x\":388,\"y\":404}],\"text\":\"Webservice\"},{\"boundingBox\":[{\"x\":1251,\"y\":386},{\"x\":1345,\"y\":386},{\"x\":1344,\"y\":409},{\"x\":1252,\"y\":409}],\"text\":\"abstract\"},{\"boundingBox\":[{\"x\":50,\"y\":412},{\"x\":173,\"y\":414},{\"x\":173,\"y\":440},{\"x\":50,\"y\":438}],\"text\":\"CallHelper\"},{\"boundingBox\":[{\"x\":389,\"y\":411},{\"x\":527,\"y\":412},{\"x\":526,\"y\":437},{\"x\":388,\"y\":436}],\"text\":\"CallHandler\"},{\"boundingBox\":[{\"x\":1231,\"y\":416},{\"x\":1367,\"y\":417},{\"x\":1367,\"y\":442},{\"x\":1231,\"y\":442}],\"text\":\"Component\"},{\"boundingBox\":[{\"x\":834,\"y\":441},{\"x\":990,\"y\":441},{\"x\":989,\"y\":465},{\"x\":835,\"y\":462}],\"text\":\"TimeDiscounting\"},{\"boundingBox\":[{\"x\":875,\"y\":467},{\"x\":951,\"y\":466},{\"x\":950,\"y\":486},{\"x\":874,\"y\":486}],\"text\":\"Relative\"},{\"boundingBox\":[{\"x\":691,\"y\":554},{\"x\":802,\"y\":552},{\"x\":802,\"y\":573},{\"x\":691,\"y\":575}],\"text\":\"aggregation\"},{\"boundingBox\":[{\"x\":815,\"y\":589},{\"x\":1011,\"y\":588},{\"x\":1010,\"y\":611},{\"x\":815,\"y\":611}],\"text\":\"CongruenceAbsolute\"},{\"boundingBox\":[{\"x\":834,\"y\":676},{\"x\":991,\"y\":678},{\"x\":991,\"y\":701},{\"x\":834,\"y\":698}],\"text\":\"TimeDiscounting\"},{\"boundingBox\":[{\"x\":874,\"y\":702},{\"x\":951,\"y\":702},{\"x\":951,\"y\":722},{\"x\":875,\"y\":722}],\"text\":\"Relative\"}]}",
        "{\"language\":\"en\",\"text\":\"Filter: Weighting: referral reduced set age-based context similarity weighted referral Aggregation: referral reputation filter (absolut) (absolute set average value congruence) set\",\"lines\":[{\"boundingBox\":[{\"x\":245,\"y\":20},{\"x\":303,\"y\":20},{\"x\":302,\"y\":41},{\"x\":244,\"y\":39}],\"text\":\"Filter:\"},{\"boundingBox\":[{\"x\":669,\"y\":4},{\"x\":783,\"y\":6},{\"x\":783,\"y\":31},{\"x\":669,\"y\":29}],\"text\":\"Weighting:\"},{\"boundingBox\":[{\"x\":26,\"y\":35},{\"x\":103,\"y\":34},{\"x\":103,\"y\":55},{\"x\":26,\"y\":56}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":450,\"y\":26},{\"x\":539,\"y\":25},{\"x\":540,\"y\":46},{\"x\":450,\"y\":47}],\"text\":\"reduced\"},{\"boundingBox\":[{\"x\":47,\"y\":65},{\"x\":83,\"y\":64},{\"x\":84,\"y\":83},{\"x\":48,\"y\":84}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":216,\"y\":51},{\"x\":331,\"y\":48},{\"x\":331,\"y\":70},{\"x\":216,\"y\":72}],\"text\":\"age-based\"},{\"boundingBox\":[{\"x\":638,\"y\":34},{\"x\":815,\"y\":33},{\"x\":815,\"y\":57},{\"x\":638,\"y\":58}],\"text\":\"context similarity\"},{\"boundingBox\":[{\"x\":916,\"y\":26},{\"x\":1011,\"y\":25},{\"x\":1011,\"y\":47},{\"x\":916,\"y\":48}],\"text\":\"weighted\"},{\"boundingBox\":[{\"x\":456,\"y\":55},{\"x\":533,\"y\":53},{\"x\":534,\"y\":73},{\"x\":456,\"y\":75}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":1110,\"y\":34},{\"x\":1247,\"y\":33},{\"x\":1247,\"y\":57},{\"x\":1110,\"y\":58}],\"text\":\"Aggregation:\"},{\"boundingBox\":[{\"x\":926,\"y\":54},{\"x\":1004,\"y\":52},{\"x\":1004,\"y\":74},{\"x\":926,\"y\":76}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":1329,\"y\":40},{\"x\":1436,\"y\":38},{\"x\":1437,\"y\":61},{\"x\":1329,\"y\":64}],\"text\":\"reputation\"},{\"boundingBox\":[{\"x\":201,\"y\":76},{\"x\":346,\"y\":76},{\"x\":346,\"y\":100},{\"x\":201,\"y\":99}],\"text\":\"filter (absolut)\"},{\"boundingBox\":[{\"x\":676,\"y\":63},{\"x\":777,\"y\":63},{\"x\":777,\"y\":85},{\"x\":677,\"y\":86}],\"text\":\"(absolute\"},{\"boundingBox\":[{\"x\":479,\"y\":84},{\"x\":512,\"y\":84},{\"x\":513,\"y\":102},{\"x\":479,\"y\":102}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":1135,\"y\":65},{\"x\":1224,\"y\":65},{\"x\":1224,\"y\":87},{\"x\":1135,\"y\":86}],\"text\":\"average\"},{\"boundingBox\":[{\"x\":1353,\"y\":68},{\"x\":1413,\"y\":68},{\"x\":1413,\"y\":88},{\"x\":1353,\"y\":89}],\"text\":\"value\"},{\"boundingBox\":[{\"x\":660,\"y\":94},{\"x\":793,\"y\":93},{\"x\":793,\"y\":114},{\"x\":660,\"y\":115}],\"text\":\"congruence)\"},{\"boundingBox\":[{\"x\":947,\"y\":84},{\"x\":983,\"y\":83},{\"x\":983,\"y\":101},{\"x\":947,\"y\":102}],\"text\":\"set\"}],\"words\":[{\"boundingBox\":[{\"x\":244,\"y\":20},{\"x\":301,\"y\":20},{\"x\":301,\"y\":41},{\"x\":244,\"y\":40}],\"text\":\"Filter:\"},{\"boundingBox\":[{\"x\":669,\"y\":5},{\"x\":784,\"y\":6},{\"x\":783,\"y\":31},{\"x\":670,\"y\":29}],\"text\":\"Weighting:\"},{\"boundingBox\":[{\"x\":27,\"y\":36},{\"x\":103,\"y\":34},{\"x\":103,\"y\":56},{\"x\":28,\"y\":56}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":451,\"y\":27},{\"x\":540,\"y\":25},{\"x\":540,\"y\":46},{\"x\":451,\"y\":47}],\"text\":\"reduced\"},{\"boundingBox\":[{\"x\":47,\"y\":65},{\"x\":83,\"y\":64},{\"x\":84,\"y\":83},{\"x\":47,\"y\":84}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":216,\"y\":51},{\"x\":331,\"y\":49},{\"x\":331,\"y\":71},{\"x\":217,\"y\":73}],\"text\":\"age-based\"},{\"boundingBox\":[{\"x\":639,\"y\":37},{\"x\":715,\"y\":35},{\"x\":715,\"y\":57},{\"x\":639,\"y\":57}],\"text\":\"context\"},{\"boundingBox\":[{\"x\":719,\"y\":35},{\"x\":815,\"y\":34},{\"x\":814,\"y\":58},{\"x\":719,\"y\":57}],\"text\":\"similarity\"},{\"boundingBox\":[{\"x\":916,\"y\":28},{\"x\":1012,\"y\":26},{\"x\":1012,\"y\":47},{\"x\":917,\"y\":47}],\"text\":\"weighted\"},{\"boundingBox\":[{\"x\":456,\"y\":56},{\"x\":534,\"y\":54},{\"x\":533,\"y\":74},{\"x\":457,\"y\":74}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":1111,\"y\":35},{\"x\":1247,\"y\":33},{\"x\":1246,\"y\":58},{\"x\":1110,\"y\":58}],\"text\":\"Aggregation:\"},{\"boundingBox\":[{\"x\":927,\"y\":55},{\"x\":1004,\"y\":52},{\"x\":1004,\"y\":75},{\"x\":927,\"y\":75}],\"text\":\"referral\"},{\"boundingBox\":[{\"x\":1329,\"y\":41},{\"x\":1436,\"y\":38},{\"x\":1436,\"y\":62},{\"x\":1329,\"y\":64}],\"text\":\"reputation\"},{\"boundingBox\":[{\"x\":201,\"y\":76},{\"x\":245,\"y\":77},{\"x\":245,\"y\":100},{\"x\":201,\"y\":100}],\"text\":\"filter\"},{\"boundingBox\":[{\"x\":250,\"y\":77},{\"x\":346,\"y\":77},{\"x\":345,\"y\":101},{\"x\":250,\"y\":100}],\"text\":\"(absolut)\"},{\"boundingBox\":[{\"x\":677,\"y\":64},{\"x\":777,\"y\":64},{\"x\":776,\"y\":85},{\"x\":677,\"y\":87}],\"text\":\"(absolute\"},{\"boundingBox\":[{\"x\":479,\"y\":84},{\"x\":513,\"y\":84},{\"x\":513,\"y\":102},{\"x\":479,\"y\":102}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":1136,\"y\":65},{\"x\":1225,\"y\":65},{\"x\":1224,\"y\":88},{\"x\":1136,\"y\":86}],\"text\":\"average\"},{\"boundingBox\":[{\"x\":1354,\"y\":69},{\"x\":1413,\"y\":68},{\"x\":1414,\"y\":88},{\"x\":1354,\"y\":89}],\"text\":\"value\"},{\"boundingBox\":[{\"x\":661,\"y\":95},{\"x\":792,\"y\":93},{\"x\":792,\"y\":115},{\"x\":660,\"y\":115}],\"text\":\"congruence)\"},{\"boundingBox\":[{\"x\":947,\"y\":84},{\"x\":982,\"y\":83},{\"x\":983,\"y\":101},{\"x\":947,\"y\":102}],\"text\":\"set\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 13 May 2015\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":15},{\"x\":897,\"y\":16},{\"x\":896,\"y\":72},{\"x\":0,\"y\":70}],\"text\":\"Published online: 13 May 2015\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":18},{\"x\":284,\"y\":16},{\"x\":284,\"y\":71},{\"x\":0,\"y\":68}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":294,\"y\":16},{\"x\":503,\"y\":16},{\"x\":503,\"y\":73},{\"x\":294,\"y\":71}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":512,\"y\":16},{\"x\":587,\"y\":16},{\"x\":588,\"y\":73},{\"x\":513,\"y\":73}],\"text\":\"13\"},{\"boundingBox\":[{\"x\":597,\"y\":16},{\"x\":737,\"y\":16},{\"x\":738,\"y\":73},{\"x\":598,\"y\":73}],\"text\":\"May\"},{\"boundingBox\":[{\"x\":747,\"y\":16},{\"x\":894,\"y\":17},{\"x\":895,\"y\":73},{\"x\":748,\"y\":73}],\"text\":\"2015\"}]}"
      ]
    },
    {
      "@search.score": 0.20760635,
      "content": "\nRESEARCH Open Access\n\nAugmented reality virtual glasses try-on\ntechnology based on iOS platform\nBoping Zhang\n\nAbstract\n\nWith the development of e-commerce, network virtual try-on, as a new online shopping mode, fills the gap that\nthe goods cannot be tried on in traditional online shopping. In the work, we discussed augmented reality virtual\nglasses try-on technology on iOS platform to achieve optimal purchase of online glasses, improving try-on speed of\nvirtual glasses, user senses of reality, and immersion. Face information was collected by the input device-monocular\ncamera. After face detection by SVM classifier, the local face features were extracted by robust SIFT. Combined with\nSDM, the feature points were iteratively solved to obtain more accurate feature point alignment model. Through\nthe head pose estimation, the virtual model was accurately superimposed on the human face, thus realizing the\ntry-on of virtual glasses. The above research was applied in iOS glasses try-on APP system to design the try-on system\nof augmented reality virtual glasses on iOS mobile platform. It is proved that the method can achieve accurate\nidentification of face features and quick try-on of virtual glasses.\n\nKeywords: Virtual try-on, Virtual glasses, Augmented reality, Computer vision, Pose estimation, iOS\n\n1 Introduction\nNetwork virtual try-on is a new way of online shopping.\nWith the development of e-commerce, it broadens the\nexternal propaganda channels of merchants to enhance\nthe interaction between consumers and merchants.\nVirtual try-on fills the gap that the goods cannot be\ntried on in traditional online shopping. As an important\npart of network virtual try-on, virtual glasses try-on\ntechnology has become a key research issue in this field\nrecently [1–4]. During virtual glasses try-on process,\nconsumers can select their favorite glasses by compar-\ning the actual wearing effects of different glasses in the\nonline shopping. The research key of virtual glasses\ntry-on system is the rapid achievement of experiential\nonline shopping.\nAR (augmented reality) calculates the position and\n\nangle of camera image in real time while adding corre-\nsponding images. The virtual world scene is superim-\nposed on a screen in real world for real-time\ninteraction [5]. Using computer technology, AR simu-\nlates physical information (vision, sound, taste, touch,\netc.) that is difficult to experience within certain time\n\nand space of real world. After superimposition of phys-\nical information, the virtual information is perceived by\nhuman senses in real world, thus achieving sensory ex-\nperience beyond reality [6].\nBased on AR principle, virtual glasses try-on technol-\n\nogy achieves optimal purchase of user online glasses and\nquick try-on of virtual glasses, improving the senses of\nreality and immersion. Monocular camera is used as the\ninput device to discuss try-on technology of AR glasses\non iOS platform. Face information is collected by mon-\nocular camera. After face detection by SVM (support\nvector machine) classifier, the local features of faces are\nextracted by robust SIFT (scale-invariant feature trans-\nform). Combined with SDM (supervised descent\nmethod), the feature points were iteratively solved to ob-\ntain more accurate feature point alignment model.\nThrough the head pose estimation, the virtual glasses\nmodel was accurately superimposed on the human face,\nthus realizing the try-on of virtual glasses. The above re-\nsearch is applied in iOS glasses try-on APP system to de-\nsign the try-on system of AR glasses on iOS mobile\nplatform. It is proved that the method can achieve ac-\ncurate identification of face features and quick try-on of\nvirtual glasses.Correspondence: bopingzhang@yeah.net\n\nSchool of Information Engineer, Xuchang University, Xuchang 461000,\nHenan, China\n\nEURASIP Journal on Image\nand Video Processing\n\n© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 \nhttps://doi.org/10.1186/s13640-018-0373-8\n\n\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-018-0373-8&domain=pdf\nhttp://orcid.org/0000-0001-7835-7622\nmailto:bopingzhang@yeah.net\nhttp://creativecommons.org/licenses/by/4.0/\n\n\n2 Research status of network virtual try-on\ntechnology\nGlasses try-on system was first applied in the USA.\nGlasses companies such as Camirror, Smart Look, Ipoint\nKisok, and Xview pioneered the online try-on function [7].\nUsers freely feel the wearing effect, enhancing the online\nshopping experience. Recently, online try-on function is\nexplored by domestic and foreign glasses sellers, such as\nMeijing [8], Kede [9] and Biyao [10].\nVirtual glasses try-on system involves computer vision,\n\naugmented reality, and image processing technology.\nRecently, research hotspots are speed, experience, and\nimmersion of try-on. At present, research results can be di-\nvided into four categories, namely 2D image superposition,\n3D glasses superimposed on 2D face images, 3D face mod-\neling, and AR technology based on video stream [11–14].\nHuang [15] introduced virtual optician system based on\n\nvision, which detects user’s face before locating user’s eyes.\nThree points are selected from face and glasses images.\nTwo corresponding isosceles triangles are formed for af-\nfine transformation, thus estimating the pose and scale of\nface in real time. This method realizes real-time head mo-\ntion tracking. However, the glasses model easily produces\nunrealistic deformation, affecting the realism of the\nglasses.\nAR technology is also applied in the virtual glasses\n\ntry-on system. Cheng et al. [16] selected a monocular\nCCD (charge-coupled device) camera as the input sensor\nto propose AR technology design based on the inter-\naction of marker and face features. Virtual glasses try-on\nsystem is established based on Android mobile platform,\nachieving good results. During virtual try-on process, we\nuse 2D image overlay or 3D modeling approach. There\nare still different defects although all kinds of virtual\nglasses try-on techniques have certain advantages. The\nsuperposition of 2D images is unsatisfactory in the sense\nof reality. Besides, the 3D modeling takes too long to\nmeet the real-time requirements of online shopping.\n\nIn-depth research is required to realize accurate tracking\nand matching. These problems can be solved by\nAR-based glasses try-on technology to a large extent,\nthus providing new ideas for virtual try-on technology.\n\n3 Methods of face recognition\nIt is necessary to integrate virtual objects into real envir-\nonment for the application of AR technology in virtual\nglasses try-on system, wherein face recognition is the\nprecondition for virtual glasses try-on system. During\ntry-on process, it is necessary to detect the face in each\nframe of the video. However, the problems of posture, il-\nlumination, and occlusion can increase the omission and\nfalse ratios of face detection. The real time of detection\nis an important indicator of system performance to en-\nhance people’s experience senses.\nGeneral face recognition process consists of face de-\n\ntection, tracking, feature extraction, dimension reduc-\ntion, and matching recognition (see Fig. 1) [17].\nIn Fig. 1, face detection is the first step to realize face\n\nrecognition. Its purpose is to automatically find face re-\ngion in an input image. If there is a face area, the spe-\ncific location and range of face needs to be located. Face\ndetection is divided into image-based and video-based\ndetection. If the input is a still image, each image is de-\ntected; if the input is a video, face detection is performed\nthroughout the video sequence.\nFeature extraction is based on face detection, and the\n\ninput is the detected face image. Common features are\nLBP (local binary patterns), HOG (histogram of oriented\ngradient), Gabor, etc. HOG [18] describes the edge fea-\ntures. Due to insensitiveness to illumination changes and\nsmall displacements, it describes the overall and local in-\nformation of human face. LBP [19] shows the local tex-\nture changes of an image, with brightness invariance.\nGabo feature [20] captures the local structural content\nof spatial position, direction selectivity, and spatial fre-\nquency. It is suitable for description of human faces.\n\nFig. 1 Face recognition process\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 2 of 19\n\n\n\n\n\nFeature dimension reduction is described as follows.\nFace feature is generally high-dimensional feature vector.\nFace recognition of high-dimensional feature vector\nincreases time and space complexity. Besides, it is difficult\nto effectively judge the description ability of high-dimen-\nsional face features. The high-dimensional face feature\nvector can be projected to the low-dimensional subspace.\nThe low-dimensional subspace information can complete\nface feature identification. After feature extraction, the ori-\nginal features are recombined to reduce vector dimension\nof face feature.\nAfter the previous links, we compare the existing tar-\n\ngets in face database and the faces to be identified based\non certain matching strategy, making final decision.\nMatching recognition can be represented by offline\nlearning and online matching models.\n\n3.1 SVM-based face detection\nFace detection is the premise of virtual glasses try-on\ntechnology. Recently, scholars proposed face detection\nmethods, such as neural network, SVM (support vector\nmachine), HMM (hidden Markov model), and AdaBoost.\nIn the work, the classic SVM algorithm is used for face\ndetection. SVM algorithm is a machine learning method\nbased on statistical theory. Figure 2 shows the network\nstructure of SVM [21]. SVM algorithm can be regarded\nas a three-layer feedforward neural network with a hid-\nden layer. Firstly, the input vector is mapped from\nlow-dimensional input space to the high-dimensional\nfeature space by nonlinear mapping. After that, the opti-\nmal hyperplane with the largest interval is constructed\nin the high-dimensional feature space.\nIt is denoted that the input vector of SVM x = (x1, x2, …, xn).\n\nEquation (1) shows the network output of output layer\nbased on x.\n\ny xð Þ ¼ sgn\nXNtrain\n\ni¼1 yi∂\n�\ni K x\n\ni; x\n� �\n\nþ b�\n� �\n\nð1Þ\n\nwherein the inner product K(x(i), x) is a kernel function\nsatisfying the Mercer condition. Common kernel func-\ntions consist of polynomial, Gauss, and Sigmoid kernel\n\nfunctions. The Gaussian kernel function Kðx; zÞ ¼ e\njjx−zjj2\n2σ2 ,\n\nand σ is the width function.\nOptimization problem of quadratic function (Eq. (2)) is\n\nsolved to obtain the optimal parameter vector ∂�\n\n¼ ð∂�1; ∂�2; …; ∂�NtrainÞ\nT in discriminant function.\n\nmin\n1\n2\nð\nXNtrain\n\ni¼1\nXNtrain\n\ni¼1 ∂i∂ jy\niyjK xi; xj\n\n� �\n−\nXNtrain\n\ni¼1 ∂i ð2Þ\n\ns:t:\nXNtrain\ni¼1\n\n∂iy\ni i ¼ 1; 2; …; Ntrain\n\n0≤∂i ≤C\n\nThe training sample xi corresponding to ∂i > 0 is used\nas a support vector. The optimization parameter b∗ can\nbe calculated by Eq. (3).\n\nb� ¼ 1\nNsv\n\nX\ni∈SV\n\nyi−\nX\n\nj∈SV\n∂�jK x\n\ni; xj\n� �� �\n\nð3Þ\n\nSVM classifier is used to determine whether the de-\ntected image is a human face. If it is not human face,\nthen the image is discarded. If it is, then the image is\nretained to output the detection result. Figure 3 shows\nthe detection process.\n\n3.2 Face recognition based on SIFT\nAfter face detection, face features are extracted for face\nrecognition, providing conditions for face alignment. In\nthe work, the robust SIFT algorithm is used for local fea-\nture extraction [22]. The algorithm finds feature points in\ndifferent scale spaces. It is irrelevant to rotation, scale, and\nbrightness changes. Besides, the algorithm has certain sta-\nbility to noise, affine transformation, and angle change.\n\n3.2.1 Basic principle of SIFT algorithm\nIn the process of feature construction by SIFT algorithm,\nit is necessary to deal with multiple details, achieving faster\noperation and higher positioning accuracy. Figure 4 shows\nflow block diagram of SIFT algorithm [21]. The generation\nprocess of local feature is described as follows [22]:\n\nFig. 2 SVM network structure\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 3 of 19\n\n\n\n\n\n① Detect extreme points\nGaussian differential functions are used for image search\n\non all scales, thus identifying potential fixed points.\n② Position key points\nThe scale on candidate position of model is confirmed.\n\nThe stability degree determines the selection of key points.\n③ Determine the direction of key points\nUsing the gradient direction histogram, each key point\n\nis assigned a direction with the highest gradient value to\ndetermine the main direction of key point.\n④ Describe the key points\nThe local gradients of image are calculated and repre-\n\nsented by a kind of symbol.\n\n3.2.2 Key point matching\n3.2.2.1 Scale space Scale space introduces a scale par-\nameter into image matching model. The continuously\nvariable scale parameter is used to obtain the scale space\nsequence. After that, the main contour of scale space is\n\ntaken as the feature vector to extract the edge features\n[23]. The larger scale leads to the more blurred image.\nTherefore, scale space can simulate the formation\nprocess of target on the retina of the human eye.\nScale space of image can be expressed as Eq. (4).\n\nL x; y; σð Þ ¼ G x; y; σð Þ � I x; yð Þ ð4Þ\nIn Eq. (4), G(x, y, σ) is the Gaussian function, I(x, y) the\n\noriginal image, and * the convolution operation.\n\n3.2.2.2 Establishing Gaussian pyramid\n\nG x; y; σð Þ ¼ 1\n2πσ2\n\ne− x−d=2ð Þ\n2þ y−b=2ð Þ2ð Þ=2σ2 ð5Þ\n\nIn Eq. (5), d and b are the dimensions of Gaussian\ntemplate, (x, y) is the pixel location, and σ the scale space\nfactor.\nGaussian pyramid is established according to Eq. (5),\n\nincluding Gaussian blur and down-sampling (see Fig. 5).\nIt is observed that the pyramids with different sizes con-\nstitute tower model from bottom to top. The original\nimage is used for the first layer, the new image obtained\nby down-sampling for the second layer. There are n\nlayers in each tower. The number of layers can be calcu-\nlated by Eq. (6).\n\nn ¼ log2 minf p; qð Þg−d dϵ 0; log2 minf p; qð Þg½ �\nð6Þ\n\nIn Eq. (6), p and q are the sizes of the original image and d\nis the logarithm of minimum dimension of tower top image.\n\n3.2.2.3 Gaussian difference pyramid After scale\nnormalization of maxima and minima of the Gaussian La-\nplace function σ2∇2G, we obtain the most stable image fea-\ntures using other feature extraction functions. The\nGaussian difference function is approximated to the Gauss-\nian Laplace function σ2∇2G after scale normalization. The\nrelationship is described as follows:\n\n∂G\n∂σ\n\n¼ σ2∇2G ð7Þ\n\nDifferential is approximately replaced by the difference:\n\nσ2∇2G ¼ ∂G\n∂σ\n\n≈\nG x; y; kσð Þ−G x; y; σð Þ\n\nkσ−σ\nð8Þ\n\nTherefore,\n\nG x; y; kσð Þ−G x; y; σð Þ ≈ k−1ð Þσ2∇2G ð9Þ\nIn Eq. (9), k − 1 is a constant.\nIn Fig. 6, the red line is the DoG operator curve; the\n\nblue line the Gauss-Laplacian curve. In extreme detection\n\nFig. 3 The detection process of SVM classifier\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 4 of 19\n\n\n\n\n\nmethod, the Laplacian operator is replaced by the DoG\noperator [24] (see Eq. (10).\n\nD x; y; σð Þ ¼ G x; y; kσð Þ−G x; y; σð Þð Þ � I x; yð Þ\n¼ L x; y; kσð Þ−L x; y; σð Þ ð10Þ\n\n3.2.2.4 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\n\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the ad-\njacent points to judge whether it is large or small (see\nFig. 6). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and\nlower scale spaces to detect extreme points.\nIn the calculation, the Gaussian difference image is the\n\ndifference between the adjacent upper and lower images\nin each group of the Gaussian pyramid (see Fig. 7).\n\n3.2.2.5 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the\nadjacent points to judge whether it is large or small\n(see Fig. 8). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and lower\nscale spaces to detect extreme points.\nIf there are N extreme points in each group, then we\n\nneed N + 2-layer DoG pyramid and N + 3-layer Gaussian\npyramid (see Fig. 8). Due to edge response, the extreme\npoints generated in this case are not all stable.\n\n3.2.2.6 Key point matching At first, the key point is\ncharacterized by position, scale, and direction. To main-\ntain the invariance of perspective and illumination\nchanges, the key point should be described by a set of vec-\ntors. Then, the descriptor consists of key points and other\ncontributive pixels. Besides, the independent characteristic\n\nFig. 4 SIFT algorithm flow chart\n\nFig. 5 Gaussian pyramid\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 5 of 19\n\n\n\n\n\n\n\nof descriptor is guaranteed to improve the probability of\ncorrect matching of feature points.\nThe gradient value of key point is calculated. The gra-\n\ndient value and direction are determined by Eq. (11).\n\nm x; yð Þ ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nN x þ 1; yð Þ−N x−1; yð Þð Þ2 þ N x; y þ 1ð Þ−N x; y−1ð Þð Þ2\n\nq\n\nθ x; yð Þ ¼ α tan2 N x; y þ 1ð Þ−N x; y−1ð Þ\nN x þ 1; yð Þ−N x−1; yð Þ\n\n� �\nð11Þ\n\nIn Eq. (11), N represents the scale space value of key point.\nGradient histogram statistics. The gradient and direc-\n\ntion of pixels in the neighborhood are represented by\nhistogram. The direction ranges from 0 to 360°. There is a\n\nFig. 6 Comparison of Gauss-Laplacian and DoG\n\nFig. 7 Gaussian pyramid of each group\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 6 of 19\n\n\n\n\n\n\n\nsquare column for every 10°, forming 36 columns [25]\n(see Fig. 9). In feature point field, the peak represents the\ngradient direction. The histogram of maximum is the\nmain direction of key point. Meanwhile, the histogram\nwith peak value greater than 80% of main direction is se-\nlected for auxiliary direction to improve the matching\nrobustness.\nAfter successful matching of key points, the entire al-\n\ngorithm is not over yet. This is because substantial mis-\nmatched points appear in the matching process. These\nmismatched points are eliminated by Ransac method in\nSIFT matching algorithm [26].\n\n3.2.3 Face recognition experiment\nTo evaluate the algorithm, the experiment is conducted\nbased on face infrared database provided by Terravic Re-\nsearch Corporation. There are a total of 20 infrared\nimage sequences with head rotation, glasses, hats, and\nlight-illuminated pictures. Three pairs of images are se-\nlected from each face, with a total of 60 pairs. Figure 10\nshows the selected 120 images. In the work, the classic\n\nSIFT matching algorithm is used as the initial matching\nmethod to manually determine matching accuracy and\nmismatch rate of each group. In other words, the match-\ning performance is described by accuracy and error de-\ngrees. Accuracy is defined by the ratio of the number of\ncorrect matches in total number. Error degree is the ra-\ntio of the number difference (between key and matched\npoints) in the total number of key points.\nThese 120 samples are conducted with abstract match-\n\ning contrast according to the variables including head\nrotation angle, illumination transformation, glasses, and\nhat wearing. Meanwhile, other variables remain the\nsame. Figures 11, 12, 13, and 14 show the matching re-\nsults, respectively:\n\n1. Matching results when head rotation angle changes\n2. Matching results when wearing glasses\n3. Matching results when wearing a hat\n4. Matching results when light and shade change\n\nThe experimental data are shown in Table 1.The ex-\nperimental image and Table 1 show:\n① SIFT matching performance is more easily affected\n\nby wearing glasses than head rotation angle, light illu-\nmination, darkness, and wearing hat.\n② In the case of the same number of matches, the\n\nsuccess rate of SIFT matching is higher than that of the\nHarris matching method [27].\nThe overall trend of results can be well presented al-\n\nthough there are inevitable errors due to the finiteness\nof experimental samples.\n\n3.3 Face alignment\nFace alignment is the positioning of face feature points.\nAfter face image detection, the SIFT algorithm automat-\nically positions the contour points of the eyebrows, eyes,\nnose, and mouth. In the try-on process of AR glasses,\nthe eyes are positioned to estimate the head posture.\nThe pose estimation is applied to the tracking registra-\ntion subsystem of glasses, thus producing perspective\n\nFig. 8 The detection of DoG space extreme point\n\nFig. 9 The histogram of the main direction\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 7 of 19\n\n\n\n\n\n\n\ntransformation. However, the pose estimation is easily af-\nfected by the positioning of face feature points, resulting\nin estimation error. The feature points are accurately posi-\ntioned to achieve good effect of head pose estimation.\nAt present, there are many face alignment algorithms.\n\nSDM is a method of finding function approximation\nproposed by Zhu et al [28] by calculating the average\nface, and local features around each feature point are ex-\ntracted to form feature vector descriptor. The offset be-\ntween average and real face is calculated to obtain the\nstep size and motion direction for iteration. The current\nface feature points are converged to the optimal position\nby repeated iterations.\nFigure 15 shows the SDM-based face alignment process.\n\nThe face alignment process is described as follows.\n\n3.3.1 Image normalization\nThe image is normalized to achieve face alignment, thus\nimproving the efficiency of training. The face image to be\ntrained is manually labeled with feature points. After rea-\nsonable translation, rotation, and scaling transformation,\nthe image is aligned to the first sample. The sample size is\nunified to arrange the original data information with con-\nfused, reducing interference other than shape factors. Fi-\nnally, the calculated average face is placed on the sample\nas the estimated face. The average is aligned with the ori-\nginal face image in the center.\n\nIt is denoted that x∗ is the optimal solution in face fea-\nture point location, x0 the initialization feature point,\nd(x) ∈ Rn × 1 the coordinates of n feature points in the\nimage, and h the nonlinear feature extraction function\nnear each feature point. If the SIFT features of 128 di-\nmensions are extracted from each feature point, then\nh(d(x)) ∈ R128n × 1. The SIFT feature extracted at x\n\n∗ can\nbe expressed as ∅∗ = h(d(x∗)). Then, the face feature\npoint alignment is converted into the operation of solv-\ning Δx, which minimizes Eq. (12).\n\nf x0 þ Δxð Þ ¼ hðd x0 þ Δxð Þk k22 ð12Þ\nThe step size Δx is calculated based on the SDM\n\nalgorithm.\n\nxk ¼ xk þ Δxk ð13Þ\nIf Rk and bk are the paths of each iteration, then\n\nEq. (11) can converge the feature point from the initial\nvalue x0 to x∗.\n\nxkþ1 ¼ xk−1 þ Rk−1∅k−1 þ bk−1 ð14Þ\nDuring training process, {di} is the set of face images,\n\n{di} the set of manually labeled feature points, and x0 the\nfeature point of each image. Face feature point location\nis transformed into a linear regression problem. For the\nproblem, the input feature is the SIFT feature ∅i0 at x0;\n\nFig. 10 Sample sequence set\n\nFig. 11 Matching results when head rotation angle changes\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 8 of 19\n\n\n\n\n\n\n\nthe result the iteration step size Δxi� ¼ xi� þ Δxi0 from x0\nto x∗; and the objective function Eq. (15).\n\nargminR0b0\nX\n\ndi\n\nX\nxi\n\nΔxi�−R0∅\ni\n�−b0\n\n\t\t \t\t2\n2\n\nð15Þ\n\nIn this way, R0 and b0 from the training set are iterated\nto obtain Rk and Rk. The two parameters are used for\nthe test phase to achieve the alignment of test images.\n\n3.3.2 Local feature extraction of SIFT algorithm\nIn the work, the principal component analysis is used to\nreduce the dimension of image [29], the impact of\nnon-critical dimensions, and the amount of data, thus\nimproving the efficiency. After the dimension reduction,\nthe local feature points are extracted from the face\nimage. To improve the alignment accuracy of feature\npoints, the robust SIFT algorithm is applied for local fea-\nture extraction. Section 3.2.2 introduces the extraction\nprocess in detail.\n\n3.3.3 SDM algorithm alignment result\nTraining samples are selected from IBUG and LFW face\ndatabases. The former contains 132 face images. Each\nimage is labeled with 71 face feature points, which are\nsaved in pts file. The latter consists of the sets of test and\ntraining samples, wherein, the set of test sample contains\n206 face images. Each image is labeled with 71 face feature\npoints, which are saved in pts file. The set of training\n\nsample contains 803 face images. Each image is labeled\nwith 68 face feature points. Figures 16 and 17 show frontal\nand lateral face alignment results, respectively.\n\n3.4 Face pose estimation\nBased on computer vision, the pose of object refers to\nits orientation and position relative to the camera. The\npose can be changed by moving the camera or object.\nGeometric model of camera imaging determines the re-\nlationship between 3D geometric position of certain\npoint on head surface and corresponding point of image.\nThese geometric model parameters are camera parame-\nters. In most cases, these parameters are obtained by ex-\nperiments. This process is called labeling [27, 29].\nCamera labeling determines the geometric and optical\nproperties, 3D position, and direction of camera relative\nto certain world coordinate system.\nThe idea of face pose estimation is described as fol-\n\nlows. Firstly, we find the projection relationship between\n2D coordinates on face image and 3D coordinates of\ncorresponding points on 3D face model. Then, the mo-\ntion coordinates of camera are calculated to estimate\nhead posture.\nA 3D rigid object has two movements relative to the camera:\n① Translation movement\nThe camera is moved from current spatial position\n\n(X, Y, Z) to new spatial position (X′, Y′, Z′), which is called\n\nFig. 12 Matching results when wearing glasses\n\nFig. 13 Matching results when wearing a hat\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 9 of 19\n\n\n\n\n\n\n\ntranslation. Translation vector is expressed as τ = (X′ − X,\nY′ − Y, Z′ − Z).\n② Rotary movement\nIf the camera is rotated around the XYZ axis, the rota-\n\ntion has six degrees of freedom. Therefore, pose estima-\ntion of 3D object means finding six numbers (three for\ntranslation and three for rotation).\n\n3.4.1 Feature point labelling\nThe 2D coordinates of N points are determined to calcu-\nlate 3D coordinates of points, thus obtaining 3D pose of\nobject in an image.\nTo determine the 2D coordinates of N points, we se-\n\nlect the points with rigid body invariance, such as the\nnose tip, corners of eyes, and mouth. In the work, there\nare six points including the nose tip, chin, left, and right\ncorners of eyes and mouth.\nSFM (Surrey Face Model) is used as general 3D face\n\nmodel to obtain 3D coordinates corresponding to se-\nlected 2D coordinates [30]. By manual labeling, we ob-\ntain the 3D coordinates (x, y, z) of six points for pose\nestimation. These points are called world coordinates in\nsome arbitrary reference/coordinate system.\n\n3.4.2 Camera labeling\nAfter determining world coordinates, the camera is reg-\nistered to obtain the camera matrix, namely focal length\nof camera, optical center, and radial distortion parame-\nters of image. Therefore, camera labeling is required. In\nthe work, the camera is labeled by Yang and Patras [31]\nto obtain the camera matrix.\n\n3.4.3 Feature point mapping\nFigure 18 shows the world, camera, and image coordin-\nate systems. In Fig. 18, O is the center of camera, c the\noptical center of 2D image plane, P the point in world\ncoordinate system, and P′ the projection of P on image\nplane. P′ can be determined according to the projection\nof the P point.\nIt is denoted that the world coordinate of P is (U, V, W).\n\nBesides, the known parameters are the rotation matrix R\n\nFig. 14 Matching results when light and shade change\n\nTable 1 Match result analysis table\n\nVariate Number of\nmatches\n\nTotal number\nof key points\n\nMatch\nratio\n\nFalse\nmatch rate\n\nHead rotation 18 158 0.129 0.871\n\nWearing glasses 15 167 0.099 0.901\n\nWearing a hat 21 106 0.247 0.753\n\nLight and shade\nchange\n\n45 281 0.191 0.809\nFig. 15 The face alignment process based on SDM\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 10 of 19\n\n\n\n\n\n\n\n(matrix 3 × 3) and translation vector τ (vector 3 × 1) from\ncamera to world coordinate. It is possible to determine\nposition O(X, Y, Z) of P in camera coordinate system.\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ R\n\nu\nv\nw\n\n2\n4\n\n3\n5 þ τ⇒\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ Rjτ½ �\n\nu\nv\nw\n\n2\n4\n\n3\n5 ð16Þ\n\nEquation (16) is expanded as follows:\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð17Þ\n\nIf plenty of points are mapped to (X, Y, Z) and (U,V, W),\nthe above problem is transformed into a system of linear\nequations with unknown (τx, τy, τz) . Then, the system of\nlinear equations can be solved.\nFirstly, the six points on 3D model are manually la-\n\nbeled to derive their world coordinates (U, V, W). Equa-\ntion (18) is used to determine 2D coordinates (X, Y) of\nsix points in image coordinate system.\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 xy\n\nZ\n\n2\n4\n\n3\n5 ð18Þ\n\nwhere fx and fy are the focal lengths in the x and y direc-\ntions, (cx, cy) is the optical center, and S the unknown scaling\nfactor. If P in 3D is connected to O, then P′ where light in-\ntersects image plane is the same image connecting all points\nin the center of the camera produced by P along the ray.\nEquation (18) is converted to the following form:\n\nS\nX\nY\nZ\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð19Þ\n\nThe image and world coordinates are known in the\nwork. Therefore, Eqs. (18) and (19) are transformed into\nthe following form:\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 r00 r01 r02 τxr10 r11 r12 τy\n\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð20Þ\n\nIf the correct poses R and τ are known, then the 2D\nposition of 3D facial point on image can be predicted by\nprojecting the 3D point onto the image (see Eq. (20)).\nThe 2D facial feature points are known. Pose estimation\ncan be performed by calculating the distance between\nthe projected 3D point and 2D facial feature. If the pose\nis correctly estimated, the 3D points projected onto\nimage plane will almost coincide with the 2D facial fea-\ntures. Otherwise, the re-projection error can be mea-\nsured. The least square method is used to calculate the\nsum of squares of the distance between the projected 3D\nand 2D facial feature points.\n\n3.5 Tracking registration system\nTracking registration technology is the process of align-\ning computer-generated virtual objects with scenes in\nthe real world. At present, there are two tracking regis-\ntration techniques. The first superimposes certain point\nof face feature with a point of virtual glasses based on\nthe face feature point tracking method [32]. The second\nis based on the geometric transformation relation track-\ning method. Face geometry and virtual glasses model are\nconducted with affine transformation. Virtual glasses\nmodel moves with the movement of human head, mak-\ning corresponding perspective changes and realizing 3D\ntry-on effect [33]. For the first technique, the virtual\nglasses cannot be changed with the movement of user\nhead, causing poor user experience. The second tech-\nnique has good tacking effect. The virtual glasses will be\ndistorted with overlarge head corner. Combined with the\ntwo methods, the glasses model is conducted with per-\nspective transformation using six degrees of freedom ob-\ntained by pose estimation in Section 3.3. After face\nsuperposition, accurate tracking is realized through bet-\nter stereoscopic changes.\n\nFig. 16 The picture of front face alignment\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 11 of 19\n\n\n\n\n\n3.5.1 Affine transformation method of glasses try-on\nIn Fig. 19, the center between two corners of the eye is\ncalculated according to the distance between them. An\nisosceles right triangle ABC is defined [34]. The coordi-\nnates of the triangle are A(a1, a2), B(b1, b2), and C(c1, c2).\nIf the threshold is determined by experiment ahead of\ntime, the coordinates of C are as follows.\n\nC c1; c2ð Þ ¼ b1−b2 þ a2; b1 þ b2−a2ð Þ ð21Þ\n\nDuring try-on process, the glasses model is matched\nto the eye of user using the affine transformation\nEq. (22).\n\nx0 ¼ ax þ by þ c y0 ¼ dx þ ey þ f ð22Þ\n\nIn the glasses model, the vertices of isosceles right\ntriangle are priori, with the coordinates of (x1, y1),\n(x2, y2), and (x3, y3). The vertices of isosceles right\ntriangle on user face (x01; y\n\n0\n1), (x\n\n0\n2; y\n\n0\n2), and (x\n\n0\n3; y\n\n0\n3) can be\n\ndetected in motion. The affine transformation parameter\nh = (a, b, c, d, e, f)T.\n\nx01\ny01\nx02\ny02\nx03\ny03\n\n2\n6666664\n\n3\n7777775\n¼\n\nx1 y1 1 0 0 0\n0 0 0 x1 y1 1\nx2 y2 1 0 0 0\n0 0 0 x2 y2 1\nx3 y3 1 0 0 0\n0 0 0 x3 y3 1\n\n2\n6666664\n\n3\n7777775\n\na\nb\nc\nd\ne\nf\n\n2\n6666664\n\n3\n7777775\n\nð23Þ\n\nEquation (23) is abbreviated as P = Ah. Finally, the af-\nfine transformation parameter h (h = (ATA)−1) is calcu-\nlated by least square method. If h is applied to the\nisosceles right triangle, then the image of glasses will be\nprojected onto the right position of the face.\n\n3.5.2 Perspective transformation method of glasses try-on\nAffine transformation can realize the tracking of 3D\nmodel. The tracked glasses are prone to deformation be-\ncause the affine transformation has the characteristics of\nflatness and parallelism based on three-point transform-\nation [35]. The six degrees of freedom are obtained from\nhead pose estimation. After perspective transformation,\nthe glasses are superimposed with eye feature points to\nachieve real-time tracking effect. When the head moves,\nthe space model of glasses should conform to human\nvisual law, with certain deformation. It is realized by per-\nspective transformation [36] (Fig. 20).\n\nFig. 17 The picture of side face alignment\n\nFig. 18 Three coordinate systems\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 12 of 19\n\n\n\n\n\n\n\n3.6 Virtual model generation system\nIn the work, the 3D glasses model is built in 3ds max\nand exported to 3DS format file. The 3DS data file can-\nnot display the 3D model in OpenGL in real time.\nFirstly, the 3DS model is conducted with analytic oper-\nation. Only by transferring the operational data to the\nOpenGL function can we draw the virtual glasses model\nin this case [37].\n\n3.7 Virtual and real synthesis system\nTo achieve the perfect combination of virtual glasses\nand realistic scenes, virtual glasses must be positioned to\nthe exact position in the real world at first. This process\nis achieved by integrating markers with natural features.\nFigure 21 shows the overall structure of fused glasses\ntry-on subsystem [38].\n\n4 iOS system application\nTo verify the effectiveness of proposed system method, we\ndevelop a mobile “AR Glasses Try-on Sales System” for\niOS platform. This system comprehensively uses the\n\ncommon controls of iOS. Besides, the controls are recon-\nstructed and optimized to improve the operating efficiency\nof the system. Meanwhile, most function blocks are modi-\nfied to minimize the application, dependencies, and main-\ntenance procedure of third-party framework. The system\nrealizes the basic functions including browsing of glasses\nproducts, user registration, login, goods collection, adding\nto carts, user address adding, modifying, deleting, goods\npurchasing, integrated Alipay payment, and order man-\nagement [39, 40]. In addition, it is embedded with glasses\ntry-on, photographing, recording video, uploading, and\nsharing WeChat and Weibo. Meanwhile, the system has\nits social platform for users to browse try-on effects of\nothers. The same glasses are quickly tried on, thus meet-\ning the needs of vast users (Fig. 22).\nCommodity browsing module is the core of the system,\n\ncovering commodity browsing, screening, try-on, photo-\ngraphing, uploading, and sharing. The try-on and quick\ntry-on subsystems need to call face recognition method in\nPart 3.\n\n4.1 Menu module\nMenu module is the framework module of the whole ap-\nplication. All the sub-modules are switched by a Menu-\nViewController controller. This control contains the\nviews of menu, home page, favorites, shopping cart,\norder, coupon, photo wall, setting modules, initialization,\nand the switching method of controllers. The menu page\nis not displayed in the default startup page. User calls up\nthe menu page by clicking the upper-left icon of the de-\nfault page or sliding to the right in the home page.\nMenu page adopts the traditional frosted glass method.\n\nFirstly, the UIImage object is obtained by taking a screen\nshot and then processed by the frosted glass tool. The\nfrosted glass UImage is used as the background of the\nmenu to realize the translucent frosted glass effect.\nMenu view is the leftmost view of MenuViewControl-\n\nler. The view is located at the top of the entire applica-\ntion. The menu can be switched out in all modules. It is\nachieved mainly by the table. The rows are selected by\nthe table to trigger the effect of selected rows.\nThe module title in the menu is clicked to trigger the\n\nproxy event of table, thus calling the method of selecting\ncurrent module for module switching. The switchable\nmodules mainly include user, commodity, collection,\nshopping cart, order, coupon, photo wall, and setting.\n\n4.2 User registration module\nUser registration module is used for the management of\nregistered users. Registered users enjoy the VIP promo-\ntion activities and prices. Unregistered users enter the\nregistration page by clicking the registration button.\n\nFig. 19 The isosceles triangle made on face and eyeglass images\n\nFig. 20 The quadrilateral made on face and eyeglass images\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 13 of 19\n\n\n\n\n\n\n\n4.3 Commodity module\nCommodity module is the key of “AR glasses sales sys-\ntem,” including commodity browsing, selection, and add-\ning to cart.\n\n4.3.1 Commodity browsing\nAll the products are browsed in the login or non-login\nstatus. In commodity browsing, the big images of glasses\nare slid to browse the front of product image and leg\nstyle. The detail page can be slid to view more commod-\nity information. More commodity information is loaded\nby sliding up.\nThe pull-up and pull-down are proxy methods based\n\non the table and its parent class (UIScrollView).\n(void)scrollViewDidScroll:(UIScrollView *)scrollView\nWhen the height of parent container offset is greater\n\nthan 20% of table height, the pull-down refresh is called.\nThe pull-up refresh is called when the height difference\n\nbetween the height of parent container and the sum of\ntable height and offset exceeds 10% of the screen height.\n\n4.3.2 Commodity screening\nIn commodity browsing, users quickly find products\nmeeting their needs and click to select multiple items. If\none item is selected, the system will feed back the num-\nber of eligible products in time. The display result but-\nton is clicked to display the screening result. Screening\nresults can be cleared by clicking on the cross on the\nright of blue subtitle.\nList screening is realized by modified and nested tables.\n\nThe segment head of custom table is used as first-level\nscreening title. Second-level screening catalog is achieved\nby nested table and custom table cells. By nesting\nsecond-level screening catalog, we obtain third-level\nscreening catalog. The subtitle of first-level catalog is\nrefreshed by recording the selected filter item in real time.\n\nFig. 21 General structure of system\n\nFig. 22 The structure of “AR Glasses Sales System”\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 14 of 19\n\n\n\n\n\n\n\nSimultaneously, the server is synchronized to get the\nremaining product information.\nIn the screening tool class, the record of third-level\n\nmenu is complicated. In implementation, the third-level\noptions are recorded for local summary, updating selected\nor unselected state. Full summary is performed by reusing\nlocal summary, indicating in the first-level menu.\n\n4.3.3 Glasses try-on\nIn the system, the face data are captured by the camera\nfor further processing. Firstly, user’s face is located at 30–\n50 cm right ahead the front camera of mobile phone. The\nface is slightly rotated, without getting out of capture area\nof camera. In addition, user should not keep his/her back\nto the light during the try-on process because the light af-\nfects the capture effect of the camera. When the camera\ndoes not capture face data, face position will be adjusted\nby a prompt. As user wiggles in front of camera, the en-\ngine re-recognizes the face information. The quick try-on\nbutton is clicked to enter the quick try-on page. User can\nput the phone 30–50 cm in front of him. Then, his/her\nface appears on the screen of the mobile phone. Mean-\nwhile, the system automatically recognizes user face data\nto put on glasses. The product details are viewed by\n\nsliding the glasses or clicking on the left detail button.\nFigure 23 shows the try-on process in detail.\nIn the try-on process, the third-party oepnframeworks is\n\nused to modify the class library according to the require-\nments. Section 3 introduces face recognition, alignment,\ntracking registration and head pose estimation, and virtual\nmodel generation methods. Combined with these\nmethods, the interface is packaged to increase the stability\nof system, reducing the dependence on third-party con-\ntrols. The part embedded in openframeworks starts with\nthe main function. Using openframeworks, the window is\ninitialized to call the Appdeleagte class of openframe-\nworks. This class is compatible with UIKit library in iOS.\nThe ofApp will be initialized to call engine loading model.\n\n4.3.3.1 AFNetworking In iOS development, the NSURL-\nConnection of XCode is competent to submit a request to\na simple page of Web site, thus obtaining the response\nfrom server. However, most web pages to be visited are\nprotected by authority. The pages cannot be visited by a\nsimple URL. This involves the processing of Session and\nCookie. Here, NSURLConnection can be used to realize\naccess, with larger complexity and difficulty.\nAFNetworking is more suitable to process requests to\n\nWeb sites, including detailed Sessions and Cookies\n\nFig. 23 The detailed flow chart of eyeglass try-on\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 15 of 19\n\n\n\n\n\nproblems. It can be used to send HTTP requests and re-\nceive HTTP responses. However, it does not cache ser-\nver responses or execute the JAvascript code in the\nHTML page. Meanwhile, AFNetworking has built-in\nJSON, plist, and XML file parsing for convenient\napplication.\nSome interfaces of the library are packaged to facilitate\n\nthe use of AFNetworking. The packaged AFNetworking\ncan record the operation due to disconnection request\nfailure. After networking, the request is re-initiated.\nWhen data needs to be requested, if get request is\n\ncalled, the following methods will be called:\n-(void)GET:(NSString *)URLString parameters:(id)para\n\nmeters WithBlock:(resultBlock)result;\nIf a image necessary upload pictures the following methods\n\nwill be call.\n// Upload pictures\n-(void)POST:(NSString *)URLString parameters:(id)para\n\nmeters WithData:(NSData *)data WithKey:(NSString *)key\nWithTypeO:(NSString*)pngOrMp4\nWithBlock:(resultBlock)result;\nIf a video necessary upload pictures the following\n\nmethods will be call.\n// Upload video\n-(void)POST:(NSString *)URLString parameters:(id)pa\n\nrameters WithDic:(NSDictionary *)dic WithTypeO:(NS\nString*)pngOrMp4 WithBlock:(resultBlock)result;\nIf post the following interface will be call.\n// post\n- (void)POST:(NSString *)URLString parameters:(id)\n\nparameters WithBlock:(resultBlock)result;\n\n4.3.3.2 SDWebImage SDWebImage is a framework for\nthird-party applications. It is used to implement asyn-\nchronous loading and caching of images. In this system,\nall network images are loaded using this framework. By\ndefining interface classes, we can easily implement asyn-\nchronous loading and caching of images.\n#import <Foundation/Foundation.h>\n@interface TGImageTool : NSObject\n+ (void)downloadImage:(NSString *)url placeholder:\n\n(UIImage *)place imageView:(UIImageView *)imageView;\n+ (void)clear;\n@end\n#import “TGImageTool.h”\n#import “UIImageView+WebCache.h”\n@implementation TGImageTool\n+ (void)downloadImage:(NSString *)url placeholder:\n\n(UIImage *)place imageView:(UIImageView *)imageView\n{[imageView setImageWithURL:[NSURL URLWithStrin\n\ng:url] placeholderImage:place options:SDWebImageLowPri\nority | SDWebImageRetryFailed];}\n+ (void)clear\n{\n\n// 1. Clear the cached images in memory\n[[SDImageCache sharedImageCache] clearMemory];\n[[SDImageCache sharedImageCache] clearDisk];\n// 2. Cancel all download requests\n[[SDWebImageManager sharedManager] cancelAll];\n}\n@end\nThe image is loaded by the above tool class method.\n\nWhen the cache is implemented, the image will be auto-\nmatically added to the cache.\nThe clear method of tool class is called to clear the cache.\n\n4.3.3.3 JSONKit JSONKit is used in this system only\nwhen the order information is submitted. It transcodes\nthe complicated parameter information to JSON strings\nfor server application. The conversion method is de-\nscribed as follows.\nNSString *new=[dic JSONString]\n\n4.3.4 Adding to cart\nThe satisfied glasses are added to shopping cart by clicking\non the “Add to Cart” button. The animation of “Add to\nCart” is realized by path and combined animation in the\nQuartzCore library.\n\n4.3.5 Buy a glasses immediately\nUser directly jumps to the page for purchasing the\nglasses without adding to cart. The function is realized\nby directly jumping to order information improvement\npage after summarizing commodity information.\n\n4.3.6 Taking photos or recording videos\nUsers with glasses can take off their glasses after logging\nin. VR glasses are tried on to take photos or record videos.\nThe try-on effects can also be watched after wearing\nglasses. The system provides functions of taking photos\nand recording videos. The photo/video button is used to\nswitch between taking pictures and recording videos. In\nthe work, this function is realized by modifying the engine\nin oepnframeworks. This system only involves the call.\nThe photos and videos are placed in the four preview\nareas below, where user can click to view the details.\n\n4.3.7 Uploading and sharing\nThe system provides uploading and sharing functions of\nphotos or videos to share satisfactory try-on results and\nwonderful moments with friends. “Share” button is\nclicked to upload videos to photo wall, friend circle,\nWeibo, or WeChat in the server. The photos or videos\nare deleted by clicking the “Delete” button.\nThe third-party AFNetworking method is used to up-\n\nload files. The files can be shared to Sina Weibo,\nWeChat circle, friends, and photo wall.\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 16 of 19\n\n\n\nThe sharing principle is to obtain the information of\nthe photo or video on server side. Then, the html5 page\nis generated, including image, video, like, and comment.\nThe URL is returned to the client and shared to WeChat\nand Sina Weibo.\nUsers can choose whether to share to photo wall at\n\nthe same time. Sharing to photo wall is to send a request\nto the server. The photo or video is backed up in the\ntable corresponding to database photo wall information.\nWhen being requested, the shared information can be\nobtained in the photo wall.\n\n4.4 Collection module\nIn the implementation of collection module, the cells of\ntable are reused in the home page. The data are replaced\nwith the data of favorite list. After logging in, the favorite\nitem is added to the collection list of personal information\nby clicking the gray heart button, which is convenient for\nnext viewing. “Collect” button is clicked to cancel the col-\nlected item, removing it from the collection list.\n\n4.5 Shopping cart module\nAfter logging in, the satisfactory item is added to shop-\nping cart in the try-on interface. In implementation, the\ncustom tool class is used to record the selected state.\nWhen clicking “Select All” button, all data in the table\nare selected. The selected state of “Select All” button is\nremoved to cancel certain item. Meanwhile, the sums of\nselected item quantities and unit prices are calculated.\nThe head position shows the number of items. “Settle”\nbutton at the bottom of table shows the total number of\nitems. Users can modify orders and postal addresses,\nwhile submitting orders and paying online.\n\n4.6 Order module\nAfter logging in, users can see their historical orders in\n“My Order.” There are two states in the order, including\npending (immediate payment) and successful payment.\n“Pay now” button is clicked to jump to payment interface.\nDuring order payment, it will jump to the immediate pay-\nment page of shopping module and then to Alipay.\n\n4.7 Coupon module\nThe coupon module is a channel through which mer-\nchants can distribute benefits to users. After logging in,\nusers check the coupons matching their own eligibilities.\nThere are three types of coupons received: available,\nused, and expired coupons. After reading coupon usage\nrules, users can select whether to use the coupon in the\ninterface of order information completion.\n\n4.8 Photo wall module\nThe photo wall is a display platform provided by the sys-\ntem to user. It is convenient for user to browse the\n\ntry-on results of others. Based on dynamic prompt func-\ntion, user quickly finds the favorite style of glasses.\nUser dynamic prompt function is implemented by de-\n\ntecting new messages. Once menu page pops up, a request\nis sent to the server, requesting a new unread message. If\nthere is a new message, it will show user avatars of last dy-\nnamic message and the number of new messages; other-\nwise, the prompt box is not displayed.\nWhile seeing the favorite try-on results, users can like,\n\ncomment, forward, or view the same item and try it on\nquickly.\n“View commodity” button is clicked to view the de-\n\ntailed information of glasses try-on results. The product\ninformation is uniquely determined according to the\nproduct ID. It is the same as quick try-on principle.\nUser can directly try on the same glasses worn by\n\nother users by clicking the quick try-on button. The\nphoto wall and product data are bound in the database\nat the beginning. Therefore, the product can be directly\nfound and tried on according to the product ID.\n\n4.9 Setting module\nThe setting module contains “check updates,” “clean up\npicture cache,” “about us,” “rating,” “feedback,” and “exit\ncurrent user.” Relatively, it is the application of native\ntable control, which is not described here.\n\n5 Results and discussion\nExperimental environment is described as follows.\nOperating system: iOS 9\nDevelopment tools: Xcode 6\nRelated libraries: OpenCV, MFC\nProgramming language: C language, Objective-C, C++\nFigure 24 shows the partial operation interface of the\n\nsystem.\nAlthough a lot of jobs are done, there are still some\n\nshortcomings in the system:\n\n1. Equipped with a try-on engine, the system has\ncertain requirements on the performance of iPhone.\nThe higher configuration of iPhone leads to the\nmore accurate identification. At present, the models\nrunning smoothly are iPhone 5s and iPhone 6 and\niPhone 6 Plus.\n\n2. It is difficult for user to perform subsequent\noperations in the case of unstable network\nenvironment, especially for failed login.\n\n3. In the system, the face data are captured by the\ncamera for further processing. Firstly, user face is\nlocated at 30–50 cm right ahead the front camera\nof the mobile phone. The face is slightly rotated,\nwithout getting out of the capture area of the camera.\n\n4. User should not keep his/her back to the light\nduring the try-on process because the light affects\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 17 of 19\n\n\n\ncapture effect of the camera. When the camera does\nnot capture face data, face position will be adjusted\nby a prompt. As user wiggles in front of camera, the\nengine re-recognizes user face information.\n\n5. At present, only the Chinese version of “AR Glasses\nSales System” has been developed. There is no\ncorresponding English version.\n\n6 Conclusions\nIn the work, we discussed augmented reality virtual\nglasses try-on technology. Face information was collected\nby monocular camera. After face detection by SVM classi-\nfier, the local face features were extracted by robust SIFT.\nCombined with SDM, the feature points were iteratively\nsolved to obtain more accurate feature point alignment\nmodel. Through the head pose estimation, the virtual\nglasses model was accurately superimposed on the human\nface, thus realizing the try-on of virtual glasses. This the-\noretical research was applied in iOS platform for the\ntry-on of virtual glasses, thus providing best services for\nuser selection. Experiments showed that the virtual glasses\nhad realistic effect and high try-on speed and user satisfac-\ntion. Consequently, AR-based glasses try-on technology\nprovided new idea for virtual try-on technology. Camera\ncapture under complex light conditions will be further\nstudied. App running test on iPhone 7 and above will be\ncarried out. Multilingual versions will be developed.\n\nAbbreviations\nAPP: Application; AR: Augmented reality; CCD: Charge-coupled device;\nHMM: Hidden Markov model; HOG: Histogram of oriented gradient;\nIBUG: Intelligent Behaviour Understanding Group; iOS: iPhone OS; LBP: Local\nbinary patterns; LFW: Labeled Faces in the Wild; SDM: Supervised descent\nmethod; SFM: Surrey Face Model; SIFT: Scale-invariant feature transform;\nSVM: Support vector machines; VR: Virtual reality\n\nAcknowledgements\nThis work is partially supported by Shanxi Province Universities Science and\nTechnology Innovation Project (2017107) and Shanxi Province Science\nFoundation for Youths (201701D12111421).\nThanks to the editor and reviewers.\n\nFunding\nThe paper is subsidized by science and technology key project of Henan\nProvince, China. NO.172102210462\n\nAvailability of data and materials\nData will not be shared; reason for not sharing the data and materials is that\nthe work submitted for review is not completed. The research is still ongoing, and\nthose data and materials are still required by my team for further investigations.\n\nAuthor’s contributions\nBZ designed the research, analyzed the data, and wrote and edited the\nmanuscript. The author read and approved the final manuscript.\n\nAuthor’s information\nBoping Zhang, female, is currently an Associate Professor at the School of\nInformation Engineering, Xuchang University, China. She received master’s\ndegree from Zhengzhou University, China, in 2006. Her current research\ninterests include computer vision, image processing, virtual reality, and\npattern recognition.\n\nEthics approval and consent to participate\nNot applicable.\n\nConsent for publication\nNot applicable.\n\nCompeting interests\nThe author declares that she has no competing interests. The author confirms\nthat the content of the manuscript has not been published or submitted for\npublication elsewhere.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published\nmaps and institutional affiliations.\n\nReceived: 15 August 2018 Accepted: 5 November 2018\n\nReferences\n1. DITTO. http://www.ditto.com/\n2. O. Deniz, M. Castrillon, J. Lorenzo, et al., Computer vision based eyewear\n\nselector. Journal of Zhejiang University-SCIENCE C (Computers & Electronics)\n11(2), 79–91 (2010)\n\n3. Gongxin Xie. A transformation road for the glasses industry[J]. China Glasses,\n2014,03:112–113\n\n4. Liu Cheng, Wang Feng, QI Changhong, et al. A method of virtual glasses\ntry-on based on augmented reality[J]. Industrial Control Computer, 2014,\n27(12):66–69\n\n5. Boping Zhang. Design of mobile augmented reality game based on image\nrecognition[J]. EURASIP Journal on Image and Video Processing, 2017, 20:2–20\n\na b c d\nFig. 24 The part of interface for system operation\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 18 of 19\n\n\n\n\n\nhttp://www.ditto.com/\n\n\n6. Yan Lei, Yang Xiaogang, et al. Mobile augmented reality system design and\napplication based on image recognition[J], Journal of Image and Graphics,\n2016, 21(2):184–191\n\n7. Niswar A, Khan I R, Farbiz F. Virtual try-on of eyeglasses using 3D model of\nthe head[C]. International Conference on Virtual Reality Continuum and ITS\nApplications in Industry. New York: ACM; 2011:435–438.\n\n8. Meijing[OL]. http://www.meijing.com/tryinon.html\n9. Kede [OL]. http://www.kede.com/frame\n10. Biyao [OL]. http://www.biyao.com/home/index.html\n11. Li Juan, Yang Jie. Eyeglasses try-on based on improved Poisson equations.\n\n2011 Conference on Multimedia Technology. New York: ICMT 2011. 2011;\n3058–3061.\n\n12. DU Yao,WANG Zhao-Zhong. Real-like virtual fitting for single image[J].\nComputer Systems Application, 2015, 24(4):19–20\n\n13. Y. Lu, W. Shi-Gang, et al., Technology of virtual eyeglasses try-on system\nbased on face pose estimation[J]. Chinese Optics 8(4), 582–588 (2015)\n\n14. Yuan M, Khan I R, Farbiz F, et al. A mixed reality virtual clothes try-on\nsystem[J]. IEEE Transactions on Multimedia. 2013;15(8):1958-968.\n\n15. Huang W Y, et al. Vision-based virtual eyeglasses fitting system[C]. IEEE,\nInternational Symposium on Consumer Electronics. New York: IEEE. 2013;45–46\n\n16. Wang Feng, Qi Changhong, Liu Cheng, Jiang Wei, Ni Zhou, Zou Ya.\nReconstruction of 3D head model based on orthogonal images [J]. Journal\nof Southeast University (Natural Science Edition). 2015;45(1):36-40.\n\n17. Zhang B. Cluster Comput. 2017. https://doi.org/10.1007/s10586-017-1330-5.\n18. Maatta J, Hadid A, Pietikainen M. Face spoofing detection from single images\n\nusing texture and local shape analysis[J]. IET Biometrics, 2012, 1(1):3–10\n19. Lin Y, Lv F, Zhu S, et al. Large-scale image classification: fast feature\n\nextraction and svm training[C]. Computer Vision and Pattern Recognition\n(CVPR), 2011 IEEE Conference on. New York: IEEE; 2011:1689–1696\n\n20. Lowe D G, Lowe D G. Distinctive image features from scale-invariant\nkeypoints[J]. Int. J. Comput. Vis., 2004, 60(2):91–110\n\n21. Zhang Boping. Research on automatic recognition of color multi\ndimensional face images under variable illumination[J]. Microelectronics &\nComputer, 2017,34(5) :128–132\n\n22. MING An-Long MA Hua-dong. Region-SIFT descriptor based\ncorrespondence between multiple cameras[J]. CHIN ESE JOURNA L OF\nCOMPUTERS, 2008, 12(4):650–662\n\n23. He Kai, Wang Xiaowen, Ge Yunfeng. Adaptive support-weight stereo\nmatching algorithm based on SIFT descriptors[J]. Journal of Tiajin University,\n2016, Vol.49(9):978–984\n\n24. D.G. Lowe, Distinctive image features from scale-invariant key points. Int. J.\nComput. Vis. 60(2), 91–110 (2004)\n\n25. Chen Guangxi, Gong Zhenting, et al. Fast image recognition method Bsded\non locality-constrained linear coding[J]. Computer Science, 2016, vol. 43(5),\n308–314\n\n26. Bai Tingzhu, Hou Xibao. An improved image matching algorithm base on\nSIFT[J]. Transaction of Beijing Institute of Technology, 2013, 33(6):622–627\n\n27. Xiong X, Tome F D L. Supervised descent method and its applications to face\nalignment[C].Computer Vision and Pattern Recognition. New York: IEEE; 2013:\n532–539\n\n28. Zhu JE, et al. Real-Time Non-rigid Shape Recovery Via Active Appearance\nModels for Augmented Reality (Proc. Of 9th European Conference on\nComputer Vision, Graz, 2006), pp. 186–197\n\n29. Huber P, Hu G, Tena R, et al. A multiresolution 3D morphable face model\nand fitting framework[C]. Visapp. 2015\n\n30. Zhang Z. A flexible new technique for camera calibration[J]. IEEE Transactions\non Pattern Analysis&Machine Intelligence, 2000, 22(11):1330–1334\n\n31. Yang H, Patras I. Sieving Regression Forest Votes for Facial Feature\nDetection in the Wild[C]. New York: ICCV; 2013:1936–1943\n\n32. Dantone M, Gall J, Fanelli G, et al. Real-time facial feature detection using\nconditional regression forests[C]. Computer Vision and Pattern Recognition.\nNew York: IEEE; 2012:2578–2585\n\n33. Google Release online AR mobile games Ingress[OL]. http://www.csdn.net/\narticle/2012-11-16/2811943-google-launches-ingress\n\n34. D. Shreiner, G. Sellers, J.M. Kessenich, B.M. Licea-Kane, OpenGL Programming\nGuide: The Official Guide to Learning OpenGL, 8th edn. (Addison-Wesley\nProfessional, United States, 2013)\n\n35. J. Kim, S. Forsythe, Adoption of virtual try-on technology for online apparel\nshopping. J. Interact. Mark. 22, 45–59 (2008)\n\n36. A. Merle, S. Senecal, A. St-Onge, Whether and how virtual try-on influences\nconsumer responses to an apparel web site. Int. J. Electron. Commer. 16,\n41–64 (2012)\n\n37. Niswar, A.; Khan, I.R.; Farbiz, F. In Virtual try-on of eyeglasses using 3d model\nof the head, Proceedings of the 10th International Conference on Virtual\nReality Continuum and Its Applications in Industry. New York: ACM; 2011.\npp 435–438\n\n38. Koestinger M, Wohlhart P, Roth PM, Bischof H. Annotated facial landmarks\nin the wild: A large-scale, real-world database for facial landmark\nlocalization. First IEEE International Workshop on Benchmarking Facial\nImage Analysis Technologies, 2011\n\n39. Q. Zhou, Multi-layer affective computing model based on emotional\npsychology. Electron. Commer. Res. 18(1), 109–124 (2018). https://doi.org/\n10.1007/s10660-017-9265-8\n\n40. Q. Zhou, Z. Xu, N.Y. Yen, User sentiment analysis based on social network\ninformation and its application in consumer reconstruction intention.\nComput. Hum. Behav. (2018) https://doi.org/10.1016/j.chb.2018.07.006\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 19 of 19\n\nhttp://www.meijing.com/tryinon.html\nhttp://www.kede.com/frame\nhttp://www.biyao.com/home/index.html\nhttps://doi.org/10.1007/s10586-017-1330-5\nhttp://www.csdn.net/article/2012-11-16/2811943-google-launches-ingress\nhttp://www.csdn.net/article/2012-11-16/2811943-google-launches-ingress\nhttps://doi.org/10.1007/s10660-017-9265-8\nhttps://doi.org/10.1007/s10660-017-9265-8\nhttps://doi.org/10.1016/j.chb.2018.07.006\n\n\tAbstract\n\tIntroduction\n\tResearch status of network virtual try-on technology\n\tMethods of face recognition\n\tSVM-based face detection\n\tFace recognition based on SIFT\n\tBasic principle of SIFT algorithm\n\tKey point matching\n\tFace recognition experiment\n\n\tFace alignment\n\tImage normalization\n\tLocal feature extraction of SIFT algorithm\n\tSDM algorithm alignment result\n\n\tFace pose estimation\n\tFeature point labelling\n\tCamera labeling\n\tFeature point mapping\n\n\tTracking registration system\n\tAffine transformation method of glasses try-on\n\tPerspective transformation method of glasses try-on\n\n\tVirtual model generation system\n\tVirtual and real synthesis system\n\n\tiOS system application\n\tMenu module\n\tUser registration module\n\tCommodity module\n\tCommodity browsing\n\tCommodity screening\n\tGlasses try-on\n\tAdding to cart\n\tBuy a glasses immediately\n\tTaking photos or recording videos\n\tUploading and sharing\n\n\tCollection module\n\tShopping cart module\n\tOrder module\n\tCoupon module\n\tPhoto wall module\n\tSetting module\n\n\tResults and discussion\n\tConclusions\n\tAbbreviations\n\tAcknowledgements\n\tFunding\n\tAvailability of data and materials\n\tAuthor’s contributions\n\tAuthor’s information\n\tEthics approval and consent to participate\n\tConsent for publication\n\tCompeting interests\n\tPublisher’s Note\n\tReferences\n\n",
      "metadata_storage_path": "aHR0cHM6Ly9lbnJpY2hlZHN0b3JhZ2VhY2NvdW50LmJsb2IuY29yZS53aW5kb3dzLm5ldC9saWJyYXJ5L3MxMzY0MC0wMTgtMDM3My04LnBkZg2",
      "authors": [
        "Boping Zhang",
        "Zhang",
        "Meijing",
        "Kede",
        "Biyao",
        "Huang",
        "Cheng",
        "Gaussian",
        "Gauss",
        "ian Laplace",
        "Laplacian",
        "Ransac",
        "Harris",
        "Zhu",
        "ginal",
        "Yang",
        "Patras"
      ],
      "institutions": [
        "iOS",
        "SVM",
        "School of Information Engineer",
        "Xuchang University",
        "EURASIP Journal",
        "Creative Commons",
        "EURASIP",
        "Camirror",
        "Smart Look",
        "Ipoint",
        "eling",
        "HMM",
        "Markov",
        "AdaBoost",
        "XNtrain",
        "Ntrain",
        "Nsv",
        "Otrain",
        "Xtrain",
        "Gaussian DOG",
        "Terravic Re",
        "search Corporation",
        "SIFT",
        "IBUG",
        "LFW",
        "Rotary",
        "Alipay",
        "Weibo",
        "oepnframeworks",
        "AFNetworking"
      ],
      "key_phrases": [
        "accurate feature point alignment model",
        "new online shopping mode",
        "Augmented reality virtual glasses",
        "external propaganda channels",
        "actual wearing effects",
        "Creative Commons Attribution",
        "lates physical information",
        "phys- ical information",
        "head pose estimation",
        "traditional online shopping",
        "virtual glasses model",
        "input device-monocular camera",
        "key research issue",
        "virtual world scene",
        "user online glasses",
        "RESEARCH Open Access",
        "1 Introduction Network virtual",
        "iOS mobile platform",
        "local face features",
        "virtual model",
        "feature points",
        "scale-invariant feature",
        "new way",
        "2018 Open Access",
        "research key",
        "local features",
        "virtual information",
        "Face information",
        "Information Engineer",
        "real world",
        "Virtual try",
        "favorite glasses",
        "different glasses",
        "user senses",
        "face detection",
        "human face",
        "iOS glasses",
        "Boping Zhang",
        "optimal purchase",
        "robust SIFT",
        "rapid achievement",
        "sponding images",
        "vector machine",
        "EURASIP Journal",
        "Video Processing",
        "The Author",
        "iOS platform",
        "AR glasses",
        "camera image",
        "quick try",
        "human senses",
        "AR principle",
        "Computer vision",
        "real time",
        "real-time interaction",
        "Xuchang University",
        "APP system",
        "descent method",
        "computer technology",
        "SVM classifier",
        "Abstract",
        "development",
        "commerce",
        "gap",
        "goods",
        "speed",
        "immersion",
        "SDM",
        "identification",
        "Keywords",
        "merchants",
        "consumers",
        "important",
        "part",
        "field",
        "experiential",
        "position",
        "angle",
        "corre",
        "screen",
        "sound",
        "taste",
        "touch",
        "space",
        "sensory",
        "perience",
        "faces",
        "bopingzhang",
        "yeah",
        "School",
        "Henan",
        "China",
        "article",
        "terms",
        "real-time head mo- tion tracking",
        "Two corresponding isosceles triangles",
        "General face recognition process",
        "original author(s",
        "charge-coupled device) camera",
        "Android mobile platform",
        "Creative Commons license",
        "real envir- onment",
        "foreign glasses sellers",
        "3D modeling approach",
        "2D image overlay",
        "2D image superposition",
        "AR technology design",
        "2D face images",
        "virtual optician system",
        "image processing technology",
        "real-time requirements",
        "2D images",
        "accurate tracking",
        "International License",
        "The superposition",
        "3D glasses",
        "glasses images",
        "network virtual",
        "virtual try",
        "virtual objects",
        "Virtual glasses",
        "unrestricted use",
        "appropriate credit",
        "Research status",
        "Smart Look",
        "wearing effect",
        "research hotspots",
        "research results",
        "four categories",
        "Three points",
        "fine transformation",
        "unrealistic deformation",
        "inter- action",
        "good results",
        "different defects",
        "depth research",
        "large extent",
        "new ideas",
        "false ratios",
        "important indicator",
        "feature extraction",
        "dimension reduc",
        "first step",
        "cific location",
        "Glasses companies",
        "glasses model",
        "AR-based glasses",
        "face features",
        "face area",
        "face needs",
        "matching recognition",
        "doi.org",
        "orcid.org",
        "shopping experience",
        "input sensor",
        "experience senses",
        "input image",
        "computer vision",
        "augmented reality",
        "video stream",
        "system performance",
        "online shopping",
        "based detection",
        "creativecommons",
        "licenses",
        "distribution",
        "reproduction",
        "medium",
        "source",
        "link",
        "changes",
        "Zhang",
        "crossmark",
        "crossref",
        "dialog",
        "USA",
        "Camirror",
        "Ipoint",
        "Kisok",
        "Xview",
        "function",
        "Users",
        "domestic",
        "Meijing",
        "Kede",
        "Biyao",
        "present",
        "Huang",
        "eyes",
        "pose",
        "scale",
        "method",
        "realism",
        "Cheng",
        "monocular",
        "CCD",
        "marker",
        "kinds",
        "techniques",
        "advantages",
        "problems",
        "application",
        "precondition",
        "frame",
        "posture",
        "lumination",
        "occlusion",
        "omission",
        "people",
        "Fig.",
        "gion",
        "range",
        "Offline Learning Training Feature Feature Image Extraction Space Testing Feature Feature Image Extraction Macthing Online Learning",
        "Face Macthing Recognition Recognition Results CLassification Face Result Input Feature Feature Searching",
        "The Gaussian kernel function Kðx",
        "Extraction Dimensionality Image Tracing",
        "Face recognition process Zhang",
        "high-dimensional face feature vector",
        "high-dimensional feature space",
        "machine learning method",
        "training sample xi",
        "online matching models",
        "Feature extraction",
        "three-layer feedforward neural network",
        "face feature identification",
        "high-dimensional feature vector",
        "Feature dimension reduction",
        "low-dimensional input space",
        "Sigmoid kernel functions",
        "detection result",
        "local binary patterns",
        "local structural content",
        "Common kernel func",
        "3.2 Face recognition",
        "Gabo feature",
        "sional face features",
        "3.1 SVM-based face detection",
        "face detection methods",
        "spatial fre- quency",
        "face image",
        "low-dimensional subspace information",
        "support vector machine",
        "optimal parameter vector",
        "Matching recognition",
        "space complexity",
        "classic SVM algorithm",
        "tected image",
        "input vector",
        "face database",
        "Nsv X",
        "detection process",
        "SVM x",
        "vector dimension",
        "Common features",
        "width function",
        "quadratic function",
        "discriminant function",
        "spatial position",
        "iyjK xi",
        "optimization parameter",
        "ginal features",
        "matching strategy",
        "network output",
        "oriented gradient",
        "illumination changes",
        "small displacements",
        "ture changes",
        "brightness invariance",
        "direction selectivity",
        "previous links",
        "final decision",
        "virtual glasses",
        "Markov model",
        "statistical theory",
        "nonlinear mapping",
        "mal hyperplane",
        "largest interval",
        "inner product",
        "Mercer condition",
        "Optimization problem",
        "video sequence",
        "description ability",
        "output layer",
        "Figure 3 shows",
        "LBP",
        "HOG",
        "histogram",
        "Gabor",
        "insensitiveness",
        "overall",
        "Page",
        "time",
        "premise",
        "technology",
        "scholars",
        "HMM",
        "AdaBoost",
        "structure",
        "Equation",
        "¼ sgn",
        "XNtrain",
        "polynomial",
        "zÞ",
        "Eq.",
        "�NtrainÞ",
        "jy",
        "xj",
        "∂i",
        "∂�jK",
        "SIFT",
        "ð1Þ",
        "2σ",
        "∗",
        "Figure 4 shows flow block diagram",
        "SVM Classifier detection Output results",
        "SVM network structure Zhang",
        "other feature extraction functions",
        "higher positioning accuracy",
        "highest gradient value",
        "ian Laplace function",
        "DoG operator curve",
        "potential fixed points",
        "Establishing Gaussian pyramid",
        "variable scale parameter",
        "Gaussian differential functions",
        "gradient direction histogram",
        "stitute tower model",
        "Key point matching",
        "different scale spaces",
        "scale space sequence",
        "scale space factor",
        "Gaussian difference pyramid",
        "Gaussian difference function",
        "image matching model",
        "robust SIFT algorithm",
        "② Position key points",
        "tower top image",
        "Gaussian function",
        "extreme detection",
        "candidate position",
        "Gauss-Laplacian curve",
        "feature construction",
        "local feature",
        "feature vector",
        "Gaussian blur",
        "extreme points",
        "face recognition",
        "face alignment",
        "brightness changes",
        "sta- bility",
        "affine transformation",
        "angle change",
        "Basic principle",
        "multiple details",
        "X1 K",
        "Sgn Otrain",
        "train Xtrain",
        "stability degree",
        "local gradients",
        "main contour",
        "edge features",
        "larger scale",
        "human eye",
        "pixel location",
        "different sizes",
        "first layer",
        "second layer",
        "minimum dimension",
        "red line",
        "blue line",
        "image search",
        "blurred image",
        "original image",
        "new image",
        "generation process",
        "main direction",
        "formation process",
        "convolution operation",
        "scale normalization",
        "The relationship",
        "conditions",
        "rotation",
        "noise",
        "scales",
        "selection",
        "kind",
        "symbol",
        "target",
        "retina",
        "dimensions",
        "template",
        "down-sampling",
        "pyramids",
        "bottom",
        "layers",
        "number",
        "¼ log2",
        "qð",
        "logarithm",
        "maxima",
        "minima",
        "2∇2G",
        "constant",
        "Import",
        "End",
        "3.2.1",
        "3.2.2",
        "σ",
        "ð6Þ",
        "Fig. 4 SIFT algorithm flow chart",
        "Image input template input Detection",
        "red intermediate detection point",
        "local characteristic region characteristic",
        "Gaussian DOG square column",
        "Wrong image matching characteristic",
        "Spatial extreme detection",
        "two adjacent layers",
        "feature point field",
        "3-layer Gaussian pyramid",
        "local extreme points",
        "2-layer DoG pyramid",
        "N extreme points",
        "Gaussian pyramid Zhang",
        "lower scale spaces",
        "Gaussian difference image",
        "scale space value",
        "Gradient histogram statistics",
        "independent characteristic",
        "matching image",
        "adjacent points",
        "extreme extracti",
        "pixel point",
        "adjacent upper",
        "key point",
        "ence space",
        "gradient value",
        "DoG operator",
        "correct matching",
        "successful matching",
        "group Zhang",
        "edge response",
        "stability characteristicmatching",
        "Octave2 Octave1",
        "direc- tion",
        "next octave",
        "lower images",
        "peak value",
        "same group",
        "gradient direction",
        "auxiliary direction",
        "Laplacian operator",
        "contributive pixels",
        "26 points",
        "¼ L",
        "calculation",
        "surrounding",
        "case",
        "invariance",
        "perspective",
        "set",
        "tors",
        "descriptor",
        "other",
        "Location",
        "Octave5",
        "Octave4",
        "Octave3",
        "probability",
        "ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi",
        "neighborhood",
        "Comparison",
        "Gauss-Laplacian",
        "36 columns",
        "maximum",
        "robustness",
        "2.5",
        "3.",
        "θ",
        "360°",
        "10°",
        "Terravic Re- search Corporation",
        "tracking registra- tion subsystem",
        "DoG space extreme point",
        "many face alignment algorithms",
        "SDM-based face alignment process",
        "match- ing performance",
        "main direction Zhang",
        "feature vector descriptor",
        "original data information",
        "face infrared database",
        "① SIFT matching performance",
        "main direction transformation",
        "20 infrared image sequences",
        "initial matching method",
        "Harris matching method",
        "Face recognition experiment",
        "SIFT matching algorithm",
        "head rotation angle",
        "face feature points",
        "face image detection",
        "3.3 Face alignment",
        "matching process",
        "SIFT algorithm",
        "motion direction",
        "matching re",
        "experimental data",
        "real face",
        "illumination transformation",
        "head posture",
        "scaling transformation",
        "Ransac method",
        "mismatched points",
        "key points",
        "contour points",
        "light-illuminated pictures",
        "mismatch rate",
        "other words",
        "success rate",
        "overall trend",
        "inevitable errors",
        "good effect",
        "function approximation",
        "step size",
        "optimal position",
        "sonable translation",
        "shape factors",
        "average face",
        "estimation error",
        "matching accuracy",
        "perimental image",
        "Image normalization",
        "number difference",
        "same number",
        "Error degree",
        "first sample",
        "sample size",
        "Three pairs",
        "correct matches",
        "other variables",
        "Table 1 show",
        "experimental samples",
        "tween average",
        "total number",
        "60 pairs",
        "120 samples",
        "hats",
        "images",
        "Figure",
        "work",
        "classic",
        "group",
        "grees",
        "sults",
        "shade",
        "darkness",
        "finiteness",
        "positioning",
        "eyebrows",
        "nose",
        "mouth",
        "Scale",
        "Zhu",
        "offset",
        "iteration",
        "current",
        "efficiency",
        "training",
        "interference",
        "3.1",
        "nonlinear feature extraction function",
        "lateral face alignment results",
        "Face feature point location",
        "3.3.3 SDM algorithm alignment result",
        "face feature point alignment",
        "principal component analysis",
        "world coordinate system",
        "LFW face databases",
        "Local feature extraction",
        "current spatial position",
        "new spatial position",
        "n feature points",
        "local feature points",
        "initialization feature point",
        "71 face feature points",
        "68 face feature points",
        "objective function Eq",
        "3D face model",
        "3.4 Face pose estimation",
        "linear regression problem",
        "The SIFT feature",
        "camera parame- ters",
        "3D rigid object",
        "3D geometric position",
        "ginal face image",
        "iteration step size",
        "geometric model parameters",
        "10 Sample sequence set",
        "Matching results",
        "alignment accuracy",
        "3D position",
        "face images",
        "The pose",
        "input feature",
        "corresponding points",
        "extraction process",
        "SIFT features",
        "3D coordinates",
        "training sample",
        "test sample",
        "optimal solution",
        "initial value",
        "head rotation",
        "two parameters",
        "critical dimensions",
        "pts file",
        "head surface",
        "most cases",
        "optical properties",
        "projection relationship",
        "two movements",
        "① Translation movement",
        "test phase",
        "test images",
        "2D coordinates",
        "tion coordinates",
        "training process",
        "camera imaging",
        "Camera labeling",
        "camera relative",
        "training set",
        "dimension reduction",
        "argminR0b0 X",
        "x∗",
        "Δx",
        "center",
        "128 di",
        "R128n",
        "operation",
        "xk",
        "Rk",
        "bk",
        "paths",
        "x0",
        "xi",
        "way",
        "impact",
        "amount",
        "Section",
        "detail",
        "IBUG",
        "former",
        "sets",
        "Figures",
        "orientation",
        "periments",
        "direction",
        "idea",
        "lows",
        "∅",
        "100",
        "200 300",
        "500",
        "radial distortion parame- ters",
        "Match result analysis table",
        "key points Match ratio",
        "general 3D face model",
        "False match rate",
        "Surrey Face Model",
        "face alignment process",
        "training face sample",
        "average face Extract",
        "face detection model",
        "rigid body invariance",
        "y direc- tions",
        "SDM Iterative Solution",
        "3.4.1 Feature point labelling",
        "4.3 Feature point mapping",
        "arbitrary reference/coordinate system",
        "unknown scaling factor",
        "tersects image plane",
        "late 3D coordinates",
        "2D image plane",
        "image coordinate system",
        "rotation matrix R",
        "3D model",
        "SDM Zhang",
        "3D pose",
        "P point",
        "world coordinates",
        "② Rotary movement",
        "XYZ axis",
        "six degrees",
        "six numbers",
        "nose tip",
        "manual labeling",
        "pose estimation",
        "focal length",
        "Variate Number",
        "Total number",
        "Head rotation",
        "position O",
        "linear equations",
        "following form",
        "same image",
        "3D object",
        "N points",
        "six points",
        "rota- tion",
        "Equa- tion",
        "optical center",
        "hat Zhang",
        "Wearing glasses",
        "S X",
        "camera matrix",
        "Translation vector",
        "Table 1",
        "5 ¼ R",
        "5 ¼ S",
        "freedom",
        "corners",
        "SFM",
        "Yang",
        "Patras",
        "systems",
        "projection",
        "parameters",
        "light",
        "matches",
        "change",
        "Input",
        "plenty",
        "problem",
        "fx",
        "fy",
        "ray",
        "300",
        "τ",
        "4.2",
        "front face alignment Zhang",
        "face feature point tracking method",
        "2D facial feature points",
        "three-point transform- ation",
        "3.5 Tracking registration system",
        "Tracking registration technology",
        "least square method",
        "Perspective transformation method",
        "computer-generated virtual objects",
        "geometric transformation relation",
        "corresponding perspective changes",
        "fine transformation parameter",
        "overlarge head corner",
        "Affine transformation method",
        "poor user experience",
        "good tacking effect",
        "3D facial point",
        "isosceles right triangle",
        "2D position",
        "right position",
        "Face geometry",
        "face superposition",
        "user face",
        "stereoscopic changes",
        "human head",
        "user head",
        "3D point",
        "correct poses",
        "projection error",
        "tration techniques",
        "first technique",
        "two methods",
        "two corners",
        "image plane",
        "Pose estimation",
        "Eqs",
        "r00",
        "distance",
        "tures",
        "sum",
        "squares",
        "scenes",
        "movement",
        "picture",
        "eye",
        "ABC",
        "threshold",
        "experiment",
        "dx",
        "vertices",
        "motion",
        "The",
        "deformation",
        "characteristics",
        "flatness",
        "parallelism",
        "664",
        "þ",
        "3.5.2",
        "T X Y Camera coordinate systems",
        "C O Z plane-coordinate system R",
        "Three coordinate systems Zhang",
        "translucent frosted glass effect",
        "traditional frosted glass method",
        "The 3DS data file",
        "3.6 Virtual model generation system",
        "World coordinate systems",
        "4.1 Menu module Menu module",
        "3DS format file",
        "frosted glass UImage",
        "eye feature points",
        "side face alignment",
        "analytic oper- ation",
        "entire applica- tion",
        "real-time tracking effect",
        "most function blocks",
        "face recognition method",
        "mobile “AR Glasses",
        "default startup page",
        "real synthesis system",
        "3D glasses model",
        "4.2 User registration module",
        "Commodity browsing module",
        "4 iOS system application",
        "3DS model",
        "glass tool",
        "operational data",
        "3ds max",
        "framework module",
        "module title",
        "current module",
        "module switching",
        "system method",
        "space model",
        "switching method",
        "Sales System",
        "perspective transformation",
        "visual law",
        "w P",
        "OpenGL function",
        "perfect combination",
        "realistic scenes",
        "exact position",
        "natural features",
        "overall structure",
        "fused glasses",
        "operating efficiency",
        "tenance procedure",
        "third-party framework",
        "basic functions",
        "glasses products",
        "goods purchasing",
        "Alipay payment",
        "social platform",
        "same glasses",
        "ap- plication",
        "ViewController controller",
        "home page",
        "shopping cart",
        "menu page",
        "upper-left icon",
        "fault page",
        "UIImage object",
        "proxy event",
        "user address",
        "photo wall",
        "Menu view",
        "leftmost view",
        "switchable modules",
        "common controls",
        "goods collection",
        "vast users",
        "setting modules",
        "3.7 Virtual",
        "human",
        "markers",
        "subsystem",
        "effectiveness",
        "dependencies",
        "login",
        "carts",
        "agement",
        "addition",
        "WeChat",
        "Weibo",
        "effects",
        "others",
        "needs",
        "core",
        "sub-modules",
        "views",
        "favorites",
        "order",
        "coupon",
        "initialization",
        "controllers",
        "right",
        "background",
        "top",
        "table",
        "Glasses model Scene acquisition Tracking registry Model generation system system",
        "commodity Product modification Address Order statistics",
        "Commodity User Collection shopping Order discount",
        "virtual world camera display system User UI",
        "Augmented reality glasses sales system",
        "commodity list Glasses photogr uploading",
        "AR Glasses Sales System",
        "Virtual reality synthesis system",
        "VIP promo- tion activities",
        "remaining product information",
        "goods trolley coupon",
        "More commodity information",
        "Second-level screening catalog",
        "photo setting browsing",
        "screening tool class",
        "first-level screening title",
        "User registration module",
        "parent container offset",
        "third-level screening catalog",
        "custom table cells",
        "eyeglass images Zhang",
        "display result",
        "List screening",
        "Commodity screening",
        "4.3 Commodity module",
        "commodity browsing",
        "Real world",
        "world cameras",
        "first-level catalog",
        "product image",
        "parent class",
        "screening result",
        "registration button",
        "big images",
        "third-level menu",
        "third-level options",
        "first-level menu",
        "4.3.3 Glasses",
        "face information",
        "isosceles triangle",
        "C B",
        "leg style",
        "proxy methods",
        "multiple items",
        "one item",
        "segment head",
        "filter item",
        "actual environment",
        "Show photos",
        "local summary",
        "unselected state",
        "Full summary",
        "mobile phone",
        "capture area",
        "capture effect",
        "registration page",
        "registered users",
        "face data",
        "The face",
        "face position",
        "login status",
        "detail page",
        "pull-up refresh",
        "screen height",
        "blue subtitle",
        "General structure",
        "real Coordinates",
        "deletion management",
        "table height",
        "eligible products",
        "front camera",
        "prices",
        "quadrilateral",
        "key",
        "pull-down",
        "UIScrollView",
        "scrollViewDidScroll",
        "ber",
        "cross",
        "tables",
        "comments",
        "aphing",
        "server",
        "record",
        "implementation",
        "prompt",
        "gine",
        "quick",
        "3.2",
        "SDM Iterative Solution Feature extraction Face Alignment Feature point Face rotation",
        "average face SVM classitier SIFT Extract",
        "face design treatment transformation",
        "3d effect Transparency perspective",
        "lens selection Angle",
        "lens End problems",
        "XML file parsing",
        "detailed flow chart",
        "3D coordinates library",
        "third-party con- trols",
        "engine loading model",
        "user face data",
        "defining interface classes",
        "model generation methods",
        "disconnection request failure",
        "most web pages",
        "left detail button",
        "perspective relationship",
        "detailed Sessions",
        "third-party oepnframeworks",
        "Web site",
        "third-party applications",
        "chronous loading",
        "product details",
        "tracking registration",
        "main function",
        "UIKit library",
        "NSURL- Connection",
        "simple URL",
        "larger complexity",
        "JAvascript code",
        "convenient application",
        "URLString parameters",
        "following interface",
        "interface TGImageTool",
        "following methods",
        "class library",
        "Appdeleagte class",
        "video necessary",
        "iOS development",
        "simple page",
        "Import Image",
        "HTTP requests",
        "HTTP responses",
        "HTML page",
        "parameters WithBlock",
        "network images",
        "front",
        "Mean",
        "system",
        "glasses",
        "ments",
        "virtual",
        "stability",
        "dependence",
        "openframeworks",
        "window",
        "ofApp",
        "AFNetworking",
        "XCode",
        "authority",
        "Cookie",
        "NSURLConnection",
        "access",
        "difficulty",
        "eyeglass",
        "2D",
        "JSON",
        "plist",
        "interfaces",
        "NSString",
        "pictures",
        "call",
        "Upload",
        "POST",
        "NSData",
        "WithKey",
        "WithTypeO",
        "resultBlock",
        "NSDictionary",
        "caching",
        "Foundation",
        "NSObject",
        "complicated parameter information",
        "information improvement page",
        "tool class method",
        "order information",
        "commodity information",
        "clear method",
        "conversion method",
        "place options",
        "download requests",
        "SDWebImageManager sharedManager",
        "JSON strings",
        "server application",
        "dic JSONString",
        "QuartzCore library",
        "four preview",
        "place imageView",
        "implementation TGImageTool",
        "photo/video button",
        "satisfied glasses",
        "VR glasses",
        "record videos",
        "sharing functions",
        "downloadImage",
        "url",
        "placeholder",
        "UIImage",
        "end",
        "import",
        "WebCache",
        "SDWebImageLowPri",
        "ority",
        "SDWebImageRetryFailed",
        "memory",
        "SDImageCache",
        "animation",
        "path",
        "combined",
        "User",
        "photos",
        "engine",
        "areas",
        "details",
        "Uploading"
      ],
      "merged_content": "\nRESEARCH Open Access\n\nAugmented reality virtual glasses try-on\ntechnology based on iOS platform\nBoping Zhang\n\nAbstract\n\nWith the development of e-commerce, network virtual try-on, as a new online shopping mode, fills the gap that\nthe goods cannot be tried on in traditional online shopping. In the work, we discussed augmented reality virtual\nglasses try-on technology on iOS platform to achieve optimal purchase of online glasses, improving try-on speed of\nvirtual glasses, user senses of reality, and immersion. Face information was collected by the input device-monocular\ncamera. After face detection by SVM classifier, the local face features were extracted by robust SIFT. Combined with\nSDM, the feature points were iteratively solved to obtain more accurate feature point alignment model. Through\nthe head pose estimation, the virtual model was accurately superimposed on the human face, thus realizing the\ntry-on of virtual glasses. The above research was applied in iOS glasses try-on APP system to design the try-on system\nof augmented reality virtual glasses on iOS mobile platform. It is proved that the method can achieve accurate\nidentification of face features and quick try-on of virtual glasses.\n\nKeywords: Virtual try-on, Virtual glasses, Augmented reality, Computer vision, Pose estimation, iOS\n\n1 Introduction\nNetwork virtual try-on is a new way of online shopping.\nWith the development of e-commerce, it broadens the\nexternal propaganda channels of merchants to enhance\nthe interaction between consumers and merchants.\nVirtual try-on fills the gap that the goods cannot be\ntried on in traditional online shopping. As an important\npart of network virtual try-on, virtual glasses try-on\ntechnology has become a key research issue in this field\nrecently [1–4]. During virtual glasses try-on process,\nconsumers can select their favorite glasses by compar-\ning the actual wearing effects of different glasses in the\nonline shopping. The research key of virtual glasses\ntry-on system is the rapid achievement of experiential\nonline shopping.\nAR (augmented reality) calculates the position and\n\nangle of camera image in real time while adding corre-\nsponding images. The virtual world scene is superim-\nposed on a screen in real world for real-time\ninteraction [5]. Using computer technology, AR simu-\nlates physical information (vision, sound, taste, touch,\netc.) that is difficult to experience within certain time\n\nand space of real world. After superimposition of phys-\nical information, the virtual information is perceived by\nhuman senses in real world, thus achieving sensory ex-\nperience beyond reality [6].\nBased on AR principle, virtual glasses try-on technol-\n\nogy achieves optimal purchase of user online glasses and\nquick try-on of virtual glasses, improving the senses of\nreality and immersion. Monocular camera is used as the\ninput device to discuss try-on technology of AR glasses\non iOS platform. Face information is collected by mon-\nocular camera. After face detection by SVM (support\nvector machine) classifier, the local features of faces are\nextracted by robust SIFT (scale-invariant feature trans-\nform). Combined with SDM (supervised descent\nmethod), the feature points were iteratively solved to ob-\ntain more accurate feature point alignment model.\nThrough the head pose estimation, the virtual glasses\nmodel was accurately superimposed on the human face,\nthus realizing the try-on of virtual glasses. The above re-\nsearch is applied in iOS glasses try-on APP system to de-\nsign the try-on system of AR glasses on iOS mobile\nplatform. It is proved that the method can achieve ac-\ncurate identification of face features and quick try-on of\nvirtual glasses.Correspondence: bopingzhang@yeah.net\n\nSchool of Information Engineer, Xuchang University, Xuchang 461000,\nHenan, China\n\nEURASIP Journal on Image\nand Video Processing\n\n© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 \nhttps://doi.org/10.1186/s13640-018-0373-8\n\n  \n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-018-0373-8&domain=pdf\nhttp://orcid.org/0000-0001-7835-7622\nmailto:bopingzhang@yeah.net\nhttp://creativecommons.org/licenses/by/4.0/\n\n\n2 Research status of network virtual try-on\ntechnology\nGlasses try-on system was first applied in the USA.\nGlasses companies such as Camirror, Smart Look, Ipoint\nKisok, and Xview pioneered the online try-on function [7].\nUsers freely feel the wearing effect, enhancing the online\nshopping experience. Recently, online try-on function is\nexplored by domestic and foreign glasses sellers, such as\nMeijing [8], Kede [9] and Biyao [10].\nVirtual glasses try-on system involves computer vision,\n\naugmented reality, and image processing technology.\nRecently, research hotspots are speed, experience, and\nimmersion of try-on. At present, research results can be di-\nvided into four categories, namely 2D image superposition,\n3D glasses superimposed on 2D face images, 3D face mod-\neling, and AR technology based on video stream [11–14].\nHuang [15] introduced virtual optician system based on\n\nvision, which detects user’s face before locating user’s eyes.\nThree points are selected from face and glasses images.\nTwo corresponding isosceles triangles are formed for af-\nfine transformation, thus estimating the pose and scale of\nface in real time. This method realizes real-time head mo-\ntion tracking. However, the glasses model easily produces\nunrealistic deformation, affecting the realism of the\nglasses.\nAR technology is also applied in the virtual glasses\n\ntry-on system. Cheng et al. [16] selected a monocular\nCCD (charge-coupled device) camera as the input sensor\nto propose AR technology design based on the inter-\naction of marker and face features. Virtual glasses try-on\nsystem is established based on Android mobile platform,\nachieving good results. During virtual try-on process, we\nuse 2D image overlay or 3D modeling approach. There\nare still different defects although all kinds of virtual\nglasses try-on techniques have certain advantages. The\nsuperposition of 2D images is unsatisfactory in the sense\nof reality. Besides, the 3D modeling takes too long to\nmeet the real-time requirements of online shopping.\n\nIn-depth research is required to realize accurate tracking\nand matching. These problems can be solved by\nAR-based glasses try-on technology to a large extent,\nthus providing new ideas for virtual try-on technology.\n\n3 Methods of face recognition\nIt is necessary to integrate virtual objects into real envir-\nonment for the application of AR technology in virtual\nglasses try-on system, wherein face recognition is the\nprecondition for virtual glasses try-on system. During\ntry-on process, it is necessary to detect the face in each\nframe of the video. However, the problems of posture, il-\nlumination, and occlusion can increase the omission and\nfalse ratios of face detection. The real time of detection\nis an important indicator of system performance to en-\nhance people’s experience senses.\nGeneral face recognition process consists of face de-\n\ntection, tracking, feature extraction, dimension reduc-\ntion, and matching recognition (see Fig. 1) [17].\nIn Fig. 1, face detection is the first step to realize face\n\nrecognition. Its purpose is to automatically find face re-\ngion in an input image. If there is a face area, the spe-\ncific location and range of face needs to be located. Face\ndetection is divided into image-based and video-based\ndetection. If the input is a still image, each image is de-\ntected; if the input is a video, face detection is performed\nthroughout the video sequence.\nFeature extraction is based on face detection, and the\n\ninput is the detected face image. Common features are\nLBP (local binary patterns), HOG (histogram of oriented\ngradient), Gabor, etc. HOG [18] describes the edge fea-\ntures. Due to insensitiveness to illumination changes and\nsmall displacements, it describes the overall and local in-\nformation of human face. LBP [19] shows the local tex-\nture changes of an image, with brightness invariance.\nGabo feature [20] captures the local structural content\nof spatial position, direction selectivity, and spatial fre-\nquency. It is suitable for description of human faces.\n\nFig. 1 Face recognition process\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 2 of 19\n\n Face Macthing Recognition Recognition Results CLassification Face Result Input Feature Feature Searching Extraction Dimensionality Image Tracing Offline Learning Training Feature Feature Image Extraction Space Testing Feature Feature Image Extraction Macthing Online Learning \n\n\n\nFeature dimension reduction is described as follows.\nFace feature is generally high-dimensional feature vector.\nFace recognition of high-dimensional feature vector\nincreases time and space complexity. Besides, it is difficult\nto effectively judge the description ability of high-dimen-\nsional face features. The high-dimensional face feature\nvector can be projected to the low-dimensional subspace.\nThe low-dimensional subspace information can complete\nface feature identification. After feature extraction, the ori-\nginal features are recombined to reduce vector dimension\nof face feature.\nAfter the previous links, we compare the existing tar-\n\ngets in face database and the faces to be identified based\non certain matching strategy, making final decision.\nMatching recognition can be represented by offline\nlearning and online matching models.\n\n3.1 SVM-based face detection\nFace detection is the premise of virtual glasses try-on\ntechnology. Recently, scholars proposed face detection\nmethods, such as neural network, SVM (support vector\nmachine), HMM (hidden Markov model), and AdaBoost.\nIn the work, the classic SVM algorithm is used for face\ndetection. SVM algorithm is a machine learning method\nbased on statistical theory. Figure 2 shows the network\nstructure of SVM [21]. SVM algorithm can be regarded\nas a three-layer feedforward neural network with a hid-\nden layer. Firstly, the input vector is mapped from\nlow-dimensional input space to the high-dimensional\nfeature space by nonlinear mapping. After that, the opti-\nmal hyperplane with the largest interval is constructed\nin the high-dimensional feature space.\nIt is denoted that the input vector of SVM x = (x1, x2, …, xn).\n\nEquation (1) shows the network output of output layer\nbased on x.\n\ny xð Þ ¼ sgn\nXNtrain\n\ni¼1 yi∂\n�\ni K x\n\ni; x\n� �\n\nþ b�\n� �\n\nð1Þ\n\nwherein the inner product K(x(i), x) is a kernel function\nsatisfying the Mercer condition. Common kernel func-\ntions consist of polynomial, Gauss, and Sigmoid kernel\n\nfunctions. The Gaussian kernel function Kðx; zÞ ¼ e\njjx−zjj2\n2σ2 ,\n\nand σ is the width function.\nOptimization problem of quadratic function (Eq. (2)) is\n\nsolved to obtain the optimal parameter vector ∂�\n\n¼ ð∂�1; ∂�2; …; ∂�NtrainÞ\nT in discriminant function.\n\nmin\n1\n2\nð\nXNtrain\n\ni¼1\nXNtrain\n\ni¼1 ∂i∂ jy\niyjK xi; xj\n\n� �\n−\nXNtrain\n\ni¼1 ∂i ð2Þ\n\ns:t:\nXNtrain\ni¼1\n\n∂iy\ni i ¼ 1; 2; …; Ntrain\n\n0≤∂i ≤C\n\nThe training sample xi corresponding to ∂i > 0 is used\nas a support vector. The optimization parameter b∗ can\nbe calculated by Eq. (3).\n\nb� ¼ 1\nNsv\n\nX\ni∈SV\n\nyi−\nX\n\nj∈SV\n∂�jK x\n\ni; xj\n� �� �\n\nð3Þ\n\nSVM classifier is used to determine whether the de-\ntected image is a human face. If it is not human face,\nthen the image is discarded. If it is, then the image is\nretained to output the detection result. Figure 3 shows\nthe detection process.\n\n3.2 Face recognition based on SIFT\nAfter face detection, face features are extracted for face\nrecognition, providing conditions for face alignment. In\nthe work, the robust SIFT algorithm is used for local fea-\nture extraction [22]. The algorithm finds feature points in\ndifferent scale spaces. It is irrelevant to rotation, scale, and\nbrightness changes. Besides, the algorithm has certain sta-\nbility to noise, affine transformation, and angle change.\n\n3.2.1 Basic principle of SIFT algorithm\nIn the process of feature construction by SIFT algorithm,\nit is necessary to deal with multiple details, achieving faster\noperation and higher positioning accuracy. Figure 4 shows\nflow block diagram of SIFT algorithm [21]. The generation\nprocess of local feature is described as follows [22]:\n\nFig. 2 SVM network structure\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 3 of 19\n\n X1 K(x ,x) K(x ,x) Sgn Otrain y train Xtrain ,x) \n\n\n\n① Detect extreme points\nGaussian differential functions are used for image search\n\non all scales, thus identifying potential fixed points.\n② Position key points\nThe scale on candidate position of model is confirmed.\n\nThe stability degree determines the selection of key points.\n③ Determine the direction of key points\nUsing the gradient direction histogram, each key point\n\nis assigned a direction with the highest gradient value to\ndetermine the main direction of key point.\n④ Describe the key points\nThe local gradients of image are calculated and repre-\n\nsented by a kind of symbol.\n\n3.2.2 Key point matching\n3.2.2.1 Scale space Scale space introduces a scale par-\nameter into image matching model. The continuously\nvariable scale parameter is used to obtain the scale space\nsequence. After that, the main contour of scale space is\n\ntaken as the feature vector to extract the edge features\n[23]. The larger scale leads to the more blurred image.\nTherefore, scale space can simulate the formation\nprocess of target on the retina of the human eye.\nScale space of image can be expressed as Eq. (4).\n\nL x; y; σð Þ ¼ G x; y; σð Þ � I x; yð Þ ð4Þ\nIn Eq. (4), G(x, y, σ) is the Gaussian function, I(x, y) the\n\noriginal image, and * the convolution operation.\n\n3.2.2.2 Establishing Gaussian pyramid\n\nG x; y; σð Þ ¼ 1\n2πσ2\n\ne− x−d=2ð Þ\n2þ y−b=2ð Þ2ð Þ=2σ2 ð5Þ\n\nIn Eq. (5), d and b are the dimensions of Gaussian\ntemplate, (x, y) is the pixel location, and σ the scale space\nfactor.\nGaussian pyramid is established according to Eq. (5),\n\nincluding Gaussian blur and down-sampling (see Fig. 5).\nIt is observed that the pyramids with different sizes con-\nstitute tower model from bottom to top. The original\nimage is used for the first layer, the new image obtained\nby down-sampling for the second layer. There are n\nlayers in each tower. The number of layers can be calcu-\nlated by Eq. (6).\n\nn ¼ log2 minf p; qð Þg−d dϵ 0; log2 minf p; qð Þg½ �\nð6Þ\n\nIn Eq. (6), p and q are the sizes of the original image and d\nis the logarithm of minimum dimension of tower top image.\n\n3.2.2.3 Gaussian difference pyramid After scale\nnormalization of maxima and minima of the Gaussian La-\nplace function σ2∇2G, we obtain the most stable image fea-\ntures using other feature extraction functions. The\nGaussian difference function is approximated to the Gauss-\nian Laplace function σ2∇2G after scale normalization. The\nrelationship is described as follows:\n\n∂G\n∂σ\n\n¼ σ2∇2G ð7Þ\n\nDifferential is approximately replaced by the difference:\n\nσ2∇2G ¼ ∂G\n∂σ\n\n≈\nG x; y; kσð Þ−G x; y; σð Þ\n\nkσ−σ\nð8Þ\n\nTherefore,\n\nG x; y; kσð Þ−G x; y; σð Þ ≈ k−1ð Þσ2∇2G ð9Þ\nIn Eq. (9), k − 1 is a constant.\nIn Fig. 6, the red line is the DoG operator curve; the\n\nblue line the Gauss-Laplacian curve. In extreme detection\n\nFig. 3 The detection process of SVM classifier\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 4 of 19\n\n Start Import Image No Does it include faces yes SVM Classifier detection Output results for face detection End \n\n\n\nmethod, the Laplacian operator is replaced by the DoG\noperator [24] (see Eq. (10).\n\nD x; y; σð Þ ¼ G x; y; kσð Þ−G x; y; σð Þð Þ � I x; yð Þ\n¼ L x; y; kσð Þ−L x; y; σð Þ ð10Þ\n\n3.2.2.4 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\n\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the ad-\njacent points to judge whether it is large or small (see\nFig. 6). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and\nlower scale spaces to detect extreme points.\nIn the calculation, the Gaussian difference image is the\n\ndifference between the adjacent upper and lower images\nin each group of the Gaussian pyramid (see Fig. 7).\n\n3.2.2.5 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the\nadjacent points to judge whether it is large or small\n(see Fig. 8). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and lower\nscale spaces to detect extreme points.\nIf there are N extreme points in each group, then we\n\nneed N + 2-layer DoG pyramid and N + 3-layer Gaussian\npyramid (see Fig. 8). Due to edge response, the extreme\npoints generated in this case are not all stable.\n\n3.2.2.6 Key point matching At first, the key point is\ncharacterized by position, scale, and direction. To main-\ntain the invariance of perspective and illumination\nchanges, the key point should be described by a set of vec-\ntors. Then, the descriptor consists of key points and other\ncontributive pixels. Besides, the independent characteristic\n\nFig. 4 SIFT algorithm flow chart\n\nFig. 5 Gaussian pyramid\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 5 of 19\n\n Image input Image input template input Detection extreme extracti on local characteristic region characteristic set Location the Key points of stability characteristicmatching Describing local characteristic region Dislodge the Wrong image matching characteristic set The input of matching image \n\n Octave5 80 Octave4 40 Octave3 20 Octave2 Octave1 \n\n\n\nof descriptor is guaranteed to improve the probability of\ncorrect matching of feature points.\nThe gradient value of key point is calculated. The gra-\n\ndient value and direction are determined by Eq. (11).\n\nm x; yð Þ ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nN x þ 1; yð Þ−N x−1; yð Þð Þ2 þ N x; y þ 1ð Þ−N x; y−1ð Þð Þ2\n\nq\n\nθ x; yð Þ ¼ α tan2 N x; y þ 1ð Þ−N x; y−1ð Þ\nN x þ 1; yð Þ−N x−1; yð Þ\n\n� �\nð11Þ\n\nIn Eq. (11), N represents the scale space value of key point.\nGradient histogram statistics. The gradient and direc-\n\ntion of pixels in the neighborhood are represented by\nhistogram. The direction ranges from 0 to 360°. There is a\n\nFig. 6 Comparison of Gauss-Laplacian and DoG\n\nFig. 7 Gaussian pyramid of each group\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 6 of 19\n\n 02 0.1 -0.1 02 -03 Laplacian - DOG -0.4 \n\n Scale (next octave) Gaussian DOG \n\n\n\nsquare column for every 10°, forming 36 columns [25]\n(see Fig. 9). In feature point field, the peak represents the\ngradient direction. The histogram of maximum is the\nmain direction of key point. Meanwhile, the histogram\nwith peak value greater than 80% of main direction is se-\nlected for auxiliary direction to improve the matching\nrobustness.\nAfter successful matching of key points, the entire al-\n\ngorithm is not over yet. This is because substantial mis-\nmatched points appear in the matching process. These\nmismatched points are eliminated by Ransac method in\nSIFT matching algorithm [26].\n\n3.2.3 Face recognition experiment\nTo evaluate the algorithm, the experiment is conducted\nbased on face infrared database provided by Terravic Re-\nsearch Corporation. There are a total of 20 infrared\nimage sequences with head rotation, glasses, hats, and\nlight-illuminated pictures. Three pairs of images are se-\nlected from each face, with a total of 60 pairs. Figure 10\nshows the selected 120 images. In the work, the classic\n\nSIFT matching algorithm is used as the initial matching\nmethod to manually determine matching accuracy and\nmismatch rate of each group. In other words, the match-\ning performance is described by accuracy and error de-\ngrees. Accuracy is defined by the ratio of the number of\ncorrect matches in total number. Error degree is the ra-\ntio of the number difference (between key and matched\npoints) in the total number of key points.\nThese 120 samples are conducted with abstract match-\n\ning contrast according to the variables including head\nrotation angle, illumination transformation, glasses, and\nhat wearing. Meanwhile, other variables remain the\nsame. Figures 11, 12, 13, and 14 show the matching re-\nsults, respectively:\n\n1. Matching results when head rotation angle changes\n2. Matching results when wearing glasses\n3. Matching results when wearing a hat\n4. Matching results when light and shade change\n\nThe experimental data are shown in Table 1.The ex-\nperimental image and Table 1 show:\n① SIFT matching performance is more easily affected\n\nby wearing glasses than head rotation angle, light illu-\nmination, darkness, and wearing hat.\n② In the case of the same number of matches, the\n\nsuccess rate of SIFT matching is higher than that of the\nHarris matching method [27].\nThe overall trend of results can be well presented al-\n\nthough there are inevitable errors due to the finiteness\nof experimental samples.\n\n3.3 Face alignment\nFace alignment is the positioning of face feature points.\nAfter face image detection, the SIFT algorithm automat-\nically positions the contour points of the eyebrows, eyes,\nnose, and mouth. In the try-on process of AR glasses,\nthe eyes are positioned to estimate the head posture.\nThe pose estimation is applied to the tracking registra-\ntion subsystem of glasses, thus producing perspective\n\nFig. 8 The detection of DoG space extreme point\n\nFig. 9 The histogram of the main direction\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 7 of 19\n\n Scale \n\n The main direction \n\n\n\ntransformation. However, the pose estimation is easily af-\nfected by the positioning of face feature points, resulting\nin estimation error. The feature points are accurately posi-\ntioned to achieve good effect of head pose estimation.\nAt present, there are many face alignment algorithms.\n\nSDM is a method of finding function approximation\nproposed by Zhu et al [28] by calculating the average\nface, and local features around each feature point are ex-\ntracted to form feature vector descriptor. The offset be-\ntween average and real face is calculated to obtain the\nstep size and motion direction for iteration. The current\nface feature points are converged to the optimal position\nby repeated iterations.\nFigure 15 shows the SDM-based face alignment process.\n\nThe face alignment process is described as follows.\n\n3.3.1 Image normalization\nThe image is normalized to achieve face alignment, thus\nimproving the efficiency of training. The face image to be\ntrained is manually labeled with feature points. After rea-\nsonable translation, rotation, and scaling transformation,\nthe image is aligned to the first sample. The sample size is\nunified to arrange the original data information with con-\nfused, reducing interference other than shape factors. Fi-\nnally, the calculated average face is placed on the sample\nas the estimated face. The average is aligned with the ori-\nginal face image in the center.\n\nIt is denoted that x∗ is the optimal solution in face fea-\nture point location, x0 the initialization feature point,\nd(x) ∈ Rn × 1 the coordinates of n feature points in the\nimage, and h the nonlinear feature extraction function\nnear each feature point. If the SIFT features of 128 di-\nmensions are extracted from each feature point, then\nh(d(x)) ∈ R128n × 1. The SIFT feature extracted at x\n\n∗ can\nbe expressed as ∅∗ = h(d(x∗)). Then, the face feature\npoint alignment is converted into the operation of solv-\ning Δx, which minimizes Eq. (12).\n\nf x0 þ Δxð Þ ¼ hðd x0 þ Δxð Þk k22 ð12Þ\nThe step size Δx is calculated based on the SDM\n\nalgorithm.\n\nxk ¼ xk þ Δxk ð13Þ\nIf Rk and bk are the paths of each iteration, then\n\nEq. (11) can converge the feature point from the initial\nvalue x0 to x∗.\n\nxkþ1 ¼ xk−1 þ Rk−1∅k−1 þ bk−1 ð14Þ\nDuring training process, {di} is the set of face images,\n\n{di} the set of manually labeled feature points, and x0 the\nfeature point of each image. Face feature point location\nis transformed into a linear regression problem. For the\nproblem, the input feature is the SIFT feature ∅i0 at x0;\n\nFig. 10 Sample sequence set\n\nFig. 11 Matching results when head rotation angle changes\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 8 of 19\n\n  \n\n 50 100 150 200 100 200 300 400 500 600 \n\n\n\nthe result the iteration step size Δxi� ¼ xi� þ Δxi0 from x0\nto x∗; and the objective function Eq. (15).\n\nargminR0b0\nX\n\ndi\n\nX\nxi\n\nΔxi�−R0∅\ni\n�−b0\n\n\t\t \t\t2\n2\n\nð15Þ\n\nIn this way, R0 and b0 from the training set are iterated\nto obtain Rk and Rk. The two parameters are used for\nthe test phase to achieve the alignment of test images.\n\n3.3.2 Local feature extraction of SIFT algorithm\nIn the work, the principal component analysis is used to\nreduce the dimension of image [29], the impact of\nnon-critical dimensions, and the amount of data, thus\nimproving the efficiency. After the dimension reduction,\nthe local feature points are extracted from the face\nimage. To improve the alignment accuracy of feature\npoints, the robust SIFT algorithm is applied for local fea-\nture extraction. Section 3.2.2 introduces the extraction\nprocess in detail.\n\n3.3.3 SDM algorithm alignment result\nTraining samples are selected from IBUG and LFW face\ndatabases. The former contains 132 face images. Each\nimage is labeled with 71 face feature points, which are\nsaved in pts file. The latter consists of the sets of test and\ntraining samples, wherein, the set of test sample contains\n206 face images. Each image is labeled with 71 face feature\npoints, which are saved in pts file. The set of training\n\nsample contains 803 face images. Each image is labeled\nwith 68 face feature points. Figures 16 and 17 show frontal\nand lateral face alignment results, respectively.\n\n3.4 Face pose estimation\nBased on computer vision, the pose of object refers to\nits orientation and position relative to the camera. The\npose can be changed by moving the camera or object.\nGeometric model of camera imaging determines the re-\nlationship between 3D geometric position of certain\npoint on head surface and corresponding point of image.\nThese geometric model parameters are camera parame-\nters. In most cases, these parameters are obtained by ex-\nperiments. This process is called labeling [27, 29].\nCamera labeling determines the geometric and optical\nproperties, 3D position, and direction of camera relative\nto certain world coordinate system.\nThe idea of face pose estimation is described as fol-\n\nlows. Firstly, we find the projection relationship between\n2D coordinates on face image and 3D coordinates of\ncorresponding points on 3D face model. Then, the mo-\ntion coordinates of camera are calculated to estimate\nhead posture.\nA 3D rigid object has two movements relative to the camera:\n① Translation movement\nThe camera is moved from current spatial position\n\n(X, Y, Z) to new spatial position (X′, Y′, Z′), which is called\n\nFig. 12 Matching results when wearing glasses\n\nFig. 13 Matching results when wearing a hat\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 9 of 19\n\n 50 100 150 200 100 200 300 400 500 600 \n\n 50 100 150 200 100 200 300 400 500 600 \n\n\n\ntranslation. Translation vector is expressed as τ = (X′ − X,\nY′ − Y, Z′ − Z).\n② Rotary movement\nIf the camera is rotated around the XYZ axis, the rota-\n\ntion has six degrees of freedom. Therefore, pose estima-\ntion of 3D object means finding six numbers (three for\ntranslation and three for rotation).\n\n3.4.1 Feature point labelling\nThe 2D coordinates of N points are determined to calcu-\nlate 3D coordinates of points, thus obtaining 3D pose of\nobject in an image.\nTo determine the 2D coordinates of N points, we se-\n\nlect the points with rigid body invariance, such as the\nnose tip, corners of eyes, and mouth. In the work, there\nare six points including the nose tip, chin, left, and right\ncorners of eyes and mouth.\nSFM (Surrey Face Model) is used as general 3D face\n\nmodel to obtain 3D coordinates corresponding to se-\nlected 2D coordinates [30]. By manual labeling, we ob-\ntain the 3D coordinates (x, y, z) of six points for pose\nestimation. These points are called world coordinates in\nsome arbitrary reference/coordinate system.\n\n3.4.2 Camera labeling\nAfter determining world coordinates, the camera is reg-\nistered to obtain the camera matrix, namely focal length\nof camera, optical center, and radial distortion parame-\nters of image. Therefore, camera labeling is required. In\nthe work, the camera is labeled by Yang and Patras [31]\nto obtain the camera matrix.\n\n3.4.3 Feature point mapping\nFigure 18 shows the world, camera, and image coordin-\nate systems. In Fig. 18, O is the center of camera, c the\noptical center of 2D image plane, P the point in world\ncoordinate system, and P′ the projection of P on image\nplane. P′ can be determined according to the projection\nof the P point.\nIt is denoted that the world coordinate of P is (U, V, W).\n\nBesides, the known parameters are the rotation matrix R\n\nFig. 14 Matching results when light and shade change\n\nTable 1 Match result analysis table\n\nVariate Number of\nmatches\n\nTotal number\nof key points\n\nMatch\nratio\n\nFalse\nmatch rate\n\nHead rotation 18 158 0.129 0.871\n\nWearing glasses 15 167 0.099 0.901\n\nWearing a hat 21 106 0.247 0.753\n\nLight and shade\nchange\n\n45 281 0.191 0.809\nFig. 15 The face alignment process based on SDM\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 10 of 19\n\n 50 100 150 200 \n\n Start Input the training face sample Image normalization. Calculate the average face Extract local features SDM Iterative Solution Generate face detection model End \n\n\n\n(matrix 3 × 3) and translation vector τ (vector 3 × 1) from\ncamera to world coordinate. It is possible to determine\nposition O(X, Y, Z) of P in camera coordinate system.\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ R\n\nu\nv\nw\n\n2\n4\n\n3\n5 þ τ⇒\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ Rjτ½ �\n\nu\nv\nw\n\n2\n4\n\n3\n5 ð16Þ\n\nEquation (16) is expanded as follows:\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð17Þ\n\nIf plenty of points are mapped to (X, Y, Z) and (U,V, W),\nthe above problem is transformed into a system of linear\nequations with unknown (τx, τy, τz) . Then, the system of\nlinear equations can be solved.\nFirstly, the six points on 3D model are manually la-\n\nbeled to derive their world coordinates (U, V, W). Equa-\ntion (18) is used to determine 2D coordinates (X, Y) of\nsix points in image coordinate system.\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 xy\n\nZ\n\n2\n4\n\n3\n5 ð18Þ\n\nwhere fx and fy are the focal lengths in the x and y direc-\ntions, (cx, cy) is the optical center, and S the unknown scaling\nfactor. If P in 3D is connected to O, then P′ where light in-\ntersects image plane is the same image connecting all points\nin the center of the camera produced by P along the ray.\nEquation (18) is converted to the following form:\n\nS\nX\nY\nZ\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð19Þ\n\nThe image and world coordinates are known in the\nwork. Therefore, Eqs. (18) and (19) are transformed into\nthe following form:\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 r00 r01 r02 τxr10 r11 r12 τy\n\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð20Þ\n\nIf the correct poses R and τ are known, then the 2D\nposition of 3D facial point on image can be predicted by\nprojecting the 3D point onto the image (see Eq. (20)).\nThe 2D facial feature points are known. Pose estimation\ncan be performed by calculating the distance between\nthe projected 3D point and 2D facial feature. If the pose\nis correctly estimated, the 3D points projected onto\nimage plane will almost coincide with the 2D facial fea-\ntures. Otherwise, the re-projection error can be mea-\nsured. The least square method is used to calculate the\nsum of squares of the distance between the projected 3D\nand 2D facial feature points.\n\n3.5 Tracking registration system\nTracking registration technology is the process of align-\ning computer-generated virtual objects with scenes in\nthe real world. At present, there are two tracking regis-\ntration techniques. The first superimposes certain point\nof face feature with a point of virtual glasses based on\nthe face feature point tracking method [32]. The second\nis based on the geometric transformation relation track-\ning method. Face geometry and virtual glasses model are\nconducted with affine transformation. Virtual glasses\nmodel moves with the movement of human head, mak-\ning corresponding perspective changes and realizing 3D\ntry-on effect [33]. For the first technique, the virtual\nglasses cannot be changed with the movement of user\nhead, causing poor user experience. The second tech-\nnique has good tacking effect. The virtual glasses will be\ndistorted with overlarge head corner. Combined with the\ntwo methods, the glasses model is conducted with per-\nspective transformation using six degrees of freedom ob-\ntained by pose estimation in Section 3.3. After face\nsuperposition, accurate tracking is realized through bet-\nter stereoscopic changes.\n\nFig. 16 The picture of front face alignment\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 11 of 19\n\n  \n\n\n\n3.5.1 Affine transformation method of glasses try-on\nIn Fig. 19, the center between two corners of the eye is\ncalculated according to the distance between them. An\nisosceles right triangle ABC is defined [34]. The coordi-\nnates of the triangle are A(a1, a2), B(b1, b2), and C(c1, c2).\nIf the threshold is determined by experiment ahead of\ntime, the coordinates of C are as follows.\n\nC c1; c2ð Þ ¼ b1−b2 þ a2; b1 þ b2−a2ð Þ ð21Þ\n\nDuring try-on process, the glasses model is matched\nto the eye of user using the affine transformation\nEq. (22).\n\nx0 ¼ ax þ by þ c y0 ¼ dx þ ey þ f ð22Þ\n\nIn the glasses model, the vertices of isosceles right\ntriangle are priori, with the coordinates of (x1, y1),\n(x2, y2), and (x3, y3). The vertices of isosceles right\ntriangle on user face (x01; y\n\n0\n1), (x\n\n0\n2; y\n\n0\n2), and (x\n\n0\n3; y\n\n0\n3) can be\n\ndetected in motion. The affine transformation parameter\nh = (a, b, c, d, e, f)T.\n\nx01\ny01\nx02\ny02\nx03\ny03\n\n2\n6666664\n\n3\n7777775\n¼\n\nx1 y1 1 0 0 0\n0 0 0 x1 y1 1\nx2 y2 1 0 0 0\n0 0 0 x2 y2 1\nx3 y3 1 0 0 0\n0 0 0 x3 y3 1\n\n2\n6666664\n\n3\n7777775\n\na\nb\nc\nd\ne\nf\n\n2\n6666664\n\n3\n7777775\n\nð23Þ\n\nEquation (23) is abbreviated as P = Ah. Finally, the af-\nfine transformation parameter h (h = (ATA)−1) is calcu-\nlated by least square method. If h is applied to the\nisosceles right triangle, then the image of glasses will be\nprojected onto the right position of the face.\n\n3.5.2 Perspective transformation method of glasses try-on\nAffine transformation can realize the tracking of 3D\nmodel. The tracked glasses are prone to deformation be-\ncause the affine transformation has the characteristics of\nflatness and parallelism based on three-point transform-\nation [35]. The six degrees of freedom are obtained from\nhead pose estimation. After perspective transformation,\nthe glasses are superimposed with eye feature points to\nachieve real-time tracking effect. When the head moves,\nthe space model of glasses should conform to human\nvisual law, with certain deformation. It is realized by per-\nspective transformation [36] (Fig. 20).\n\nFig. 17 The picture of side face alignment\n\nFig. 18 Three coordinate systems\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 12 of 19\n\n  \n\n w P x World coordinate systems U C O Z plane-coordinate system R, T X Y Camera coordinate systems \n\n\n\n3.6 Virtual model generation system\nIn the work, the 3D glasses model is built in 3ds max\nand exported to 3DS format file. The 3DS data file can-\nnot display the 3D model in OpenGL in real time.\nFirstly, the 3DS model is conducted with analytic oper-\nation. Only by transferring the operational data to the\nOpenGL function can we draw the virtual glasses model\nin this case [37].\n\n3.7 Virtual and real synthesis system\nTo achieve the perfect combination of virtual glasses\nand realistic scenes, virtual glasses must be positioned to\nthe exact position in the real world at first. This process\nis achieved by integrating markers with natural features.\nFigure 21 shows the overall structure of fused glasses\ntry-on subsystem [38].\n\n4 iOS system application\nTo verify the effectiveness of proposed system method, we\ndevelop a mobile “AR Glasses Try-on Sales System” for\niOS platform. This system comprehensively uses the\n\ncommon controls of iOS. Besides, the controls are recon-\nstructed and optimized to improve the operating efficiency\nof the system. Meanwhile, most function blocks are modi-\nfied to minimize the application, dependencies, and main-\ntenance procedure of third-party framework. The system\nrealizes the basic functions including browsing of glasses\nproducts, user registration, login, goods collection, adding\nto carts, user address adding, modifying, deleting, goods\npurchasing, integrated Alipay payment, and order man-\nagement [39, 40]. In addition, it is embedded with glasses\ntry-on, photographing, recording video, uploading, and\nsharing WeChat and Weibo. Meanwhile, the system has\nits social platform for users to browse try-on effects of\nothers. The same glasses are quickly tried on, thus meet-\ning the needs of vast users (Fig. 22).\nCommodity browsing module is the core of the system,\n\ncovering commodity browsing, screening, try-on, photo-\ngraphing, uploading, and sharing. The try-on and quick\ntry-on subsystems need to call face recognition method in\nPart 3.\n\n4.1 Menu module\nMenu module is the framework module of the whole ap-\nplication. All the sub-modules are switched by a Menu-\nViewController controller. This control contains the\nviews of menu, home page, favorites, shopping cart,\norder, coupon, photo wall, setting modules, initialization,\nand the switching method of controllers. The menu page\nis not displayed in the default startup page. User calls up\nthe menu page by clicking the upper-left icon of the de-\nfault page or sliding to the right in the home page.\nMenu page adopts the traditional frosted glass method.\n\nFirstly, the UIImage object is obtained by taking a screen\nshot and then processed by the frosted glass tool. The\nfrosted glass UImage is used as the background of the\nmenu to realize the translucent frosted glass effect.\nMenu view is the leftmost view of MenuViewControl-\n\nler. The view is located at the top of the entire applica-\ntion. The menu can be switched out in all modules. It is\nachieved mainly by the table. The rows are selected by\nthe table to trigger the effect of selected rows.\nThe module title in the menu is clicked to trigger the\n\nproxy event of table, thus calling the method of selecting\ncurrent module for module switching. The switchable\nmodules mainly include user, commodity, collection,\nshopping cart, order, coupon, photo wall, and setting.\n\n4.2 User registration module\nUser registration module is used for the management of\nregistered users. Registered users enjoy the VIP promo-\ntion activities and prices. Unregistered users enter the\nregistration page by clicking the registration button.\n\nFig. 19 The isosceles triangle made on face and eyeglass images\n\nFig. 20 The quadrilateral made on face and eyeglass images\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 13 of 19\n\n C \n\n B \n\n\n\n4.3 Commodity module\nCommodity module is the key of “AR glasses sales sys-\ntem,” including commodity browsing, selection, and add-\ning to cart.\n\n4.3.1 Commodity browsing\nAll the products are browsed in the login or non-login\nstatus. In commodity browsing, the big images of glasses\nare slid to browse the front of product image and leg\nstyle. The detail page can be slid to view more commod-\nity information. More commodity information is loaded\nby sliding up.\nThe pull-up and pull-down are proxy methods based\n\non the table and its parent class (UIScrollView).\n(void)scrollViewDidScroll:(UIScrollView *)scrollView\nWhen the height of parent container offset is greater\n\nthan 20% of table height, the pull-down refresh is called.\nThe pull-up refresh is called when the height difference\n\nbetween the height of parent container and the sum of\ntable height and offset exceeds 10% of the screen height.\n\n4.3.2 Commodity screening\nIn commodity browsing, users quickly find products\nmeeting their needs and click to select multiple items. If\none item is selected, the system will feed back the num-\nber of eligible products in time. The display result but-\nton is clicked to display the screening result. Screening\nresults can be cleared by clicking on the cross on the\nright of blue subtitle.\nList screening is realized by modified and nested tables.\n\nThe segment head of custom table is used as first-level\nscreening title. Second-level screening catalog is achieved\nby nested table and custom table cells. By nesting\nsecond-level screening catalog, we obtain third-level\nscreening catalog. The subtitle of first-level catalog is\nrefreshed by recording the selected filter item in real time.\n\nFig. 21 General structure of system\n\nFig. 22 The structure of “AR Glasses Sales System”\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 14 of 19\n\n Real world coordinates Virtual world coordinates actual environment Glasses model Scene acquisition Tracking registry Model generation system system Virtual reality synthesis system Coordinates of real Coordinates of the world cameras virtual world camera display system User UI \n\n Augmented reality glasses sales system Commodity User Collection shopping Order discount photo setting browsing of goods trolley coupon wall up commodity list Glasses photogr uploading sharing Show photos comments sharing Try on browsing screening try-on aphing of trying on and likes quickly commodity Product modification Address Order statistics and deletion management Generated \n\n\n\nSimultaneously, the server is synchronized to get the\nremaining product information.\nIn the screening tool class, the record of third-level\n\nmenu is complicated. In implementation, the third-level\noptions are recorded for local summary, updating selected\nor unselected state. Full summary is performed by reusing\nlocal summary, indicating in the first-level menu.\n\n4.3.3 Glasses try-on\nIn the system, the face data are captured by the camera\nfor further processing. Firstly, user’s face is located at 30–\n50 cm right ahead the front camera of mobile phone. The\nface is slightly rotated, without getting out of capture area\nof camera. In addition, user should not keep his/her back\nto the light during the try-on process because the light af-\nfects the capture effect of the camera. When the camera\ndoes not capture face data, face position will be adjusted\nby a prompt. As user wiggles in front of camera, the en-\ngine re-recognizes the face information. The quick try-on\nbutton is clicked to enter the quick try-on page. User can\nput the phone 30–50 cm in front of him. Then, his/her\nface appears on the screen of the mobile phone. Mean-\nwhile, the system automatically recognizes user face data\nto put on glasses. The product details are viewed by\n\nsliding the glasses or clicking on the left detail button.\nFigure 23 shows the try-on process in detail.\nIn the try-on process, the third-party oepnframeworks is\n\nused to modify the class library according to the require-\nments. Section 3 introduces face recognition, alignment,\ntracking registration and head pose estimation, and virtual\nmodel generation methods. Combined with these\nmethods, the interface is packaged to increase the stability\nof system, reducing the dependence on third-party con-\ntrols. The part embedded in openframeworks starts with\nthe main function. Using openframeworks, the window is\ninitialized to call the Appdeleagte class of openframe-\nworks. This class is compatible with UIKit library in iOS.\nThe ofApp will be initialized to call engine loading model.\n\n4.3.3.1 AFNetworking In iOS development, the NSURL-\nConnection of XCode is competent to submit a request to\na simple page of Web site, thus obtaining the response\nfrom server. However, most web pages to be visited are\nprotected by authority. The pages cannot be visited by a\nsimple URL. This involves the processing of Session and\nCookie. Here, NSURLConnection can be used to realize\naccess, with larger complexity and difficulty.\nAFNetworking is more suitable to process requests to\n\nWeb sites, including detailed Sessions and Cookies\n\nFig. 23 The detailed flow chart of eyeglass try-on\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 15 of 19\n\n Start Import Image Input the training face sample Is there a face? Image normalization, Calculate the average face SVM classitier SIFT Extract local features Is there a face? SDM Iterative Solution Feature extraction Face Alignment Feature point Face rotation Calculate the perspective relationship The lens selection Angle between 2D and 3D coordinates library Shoot the The glasses 3d effect Transparency perspective picking try-on effect overlap the face design treatment transformation out a lens End \n\n\n\nproblems. It can be used to send HTTP requests and re-\nceive HTTP responses. However, it does not cache ser-\nver responses or execute the JAvascript code in the\nHTML page. Meanwhile, AFNetworking has built-in\nJSON, plist, and XML file parsing for convenient\napplication.\nSome interfaces of the library are packaged to facilitate\n\nthe use of AFNetworking. The packaged AFNetworking\ncan record the operation due to disconnection request\nfailure. After networking, the request is re-initiated.\nWhen data needs to be requested, if get request is\n\ncalled, the following methods will be called:\n-(void)GET:(NSString *)URLString parameters:(id)para\n\nmeters WithBlock:(resultBlock)result;\nIf a image necessary upload pictures the following methods\n\nwill be call.\n// Upload pictures\n-(void)POST:(NSString *)URLString parameters:(id)para\n\nmeters WithData:(NSData *)data WithKey:(NSString *)key\nWithTypeO:(NSString*)pngOrMp4\nWithBlock:(resultBlock)result;\nIf a video necessary upload pictures the following\n\nmethods will be call.\n// Upload video\n-(void)POST:(NSString *)URLString parameters:(id)pa\n\nrameters WithDic:(NSDictionary *)dic WithTypeO:(NS\nString*)pngOrMp4 WithBlock:(resultBlock)result;\nIf post the following interface will be call.\n// post\n- (void)POST:(NSString *)URLString parameters:(id)\n\nparameters WithBlock:(resultBlock)result;\n\n4.3.3.2 SDWebImage SDWebImage is a framework for\nthird-party applications. It is used to implement asyn-\nchronous loading and caching of images. In this system,\nall network images are loaded using this framework. By\ndefining interface classes, we can easily implement asyn-\nchronous loading and caching of images.\n#import <Foundation/Foundation.h>\n@interface TGImageTool : NSObject\n+ (void)downloadImage:(NSString *)url placeholder:\n\n(UIImage *)place imageView:(UIImageView *)imageView;\n+ (void)clear;\n@end\n#import “TGImageTool.h”\n#import “UIImageView+WebCache.h”\n@implementation TGImageTool\n+ (void)downloadImage:(NSString *)url placeholder:\n\n(UIImage *)place imageView:(UIImageView *)imageView\n{[imageView setImageWithURL:[NSURL URLWithStrin\n\ng:url] placeholderImage:place options:SDWebImageLowPri\nority | SDWebImageRetryFailed];}\n+ (void)clear\n{\n\n// 1. Clear the cached images in memory\n[[SDImageCache sharedImageCache] clearMemory];\n[[SDImageCache sharedImageCache] clearDisk];\n// 2. Cancel all download requests\n[[SDWebImageManager sharedManager] cancelAll];\n}\n@end\nThe image is loaded by the above tool class method.\n\nWhen the cache is implemented, the image will be auto-\nmatically added to the cache.\nThe clear method of tool class is called to clear the cache.\n\n4.3.3.3 JSONKit JSONKit is used in this system only\nwhen the order information is submitted. It transcodes\nthe complicated parameter information to JSON strings\nfor server application. The conversion method is de-\nscribed as follows.\nNSString *new=[dic JSONString]\n\n4.3.4 Adding to cart\nThe satisfied glasses are added to shopping cart by clicking\non the “Add to Cart” button. The animation of “Add to\nCart” is realized by path and combined animation in the\nQuartzCore library.\n\n4.3.5 Buy a glasses immediately\nUser directly jumps to the page for purchasing the\nglasses without adding to cart. The function is realized\nby directly jumping to order information improvement\npage after summarizing commodity information.\n\n4.3.6 Taking photos or recording videos\nUsers with glasses can take off their glasses after logging\nin. VR glasses are tried on to take photos or record videos.\nThe try-on effects can also be watched after wearing\nglasses. The system provides functions of taking photos\nand recording videos. The photo/video button is used to\nswitch between taking pictures and recording videos. In\nthe work, this function is realized by modifying the engine\nin oepnframeworks. This system only involves the call.\nThe photos and videos are placed in the four preview\nareas below, where user can click to view the details.\n\n4.3.7 Uploading and sharing\nThe system provides uploading and sharing functions of\nphotos or videos to share satisfactory try-on results and\nwonderful moments with friends. “Share” button is\nclicked to upload videos to photo wall, friend circle,\nWeibo, or WeChat in the server. The photos or videos\nare deleted by clicking the “Delete” button.\nThe third-party AFNetworking method is used to up-\n\nload files. The files can be shared to Sina Weibo,\nWeChat circle, friends, and photo wall.\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 16 of 19\n\n\n\nThe sharing principle is to obtain the information of\nthe photo or video on server side. Then, the html5 page\nis generated, including image, video, like, and comment.\nThe URL is returned to the client and shared to WeChat\nand Sina Weibo.\nUsers can choose whether to share to photo wall at\n\nthe same time. Sharing to photo wall is to send a request\nto the server. The photo or video is backed up in the\ntable corresponding to database photo wall information.\nWhen being requested, the shared information can be\nobtained in the photo wall.\n\n4.4 Collection module\nIn the implementation of collection module, the cells of\ntable are reused in the home page. The data are replaced\nwith the data of favorite list. After logging in, the favorite\nitem is added to the collection list of personal information\nby clicking the gray heart button, which is convenient for\nnext viewing. “Collect” button is clicked to cancel the col-\nlected item, removing it from the collection list.\n\n4.5 Shopping cart module\nAfter logging in, the satisfactory item is added to shop-\nping cart in the try-on interface. In implementation, the\ncustom tool class is used to record the selected state.\nWhen clicking “Select All” button, all data in the table\nare selected. The selected state of “Select All” button is\nremoved to cancel certain item. Meanwhile, the sums of\nselected item quantities and unit prices are calculated.\nThe head position shows the number of items. “Settle”\nbutton at the bottom of table shows the total number of\nitems. Users can modify orders and postal addresses,\nwhile submitting orders and paying online.\n\n4.6 Order module\nAfter logging in, users can see their historical orders in\n“My Order.” There are two states in the order, including\npending (immediate payment) and successful payment.\n“Pay now” button is clicked to jump to payment interface.\nDuring order payment, it will jump to the immediate pay-\nment page of shopping module and then to Alipay.\n\n4.7 Coupon module\nThe coupon module is a channel through which mer-\nchants can distribute benefits to users. After logging in,\nusers check the coupons matching their own eligibilities.\nThere are three types of coupons received: available,\nused, and expired coupons. After reading coupon usage\nrules, users can select whether to use the coupon in the\ninterface of order information completion.\n\n4.8 Photo wall module\nThe photo wall is a display platform provided by the sys-\ntem to user. It is convenient for user to browse the\n\ntry-on results of others. Based on dynamic prompt func-\ntion, user quickly finds the favorite style of glasses.\nUser dynamic prompt function is implemented by de-\n\ntecting new messages. Once menu page pops up, a request\nis sent to the server, requesting a new unread message. If\nthere is a new message, it will show user avatars of last dy-\nnamic message and the number of new messages; other-\nwise, the prompt box is not displayed.\nWhile seeing the favorite try-on results, users can like,\n\ncomment, forward, or view the same item and try it on\nquickly.\n“View commodity” button is clicked to view the de-\n\ntailed information of glasses try-on results. The product\ninformation is uniquely determined according to the\nproduct ID. It is the same as quick try-on principle.\nUser can directly try on the same glasses worn by\n\nother users by clicking the quick try-on button. The\nphoto wall and product data are bound in the database\nat the beginning. Therefore, the product can be directly\nfound and tried on according to the product ID.\n\n4.9 Setting module\nThe setting module contains “check updates,” “clean up\npicture cache,” “about us,” “rating,” “feedback,” and “exit\ncurrent user.” Relatively, it is the application of native\ntable control, which is not described here.\n\n5 Results and discussion\nExperimental environment is described as follows.\nOperating system: iOS 9\nDevelopment tools: Xcode 6\nRelated libraries: OpenCV, MFC\nProgramming language: C language, Objective-C, C++\nFigure 24 shows the partial operation interface of the\n\nsystem.\nAlthough a lot of jobs are done, there are still some\n\nshortcomings in the system:\n\n1. Equipped with a try-on engine, the system has\ncertain requirements on the performance of iPhone.\nThe higher configuration of iPhone leads to the\nmore accurate identification. At present, the models\nrunning smoothly are iPhone 5s and iPhone 6 and\niPhone 6 Plus.\n\n2. It is difficult for user to perform subsequent\noperations in the case of unstable network\nenvironment, especially for failed login.\n\n3. In the system, the face data are captured by the\ncamera for further processing. Firstly, user face is\nlocated at 30–50 cm right ahead the front camera\nof the mobile phone. The face is slightly rotated,\nwithout getting out of the capture area of the camera.\n\n4. User should not keep his/her back to the light\nduring the try-on process because the light affects\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 17 of 19\n\n\n\ncapture effect of the camera. When the camera does\nnot capture face data, face position will be adjusted\nby a prompt. As user wiggles in front of camera, the\nengine re-recognizes user face information.\n\n5. At present, only the Chinese version of “AR Glasses\nSales System” has been developed. There is no\ncorresponding English version.\n\n6 Conclusions\nIn the work, we discussed augmented reality virtual\nglasses try-on technology. Face information was collected\nby monocular camera. After face detection by SVM classi-\nfier, the local face features were extracted by robust SIFT.\nCombined with SDM, the feature points were iteratively\nsolved to obtain more accurate feature point alignment\nmodel. Through the head pose estimation, the virtual\nglasses model was accurately superimposed on the human\nface, thus realizing the try-on of virtual glasses. This the-\noretical research was applied in iOS platform for the\ntry-on of virtual glasses, thus providing best services for\nuser selection. Experiments showed that the virtual glasses\nhad realistic effect and high try-on speed and user satisfac-\ntion. Consequently, AR-based glasses try-on technology\nprovided new idea for virtual try-on technology. Camera\ncapture under complex light conditions will be further\nstudied. App running test on iPhone 7 and above will be\ncarried out. Multilingual versions will be developed.\n\nAbbreviations\nAPP: Application; AR: Augmented reality; CCD: Charge-coupled device;\nHMM: Hidden Markov model; HOG: Histogram of oriented gradient;\nIBUG: Intelligent Behaviour Understanding Group; iOS: iPhone OS; LBP: Local\nbinary patterns; LFW: Labeled Faces in the Wild; SDM: Supervised descent\nmethod; SFM: Surrey Face Model; SIFT: Scale-invariant feature transform;\nSVM: Support vector machines; VR: Virtual reality\n\nAcknowledgements\nThis work is partially supported by Shanxi Province Universities Science and\nTechnology Innovation Project (2017107) and Shanxi Province Science\nFoundation for Youths (201701D12111421).\nThanks to the editor and reviewers.\n\nFunding\nThe paper is subsidized by science and technology key project of Henan\nProvince, China. NO.172102210462\n\nAvailability of data and materials\nData will not be shared; reason for not sharing the data and materials is that\nthe work submitted for review is not completed. The research is still ongoing, and\nthose data and materials are still required by my team for further investigations.\n\nAuthor’s contributions\nBZ designed the research, analyzed the data, and wrote and edited the\nmanuscript. The author read and approved the final manuscript.\n\nAuthor’s information\nBoping Zhang, female, is currently an Associate Professor at the School of\nInformation Engineering, Xuchang University, China. She received master’s\ndegree from Zhengzhou University, China, in 2006. Her current research\ninterests include computer vision, image processing, virtual reality, and\npattern recognition.\n\nEthics approval and consent to participate\nNot applicable.\n\nConsent for publication\nNot applicable.\n\nCompeting interests\nThe author declares that she has no competing interests. The author confirms\nthat the content of the manuscript has not been published or submitted for\npublication elsewhere.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published\nmaps and institutional affiliations.\n\nReceived: 15 August 2018 Accepted: 5 November 2018\n\nReferences\n1. DITTO. http://www.ditto.com/\n2. O. Deniz, M. Castrillon, J. Lorenzo, et al., Computer vision based eyewear\n\nselector. Journal of Zhejiang University-SCIENCE C (Computers & Electronics)\n11(2), 79–91 (2010)\n\n3. Gongxin Xie. A transformation road for the glasses industry[J]. China Glasses,\n2014,03:112–113\n\n4. Liu Cheng, Wang Feng, QI Changhong, et al. A method of virtual glasses\ntry-on based on augmented reality[J]. Industrial Control Computer, 2014,\n27(12):66–69\n\n5. Boping Zhang. Design of mobile augmented reality game based on image\nrecognition[J]. EURASIP Journal on Image and Video Processing, 2017, 20:2–20\n\na b c d\nFig. 24 The part of interface for system operation\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 18 of 19\n\n Published online: 27 November 2018 \n\n -108 PmBY 19:18 @¥( 100%* 15:48 < # AGLASSES = GLASSES Rayban xfett RB33 ... . - ...... *FEWQrb3025 112 ... ¥ 450 ¥ 560 2 3 5 6 7 8 O i - & @ .? ! ABC space Done V ¥ 1010 \n\nhttp://www.ditto.com/\n\n\n6. Yan Lei, Yang Xiaogang, et al. Mobile augmented reality system design and\napplication based on image recognition[J], Journal of Image and Graphics,\n2016, 21(2):184–191\n\n7. Niswar A, Khan I R, Farbiz F. Virtual try-on of eyeglasses using 3D model of\nthe head[C]. International Conference on Virtual Reality Continuum and ITS\nApplications in Industry. New York: ACM; 2011:435–438.\n\n8. Meijing[OL]. http://www.meijing.com/tryinon.html\n9. Kede [OL]. http://www.kede.com/frame\n10. Biyao [OL]. http://www.biyao.com/home/index.html\n11. Li Juan, Yang Jie. Eyeglasses try-on based on improved Poisson equations.\n\n2011 Conference on Multimedia Technology. New York: ICMT 2011. 2011;\n3058–3061.\n\n12. DU Yao,WANG Zhao-Zhong. Real-like virtual fitting for single image[J].\nComputer Systems Application, 2015, 24(4):19–20\n\n13. Y. Lu, W. Shi-Gang, et al., Technology of virtual eyeglasses try-on system\nbased on face pose estimation[J]. Chinese Optics 8(4), 582–588 (2015)\n\n14. Yuan M, Khan I R, Farbiz F, et al. A mixed reality virtual clothes try-on\nsystem[J]. IEEE Transactions on Multimedia. 2013;15(8):1958-968.\n\n15. Huang W Y, et al. Vision-based virtual eyeglasses fitting system[C]. IEEE,\nInternational Symposium on Consumer Electronics. New York: IEEE. 2013;45–46\n\n16. Wang Feng, Qi Changhong, Liu Cheng, Jiang Wei, Ni Zhou, Zou Ya.\nReconstruction of 3D head model based on orthogonal images [J]. Journal\nof Southeast University (Natural Science Edition). 2015;45(1):36-40.\n\n17. Zhang B. Cluster Comput. 2017. https://doi.org/10.1007/s10586-017-1330-5.\n18. Maatta J, Hadid A, Pietikainen M. Face spoofing detection from single images\n\nusing texture and local shape analysis[J]. IET Biometrics, 2012, 1(1):3–10\n19. Lin Y, Lv F, Zhu S, et al. Large-scale image classification: fast feature\n\nextraction and svm training[C]. Computer Vision and Pattern Recognition\n(CVPR), 2011 IEEE Conference on. New York: IEEE; 2011:1689–1696\n\n20. Lowe D G, Lowe D G. Distinctive image features from scale-invariant\nkeypoints[J]. Int. J. Comput. Vis., 2004, 60(2):91–110\n\n21. Zhang Boping. Research on automatic recognition of color multi\ndimensional face images under variable illumination[J]. Microelectronics &\nComputer, 2017,34(5) :128–132\n\n22. MING An-Long MA Hua-dong. Region-SIFT descriptor based\ncorrespondence between multiple cameras[J]. CHIN ESE JOURNA L OF\nCOMPUTERS, 2008, 12(4):650–662\n\n23. He Kai, Wang Xiaowen, Ge Yunfeng. Adaptive support-weight stereo\nmatching algorithm based on SIFT descriptors[J]. Journal of Tiajin University,\n2016, Vol.49(9):978–984\n\n24. D.G. Lowe, Distinctive image features from scale-invariant key points. Int. J.\nComput. Vis. 60(2), 91–110 (2004)\n\n25. Chen Guangxi, Gong Zhenting, et al. Fast image recognition method Bsded\non locality-constrained linear coding[J]. Computer Science, 2016, vol. 43(5),\n308–314\n\n26. Bai Tingzhu, Hou Xibao. An improved image matching algorithm base on\nSIFT[J]. Transaction of Beijing Institute of Technology, 2013, 33(6):622–627\n\n27. Xiong X, Tome F D L. Supervised descent method and its applications to face\nalignment[C].Computer Vision and Pattern Recognition. New York: IEEE; 2013:\n532–539\n\n28. Zhu JE, et al. Real-Time Non-rigid Shape Recovery Via Active Appearance\nModels for Augmented Reality (Proc. Of 9th European Conference on\nComputer Vision, Graz, 2006), pp. 186–197\n\n29. Huber P, Hu G, Tena R, et al. A multiresolution 3D morphable face model\nand fitting framework[C]. Visapp. 2015\n\n30. Zhang Z. A flexible new technique for camera calibration[J]. IEEE Transactions\non Pattern Analysis&Machine Intelligence, 2000, 22(11):1330–1334\n\n31. Yang H, Patras I. Sieving Regression Forest Votes for Facial Feature\nDetection in the Wild[C]. New York: ICCV; 2013:1936–1943\n\n32. Dantone M, Gall J, Fanelli G, et al. Real-time facial feature detection using\nconditional regression forests[C]. Computer Vision and Pattern Recognition.\nNew York: IEEE; 2012:2578–2585\n\n33. Google Release online AR mobile games Ingress[OL]. http://www.csdn.net/\narticle/2012-11-16/2811943-google-launches-ingress\n\n34. D. Shreiner, G. Sellers, J.M. Kessenich, B.M. Licea-Kane, OpenGL Programming\nGuide: The Official Guide to Learning OpenGL, 8th edn. (Addison-Wesley\nProfessional, United States, 2013)\n\n35. J. Kim, S. Forsythe, Adoption of virtual try-on technology for online apparel\nshopping. J. Interact. Mark. 22, 45–59 (2008)\n\n36. A. Merle, S. Senecal, A. St-Onge, Whether and how virtual try-on influences\nconsumer responses to an apparel web site. Int. J. Electron. Commer. 16,\n41–64 (2012)\n\n37. Niswar, A.; Khan, I.R.; Farbiz, F. In Virtual try-on of eyeglasses using 3d model\nof the head, Proceedings of the 10th International Conference on Virtual\nReality Continuum and Its Applications in Industry. New York: ACM; 2011.\npp 435–438\n\n38. Koestinger M, Wohlhart P, Roth PM, Bischof H. Annotated facial landmarks\nin the wild: A large-scale, real-world database for facial landmark\nlocalization. First IEEE International Workshop on Benchmarking Facial\nImage Analysis Technologies, 2011\n\n39. Q. Zhou, Multi-layer affective computing model based on emotional\npsychology. Electron. Commer. Res. 18(1), 109–124 (2018). https://doi.org/\n10.1007/s10660-017-9265-8\n\n40. Q. Zhou, Z. Xu, N.Y. Yen, User sentiment analysis based on social network\ninformation and its application in consumer reconstruction intention.\nComput. Hum. Behav. (2018) https://doi.org/10.1016/j.chb.2018.07.006\n\nZhang EURASIP Journal on Image and Video Processing        (2018) 2018:132 Page 19 of 19\n\nhttp://www.meijing.com/tryinon.html\nhttp://www.kede.com/frame\nhttp://www.biyao.com/home/index.html\nhttps://doi.org/10.1007/s10586-017-1330-5\nhttp://www.csdn.net/article/2012-11-16/2811943-google-launches-ingress\nhttp://www.csdn.net/article/2012-11-16/2811943-google-launches-ingress\nhttps://doi.org/10.1007/s10660-017-9265-8\nhttps://doi.org/10.1007/s10660-017-9265-8\nhttps://doi.org/10.1016/j.chb.2018.07.006\n\n\tAbstract\n\tIntroduction\n\tResearch status of network virtual try-on technology\n\tMethods of face recognition\n\tSVM-based face detection\n\tFace recognition based on SIFT\n\tBasic principle of SIFT algorithm\n\tKey point matching\n\tFace recognition experiment\n\n\tFace alignment\n\tImage normalization\n\tLocal feature extraction of SIFT algorithm\n\tSDM algorithm alignment result\n\n\tFace pose estimation\n\tFeature point labelling\n\tCamera labeling\n\tFeature point mapping\n\n\tTracking registration system\n\tAffine transformation method of glasses try-on\n\tPerspective transformation method of glasses try-on\n\n\tVirtual model generation system\n\tVirtual and real synthesis system\n\n\tiOS system application\n\tMenu module\n\tUser registration module\n\tCommodity module\n\tCommodity browsing\n\tCommodity screening\n\tGlasses try-on\n\tAdding to cart\n\tBuy a glasses immediately\n\tTaking photos or recording videos\n\tUploading and sharing\n\n\tCollection module\n\tShopping cart module\n\tOrder module\n\tCoupon module\n\tPhoto wall module\n\tSetting module\n\n\tResults and discussion\n\tConclusions\n\tAbbreviations\n\tAcknowledgements\n\tFunding\n\tAvailability of data and materials\n\tAuthor’s contributions\n\tAuthor’s information\n\tEthics approval and consent to participate\n\tConsent for publication\n\tCompeting interests\n\tPublisher’s Note\n\tReferences\n\n",
      "text": [
        "",
        "Face Macthing Recognition Recognition Results CLassification Face Result Input Feature Feature Searching Extraction Dimensionality Image Tracing Offline Learning Training Feature Feature Image Extraction Space Testing Feature Feature Image Extraction Macthing Online Learning",
        "X1 K(x ,x) K(x ,x) Sgn Otrain y train Xtrain ,x)",
        "Start Import Image No Does it include faces yes SVM Classifier detection Output results for face detection End",
        "Image input Image input template input Detection extreme extracti on local characteristic region characteristic set Location the Key points of stability characteristicmatching Describing local characteristic region Dislodge the Wrong image matching characteristic set The input of matching image",
        "Octave5 80 Octave4 40 Octave3 20 Octave2 Octave1",
        "02 0.1 -0.1 02 -03 Laplacian - DOG -0.4",
        "Scale (next octave) Gaussian DOG",
        "Scale",
        "The main direction",
        "",
        "50 100 150 200 100 200 300 400 500 600",
        "50 100 150 200 100 200 300 400 500 600",
        "50 100 150 200 100 200 300 400 500 600",
        "50 100 150 200",
        "Start Input the training face sample Image normalization. Calculate the average face Extract local features SDM Iterative Solution Generate face detection model End",
        "",
        "",
        "w P x World coordinate systems U C O Z plane-coordinate system R, T X Y Camera coordinate systems",
        "C",
        "B",
        "Real world coordinates Virtual world coordinates actual environment Glasses model Scene acquisition Tracking registry Model generation system system Virtual reality synthesis system Coordinates of real Coordinates of the world cameras virtual world camera display system User UI",
        "Augmented reality glasses sales system Commodity User Collection shopping Order discount photo setting browsing of goods trolley coupon wall up commodity list Glasses photogr uploading sharing Show photos comments sharing Try on browsing screening try-on aphing of trying on and likes quickly commodity Product modification Address Order statistics and deletion management Generated",
        "Start Import Image Input the training face sample Is there a face? Image normalization, Calculate the average face SVM classitier SIFT Extract local features Is there a face? SDM Iterative Solution Feature extraction Face Alignment Feature point Face rotation Calculate the perspective relationship The lens selection Angle between 2D and 3D coordinates library Shoot the The glasses 3d effect Transparency perspective picking try-on effect overlap the face design treatment transformation out a lens End",
        "Published online: 27 November 2018",
        "-108 PmBY 19:18 @¥( 100%* 15:48 < # AGLASSES = GLASSES Rayban xfett RB33 ... . - ...... *FEWQrb3025 112 ... ¥ 450 ¥ 560 2 3 5 6 7 8 O i - & @ .? ! ABC space Done V ¥ 1010"
      ],
      "layoutText": [
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"Face Macthing Recognition Recognition Results CLassification Face Result Input Feature Feature Searching Extraction Dimensionality Image Tracing Offline Learning Training Feature Feature Image Extraction Space Testing Feature Feature Image Extraction Macthing Online Learning\",\"lines\":[{\"boundingBox\":[{\"x\":1000,\"y\":15},{\"x\":1201,\"y\":16},{\"x\":1201,\"y\":49},{\"x\":1000,\"y\":48}],\"text\":\"Face Macthing\"},{\"boundingBox\":[{\"x\":1248,\"y\":21},{\"x\":1405,\"y\":21},{\"x\":1405,\"y\":47},{\"x\":1248,\"y\":47}],\"text\":\"Recognition\"},{\"boundingBox\":[{\"x\":1026,\"y\":55},{\"x\":1181,\"y\":56},{\"x\":1180,\"y\":83},{\"x\":1026,\"y\":82}],\"text\":\"Recognition\"},{\"boundingBox\":[{\"x\":1278,\"y\":78},{\"x\":1376,\"y\":78},{\"x\":1376,\"y\":104},{\"x\":1278,\"y\":103}],\"text\":\"Results\"},{\"boundingBox\":[{\"x\":1011,\"y\":153},{\"x\":1194,\"y\":155},{\"x\":1194,\"y\":182},{\"x\":1011,\"y\":180}],\"text\":\"CLassification\"},{\"boundingBox\":[{\"x\":146,\"y\":180},{\"x\":207,\"y\":181},{\"x\":206,\"y\":205},{\"x\":145,\"y\":203}],\"text\":\"Face\"},{\"boundingBox\":[{\"x\":1061,\"y\":191},{\"x\":1146,\"y\":191},{\"x\":1146,\"y\":217},{\"x\":1062,\"y\":218}],\"text\":\"Result\"},{\"boundingBox\":[{\"x\":6,\"y\":194},{\"x\":78,\"y\":197},{\"x\":78,\"y\":221},{\"x\":8,\"y\":222}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":348,\"y\":197},{\"x\":448,\"y\":199},{\"x\":447,\"y\":225},{\"x\":348,\"y\":222}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":596,\"y\":197},{\"x\":694,\"y\":198},{\"x\":693,\"y\":225},{\"x\":595,\"y\":223}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":103,\"y\":212},{\"x\":240,\"y\":215},{\"x\":239,\"y\":248},{\"x\":102,\"y\":244}],\"text\":\"Searching\"},{\"boundingBox\":[{\"x\":332,\"y\":235},{\"x\":463,\"y\":236},{\"x\":462,\"y\":259},{\"x\":332,\"y\":258}],\"text\":\"Extraction\"},{\"boundingBox\":[{\"x\":546,\"y\":233},{\"x\":741,\"y\":234},{\"x\":741,\"y\":260},{\"x\":546,\"y\":260}],\"text\":\"Dimensionality\"},{\"boundingBox\":[{\"x\":9,\"y\":251},{\"x\":92,\"y\":253},{\"x\":92,\"y\":278},{\"x\":8,\"y\":277}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":123,\"y\":252},{\"x\":227,\"y\":255},{\"x\":227,\"y\":283},{\"x\":123,\"y\":280}],\"text\":\"Tracing\"},{\"boundingBox\":[{\"x\":873,\"y\":278},{\"x\":1091,\"y\":282},{\"x\":1090,\"y\":311},{\"x\":873,\"y\":305}],\"text\":\"Offline Learning\"},{\"boundingBox\":[{\"x\":862,\"y\":327},{\"x\":973,\"y\":329},{\"x\":972,\"y\":354},{\"x\":862,\"y\":351}],\"text\":\"Training\"},{\"boundingBox\":[{\"x\":1056,\"y\":325},{\"x\":1152,\"y\":326},{\"x\":1152,\"y\":350},{\"x\":1056,\"y\":348}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":1260,\"y\":324},{\"x\":1357,\"y\":325},{\"x\":1356,\"y\":350},{\"x\":1259,\"y\":348}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":874,\"y\":365},{\"x\":957,\"y\":366},{\"x\":957,\"y\":391},{\"x\":873,\"y\":389}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":1039,\"y\":361},{\"x\":1173,\"y\":362},{\"x\":1172,\"y\":387},{\"x\":1039,\"y\":385}],\"text\":\"Extraction\"},{\"boundingBox\":[{\"x\":1266,\"y\":362},{\"x\":1345,\"y\":362},{\"x\":1345,\"y\":388},{\"x\":1266,\"y\":387}],\"text\":\"Space\"},{\"boundingBox\":[{\"x\":872,\"y\":468},{\"x\":968,\"y\":472},{\"x\":966,\"y\":498},{\"x\":872,\"y\":493}],\"text\":\"Testing\"},{\"boundingBox\":[{\"x\":1070,\"y\":468},{\"x\":1167,\"y\":470},{\"x\":1167,\"y\":494},{\"x\":1069,\"y\":492}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":1260,\"y\":468},{\"x\":1356,\"y\":470},{\"x\":1355,\"y\":494},{\"x\":1259,\"y\":491}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":877,\"y\":505},{\"x\":961,\"y\":509},{\"x\":959,\"y\":534},{\"x\":875,\"y\":530}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":1053,\"y\":505},{\"x\":1184,\"y\":507},{\"x\":1184,\"y\":531},{\"x\":1053,\"y\":530}],\"text\":\"Extraction\"},{\"boundingBox\":[{\"x\":1247,\"y\":504},{\"x\":1368,\"y\":507},{\"x\":1367,\"y\":534},{\"x\":1246,\"y\":530}],\"text\":\"Macthing\"},{\"boundingBox\":[{\"x\":881,\"y\":552},{\"x\":1094,\"y\":553},{\"x\":1093,\"y\":581},{\"x\":881,\"y\":579}],\"text\":\"Online Learning\"}],\"words\":[{\"boundingBox\":[{\"x\":1000,\"y\":15},{\"x\":1063,\"y\":16},{\"x\":1062,\"y\":48},{\"x\":1000,\"y\":48}],\"text\":\"Face\"},{\"boundingBox\":[{\"x\":1069,\"y\":16},{\"x\":1202,\"y\":16},{\"x\":1202,\"y\":50},{\"x\":1069,\"y\":48}],\"text\":\"Macthing\"},{\"boundingBox\":[{\"x\":1249,\"y\":22},{\"x\":1404,\"y\":22},{\"x\":1405,\"y\":48},{\"x\":1249,\"y\":48}],\"text\":\"Recognition\"},{\"boundingBox\":[{\"x\":1027,\"y\":56},{\"x\":1180,\"y\":56},{\"x\":1180,\"y\":84},{\"x\":1026,\"y\":83}],\"text\":\"Recognition\"},{\"boundingBox\":[{\"x\":1279,\"y\":78},{\"x\":1376,\"y\":79},{\"x\":1376,\"y\":104},{\"x\":1279,\"y\":104}],\"text\":\"Results\"},{\"boundingBox\":[{\"x\":1012,\"y\":154},{\"x\":1194,\"y\":156},{\"x\":1194,\"y\":182},{\"x\":1011,\"y\":181}],\"text\":\"CLassification\"},{\"boundingBox\":[{\"x\":145,\"y\":180},{\"x\":206,\"y\":181},{\"x\":205,\"y\":205},{\"x\":145,\"y\":203}],\"text\":\"Face\"},{\"boundingBox\":[{\"x\":1062,\"y\":193},{\"x\":1146,\"y\":192},{\"x\":1146,\"y\":218},{\"x\":1062,\"y\":218}],\"text\":\"Result\"},{\"boundingBox\":[{\"x\":6,\"y\":194},{\"x\":78,\"y\":195},{\"x\":78,\"y\":222},{\"x\":6,\"y\":221}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":348,\"y\":197},{\"x\":448,\"y\":200},{\"x\":448,\"y\":225},{\"x\":348,\"y\":223}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":596,\"y\":198},{\"x\":693,\"y\":200},{\"x\":693,\"y\":225},{\"x\":596,\"y\":224}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":106,\"y\":213},{\"x\":240,\"y\":216},{\"x\":239,\"y\":248},{\"x\":105,\"y\":245}],\"text\":\"Searching\"},{\"boundingBox\":[{\"x\":333,\"y\":235},{\"x\":463,\"y\":237},{\"x\":462,\"y\":260},{\"x\":332,\"y\":259}],\"text\":\"Extraction\"},{\"boundingBox\":[{\"x\":547,\"y\":234},{\"x\":741,\"y\":235},{\"x\":740,\"y\":261},{\"x\":546,\"y\":260}],\"text\":\"Dimensionality\"},{\"boundingBox\":[{\"x\":9,\"y\":252},{\"x\":93,\"y\":254},{\"x\":92,\"y\":278},{\"x\":9,\"y\":277}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":129,\"y\":253},{\"x\":228,\"y\":256},{\"x\":226,\"y\":284},{\"x\":129,\"y\":280}],\"text\":\"Tracing\"},{\"boundingBox\":[{\"x\":874,\"y\":278},{\"x\":967,\"y\":279},{\"x\":967,\"y\":306},{\"x\":875,\"y\":305}],\"text\":\"Offline\"},{\"boundingBox\":[{\"x\":972,\"y\":279},{\"x\":1091,\"y\":284},{\"x\":1090,\"y\":312},{\"x\":972,\"y\":306}],\"text\":\"Learning\"},{\"boundingBox\":[{\"x\":862,\"y\":327},{\"x\":973,\"y\":330},{\"x\":973,\"y\":354},{\"x\":862,\"y\":351}],\"text\":\"Training\"},{\"boundingBox\":[{\"x\":1057,\"y\":326},{\"x\":1152,\"y\":327},{\"x\":1152,\"y\":350},{\"x\":1056,\"y\":349}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":1260,\"y\":324},{\"x\":1357,\"y\":326},{\"x\":1357,\"y\":350},{\"x\":1260,\"y\":348}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":874,\"y\":365},{\"x\":956,\"y\":366},{\"x\":956,\"y\":392},{\"x\":874,\"y\":389}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":1040,\"y\":361},{\"x\":1172,\"y\":363},{\"x\":1170,\"y\":388},{\"x\":1039,\"y\":386}],\"text\":\"Extraction\"},{\"boundingBox\":[{\"x\":1267,\"y\":362},{\"x\":1345,\"y\":363},{\"x\":1345,\"y\":388},{\"x\":1267,\"y\":388}],\"text\":\"Space\"},{\"boundingBox\":[{\"x\":873,\"y\":468},{\"x\":967,\"y\":473},{\"x\":966,\"y\":498},{\"x\":873,\"y\":494}],\"text\":\"Testing\"},{\"boundingBox\":[{\"x\":1070,\"y\":469},{\"x\":1166,\"y\":470},{\"x\":1166,\"y\":494},{\"x\":1070,\"y\":492}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":1260,\"y\":468},{\"x\":1355,\"y\":471},{\"x\":1355,\"y\":494},{\"x\":1260,\"y\":492}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":877,\"y\":505},{\"x\":961,\"y\":509},{\"x\":960,\"y\":535},{\"x\":876,\"y\":531}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":1055,\"y\":506},{\"x\":1184,\"y\":508},{\"x\":1183,\"y\":532},{\"x\":1053,\"y\":530}],\"text\":\"Extraction\"},{\"boundingBox\":[{\"x\":1247,\"y\":505},{\"x\":1369,\"y\":507},{\"x\":1368,\"y\":535},{\"x\":1247,\"y\":530}],\"text\":\"Macthing\"},{\"boundingBox\":[{\"x\":881,\"y\":552},{\"x\":971,\"y\":553},{\"x\":971,\"y\":580},{\"x\":882,\"y\":579}],\"text\":\"Online\"},{\"boundingBox\":[{\"x\":976,\"y\":553},{\"x\":1094,\"y\":554},{\"x\":1094,\"y\":582},{\"x\":976,\"y\":580}],\"text\":\"Learning\"}]}",
        "{\"language\":\"en\",\"text\":\"X1 K(x ,x) K(x ,x) Sgn Otrain y train Xtrain ,x)\",\"lines\":[{\"boundingBox\":[{\"x\":20,\"y\":34},{\"x\":57,\"y\":34},{\"x\":57,\"y\":71},{\"x\":19,\"y\":71}],\"text\":\"X1\"},{\"boundingBox\":[{\"x\":370,\"y\":31},{\"x\":527,\"y\":34},{\"x\":526,\"y\":74},{\"x\":370,\"y\":70}],\"text\":\"K(x ,x)\"},{\"boundingBox\":[{\"x\":369,\"y\":196},{\"x\":523,\"y\":199},{\"x\":521,\"y\":241},{\"x\":369,\"y\":235}],\"text\":\"K(x ,x)\"},{\"boundingBox\":[{\"x\":899,\"y\":201},{\"x\":971,\"y\":204},{\"x\":970,\"y\":237},{\"x\":898,\"y\":235}],\"text\":\"Sgn\"},{\"boundingBox\":[{\"x\":616,\"y\":301},{\"x\":752,\"y\":230},{\"x\":768,\"y\":260},{\"x\":631,\"y\":332}],\"text\":\"Otrain y train\"},{\"boundingBox\":[{\"x\":4,\"y\":373},{\"x\":80,\"y\":383},{\"x\":79,\"y\":409},{\"x\":3,\"y\":403}],\"text\":\"Xtrain\"},{\"boundingBox\":[{\"x\":489,\"y\":385},{\"x\":539,\"y\":381},{\"x\":541,\"y\":418},{\"x\":491,\"y\":421}],\"text\":\",x)\"}],\"words\":[{\"boundingBox\":[{\"x\":19,\"y\":34},{\"x\":56,\"y\":34},{\"x\":56,\"y\":71},{\"x\":19,\"y\":71}],\"text\":\"X1\"},{\"boundingBox\":[{\"x\":370,\"y\":33},{\"x\":451,\"y\":33},{\"x\":452,\"y\":72},{\"x\":372,\"y\":71}],\"text\":\"K(x\"},{\"boundingBox\":[{\"x\":466,\"y\":33},{\"x\":524,\"y\":36},{\"x\":525,\"y\":74},{\"x\":467,\"y\":73}],\"text\":\",x)\"},{\"boundingBox\":[{\"x\":369,\"y\":197},{\"x\":454,\"y\":197},{\"x\":454,\"y\":237},{\"x\":369,\"y\":236}],\"text\":\"K(x\"},{\"boundingBox\":[{\"x\":467,\"y\":198},{\"x\":521,\"y\":202},{\"x\":520,\"y\":242},{\"x\":467,\"y\":238}],\"text\":\",x)\"},{\"boundingBox\":[{\"x\":899,\"y\":201},{\"x\":969,\"y\":203},{\"x\":968,\"y\":237},{\"x\":898,\"y\":234}],\"text\":\"Sgn\"},{\"boundingBox\":[{\"x\":622,\"y\":300},{\"x\":681,\"y\":267},{\"x\":696,\"y\":297},{\"x\":637,\"y\":329}],\"text\":\"Otrain\"},{\"boundingBox\":[{\"x\":686,\"y\":264},{\"x\":692,\"y\":261},{\"x\":707,\"y\":292},{\"x\":701,\"y\":294}],\"text\":\"y\"},{\"boundingBox\":[{\"x\":698,\"y\":258},{\"x\":752,\"y\":231},{\"x\":767,\"y\":262},{\"x\":712,\"y\":289}],\"text\":\"train\"},{\"boundingBox\":[{\"x\":10,\"y\":374},{\"x\":81,\"y\":381},{\"x\":78,\"y\":410},{\"x\":7,\"y\":403}],\"text\":\"Xtrain\"},{\"boundingBox\":[{\"x\":489,\"y\":384},{\"x\":538,\"y\":381},{\"x\":540,\"y\":417},{\"x\":491,\"y\":421}],\"text\":\",x)\"}]}",
        "{\"language\":\"en\",\"text\":\"Start Import Image No Does it include faces yes SVM Classifier detection Output results for face detection End\",\"lines\":[{\"boundingBox\":[{\"x\":404,\"y\":32},{\"x\":531,\"y\":33},{\"x\":529,\"y\":79},{\"x\":403,\"y\":77}],\"text\":\"Start\"},{\"boundingBox\":[{\"x\":287,\"y\":264},{\"x\":644,\"y\":268},{\"x\":643,\"y\":321},{\"x\":287,\"y\":316}],\"text\":\"Import Image\"},{\"boundingBox\":[{\"x\":16,\"y\":494},{\"x\":96,\"y\":494},{\"x\":95,\"y\":542},{\"x\":15,\"y\":542}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":198,\"y\":554},{\"x\":734,\"y\":554},{\"x\":734,\"y\":610},{\"x\":198,\"y\":609}],\"text\":\"Does it include faces\"},{\"boundingBox\":[{\"x\":533,\"y\":742},{\"x\":626,\"y\":737},{\"x\":628,\"y\":784},{\"x\":534,\"y\":788}],\"text\":\"yes\"},{\"boundingBox\":[{\"x\":137,\"y\":899},{\"x\":796,\"y\":900},{\"x\":796,\"y\":956},{\"x\":137,\"y\":954}],\"text\":\"SVM Classifier detection\"},{\"boundingBox\":[{\"x\":239,\"y\":1202},{\"x\":698,\"y\":1196},{\"x\":699,\"y\":1251},{\"x\":239,\"y\":1256}],\"text\":\"Output results for\"},{\"boundingBox\":[{\"x\":287,\"y\":1272},{\"x\":649,\"y\":1274},{\"x\":649,\"y\":1328},{\"x\":287,\"y\":1326}],\"text\":\"face detection\"},{\"boundingBox\":[{\"x\":415,\"y\":1498},{\"x\":519,\"y\":1497},{\"x\":520,\"y\":1546},{\"x\":415,\"y\":1545}],\"text\":\"End\"}],\"words\":[{\"boundingBox\":[{\"x\":403,\"y\":32},{\"x\":529,\"y\":33},{\"x\":528,\"y\":79},{\"x\":403,\"y\":77}],\"text\":\"Start\"},{\"boundingBox\":[{\"x\":288,\"y\":264},{\"x\":466,\"y\":266},{\"x\":465,\"y\":320},{\"x\":288,\"y\":318}],\"text\":\"Import\"},{\"boundingBox\":[{\"x\":476,\"y\":266},{\"x\":644,\"y\":269},{\"x\":642,\"y\":321},{\"x\":475,\"y\":320}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":15,\"y\":494},{\"x\":95,\"y\":494},{\"x\":95,\"y\":542},{\"x\":15,\"y\":542}],\"text\":\"No\"},{\"boundingBox\":[{\"x\":199,\"y\":556},{\"x\":327,\"y\":555},{\"x\":328,\"y\":609},{\"x\":199,\"y\":607}],\"text\":\"Does\"},{\"boundingBox\":[{\"x\":337,\"y\":555},{\"x\":381,\"y\":555},{\"x\":382,\"y\":609},{\"x\":338,\"y\":609}],\"text\":\"it\"},{\"boundingBox\":[{\"x\":391,\"y\":555},{\"x\":587,\"y\":554},{\"x\":587,\"y\":611},{\"x\":392,\"y\":609}],\"text\":\"include\"},{\"boundingBox\":[{\"x\":597,\"y\":554},{\"x\":733,\"y\":554},{\"x\":732,\"y\":611},{\"x\":597,\"y\":611}],\"text\":\"faces\"},{\"boundingBox\":[{\"x\":535,\"y\":741},{\"x\":626,\"y\":737},{\"x\":628,\"y\":783},{\"x\":537,\"y\":788}],\"text\":\"yes\"},{\"boundingBox\":[{\"x\":138,\"y\":903},{\"x\":266,\"y\":901},{\"x\":265,\"y\":955},{\"x\":137,\"y\":954}],\"text\":\"SVM\"},{\"boundingBox\":[{\"x\":292,\"y\":900},{\"x\":538,\"y\":900},{\"x\":538,\"y\":956},{\"x\":292,\"y\":955}],\"text\":\"Classifier\"},{\"boundingBox\":[{\"x\":548,\"y\":900},{\"x\":793,\"y\":904},{\"x\":794,\"y\":957},{\"x\":548,\"y\":956}],\"text\":\"detection\"},{\"boundingBox\":[{\"x\":241,\"y\":1203},{\"x\":414,\"y\":1200},{\"x\":413,\"y\":1256},{\"x\":239,\"y\":1255}],\"text\":\"Output\"},{\"boundingBox\":[{\"x\":425,\"y\":1200},{\"x\":605,\"y\":1197},{\"x\":604,\"y\":1253},{\"x\":423,\"y\":1256}],\"text\":\"results\"},{\"boundingBox\":[{\"x\":615,\"y\":1197},{\"x\":697,\"y\":1196},{\"x\":696,\"y\":1249},{\"x\":614,\"y\":1252}],\"text\":\"for\"},{\"boundingBox\":[{\"x\":289,\"y\":1273},{\"x\":393,\"y\":1273},{\"x\":391,\"y\":1328},{\"x\":287,\"y\":1327}],\"text\":\"face\"},{\"boundingBox\":[{\"x\":403,\"y\":1273},{\"x\":646,\"y\":1276},{\"x\":647,\"y\":1328},{\"x\":402,\"y\":1328}],\"text\":\"detection\"},{\"boundingBox\":[{\"x\":415,\"y\":1497},{\"x\":520,\"y\":1497},{\"x\":520,\"y\":1546},{\"x\":415,\"y\":1546}],\"text\":\"End\"}]}",
        "{\"language\":\"en\",\"text\":\"Image input Image input template input Detection extreme extracti on local characteristic region characteristic set Location the Key points of stability characteristicmatching Describing local characteristic region Dislodge the Wrong image matching characteristic set The input of matching image\",\"lines\":[{\"boundingBox\":[{\"x\":160,\"y\":101},{\"x\":355,\"y\":103},{\"x\":354,\"y\":141},{\"x\":160,\"y\":139}],\"text\":\"Image input\"},{\"boundingBox\":[{\"x\":675,\"y\":183},{\"x\":869,\"y\":184},{\"x\":868,\"y\":222},{\"x\":674,\"y\":220}],\"text\":\"Image input\"},{\"boundingBox\":[{\"x\":1120,\"y\":184},{\"x\":1350,\"y\":183},{\"x\":1350,\"y\":220},{\"x\":1120,\"y\":221}],\"text\":\"template input\"},{\"boundingBox\":[{\"x\":108,\"y\":259},{\"x\":410,\"y\":261},{\"x\":410,\"y\":296},{\"x\":108,\"y\":294}],\"text\":\"Detection extreme\"},{\"boundingBox\":[{\"x\":661,\"y\":304},{\"x\":907,\"y\":302},{\"x\":907,\"y\":336},{\"x\":661,\"y\":337}],\"text\":\"extracti on local\"},{\"boundingBox\":[{\"x\":621,\"y\":349},{\"x\":952,\"y\":352},{\"x\":952,\"y\":385},{\"x\":621,\"y\":382}],\"text\":\"characteristic region\"},{\"boundingBox\":[{\"x\":1101,\"y\":336},{\"x\":1380,\"y\":336},{\"x\":1380,\"y\":369},{\"x\":1101,\"y\":370}],\"text\":\"characteristic set\"},{\"boundingBox\":[{\"x\":63,\"y\":431},{\"x\":449,\"y\":432},{\"x\":449,\"y\":472},{\"x\":63,\"y\":469}],\"text\":\"Location the Key points\"},{\"boundingBox\":[{\"x\":173,\"y\":480},{\"x\":347,\"y\":480},{\"x\":346,\"y\":520},{\"x\":173,\"y\":518}],\"text\":\"of stability\"},{\"boundingBox\":[{\"x\":825,\"y\":542},{\"x\":1192,\"y\":542},{\"x\":1192,\"y\":582},{\"x\":825,\"y\":580}],\"text\":\"characteristicmatching\"},{\"boundingBox\":[{\"x\":119,\"y\":620},{\"x\":395,\"y\":620},{\"x\":395,\"y\":665},{\"x\":120,\"y\":666}],\"text\":\"Describing local\"},{\"boundingBox\":[{\"x\":95,\"y\":672},{\"x\":427,\"y\":674},{\"x\":427,\"y\":711},{\"x\":95,\"y\":708}],\"text\":\"characteristic region\"},{\"boundingBox\":[{\"x\":844,\"y\":671},{\"x\":1170,\"y\":672},{\"x\":1170,\"y\":713},{\"x\":844,\"y\":710}],\"text\":\"Dislodge the Wrong\"},{\"boundingBox\":[{\"x\":875,\"y\":718},{\"x\":1136,\"y\":718},{\"x\":1136,\"y\":758},{\"x\":875,\"y\":758}],\"text\":\"image matching\"},{\"boundingBox\":[{\"x\":124,\"y\":847},{\"x\":403,\"y\":847},{\"x\":403,\"y\":884},{\"x\":124,\"y\":884}],\"text\":\"characteristic set\"},{\"boundingBox\":[{\"x\":914,\"y\":849},{\"x\":1113,\"y\":850},{\"x\":1113,\"y\":886},{\"x\":914,\"y\":885}],\"text\":\"The input of\"},{\"boundingBox\":[{\"x\":884,\"y\":901},{\"x\":1148,\"y\":902},{\"x\":1148,\"y\":937},{\"x\":884,\"y\":935}],\"text\":\"matching image\"}],\"words\":[{\"boundingBox\":[{\"x\":161,\"y\":102},{\"x\":260,\"y\":103},{\"x\":261,\"y\":141},{\"x\":162,\"y\":138}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":268,\"y\":103},{\"x\":356,\"y\":105},{\"x\":355,\"y\":139},{\"x\":268,\"y\":141}],\"text\":\"input\"},{\"boundingBox\":[{\"x\":676,\"y\":183},{\"x\":777,\"y\":185},{\"x\":776,\"y\":222},{\"x\":675,\"y\":219}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":784,\"y\":185},{\"x\":868,\"y\":185},{\"x\":868,\"y\":221},{\"x\":783,\"y\":222}],\"text\":\"input\"},{\"boundingBox\":[{\"x\":1121,\"y\":185},{\"x\":1257,\"y\":184},{\"x\":1256,\"y\":221},{\"x\":1121,\"y\":219}],\"text\":\"template\"},{\"boundingBox\":[{\"x\":1263,\"y\":184},{\"x\":1349,\"y\":184},{\"x\":1349,\"y\":219},{\"x\":1263,\"y\":221}],\"text\":\"input\"},{\"boundingBox\":[{\"x\":109,\"y\":259},{\"x\":269,\"y\":262},{\"x\":268,\"y\":296},{\"x\":108,\"y\":295}],\"text\":\"Detection\"},{\"boundingBox\":[{\"x\":276,\"y\":262},{\"x\":410,\"y\":264},{\"x\":410,\"y\":296},{\"x\":276,\"y\":296}],\"text\":\"extreme\"},{\"boundingBox\":[{\"x\":661,\"y\":306},{\"x\":778,\"y\":303},{\"x\":778,\"y\":337},{\"x\":662,\"y\":337}],\"text\":\"extracti\"},{\"boundingBox\":[{\"x\":784,\"y\":303},{\"x\":819,\"y\":303},{\"x\":819,\"y\":337},{\"x\":784,\"y\":337}],\"text\":\"on\"},{\"boundingBox\":[{\"x\":826,\"y\":303},{\"x\":907,\"y\":302},{\"x\":907,\"y\":337},{\"x\":826,\"y\":337}],\"text\":\"local\"},{\"boundingBox\":[{\"x\":622,\"y\":350},{\"x\":842,\"y\":351},{\"x\":842,\"y\":385},{\"x\":623,\"y\":382}],\"text\":\"characteristic\"},{\"boundingBox\":[{\"x\":848,\"y\":351},{\"x\":953,\"y\":353},{\"x\":952,\"y\":384},{\"x\":848,\"y\":385}],\"text\":\"region\"},{\"boundingBox\":[{\"x\":1102,\"y\":337},{\"x\":1321,\"y\":337},{\"x\":1321,\"y\":370},{\"x\":1102,\"y\":371}],\"text\":\"characteristic\"},{\"boundingBox\":[{\"x\":1328,\"y\":337},{\"x\":1380,\"y\":337},{\"x\":1380,\"y\":370},{\"x\":1328,\"y\":370}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":63,\"y\":432},{\"x\":207,\"y\":431},{\"x\":207,\"y\":471},{\"x\":64,\"y\":468}],\"text\":\"Location\"},{\"boundingBox\":[{\"x\":214,\"y\":431},{\"x\":263,\"y\":431},{\"x\":263,\"y\":471},{\"x\":214,\"y\":471}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":270,\"y\":431},{\"x\":341,\"y\":432},{\"x\":341,\"y\":472},{\"x\":270,\"y\":471}],\"text\":\"Key\"},{\"boundingBox\":[{\"x\":348,\"y\":432},{\"x\":449,\"y\":433},{\"x\":449,\"y\":471},{\"x\":348,\"y\":472}],\"text\":\"points\"},{\"boundingBox\":[{\"x\":174,\"y\":480},{\"x\":207,\"y\":481},{\"x\":208,\"y\":519},{\"x\":176,\"y\":519}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":214,\"y\":481},{\"x\":347,\"y\":480},{\"x\":346,\"y\":521},{\"x\":216,\"y\":519}],\"text\":\"stability\"},{\"boundingBox\":[{\"x\":825,\"y\":545},{\"x\":1192,\"y\":544},{\"x\":1191,\"y\":583},{\"x\":826,\"y\":578}],\"text\":\"characteristicmatching\"},{\"boundingBox\":[{\"x\":132,\"y\":621},{\"x\":293,\"y\":620},{\"x\":293,\"y\":665},{\"x\":132,\"y\":667}],\"text\":\"Describing\"},{\"boundingBox\":[{\"x\":302,\"y\":620},{\"x\":396,\"y\":621},{\"x\":395,\"y\":666},{\"x\":302,\"y\":665}],\"text\":\"local\"},{\"boundingBox\":[{\"x\":96,\"y\":673},{\"x\":314,\"y\":674},{\"x\":314,\"y\":711},{\"x\":96,\"y\":709}],\"text\":\"characteristic\"},{\"boundingBox\":[{\"x\":321,\"y\":674},{\"x\":427,\"y\":676},{\"x\":427,\"y\":711},{\"x\":321,\"y\":711}],\"text\":\"region\"},{\"boundingBox\":[{\"x\":844,\"y\":672},{\"x\":984,\"y\":671},{\"x\":984,\"y\":711},{\"x\":846,\"y\":710}],\"text\":\"Dislodge\"},{\"boundingBox\":[{\"x\":991,\"y\":671},{\"x\":1050,\"y\":672},{\"x\":1050,\"y\":712},{\"x\":992,\"y\":711}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":1057,\"y\":672},{\"x\":1170,\"y\":673},{\"x\":1169,\"y\":714},{\"x\":1057,\"y\":712}],\"text\":\"Wrong\"},{\"boundingBox\":[{\"x\":876,\"y\":723},{\"x\":976,\"y\":722},{\"x\":977,\"y\":755},{\"x\":877,\"y\":755}],\"text\":\"image\"},{\"boundingBox\":[{\"x\":983,\"y\":721},{\"x\":1137,\"y\":719},{\"x\":1137,\"y\":759},{\"x\":983,\"y\":755}],\"text\":\"matching\"},{\"boundingBox\":[{\"x\":125,\"y\":849},{\"x\":343,\"y\":848},{\"x\":344,\"y\":883},{\"x\":128,\"y\":885}],\"text\":\"characteristic\"},{\"boundingBox\":[{\"x\":350,\"y\":848},{\"x\":403,\"y\":849},{\"x\":403,\"y\":884},{\"x\":351,\"y\":883}],\"text\":\"set\"},{\"boundingBox\":[{\"x\":916,\"y\":850},{\"x\":968,\"y\":851},{\"x\":967,\"y\":886},{\"x\":915,\"y\":885}],\"text\":\"The\"},{\"boundingBox\":[{\"x\":975,\"y\":851},{\"x\":1067,\"y\":851},{\"x\":1066,\"y\":887},{\"x\":974,\"y\":886}],\"text\":\"input\"},{\"boundingBox\":[{\"x\":1074,\"y\":851},{\"x\":1113,\"y\":850},{\"x\":1112,\"y\":887},{\"x\":1073,\"y\":887}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":884,\"y\":902},{\"x\":1037,\"y\":902},{\"x\":1037,\"y\":938},{\"x\":885,\"y\":936}],\"text\":\"matching\"},{\"boundingBox\":[{\"x\":1044,\"y\":902},{\"x\":1149,\"y\":904},{\"x\":1148,\"y\":938},{\"x\":1044,\"y\":938}],\"text\":\"image\"}]}",
        "{\"language\":\"en\",\"text\":\"Octave5 80 Octave4 40 Octave3 20 Octave2 Octave1\",\"lines\":[{\"boundingBox\":[{\"x\":741,\"y\":12},{\"x\":894,\"y\":11},{\"x\":895,\"y\":43},{\"x\":742,\"y\":46}],\"text\":\"Octave5\"},{\"boundingBox\":[{\"x\":0,\"y\":99},{\"x\":55,\"y\":98},{\"x\":57,\"y\":129},{\"x\":0,\"y\":133}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":741,\"y\":93},{\"x\":894,\"y\":93},{\"x\":895,\"y\":131},{\"x\":741,\"y\":132}],\"text\":\"Octave4\"},{\"boundingBox\":[{\"x\":0,\"y\":170},{\"x\":55,\"y\":167},{\"x\":58,\"y\":199},{\"x\":0,\"y\":205}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":743,\"y\":173},{\"x\":894,\"y\":173},{\"x\":895,\"y\":205},{\"x\":743,\"y\":208}],\"text\":\"Octave3\"},{\"boundingBox\":[{\"x\":9,\"y\":326},{\"x\":57,\"y\":326},{\"x\":57,\"y\":364},{\"x\":9,\"y\":364}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":745,\"y\":303},{\"x\":893,\"y\":303},{\"x\":893,\"y\":343},{\"x\":745,\"y\":343}],\"text\":\"Octave2\"},{\"boundingBox\":[{\"x\":746,\"y\":696},{\"x\":891,\"y\":696},{\"x\":891,\"y\":730},{\"x\":746,\"y\":731}],\"text\":\"Octave1\"}],\"words\":[{\"boundingBox\":[{\"x\":742,\"y\":12},{\"x\":894,\"y\":11},{\"x\":894,\"y\":40},{\"x\":742,\"y\":45}],\"text\":\"Octave5\"},{\"boundingBox\":[{\"x\":0,\"y\":99},{\"x\":53,\"y\":98},{\"x\":55,\"y\":130},{\"x\":0,\"y\":132}],\"text\":\"80\"},{\"boundingBox\":[{\"x\":742,\"y\":94},{\"x\":895,\"y\":93},{\"x\":895,\"y\":132},{\"x\":741,\"y\":132}],\"text\":\"Octave4\"},{\"boundingBox\":[{\"x\":0,\"y\":170},{\"x\":54,\"y\":167},{\"x\":56,\"y\":200},{\"x\":0,\"y\":204}],\"text\":\"40\"},{\"boundingBox\":[{\"x\":744,\"y\":174},{\"x\":895,\"y\":173},{\"x\":895,\"y\":203},{\"x\":743,\"y\":207}],\"text\":\"Octave3\"},{\"boundingBox\":[{\"x\":9,\"y\":326},{\"x\":55,\"y\":326},{\"x\":55,\"y\":364},{\"x\":9,\"y\":364}],\"text\":\"20\"},{\"boundingBox\":[{\"x\":746,\"y\":304},{\"x\":893,\"y\":303},{\"x\":893,\"y\":343},{\"x\":746,\"y\":343}],\"text\":\"Octave2\"},{\"boundingBox\":[{\"x\":747,\"y\":697},{\"x\":891,\"y\":696},{\"x\":888,\"y\":731},{\"x\":747,\"y\":732}],\"text\":\"Octave1\"}]}",
        "{\"language\":\"en\",\"text\":\"02 0.1 -0.1 02 -03 Laplacian - DOG -0.4\",\"lines\":[{\"boundingBox\":[{\"x\":16,\"y\":35},{\"x\":68,\"y\":35},{\"x\":68,\"y\":72},{\"x\":16,\"y\":72}],\"text\":\"02\"},{\"boundingBox\":[{\"x\":16,\"y\":191},{\"x\":64,\"y\":191},{\"x\":66,\"y\":224},{\"x\":16,\"y\":225}],\"text\":\"0.1\"},{\"boundingBox\":[{\"x\":10,\"y\":500},{\"x\":64,\"y\":501},{\"x\":63,\"y\":535},{\"x\":9,\"y\":533}],\"text\":\"-0.1\"},{\"boundingBox\":[{\"x\":13,\"y\":654},{\"x\":77,\"y\":655},{\"x\":77,\"y\":689},{\"x\":12,\"y\":689}],\"text\":\"02\"},{\"boundingBox\":[{\"x\":11,\"y\":808},{\"x\":66,\"y\":809},{\"x\":66,\"y\":845},{\"x\":11,\"y\":844}],\"text\":\"-03\"},{\"boundingBox\":[{\"x\":996,\"y\":873},{\"x\":1191,\"y\":871},{\"x\":1192,\"y\":922},{\"x\":996,\"y\":925}],\"text\":\"Laplacian\"},{\"boundingBox\":[{\"x\":936,\"y\":919},{\"x\":997,\"y\":920},{\"x\":993,\"y\":960},{\"x\":935,\"y\":957}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1003,\"y\":924},{\"x\":1097,\"y\":924},{\"x\":1096,\"y\":960},{\"x\":1003,\"y\":960}],\"text\":\"DOG\"},{\"boundingBox\":[{\"x\":7,\"y\":963},{\"x\":66,\"y\":964},{\"x\":65,\"y\":999},{\"x\":7,\"y\":998}],\"text\":\"-0.4\"}],\"words\":[{\"boundingBox\":[{\"x\":16,\"y\":35},{\"x\":68,\"y\":35},{\"x\":68,\"y\":72},{\"x\":16,\"y\":72}],\"text\":\"02\"},{\"boundingBox\":[{\"x\":16,\"y\":191},{\"x\":66,\"y\":191},{\"x\":66,\"y\":224},{\"x\":16,\"y\":225}],\"text\":\"0.1\"},{\"boundingBox\":[{\"x\":9,\"y\":500},{\"x\":63,\"y\":501},{\"x\":62,\"y\":535},{\"x\":9,\"y\":533}],\"text\":\"-0.1\"},{\"boundingBox\":[{\"x\":14,\"y\":654},{\"x\":75,\"y\":654},{\"x\":75,\"y\":689},{\"x\":14,\"y\":688}],\"text\":\"02\"},{\"boundingBox\":[{\"x\":11,\"y\":808},{\"x\":64,\"y\":809},{\"x\":63,\"y\":845},{\"x\":11,\"y\":844}],\"text\":\"-03\"},{\"boundingBox\":[{\"x\":1000,\"y\":874},{\"x\":1191,\"y\":872},{\"x\":1191,\"y\":924},{\"x\":1000,\"y\":925}],\"text\":\"Laplacian\"},{\"boundingBox\":[{\"x\":936,\"y\":919},{\"x\":968,\"y\":919},{\"x\":967,\"y\":959},{\"x\":935,\"y\":958}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":1005,\"y\":924},{\"x\":1094,\"y\":924},{\"x\":1094,\"y\":960},{\"x\":1005,\"y\":960}],\"text\":\"DOG\"},{\"boundingBox\":[{\"x\":7,\"y\":963},{\"x\":65,\"y\":964},{\"x\":65,\"y\":999},{\"x\":7,\"y\":998}],\"text\":\"-0.4\"}]}",
        "{\"language\":\"en\",\"text\":\"Scale (next octave) Gaussian DOG\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":92},{\"x\":206,\"y\":91},{\"x\":207,\"y\":126},{\"x\":0,\"y\":127}],\"text\":\"Scale (next\"},{\"boundingBox\":[{\"x\":37,\"y\":147},{\"x\":164,\"y\":146},{\"x\":165,\"y\":181},{\"x\":37,\"y\":182}],\"text\":\"octave)\"},{\"boundingBox\":[{\"x\":251,\"y\":810},{\"x\":402,\"y\":810},{\"x\":402,\"y\":841},{\"x\":251,\"y\":841}],\"text\":\"Gaussian\"},{\"boundingBox\":[{\"x\":955,\"y\":805},{\"x\":1033,\"y\":805},{\"x\":1032,\"y\":835},{\"x\":954,\"y\":836}],\"text\":\"DOG\"}],\"words\":[{\"boundingBox\":[{\"x\":0,\"y\":95},{\"x\":96,\"y\":92},{\"x\":96,\"y\":127},{\"x\":0,\"y\":127}],\"text\":\"Scale\"},{\"boundingBox\":[{\"x\":106,\"y\":92},{\"x\":206,\"y\":91},{\"x\":206,\"y\":127},{\"x\":106,\"y\":127}],\"text\":\"(next\"},{\"boundingBox\":[{\"x\":37,\"y\":150},{\"x\":165,\"y\":146},{\"x\":164,\"y\":182},{\"x\":40,\"y\":181}],\"text\":\"octave)\"},{\"boundingBox\":[{\"x\":252,\"y\":811},{\"x\":402,\"y\":811},{\"x\":402,\"y\":842},{\"x\":251,\"y\":842}],\"text\":\"Gaussian\"},{\"boundingBox\":[{\"x\":954,\"y\":805},{\"x\":1032,\"y\":805},{\"x\":1033,\"y\":835},{\"x\":954,\"y\":836}],\"text\":\"DOG\"}]}",
        "{\"language\":\"en\",\"text\":\"Scale\",\"lines\":[{\"boundingBox\":[{\"x\":689,\"y\":411},{\"x\":894,\"y\":411},{\"x\":893,\"y\":472},{\"x\":688,\"y\":470}],\"text\":\"Scale\"}],\"words\":[{\"boundingBox\":[{\"x\":693,\"y\":413},{\"x\":894,\"y\":412},{\"x\":894,\"y\":473},{\"x\":689,\"y\":469}],\"text\":\"Scale\"}]}",
        "{\"language\":\"en\",\"text\":\"The main direction\",\"lines\":[{\"boundingBox\":[{\"x\":929,\"y\":2},{\"x\":1368,\"y\":3},{\"x\":1367,\"y\":39},{\"x\":929,\"y\":38}],\"text\":\"The main direction\"}],\"words\":[{\"boundingBox\":[{\"x\":930,\"y\":3},{\"x\":1010,\"y\":3},{\"x\":1010,\"y\":39},{\"x\":930,\"y\":38}],\"text\":\"The\"},{\"boundingBox\":[{\"x\":1022,\"y\":3},{\"x\":1129,\"y\":3},{\"x\":1130,\"y\":40},{\"x\":1022,\"y\":39}],\"text\":\"main\"},{\"boundingBox\":[{\"x\":1146,\"y\":3},{\"x\":1368,\"y\":4},{\"x\":1368,\"y\":39},{\"x\":1146,\"y\":40}],\"text\":\"direction\"}]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"50 100 150 200 100 200 300 400 500 600\",\"lines\":[{\"boundingBox\":[{\"x\":17,\"y\":87},{\"x\":52,\"y\":87},{\"x\":52,\"y\":111},{\"x\":18,\"y\":112}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":3,\"y\":177},{\"x\":52,\"y\":176},{\"x\":52,\"y\":202},{\"x\":4,\"y\":202}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":5,\"y\":267},{\"x\":50,\"y\":267},{\"x\":50,\"y\":294},{\"x\":5,\"y\":294}],\"text\":\"150\"},{\"boundingBox\":[{\"x\":2,\"y\":360},{\"x\":52,\"y\":360},{\"x\":52,\"y\":386},{\"x\":3,\"y\":386}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":210,\"y\":465},{\"x\":261,\"y\":465},{\"x\":262,\"y\":490},{\"x\":211,\"y\":490}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":383,\"y\":465},{\"x\":434,\"y\":465},{\"x\":434,\"y\":490},{\"x\":385,\"y\":491}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":556,\"y\":464},{\"x\":609,\"y\":464},{\"x\":610,\"y\":490},{\"x\":556,\"y\":491}],\"text\":\"300\"},{\"boundingBox\":[{\"x\":732,\"y\":464},{\"x\":785,\"y\":464},{\"x\":784,\"y\":491},{\"x\":733,\"y\":490}],\"text\":\"400\"},{\"boundingBox\":[{\"x\":906,\"y\":466},{\"x\":957,\"y\":465},{\"x\":957,\"y\":490},{\"x\":906,\"y\":490}],\"text\":\"500\"},{\"boundingBox\":[{\"x\":1080,\"y\":466},{\"x\":1130,\"y\":465},{\"x\":1131,\"y\":489},{\"x\":1080,\"y\":490}],\"text\":\"600\"}],\"words\":[{\"boundingBox\":[{\"x\":17,\"y\":87},{\"x\":52,\"y\":87},{\"x\":52,\"y\":111},{\"x\":17,\"y\":112}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":3,\"y\":176},{\"x\":51,\"y\":176},{\"x\":52,\"y\":202},{\"x\":3,\"y\":202}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":5,\"y\":267},{\"x\":50,\"y\":267},{\"x\":50,\"y\":294},{\"x\":5,\"y\":294}],\"text\":\"150\"},{\"boundingBox\":[{\"x\":2,\"y\":360},{\"x\":51,\"y\":360},{\"x\":51,\"y\":386},{\"x\":2,\"y\":386}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":212,\"y\":465},{\"x\":262,\"y\":465},{\"x\":262,\"y\":490},{\"x\":212,\"y\":490}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":384,\"y\":465},{\"x\":433,\"y\":465},{\"x\":433,\"y\":490},{\"x\":385,\"y\":491}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":556,\"y\":464},{\"x\":610,\"y\":464},{\"x\":610,\"y\":490},{\"x\":556,\"y\":491}],\"text\":\"300\"},{\"boundingBox\":[{\"x\":732,\"y\":464},{\"x\":784,\"y\":464},{\"x\":784,\"y\":491},{\"x\":732,\"y\":490}],\"text\":\"400\"},{\"boundingBox\":[{\"x\":906,\"y\":465},{\"x\":956,\"y\":465},{\"x\":956,\"y\":490},{\"x\":906,\"y\":490}],\"text\":\"500\"},{\"boundingBox\":[{\"x\":1080,\"y\":465},{\"x\":1129,\"y\":465},{\"x\":1130,\"y\":489},{\"x\":1080,\"y\":490}],\"text\":\"600\"}]}",
        "{\"language\":\"en\",\"text\":\"50 100 150 200 100 200 300 400 500 600\",\"lines\":[{\"boundingBox\":[{\"x\":17,\"y\":86},{\"x\":50,\"y\":85},{\"x\":51,\"y\":112},{\"x\":17,\"y\":112}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":4,\"y\":176},{\"x\":50,\"y\":176},{\"x\":50,\"y\":202},{\"x\":4,\"y\":203}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":0,\"y\":270},{\"x\":50,\"y\":267},{\"x\":50,\"y\":292},{\"x\":0,\"y\":291}],\"text\":\"150\"},{\"boundingBox\":[{\"x\":0,\"y\":362},{\"x\":51,\"y\":360},{\"x\":51,\"y\":386},{\"x\":0,\"y\":385}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":209,\"y\":466},{\"x\":261,\"y\":465},{\"x\":261,\"y\":489},{\"x\":210,\"y\":490}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":381,\"y\":466},{\"x\":434,\"y\":465},{\"x\":434,\"y\":490},{\"x\":382,\"y\":491}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":555,\"y\":465},{\"x\":608,\"y\":464},{\"x\":608,\"y\":491},{\"x\":555,\"y\":491}],\"text\":\"300\"},{\"boundingBox\":[{\"x\":733,\"y\":466},{\"x\":783,\"y\":466},{\"x\":782,\"y\":490},{\"x\":734,\"y\":489}],\"text\":\"400\"},{\"boundingBox\":[{\"x\":905,\"y\":467},{\"x\":957,\"y\":465},{\"x\":957,\"y\":489},{\"x\":906,\"y\":490}],\"text\":\"500\"},{\"boundingBox\":[{\"x\":1080,\"y\":466},{\"x\":1132,\"y\":466},{\"x\":1132,\"y\":489},{\"x\":1081,\"y\":490}],\"text\":\"600\"}],\"words\":[{\"boundingBox\":[{\"x\":17,\"y\":85},{\"x\":49,\"y\":85},{\"x\":49,\"y\":112},{\"x\":17,\"y\":112}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":4,\"y\":176},{\"x\":49,\"y\":176},{\"x\":49,\"y\":203},{\"x\":4,\"y\":203}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":0,\"y\":268},{\"x\":49,\"y\":267},{\"x\":50,\"y\":292},{\"x\":0,\"y\":293}],\"text\":\"150\"},{\"boundingBox\":[{\"x\":0,\"y\":360},{\"x\":50,\"y\":360},{\"x\":50,\"y\":386},{\"x\":0,\"y\":386}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":209,\"y\":466},{\"x\":260,\"y\":465},{\"x\":260,\"y\":489},{\"x\":209,\"y\":490}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":382,\"y\":466},{\"x\":432,\"y\":465},{\"x\":433,\"y\":490},{\"x\":383,\"y\":491}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":555,\"y\":464},{\"x\":607,\"y\":464},{\"x\":607,\"y\":491},{\"x\":555,\"y\":491}],\"text\":\"300\"},{\"boundingBox\":[{\"x\":733,\"y\":466},{\"x\":783,\"y\":466},{\"x\":782,\"y\":490},{\"x\":733,\"y\":489}],\"text\":\"400\"},{\"boundingBox\":[{\"x\":905,\"y\":466},{\"x\":955,\"y\":465},{\"x\":956,\"y\":489},{\"x\":905,\"y\":490}],\"text\":\"500\"},{\"boundingBox\":[{\"x\":1080,\"y\":466},{\"x\":1131,\"y\":466},{\"x\":1131,\"y\":489},{\"x\":1080,\"y\":490}],\"text\":\"600\"}]}",
        "{\"language\":\"en\",\"text\":\"50 100 150 200 100 200 300 400 500 600\",\"lines\":[{\"boundingBox\":[{\"x\":19,\"y\":81},{\"x\":56,\"y\":80},{\"x\":55,\"y\":107},{\"x\":19,\"y\":107}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":2,\"y\":164},{\"x\":54,\"y\":163},{\"x\":54,\"y\":190},{\"x\":2,\"y\":191}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":4,\"y\":249},{\"x\":54,\"y\":249},{\"x\":53,\"y\":275},{\"x\":5,\"y\":273}],\"text\":\"150\"},{\"boundingBox\":[{\"x\":1,\"y\":334},{\"x\":53,\"y\":333},{\"x\":54,\"y\":361},{\"x\":1,\"y\":361}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":212,\"y\":434},{\"x\":263,\"y\":434},{\"x\":264,\"y\":457},{\"x\":212,\"y\":457}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":385,\"y\":432},{\"x\":436,\"y\":431},{\"x\":437,\"y\":459},{\"x\":386,\"y\":459}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":559,\"y\":434},{\"x\":606,\"y\":434},{\"x\":607,\"y\":457},{\"x\":558,\"y\":458}],\"text\":\"300\"},{\"boundingBox\":[{\"x\":733,\"y\":434},{\"x\":783,\"y\":434},{\"x\":783,\"y\":457},{\"x\":733,\"y\":457}],\"text\":\"400\"},{\"boundingBox\":[{\"x\":906,\"y\":433},{\"x\":956,\"y\":434},{\"x\":957,\"y\":457},{\"x\":905,\"y\":457}],\"text\":\"500\"},{\"boundingBox\":[{\"x\":1080,\"y\":434},{\"x\":1130,\"y\":434},{\"x\":1131,\"y\":457},{\"x\":1080,\"y\":458}],\"text\":\"600\"}],\"words\":[{\"boundingBox\":[{\"x\":19,\"y\":80},{\"x\":55,\"y\":80},{\"x\":55,\"y\":107},{\"x\":19,\"y\":107}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":2,\"y\":164},{\"x\":52,\"y\":163},{\"x\":52,\"y\":190},{\"x\":2,\"y\":191}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":4,\"y\":249},{\"x\":53,\"y\":249},{\"x\":52,\"y\":275},{\"x\":4,\"y\":274}],\"text\":\"150\"},{\"boundingBox\":[{\"x\":3,\"y\":333},{\"x\":53,\"y\":333},{\"x\":53,\"y\":360},{\"x\":3,\"y\":361}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":212,\"y\":434},{\"x\":263,\"y\":434},{\"x\":263,\"y\":457},{\"x\":212,\"y\":457}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":385,\"y\":431},{\"x\":435,\"y\":431},{\"x\":435,\"y\":459},{\"x\":385,\"y\":459}],\"text\":\"200\"},{\"boundingBox\":[{\"x\":558,\"y\":434},{\"x\":606,\"y\":434},{\"x\":606,\"y\":458},{\"x\":558,\"y\":458}],\"text\":\"300\"},{\"boundingBox\":[{\"x\":733,\"y\":434},{\"x\":782,\"y\":434},{\"x\":782,\"y\":457},{\"x\":733,\"y\":457}],\"text\":\"400\"},{\"boundingBox\":[{\"x\":905,\"y\":433},{\"x\":956,\"y\":433},{\"x\":956,\"y\":457},{\"x\":905,\"y\":456}],\"text\":\"500\"},{\"boundingBox\":[{\"x\":1080,\"y\":434},{\"x\":1129,\"y\":434},{\"x\":1130,\"y\":458},{\"x\":1080,\"y\":458}],\"text\":\"600\"}]}",
        "{\"language\":\"en\",\"text\":\"50 100 150 200\",\"lines\":[{\"boundingBox\":[{\"x\":17,\"y\":86},{\"x\":52,\"y\":85},{\"x\":52,\"y\":112},{\"x\":17,\"y\":113}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":2,\"y\":176},{\"x\":52,\"y\":176},{\"x\":52,\"y\":202},{\"x\":2,\"y\":202}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":1,\"y\":267},{\"x\":52,\"y\":267},{\"x\":52,\"y\":293},{\"x\":3,\"y\":293}],\"text\":\"150\"},{\"boundingBox\":[{\"x\":1,\"y\":360},{\"x\":52,\"y\":360},{\"x\":52,\"y\":387},{\"x\":2,\"y\":387}],\"text\":\"200\"}],\"words\":[{\"boundingBox\":[{\"x\":17,\"y\":86},{\"x\":50,\"y\":85},{\"x\":51,\"y\":112},{\"x\":17,\"y\":113}],\"text\":\"50\"},{\"boundingBox\":[{\"x\":2,\"y\":176},{\"x\":51,\"y\":176},{\"x\":51,\"y\":202},{\"x\":2,\"y\":202}],\"text\":\"100\"},{\"boundingBox\":[{\"x\":1,\"y\":267},{\"x\":51,\"y\":267},{\"x\":51,\"y\":293},{\"x\":1,\"y\":293}],\"text\":\"150\"},{\"boundingBox\":[{\"x\":1,\"y\":360},{\"x\":51,\"y\":360},{\"x\":51,\"y\":387},{\"x\":1,\"y\":387}],\"text\":\"200\"}]}",
        "{\"language\":\"en\",\"text\":\"Start Input the training face sample Image normalization. Calculate the average face Extract local features SDM Iterative Solution Generate face detection model End\",\"lines\":[{\"boundingBox\":[{\"x\":393,\"y\":27},{\"x\":499,\"y\":29},{\"x\":498,\"y\":69},{\"x\":392,\"y\":68}],\"text\":\"Start\"},{\"boundingBox\":[{\"x\":119,\"y\":211},{\"x\":764,\"y\":211},{\"x\":764,\"y\":262},{\"x\":119,\"y\":262}],\"text\":\"Input the training face sample\"},{\"boundingBox\":[{\"x\":211,\"y\":440},{\"x\":668,\"y\":438},{\"x\":668,\"y\":486},{\"x\":211,\"y\":488}],\"text\":\"Image normalization.\"},{\"boundingBox\":[{\"x\":159,\"y\":502},{\"x\":729,\"y\":505},{\"x\":729,\"y\":553},{\"x\":159,\"y\":551}],\"text\":\"Calculate the average face\"},{\"boundingBox\":[{\"x\":213,\"y\":715},{\"x\":674,\"y\":716},{\"x\":674,\"y\":763},{\"x\":213,\"y\":762}],\"text\":\"Extract local features\"},{\"boundingBox\":[{\"x\":189,\"y\":936},{\"x\":699,\"y\":936},{\"x\":699,\"y\":984},{\"x\":189,\"y\":982}],\"text\":\"SDM Iterative Solution\"},{\"boundingBox\":[{\"x\":100,\"y\":1141},{\"x\":782,\"y\":1141},{\"x\":782,\"y\":1187},{\"x\":100,\"y\":1188}],\"text\":\"Generate face detection model\"},{\"boundingBox\":[{\"x\":398,\"y\":1332},{\"x\":489,\"y\":1333},{\"x\":488,\"y\":1375},{\"x\":398,\"y\":1376}],\"text\":\"End\"}],\"words\":[{\"boundingBox\":[{\"x\":392,\"y\":27},{\"x\":499,\"y\":28},{\"x\":498,\"y\":69},{\"x\":392,\"y\":67}],\"text\":\"Start\"},{\"boundingBox\":[{\"x\":121,\"y\":213},{\"x\":233,\"y\":212},{\"x\":232,\"y\":262},{\"x\":120,\"y\":262}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":242,\"y\":212},{\"x\":310,\"y\":212},{\"x\":309,\"y\":263},{\"x\":242,\"y\":262}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":319,\"y\":212},{\"x\":493,\"y\":212},{\"x\":493,\"y\":263},{\"x\":319,\"y\":263}],\"text\":\"training\"},{\"boundingBox\":[{\"x\":502,\"y\":212},{\"x\":592,\"y\":212},{\"x\":592,\"y\":263},{\"x\":502,\"y\":263}],\"text\":\"face\"},{\"boundingBox\":[{\"x\":602,\"y\":212},{\"x\":762,\"y\":212},{\"x\":763,\"y\":263},{\"x\":602,\"y\":263}],\"text\":\"sample\"},{\"boundingBox\":[{\"x\":212,\"y\":441},{\"x\":347,\"y\":441},{\"x\":346,\"y\":488},{\"x\":212,\"y\":488}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":356,\"y\":441},{\"x\":667,\"y\":439},{\"x\":667,\"y\":486},{\"x\":355,\"y\":488}],\"text\":\"normalization.\"},{\"boundingBox\":[{\"x\":160,\"y\":503},{\"x\":361,\"y\":504},{\"x\":361,\"y\":553},{\"x\":159,\"y\":549}],\"text\":\"Calculate\"},{\"boundingBox\":[{\"x\":371,\"y\":504},{\"x\":441,\"y\":504},{\"x\":441,\"y\":553},{\"x\":370,\"y\":553}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":450,\"y\":504},{\"x\":621,\"y\":505},{\"x\":621,\"y\":553},{\"x\":450,\"y\":553}],\"text\":\"average\"},{\"boundingBox\":[{\"x\":630,\"y\":505},{\"x\":728,\"y\":505},{\"x\":728,\"y\":551},{\"x\":630,\"y\":553}],\"text\":\"face\"},{\"boundingBox\":[{\"x\":215,\"y\":719},{\"x\":371,\"y\":716},{\"x\":371,\"y\":762},{\"x\":214,\"y\":762}],\"text\":\"Extract\"},{\"boundingBox\":[{\"x\":380,\"y\":716},{\"x\":492,\"y\":716},{\"x\":491,\"y\":763},{\"x\":379,\"y\":762}],\"text\":\"local\"},{\"boundingBox\":[{\"x\":500,\"y\":716},{\"x\":674,\"y\":720},{\"x\":673,\"y\":764},{\"x\":499,\"y\":763}],\"text\":\"features\"},{\"boundingBox\":[{\"x\":189,\"y\":937},{\"x\":299,\"y\":936},{\"x\":299,\"y\":983},{\"x\":189,\"y\":982}],\"text\":\"SDM\"},{\"boundingBox\":[{\"x\":317,\"y\":936},{\"x\":500,\"y\":936},{\"x\":499,\"y\":984},{\"x\":316,\"y\":983}],\"text\":\"Iterative\"},{\"boundingBox\":[{\"x\":509,\"y\":936},{\"x\":698,\"y\":937},{\"x\":697,\"y\":985},{\"x\":508,\"y\":984}],\"text\":\"Solution\"},{\"boundingBox\":[{\"x\":101,\"y\":1144},{\"x\":304,\"y\":1142},{\"x\":303,\"y\":1189},{\"x\":101,\"y\":1188}],\"text\":\"Generate\"},{\"boundingBox\":[{\"x\":312,\"y\":1142},{\"x\":408,\"y\":1141},{\"x\":407,\"y\":1189},{\"x\":312,\"y\":1189}],\"text\":\"face\"},{\"boundingBox\":[{\"x\":416,\"y\":1141},{\"x\":627,\"y\":1141},{\"x\":627,\"y\":1189},{\"x\":416,\"y\":1189}],\"text\":\"detection\"},{\"boundingBox\":[{\"x\":636,\"y\":1141},{\"x\":781,\"y\":1142},{\"x\":780,\"y\":1188},{\"x\":636,\"y\":1189}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":398,\"y\":1332},{\"x\":489,\"y\":1332},{\"x\":489,\"y\":1376},{\"x\":398,\"y\":1376}],\"text\":\"End\"}]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"\",\"lines\":[],\"words\":[]}",
        "{\"language\":\"en\",\"text\":\"w P x World coordinate systems U C O Z plane-coordinate system R, T X Y Camera coordinate systems\",\"lines\":[{\"boundingBox\":[{\"x\":1056,\"y\":14},{\"x\":1087,\"y\":14},{\"x\":1086,\"y\":43},{\"x\":1055,\"y\":43}],\"text\":\"w\"},{\"boundingBox\":[{\"x\":910,\"y\":53},{\"x\":930,\"y\":49},{\"x\":933,\"y\":68},{\"x\":913,\"y\":72}],\"text\":\"P\"},{\"boundingBox\":[{\"x\":550,\"y\":79},{\"x\":569,\"y\":78},{\"x\":570,\"y\":97},{\"x\":551,\"y\":99}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":1085,\"y\":191},{\"x\":1419,\"y\":193},{\"x\":1418,\"y\":222},{\"x\":1085,\"y\":220}],\"text\":\"World coordinate systems\"},{\"boundingBox\":[{\"x\":974,\"y\":243},{\"x\":994,\"y\":248},{\"x\":989,\"y\":272},{\"x\":969,\"y\":267}],\"text\":\"U\"},{\"boundingBox\":[{\"x\":621,\"y\":302},{\"x\":641,\"y\":303},{\"x\":641,\"y\":322},{\"x\":621,\"y\":321}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":141,\"y\":405},{\"x\":138,\"y\":383},{\"x\":153,\"y\":379},{\"x\":156,\"y\":402}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":340,\"y\":375},{\"x\":360,\"y\":374},{\"x\":360,\"y\":396},{\"x\":340,\"y\":397}],\"text\":\"Z\"},{\"boundingBox\":[{\"x\":454,\"y\":425},{\"x\":768,\"y\":425},{\"x\":768,\"y\":455},{\"x\":454,\"y\":456}],\"text\":\"plane-coordinate system\"},{\"boundingBox\":[{\"x\":876,\"y\":438},{\"x\":924,\"y\":436},{\"x\":926,\"y\":468},{\"x\":877,\"y\":470}],\"text\":\"R, T\"},{\"boundingBox\":[{\"x\":286,\"y\":511},{\"x\":289,\"y\":477},{\"x\":309,\"y\":478},{\"x\":306,\"y\":512}],\"text\":\"X\"},{\"boundingBox\":[{\"x\":185,\"y\":508},{\"x\":209,\"y\":509},{\"x\":208,\"y\":534},{\"x\":184,\"y\":533}],\"text\":\"Y\"},{\"boundingBox\":[{\"x\":0,\"y\":551},{\"x\":354,\"y\":553},{\"x\":353,\"y\":581},{\"x\":0,\"y\":580}],\"text\":\"Camera coordinate systems\"}],\"words\":[{\"boundingBox\":[{\"x\":1055,\"y\":14},{\"x\":1082,\"y\":14},{\"x\":1082,\"y\":43},{\"x\":1055,\"y\":43}],\"text\":\"w\"},{\"boundingBox\":[{\"x\":910,\"y\":53},{\"x\":929,\"y\":49},{\"x\":933,\"y\":68},{\"x\":914,\"y\":72}],\"text\":\"P\"},{\"boundingBox\":[{\"x\":552,\"y\":79},{\"x\":568,\"y\":78},{\"x\":569,\"y\":97},{\"x\":554,\"y\":99}],\"text\":\"x\"},{\"boundingBox\":[{\"x\":1086,\"y\":191},{\"x\":1165,\"y\":192},{\"x\":1164,\"y\":221},{\"x\":1086,\"y\":219}],\"text\":\"World\"},{\"boundingBox\":[{\"x\":1170,\"y\":192},{\"x\":1306,\"y\":193},{\"x\":1306,\"y\":222},{\"x\":1169,\"y\":221}],\"text\":\"coordinate\"},{\"boundingBox\":[{\"x\":1312,\"y\":193},{\"x\":1419,\"y\":195},{\"x\":1419,\"y\":220},{\"x\":1312,\"y\":222}],\"text\":\"systems\"},{\"boundingBox\":[{\"x\":973,\"y\":243},{\"x\":993,\"y\":248},{\"x\":987,\"y\":271},{\"x\":969,\"y\":266}],\"text\":\"U\"},{\"boundingBox\":[{\"x\":624,\"y\":302},{\"x\":640,\"y\":303},{\"x\":639,\"y\":322},{\"x\":623,\"y\":321}],\"text\":\"C\"},{\"boundingBox\":[{\"x\":141,\"y\":405},{\"x\":139,\"y\":391},{\"x\":154,\"y\":389},{\"x\":156,\"y\":403}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":342,\"y\":375},{\"x\":360,\"y\":374},{\"x\":361,\"y\":395},{\"x\":343,\"y\":396}],\"text\":\"Z\"},{\"boundingBox\":[{\"x\":455,\"y\":427},{\"x\":668,\"y\":426},{\"x\":668,\"y\":454},{\"x\":455,\"y\":457}],\"text\":\"plane-coordinate\"},{\"boundingBox\":[{\"x\":674,\"y\":426},{\"x\":768,\"y\":428},{\"x\":768,\"y\":454},{\"x\":674,\"y\":454}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":876,\"y\":437},{\"x\":902,\"y\":436},{\"x\":903,\"y\":469},{\"x\":877,\"y\":470}],\"text\":\"R,\"},{\"boundingBox\":[{\"x\":909,\"y\":436},{\"x\":924,\"y\":436},{\"x\":925,\"y\":468},{\"x\":910,\"y\":469}],\"text\":\"T\"},{\"boundingBox\":[{\"x\":286,\"y\":508},{\"x\":288,\"y\":488},{\"x\":308,\"y\":490},{\"x\":306,\"y\":510}],\"text\":\"X\"},{\"boundingBox\":[{\"x\":192,\"y\":508},{\"x\":208,\"y\":509},{\"x\":207,\"y\":534},{\"x\":191,\"y\":533}],\"text\":\"Y\"},{\"boundingBox\":[{\"x\":1,\"y\":552},{\"x\":97,\"y\":552},{\"x\":97,\"y\":581},{\"x\":2,\"y\":581}],\"text\":\"Camera\"},{\"boundingBox\":[{\"x\":103,\"y\":552},{\"x\":240,\"y\":553},{\"x\":240,\"y\":581},{\"x\":103,\"y\":581}],\"text\":\"coordinate\"},{\"boundingBox\":[{\"x\":246,\"y\":553},{\"x\":353,\"y\":556},{\"x\":353,\"y\":581},{\"x\":246,\"y\":581}],\"text\":\"systems\"}]}",
        "{\"language\":\"en\",\"text\":\"C\",\"lines\":[{\"boundingBox\":[{\"x\":236,\"y\":159},{\"x\":274,\"y\":160},{\"x\":273,\"y\":204},{\"x\":236,\"y\":204}],\"text\":\"C\"}],\"words\":[{\"boundingBox\":[{\"x\":236,\"y\":159},{\"x\":272,\"y\":159},{\"x\":271,\"y\":204},{\"x\":236,\"y\":203}],\"text\":\"C\"}]}",
        "{\"language\":\"en\",\"text\":\"B\",\"lines\":[{\"boundingBox\":[{\"x\":813,\"y\":392},{\"x\":843,\"y\":390},{\"x\":844,\"y\":425},{\"x\":814,\"y\":426}],\"text\":\"B\"}],\"words\":[{\"boundingBox\":[{\"x\":813,\"y\":391},{\"x\":840,\"y\":390},{\"x\":842,\"y\":425},{\"x\":814,\"y\":426}],\"text\":\"B\"}]}",
        "{\"language\":\"en\",\"text\":\"Real world coordinates Virtual world coordinates actual environment Glasses model Scene acquisition Tracking registry Model generation system system Virtual reality synthesis system Coordinates of real Coordinates of the world cameras virtual world camera display system User UI\",\"lines\":[{\"boundingBox\":[{\"x\":18,\"y\":24},{\"x\":333,\"y\":24},{\"x\":333,\"y\":53},{\"x\":18,\"y\":53}],\"text\":\"Real world coordinates\"},{\"boundingBox\":[{\"x\":758,\"y\":23},{\"x\":1106,\"y\":24},{\"x\":1106,\"y\":53},{\"x\":758,\"y\":52}],\"text\":\"Virtual world coordinates\"},{\"boundingBox\":[{\"x\":42,\"y\":144},{\"x\":307,\"y\":144},{\"x\":307,\"y\":173},{\"x\":42,\"y\":173}],\"text\":\"actual environment\"},{\"boundingBox\":[{\"x\":833,\"y\":143},{\"x\":1033,\"y\":143},{\"x\":1033,\"y\":172},{\"x\":833,\"y\":172}],\"text\":\"Glasses model\"},{\"boundingBox\":[{\"x\":53,\"y\":244},{\"x\":295,\"y\":246},{\"x\":295,\"y\":279},{\"x\":52,\"y\":276}],\"text\":\"Scene acquisition\"},{\"boundingBox\":[{\"x\":411,\"y\":265},{\"x\":648,\"y\":267},{\"x\":647,\"y\":299},{\"x\":411,\"y\":296}],\"text\":\"Tracking registry\"},{\"boundingBox\":[{\"x\":762,\"y\":265},{\"x\":1105,\"y\":267},{\"x\":1105,\"y\":297},{\"x\":762,\"y\":295}],\"text\":\"Model generation system\"},{\"boundingBox\":[{\"x\":122,\"y\":291},{\"x\":223,\"y\":291},{\"x\":223,\"y\":313},{\"x\":122,\"y\":313}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":339,\"y\":374},{\"x\":766,\"y\":375},{\"x\":766,\"y\":407},{\"x\":339,\"y\":405}],\"text\":\"Virtual reality synthesis system\"},{\"boundingBox\":[{\"x\":28,\"y\":408},{\"x\":291,\"y\":408},{\"x\":291,\"y\":440},{\"x\":28,\"y\":441}],\"text\":\"Coordinates of real\"},{\"boundingBox\":[{\"x\":839,\"y\":406},{\"x\":1095,\"y\":405},{\"x\":1095,\"y\":437},{\"x\":839,\"y\":437}],\"text\":\"Coordinates of the\"},{\"boundingBox\":[{\"x\":62,\"y\":452},{\"x\":261,\"y\":452},{\"x\":261,\"y\":480},{\"x\":62,\"y\":479}],\"text\":\"world cameras\"},{\"boundingBox\":[{\"x\":826,\"y\":448},{\"x\":1109,\"y\":448},{\"x\":1109,\"y\":477},{\"x\":826,\"y\":476}],\"text\":\"virtual world camera\"},{\"boundingBox\":[{\"x\":455,\"y\":492},{\"x\":659,\"y\":493},{\"x\":659,\"y\":525},{\"x\":455,\"y\":524}],\"text\":\"display system\"},{\"boundingBox\":[{\"x\":521,\"y\":613},{\"x\":587,\"y\":615},{\"x\":587,\"y\":641},{\"x\":520,\"y\":640}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":840,\"y\":612},{\"x\":878,\"y\":612},{\"x\":877,\"y\":639},{\"x\":840,\"y\":638}],\"text\":\"UI\"}],\"words\":[{\"boundingBox\":[{\"x\":19,\"y\":25},{\"x\":80,\"y\":24},{\"x\":79,\"y\":54},{\"x\":19,\"y\":53}],\"text\":\"Real\"},{\"boundingBox\":[{\"x\":85,\"y\":24},{\"x\":167,\"y\":24},{\"x\":167,\"y\":54},{\"x\":85,\"y\":54}],\"text\":\"world\"},{\"boundingBox\":[{\"x\":172,\"y\":24},{\"x\":333,\"y\":25},{\"x\":333,\"y\":54},{\"x\":172,\"y\":54}],\"text\":\"coordinates\"},{\"boundingBox\":[{\"x\":761,\"y\":24},{\"x\":854,\"y\":24},{\"x\":853,\"y\":53},{\"x\":760,\"y\":53}],\"text\":\"Virtual\"},{\"boundingBox\":[{\"x\":859,\"y\":24},{\"x\":941,\"y\":24},{\"x\":941,\"y\":54},{\"x\":859,\"y\":53}],\"text\":\"world\"},{\"boundingBox\":[{\"x\":946,\"y\":24},{\"x\":1105,\"y\":25},{\"x\":1106,\"y\":54},{\"x\":946,\"y\":54}],\"text\":\"coordinates\"},{\"boundingBox\":[{\"x\":43,\"y\":147},{\"x\":126,\"y\":145},{\"x\":126,\"y\":173},{\"x\":43,\"y\":173}],\"text\":\"actual\"},{\"boundingBox\":[{\"x\":131,\"y\":145},{\"x\":307,\"y\":147},{\"x\":307,\"y\":174},{\"x\":131,\"y\":173}],\"text\":\"environment\"},{\"boundingBox\":[{\"x\":834,\"y\":143},{\"x\":936,\"y\":144},{\"x\":936,\"y\":173},{\"x\":834,\"y\":172}],\"text\":\"Glasses\"},{\"boundingBox\":[{\"x\":941,\"y\":144},{\"x\":1033,\"y\":143},{\"x\":1032,\"y\":173},{\"x\":941,\"y\":173}],\"text\":\"model\"},{\"boundingBox\":[{\"x\":54,\"y\":244},{\"x\":134,\"y\":245},{\"x\":134,\"y\":278},{\"x\":53,\"y\":277}],\"text\":\"Scene\"},{\"boundingBox\":[{\"x\":140,\"y\":245},{\"x\":295,\"y\":247},{\"x\":295,\"y\":280},{\"x\":140,\"y\":279}],\"text\":\"acquisition\"},{\"boundingBox\":[{\"x\":414,\"y\":266},{\"x\":533,\"y\":266},{\"x\":532,\"y\":298},{\"x\":414,\"y\":295}],\"text\":\"Tracking\"},{\"boundingBox\":[{\"x\":539,\"y\":266},{\"x\":647,\"y\":268},{\"x\":645,\"y\":300},{\"x\":538,\"y\":298}],\"text\":\"registry\"},{\"boundingBox\":[{\"x\":763,\"y\":266},{\"x\":851,\"y\":266},{\"x\":851,\"y\":296},{\"x\":763,\"y\":295}],\"text\":\"Model\"},{\"boundingBox\":[{\"x\":857,\"y\":266},{\"x\":1001,\"y\":267},{\"x\":1000,\"y\":297},{\"x\":857,\"y\":296}],\"text\":\"generation\"},{\"boundingBox\":[{\"x\":1006,\"y\":267},{\"x\":1102,\"y\":269},{\"x\":1102,\"y\":296},{\"x\":1006,\"y\":297}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":124,\"y\":291},{\"x\":215,\"y\":292},{\"x\":214,\"y\":314},{\"x\":125,\"y\":313}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":341,\"y\":375},{\"x\":433,\"y\":375},{\"x\":433,\"y\":406},{\"x\":342,\"y\":405}],\"text\":\"Virtual\"},{\"boundingBox\":[{\"x\":439,\"y\":374},{\"x\":527,\"y\":375},{\"x\":527,\"y\":407},{\"x\":439,\"y\":406}],\"text\":\"reality\"},{\"boundingBox\":[{\"x\":533,\"y\":375},{\"x\":660,\"y\":376},{\"x\":659,\"y\":407},{\"x\":533,\"y\":407}],\"text\":\"synthesis\"},{\"boundingBox\":[{\"x\":666,\"y\":376},{\"x\":763,\"y\":378},{\"x\":763,\"y\":406},{\"x\":665,\"y\":407}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":29,\"y\":409},{\"x\":193,\"y\":409},{\"x\":194,\"y\":441},{\"x\":29,\"y\":441}],\"text\":\"Coordinates\"},{\"boundingBox\":[{\"x\":200,\"y\":409},{\"x\":229,\"y\":409},{\"x\":230,\"y\":441},{\"x\":200,\"y\":441}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":236,\"y\":409},{\"x\":290,\"y\":408},{\"x\":291,\"y\":441},{\"x\":236,\"y\":441}],\"text\":\"real\"},{\"boundingBox\":[{\"x\":840,\"y\":407},{\"x\":1006,\"y\":406},{\"x\":1006,\"y\":438},{\"x\":840,\"y\":438}],\"text\":\"Coordinates\"},{\"boundingBox\":[{\"x\":1012,\"y\":406},{\"x\":1043,\"y\":406},{\"x\":1042,\"y\":438},{\"x\":1012,\"y\":438}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":1049,\"y\":406},{\"x\":1095,\"y\":406},{\"x\":1095,\"y\":438},{\"x\":1048,\"y\":438}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":62,\"y\":453},{\"x\":141,\"y\":452},{\"x\":141,\"y\":480},{\"x\":64,\"y\":480}],\"text\":\"world\"},{\"boundingBox\":[{\"x\":146,\"y\":452},{\"x\":262,\"y\":454},{\"x\":262,\"y\":480},{\"x\":147,\"y\":480}],\"text\":\"cameras\"},{\"boundingBox\":[{\"x\":827,\"y\":450},{\"x\":916,\"y\":449},{\"x\":916,\"y\":477},{\"x\":827,\"y\":477}],\"text\":\"virtual\"},{\"boundingBox\":[{\"x\":921,\"y\":449},{\"x\":1004,\"y\":449},{\"x\":1004,\"y\":477},{\"x\":922,\"y\":477}],\"text\":\"world\"},{\"boundingBox\":[{\"x\":1009,\"y\":449},{\"x\":1110,\"y\":451},{\"x\":1110,\"y\":478},{\"x\":1009,\"y\":477}],\"text\":\"camera\"},{\"boundingBox\":[{\"x\":456,\"y\":492},{\"x\":552,\"y\":494},{\"x\":552,\"y\":525},{\"x\":455,\"y\":525}],\"text\":\"display\"},{\"boundingBox\":[{\"x\":559,\"y\":494},{\"x\":660,\"y\":495},{\"x\":660,\"y\":524},{\"x\":558,\"y\":525}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":520,\"y\":613},{\"x\":587,\"y\":614},{\"x\":586,\"y\":641},{\"x\":520,\"y\":639}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":840,\"y\":612},{\"x\":878,\"y\":612},{\"x\":877,\"y\":639},{\"x\":840,\"y\":638}],\"text\":\"UI\"}]}",
        "{\"language\":\"en\",\"text\":\"Augmented reality glasses sales system Commodity User Collection shopping Order discount photo setting browsing of goods trolley coupon wall up commodity list Glasses photogr uploading sharing Show photos comments sharing Try on browsing screening try-on aphing of trying on and likes quickly commodity Product modification Address Order statistics and deletion management Generated\",\"lines\":[{\"boundingBox\":[{\"x\":891,\"y\":19},{\"x\":1455,\"y\":19},{\"x\":1455,\"y\":55},{\"x\":891,\"y\":56}],\"text\":\"Augmented reality glasses sales system\"},{\"boundingBox\":[{\"x\":434,\"y\":231},{\"x\":611,\"y\":232},{\"x\":610,\"y\":267},{\"x\":434,\"y\":265}],\"text\":\"Commodity\"},{\"boundingBox\":[{\"x\":659,\"y\":229},{\"x\":728,\"y\":231},{\"x\":726,\"y\":259},{\"x\":658,\"y\":256}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":790,\"y\":228},{\"x\":948,\"y\":229},{\"x\":948,\"y\":259},{\"x\":790,\"y\":258}],\"text\":\"Collection\"},{\"boundingBox\":[{\"x\":1035,\"y\":230},{\"x\":1173,\"y\":231},{\"x\":1172,\"y\":264},{\"x\":1034,\"y\":262}],\"text\":\"shopping\"},{\"boundingBox\":[{\"x\":1226,\"y\":229},{\"x\":1318,\"y\":230},{\"x\":1318,\"y\":258},{\"x\":1226,\"y\":258}],\"text\":\"Order\"},{\"boundingBox\":[{\"x\":1359,\"y\":229},{\"x\":1491,\"y\":230},{\"x\":1490,\"y\":260},{\"x\":1359,\"y\":258}],\"text\":\"discount\"},{\"boundingBox\":[{\"x\":1526,\"y\":231},{\"x\":1613,\"y\":229},{\"x\":1613,\"y\":261},{\"x\":1526,\"y\":262}],\"text\":\"photo\"},{\"boundingBox\":[{\"x\":1669,\"y\":232},{\"x\":1772,\"y\":233},{\"x\":1771,\"y\":263},{\"x\":1669,\"y\":261}],\"text\":\"setting\"},{\"boundingBox\":[{\"x\":449,\"y\":274},{\"x\":589,\"y\":278},{\"x\":588,\"y\":310},{\"x\":449,\"y\":305}],\"text\":\"browsing\"},{\"boundingBox\":[{\"x\":802,\"y\":271},{\"x\":932,\"y\":270},{\"x\":932,\"y\":304},{\"x\":802,\"y\":305}],\"text\":\"of goods\"},{\"boundingBox\":[{\"x\":1054,\"y\":270},{\"x\":1152,\"y\":272},{\"x\":1151,\"y\":305},{\"x\":1053,\"y\":303}],\"text\":\"trolley\"},{\"boundingBox\":[{\"x\":1370,\"y\":275},{\"x\":1478,\"y\":275},{\"x\":1478,\"y\":305},{\"x\":1370,\"y\":305}],\"text\":\"coupon\"},{\"boundingBox\":[{\"x\":1537,\"y\":273},{\"x\":1604,\"y\":271},{\"x\":1603,\"y\":300},{\"x\":1538,\"y\":301}],\"text\":\"wall\"},{\"boundingBox\":[{\"x\":1700,\"y\":275},{\"x\":1741,\"y\":278},{\"x\":1738,\"y\":306},{\"x\":1697,\"y\":303}],\"text\":\"up\"},{\"boundingBox\":[{\"x\":19,\"y\":444},{\"x\":183,\"y\":444},{\"x\":183,\"y\":473},{\"x\":19,\"y\":473}],\"text\":\"commodity\"},{\"boundingBox\":[{\"x\":266,\"y\":440},{\"x\":315,\"y\":442},{\"x\":315,\"y\":470},{\"x\":265,\"y\":469}],\"text\":\"list\"},{\"boundingBox\":[{\"x\":401,\"y\":441},{\"x\":515,\"y\":443},{\"x\":514,\"y\":473},{\"x\":401,\"y\":471}],\"text\":\"Glasses\"},{\"boundingBox\":[{\"x\":562,\"y\":443},{\"x\":677,\"y\":444},{\"x\":677,\"y\":476},{\"x\":562,\"y\":475}],\"text\":\"photogr\"},{\"boundingBox\":[{\"x\":712,\"y\":438},{\"x\":865,\"y\":438},{\"x\":865,\"y\":481},{\"x\":712,\"y\":482}],\"text\":\"uploading\"},{\"boundingBox\":[{\"x\":893,\"y\":443},{\"x\":1004,\"y\":446},{\"x\":1004,\"y\":477},{\"x\":892,\"y\":474}],\"text\":\"sharing\"},{\"boundingBox\":[{\"x\":1152,\"y\":434},{\"x\":1344,\"y\":435},{\"x\":1344,\"y\":468},{\"x\":1152,\"y\":467}],\"text\":\"Show photos\"},{\"boundingBox\":[{\"x\":1431,\"y\":439},{\"x\":1582,\"y\":440},{\"x\":1582,\"y\":464},{\"x\":1431,\"y\":464}],\"text\":\"comments\"},{\"boundingBox\":[{\"x\":1629,\"y\":434},{\"x\":1741,\"y\":437},{\"x\":1740,\"y\":469},{\"x\":1628,\"y\":464}],\"text\":\"sharing\"},{\"boundingBox\":[{\"x\":1802,\"y\":429},{\"x\":1900,\"y\":429},{\"x\":1900,\"y\":472},{\"x\":1803,\"y\":472}],\"text\":\"Try on\"},{\"boundingBox\":[{\"x\":33,\"y\":485},{\"x\":175,\"y\":487},{\"x\":174,\"y\":519},{\"x\":32,\"y\":516}],\"text\":\"browsing\"},{\"boundingBox\":[{\"x\":216,\"y\":487},{\"x\":360,\"y\":488},{\"x\":360,\"y\":518},{\"x\":215,\"y\":516}],\"text\":\"screening\"},{\"boundingBox\":[{\"x\":411,\"y\":489},{\"x\":501,\"y\":488},{\"x\":501,\"y\":516},{\"x\":411,\"y\":516}],\"text\":\"try-on\"},{\"boundingBox\":[{\"x\":570,\"y\":485},{\"x\":670,\"y\":486},{\"x\":669,\"y\":519},{\"x\":570,\"y\":519}],\"text\":\"aphing\"},{\"boundingBox\":[{\"x\":1161,\"y\":475},{\"x\":1331,\"y\":477},{\"x\":1331,\"y\":512},{\"x\":1161,\"y\":511}],\"text\":\"of trying on\"},{\"boundingBox\":[{\"x\":1440,\"y\":476},{\"x\":1571,\"y\":476},{\"x\":1572,\"y\":506},{\"x\":1440,\"y\":507}],\"text\":\"and likes\"},{\"boundingBox\":[{\"x\":1794,\"y\":476},{\"x\":1904,\"y\":476},{\"x\":1904,\"y\":510},{\"x\":1794,\"y\":510}],\"text\":\"quickly\"},{\"boundingBox\":[{\"x\":576,\"y\":660},{\"x\":742,\"y\":660},{\"x\":742,\"y\":692},{\"x\":576,\"y\":692}],\"text\":\"commodity\"},{\"boundingBox\":[{\"x\":792,\"y\":657},{\"x\":1095,\"y\":658},{\"x\":1095,\"y\":690},{\"x\":792,\"y\":690}],\"text\":\"Product modification\"},{\"boundingBox\":[{\"x\":1201,\"y\":658},{\"x\":1324,\"y\":659},{\"x\":1324,\"y\":690},{\"x\":1201,\"y\":689}],\"text\":\"Address\"},{\"boundingBox\":[{\"x\":1447,\"y\":658},{\"x\":1536,\"y\":660},{\"x\":1536,\"y\":690},{\"x\":1446,\"y\":688}],\"text\":\"Order\"},{\"boundingBox\":[{\"x\":593,\"y\":703},{\"x\":723,\"y\":703},{\"x\":723,\"y\":733},{\"x\":593,\"y\":733}],\"text\":\"statistics\"},{\"boundingBox\":[{\"x\":854,\"y\":701},{\"x\":1033,\"y\":701},{\"x\":1033,\"y\":733},{\"x\":854,\"y\":733}],\"text\":\"and deletion\"},{\"boundingBox\":[{\"x\":1169,\"y\":706},{\"x\":1356,\"y\":705},{\"x\":1356,\"y\":733},{\"x\":1169,\"y\":734}],\"text\":\"management\"},{\"boundingBox\":[{\"x\":1414,\"y\":699},{\"x\":1562,\"y\":699},{\"x\":1562,\"y\":732},{\"x\":1414,\"y\":732}],\"text\":\"Generated\"}],\"words\":[{\"boundingBox\":[{\"x\":892,\"y\":20},{\"x\":1054,\"y\":19},{\"x\":1054,\"y\":56},{\"x\":892,\"y\":56}],\"text\":\"Augmented\"},{\"boundingBox\":[{\"x\":1061,\"y\":19},{\"x\":1156,\"y\":19},{\"x\":1156,\"y\":56},{\"x\":1061,\"y\":56}],\"text\":\"reality\"},{\"boundingBox\":[{\"x\":1163,\"y\":19},{\"x\":1263,\"y\":19},{\"x\":1263,\"y\":56},{\"x\":1163,\"y\":56}],\"text\":\"glasses\"},{\"boundingBox\":[{\"x\":1270,\"y\":19},{\"x\":1342,\"y\":20},{\"x\":1341,\"y\":56},{\"x\":1270,\"y\":56}],\"text\":\"sales\"},{\"boundingBox\":[{\"x\":1349,\"y\":20},{\"x\":1456,\"y\":21},{\"x\":1455,\"y\":55},{\"x\":1348,\"y\":56}],\"text\":\"system\"},{\"boundingBox\":[{\"x\":435,\"y\":232},{\"x\":610,\"y\":234},{\"x\":608,\"y\":268},{\"x\":435,\"y\":265}],\"text\":\"Commodity\"},{\"boundingBox\":[{\"x\":659,\"y\":229},{\"x\":726,\"y\":231},{\"x\":725,\"y\":259},{\"x\":658,\"y\":256}],\"text\":\"User\"},{\"boundingBox\":[{\"x\":791,\"y\":229},{\"x\":948,\"y\":230},{\"x\":949,\"y\":260},{\"x\":791,\"y\":259}],\"text\":\"Collection\"},{\"boundingBox\":[{\"x\":1035,\"y\":230},{\"x\":1174,\"y\":232},{\"x\":1172,\"y\":265},{\"x\":1034,\"y\":261}],\"text\":\"shopping\"},{\"boundingBox\":[{\"x\":1228,\"y\":230},{\"x\":1319,\"y\":232},{\"x\":1318,\"y\":259},{\"x\":1227,\"y\":258}],\"text\":\"Order\"},{\"boundingBox\":[{\"x\":1360,\"y\":230},{\"x\":1492,\"y\":232},{\"x\":1491,\"y\":260},{\"x\":1360,\"y\":259}],\"text\":\"discount\"},{\"boundingBox\":[{\"x\":1526,\"y\":230},{\"x\":1611,\"y\":229},{\"x\":1611,\"y\":261},{\"x\":1526,\"y\":262}],\"text\":\"photo\"},{\"boundingBox\":[{\"x\":1669,\"y\":233},{\"x\":1772,\"y\":233},{\"x\":1770,\"y\":264},{\"x\":1670,\"y\":260}],\"text\":\"setting\"},{\"boundingBox\":[{\"x\":450,\"y\":275},{\"x\":589,\"y\":279},{\"x\":588,\"y\":311},{\"x\":449,\"y\":306}],\"text\":\"browsing\"},{\"boundingBox\":[{\"x\":803,\"y\":273},{\"x\":833,\"y\":272},{\"x\":834,\"y\":306},{\"x\":803,\"y\":306}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":839,\"y\":272},{\"x\":930,\"y\":271},{\"x\":931,\"y\":305},{\"x\":840,\"y\":306}],\"text\":\"goods\"},{\"boundingBox\":[{\"x\":1055,\"y\":270},{\"x\":1152,\"y\":273},{\"x\":1149,\"y\":306},{\"x\":1054,\"y\":302}],\"text\":\"trolley\"},{\"boundingBox\":[{\"x\":1372,\"y\":276},{\"x\":1477,\"y\":276},{\"x\":1477,\"y\":306},{\"x\":1371,\"y\":304}],\"text\":\"coupon\"},{\"boundingBox\":[{\"x\":1537,\"y\":272},{\"x\":1602,\"y\":271},{\"x\":1603,\"y\":300},{\"x\":1537,\"y\":301}],\"text\":\"wall\"},{\"boundingBox\":[{\"x\":1701,\"y\":275},{\"x\":1740,\"y\":278},{\"x\":1738,\"y\":306},{\"x\":1699,\"y\":303}],\"text\":\"up\"},{\"boundingBox\":[{\"x\":20,\"y\":445},{\"x\":184,\"y\":444},{\"x\":182,\"y\":474},{\"x\":19,\"y\":473}],\"text\":\"commodity\"},{\"boundingBox\":[{\"x\":265,\"y\":440},{\"x\":313,\"y\":441},{\"x\":312,\"y\":470},{\"x\":265,\"y\":468}],\"text\":\"list\"},{\"boundingBox\":[{\"x\":401,\"y\":441},{\"x\":515,\"y\":443},{\"x\":514,\"y\":474},{\"x\":401,\"y\":472}],\"text\":\"Glasses\"},{\"boundingBox\":[{\"x\":562,\"y\":443},{\"x\":677,\"y\":445},{\"x\":676,\"y\":477},{\"x\":563,\"y\":476}],\"text\":\"photogr\"},{\"boundingBox\":[{\"x\":713,\"y\":439},{\"x\":866,\"y\":438},{\"x\":866,\"y\":483},{\"x\":713,\"y\":483}],\"text\":\"uploading\"},{\"boundingBox\":[{\"x\":894,\"y\":443},{\"x\":1005,\"y\":446},{\"x\":1003,\"y\":478},{\"x\":893,\"y\":474}],\"text\":\"sharing\"},{\"boundingBox\":[{\"x\":1152,\"y\":435},{\"x\":1233,\"y\":435},{\"x\":1233,\"y\":469},{\"x\":1152,\"y\":467}],\"text\":\"Show\"},{\"boundingBox\":[{\"x\":1239,\"y\":435},{\"x\":1343,\"y\":437},{\"x\":1342,\"y\":468},{\"x\":1239,\"y\":469}],\"text\":\"photos\"},{\"boundingBox\":[{\"x\":1432,\"y\":440},{\"x\":1581,\"y\":440},{\"x\":1581,\"y\":465},{\"x\":1431,\"y\":464}],\"text\":\"comments\"},{\"boundingBox\":[{\"x\":1629,\"y\":435},{\"x\":1741,\"y\":439},{\"x\":1740,\"y\":470},{\"x\":1629,\"y\":465}],\"text\":\"sharing\"},{\"boundingBox\":[{\"x\":1802,\"y\":429},{\"x\":1848,\"y\":429},{\"x\":1848,\"y\":472},{\"x\":1802,\"y\":472}],\"text\":\"Try\"},{\"boundingBox\":[{\"x\":1856,\"y\":429},{\"x\":1899,\"y\":429},{\"x\":1899,\"y\":472},{\"x\":1856,\"y\":472}],\"text\":\"on\"},{\"boundingBox\":[{\"x\":33,\"y\":485},{\"x\":175,\"y\":487},{\"x\":173,\"y\":520},{\"x\":32,\"y\":515}],\"text\":\"browsing\"},{\"boundingBox\":[{\"x\":216,\"y\":487},{\"x\":360,\"y\":488},{\"x\":359,\"y\":519},{\"x\":216,\"y\":515}],\"text\":\"screening\"},{\"boundingBox\":[{\"x\":411,\"y\":489},{\"x\":501,\"y\":489},{\"x\":501,\"y\":516},{\"x\":411,\"y\":517}],\"text\":\"try-on\"},{\"boundingBox\":[{\"x\":570,\"y\":486},{\"x\":670,\"y\":486},{\"x\":669,\"y\":520},{\"x\":570,\"y\":520}],\"text\":\"aphing\"},{\"boundingBox\":[{\"x\":1162,\"y\":475},{\"x\":1188,\"y\":476},{\"x\":1187,\"y\":512},{\"x\":1161,\"y\":512}],\"text\":\"of\"},{\"boundingBox\":[{\"x\":1195,\"y\":476},{\"x\":1284,\"y\":478},{\"x\":1283,\"y\":512},{\"x\":1195,\"y\":512}],\"text\":\"trying\"},{\"boundingBox\":[{\"x\":1291,\"y\":479},{\"x\":1332,\"y\":479},{\"x\":1331,\"y\":512},{\"x\":1290,\"y\":512}],\"text\":\"on\"},{\"boundingBox\":[{\"x\":1441,\"y\":478},{\"x\":1493,\"y\":477},{\"x\":1494,\"y\":506},{\"x\":1441,\"y\":508}],\"text\":\"and\"},{\"boundingBox\":[{\"x\":1499,\"y\":477},{\"x\":1571,\"y\":477},{\"x\":1571,\"y\":507},{\"x\":1499,\"y\":506}],\"text\":\"likes\"},{\"boundingBox\":[{\"x\":1795,\"y\":477},{\"x\":1904,\"y\":477},{\"x\":1903,\"y\":511},{\"x\":1794,\"y\":510}],\"text\":\"quickly\"},{\"boundingBox\":[{\"x\":577,\"y\":662},{\"x\":742,\"y\":661},{\"x\":741,\"y\":693},{\"x\":577,\"y\":691}],\"text\":\"commodity\"},{\"boundingBox\":[{\"x\":793,\"y\":659},{\"x\":901,\"y\":658},{\"x\":901,\"y\":691},{\"x\":793,\"y\":690}],\"text\":\"Product\"},{\"boundingBox\":[{\"x\":907,\"y\":658},{\"x\":1094,\"y\":659},{\"x\":1094,\"y\":691},{\"x\":907,\"y\":691}],\"text\":\"modification\"},{\"boundingBox\":[{\"x\":1203,\"y\":659},{\"x\":1324,\"y\":661},{\"x\":1323,\"y\":690},{\"x\":1202,\"y\":690}],\"text\":\"Address\"},{\"boundingBox\":[{\"x\":1446,\"y\":658},{\"x\":1536,\"y\":660},{\"x\":1535,\"y\":690},{\"x\":1446,\"y\":688}],\"text\":\"Order\"},{\"boundingBox\":[{\"x\":593,\"y\":704},{\"x\":724,\"y\":704},{\"x\":724,\"y\":734},{\"x\":593,\"y\":734}],\"text\":\"statistics\"},{\"boundingBox\":[{\"x\":854,\"y\":703},{\"x\":906,\"y\":702},{\"x\":906,\"y\":733},{\"x\":855,\"y\":733}],\"text\":\"and\"},{\"boundingBox\":[{\"x\":912,\"y\":702},{\"x\":1032,\"y\":701},{\"x\":1033,\"y\":734},{\"x\":912,\"y\":733}],\"text\":\"deletion\"},{\"boundingBox\":[{\"x\":1170,\"y\":708},{\"x\":1356,\"y\":706},{\"x\":1356,\"y\":734},{\"x\":1170,\"y\":733}],\"text\":\"management\"},{\"boundingBox\":[{\"x\":1415,\"y\":701},{\"x\":1560,\"y\":700},{\"x\":1561,\"y\":733},{\"x\":1414,\"y\":731}],\"text\":\"Generated\"}]}",
        "{\"language\":\"en\",\"text\":\"Start Import Image Input the training face sample Is there a face? Image normalization, Calculate the average face SVM classitier SIFT Extract local features Is there a face? SDM Iterative Solution Feature extraction Face Alignment Feature point Face rotation Calculate the perspective relationship The lens selection Angle between 2D and 3D coordinates library Shoot the The glasses 3d effect Transparency perspective picking try-on effect overlap the face design treatment transformation out a lens End\",\"lines\":[{\"boundingBox\":[{\"x\":320,\"y\":17},{\"x\":390,\"y\":18},{\"x\":390,\"y\":47},{\"x\":319,\"y\":46}],\"text\":\"Start\"},{\"boundingBox\":[{\"x\":254,\"y\":140},{\"x\":451,\"y\":142},{\"x\":450,\"y\":175},{\"x\":254,\"y\":172}],\"text\":\"Import Image\"},{\"boundingBox\":[{\"x\":696,\"y\":122},{\"x\":944,\"y\":123},{\"x\":943,\"y\":160},{\"x\":696,\"y\":158}],\"text\":\"Input the training\"},{\"boundingBox\":[{\"x\":737,\"y\":164},{\"x\":906,\"y\":167},{\"x\":905,\"y\":200},{\"x\":737,\"y\":197}],\"text\":\"face sample\"},{\"boundingBox\":[{\"x\":238,\"y\":301},{\"x\":452,\"y\":302},{\"x\":451,\"y\":335},{\"x\":238,\"y\":333}],\"text\":\"Is there a face?\"},{\"boundingBox\":[{\"x\":671,\"y\":283},{\"x\":970,\"y\":283},{\"x\":970,\"y\":318},{\"x\":671,\"y\":318}],\"text\":\"Image normalization,\"},{\"boundingBox\":[{\"x\":634,\"y\":322},{\"x\":1007,\"y\":325},{\"x\":1007,\"y\":359},{\"x\":633,\"y\":357}],\"text\":\"Calculate the average face\"},{\"boundingBox\":[{\"x\":246,\"y\":443},{\"x\":464,\"y\":444},{\"x\":464,\"y\":477},{\"x\":246,\"y\":476}],\"text\":\"SVM classitier\"},{\"boundingBox\":[{\"x\":627,\"y\":458},{\"x\":1012,\"y\":461},{\"x\":1012,\"y\":493},{\"x\":627,\"y\":490}],\"text\":\"SIFT Extract local features\"},{\"boundingBox\":[{\"x\":217,\"y\":577},{\"x\":450,\"y\":579},{\"x\":450,\"y\":612},{\"x\":217,\"y\":610}],\"text\":\"Is there a face?\"},{\"boundingBox\":[{\"x\":650,\"y\":594},{\"x\":985,\"y\":595},{\"x\":985,\"y\":627},{\"x\":650,\"y\":625}],\"text\":\"SDM Iterative Solution\"},{\"boundingBox\":[{\"x\":222,\"y\":713},{\"x\":486,\"y\":715},{\"x\":486,\"y\":745},{\"x\":222,\"y\":743}],\"text\":\"Feature extraction\"},{\"boundingBox\":[{\"x\":700,\"y\":713},{\"x\":931,\"y\":715},{\"x\":931,\"y\":747},{\"x\":699,\"y\":745}],\"text\":\"Face Alignment\"},{\"boundingBox\":[{\"x\":76,\"y\":880},{\"x\":277,\"y\":880},{\"x\":277,\"y\":914},{\"x\":76,\"y\":914}],\"text\":\"Feature point\"},{\"boundingBox\":[{\"x\":397,\"y\":886},{\"x\":583,\"y\":886},{\"x\":583,\"y\":916},{\"x\":397,\"y\":915}],\"text\":\"Face rotation\"},{\"boundingBox\":[{\"x\":671,\"y\":880},{\"x\":1193,\"y\":880},{\"x\":1193,\"y\":917},{\"x\":671,\"y\":915}],\"text\":\"Calculate the perspective relationship\"},{\"boundingBox\":[{\"x\":1253,\"y\":890},{\"x\":1377,\"y\":891},{\"x\":1377,\"y\":922},{\"x\":1253,\"y\":920}],\"text\":\"The lens\"},{\"boundingBox\":[{\"x\":111,\"y\":925},{\"x\":240,\"y\":923},{\"x\":240,\"y\":951},{\"x\":112,\"y\":954}],\"text\":\"selection\"},{\"boundingBox\":[{\"x\":448,\"y\":927},{\"x\":533,\"y\":929},{\"x\":531,\"y\":958},{\"x\":446,\"y\":958}],\"text\":\"Angle\"},{\"boundingBox\":[{\"x\":708,\"y\":922},{\"x\":1156,\"y\":923},{\"x\":1155,\"y\":958},{\"x\":708,\"y\":956}],\"text\":\"between 2D and 3D coordinates\"},{\"boundingBox\":[{\"x\":1268,\"y\":931},{\"x\":1362,\"y\":932},{\"x\":1362,\"y\":963},{\"x\":1268,\"y\":961}],\"text\":\"library\"},{\"boundingBox\":[{\"x\":38,\"y\":1043},{\"x\":176,\"y\":1046},{\"x\":175,\"y\":1077},{\"x\":37,\"y\":1075}],\"text\":\"Shoot the\"},{\"boundingBox\":[{\"x\":275,\"y\":1044},{\"x\":441,\"y\":1046},{\"x\":440,\"y\":1080},{\"x\":275,\"y\":1077}],\"text\":\"The glasses\"},{\"boundingBox\":[{\"x\":519,\"y\":1042},{\"x\":647,\"y\":1044},{\"x\":646,\"y\":1075},{\"x\":518,\"y\":1072}],\"text\":\"3d effect\"},{\"boundingBox\":[{\"x\":707,\"y\":1044},{\"x\":892,\"y\":1048},{\"x\":892,\"y\":1079},{\"x\":707,\"y\":1074}],\"text\":\"Transparency\"},{\"boundingBox\":[{\"x\":998,\"y\":1046},{\"x\":1163,\"y\":1044},{\"x\":1163,\"y\":1075},{\"x\":999,\"y\":1079}],\"text\":\"perspective\"},{\"boundingBox\":[{\"x\":1264,\"y\":1048},{\"x\":1377,\"y\":1049},{\"x\":1377,\"y\":1082},{\"x\":1264,\"y\":1080}],\"text\":\"picking\"},{\"boundingBox\":[{\"x\":16,\"y\":1089},{\"x\":198,\"y\":1087},{\"x\":199,\"y\":1117},{\"x\":17,\"y\":1120}],\"text\":\"try-on effect\"},{\"boundingBox\":[{\"x\":245,\"y\":1088},{\"x\":472,\"y\":1087},{\"x\":472,\"y\":1119},{\"x\":245,\"y\":1119}],\"text\":\"overlap the face\"},{\"boundingBox\":[{\"x\":535,\"y\":1084},{\"x\":632,\"y\":1087},{\"x\":630,\"y\":1119},{\"x\":534,\"y\":1117}],\"text\":\"design\"},{\"boundingBox\":[{\"x\":730,\"y\":1089},{\"x\":868,\"y\":1089},{\"x\":868,\"y\":1117},{\"x\":730,\"y\":1117}],\"text\":\"treatment\"},{\"boundingBox\":[{\"x\":976,\"y\":1087},{\"x\":1185,\"y\":1085},{\"x\":1186,\"y\":1116},{\"x\":976,\"y\":1118}],\"text\":\"transformation\"},{\"boundingBox\":[{\"x\":1243,\"y\":1088},{\"x\":1385,\"y\":1087},{\"x\":1386,\"y\":1116},{\"x\":1243,\"y\":1117}],\"text\":\"out a lens\"},{\"boundingBox\":[{\"x\":76,\"y\":1208},{\"x\":137,\"y\":1208},{\"x\":137,\"y\":1236},{\"x\":76,\"y\":1237}],\"text\":\"End\"}],\"words\":[{\"boundingBox\":[{\"x\":319,\"y\":17},{\"x\":389,\"y\":18},{\"x\":388,\"y\":47},{\"x\":319,\"y\":46}],\"text\":\"Start\"},{\"boundingBox\":[{\"x\":254,\"y\":140},{\"x\":351,\"y\":142},{\"x\":351,\"y\":175},{\"x\":255,\"y\":172}],\"text\":\"Import\"},{\"boundingBox\":[{\"x\":357,\"y\":142},{\"x\":450,\"y\":143},{\"x\":449,\"y\":176},{\"x\":357,\"y\":175}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":696,\"y\":124},{\"x\":770,\"y\":123},{\"x\":770,\"y\":158},{\"x\":696,\"y\":157}],\"text\":\"Input\"},{\"boundingBox\":[{\"x\":777,\"y\":123},{\"x\":820,\"y\":123},{\"x\":819,\"y\":158},{\"x\":776,\"y\":158}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":827,\"y\":123},{\"x\":943,\"y\":124},{\"x\":941,\"y\":161},{\"x\":826,\"y\":158}],\"text\":\"training\"},{\"boundingBox\":[{\"x\":738,\"y\":165},{\"x\":793,\"y\":165},{\"x\":793,\"y\":199},{\"x\":738,\"y\":196}],\"text\":\"face\"},{\"boundingBox\":[{\"x\":800,\"y\":165},{\"x\":905,\"y\":168},{\"x\":902,\"y\":199},{\"x\":799,\"y\":199}],\"text\":\"sample\"},{\"boundingBox\":[{\"x\":239,\"y\":302},{\"x\":264,\"y\":302},{\"x\":264,\"y\":333},{\"x\":238,\"y\":332}],\"text\":\"Is\"},{\"boundingBox\":[{\"x\":270,\"y\":302},{\"x\":341,\"y\":303},{\"x\":340,\"y\":335},{\"x\":270,\"y\":333}],\"text\":\"there\"},{\"boundingBox\":[{\"x\":347,\"y\":303},{\"x\":368,\"y\":303},{\"x\":367,\"y\":335},{\"x\":346,\"y\":335}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":374,\"y\":303},{\"x\":452,\"y\":303},{\"x\":452,\"y\":335},{\"x\":373,\"y\":335}],\"text\":\"face?\"},{\"boundingBox\":[{\"x\":672,\"y\":285},{\"x\":755,\"y\":284},{\"x\":754,\"y\":319},{\"x\":672,\"y\":317}],\"text\":\"Image\"},{\"boundingBox\":[{\"x\":761,\"y\":284},{\"x\":970,\"y\":284},{\"x\":969,\"y\":318},{\"x\":761,\"y\":319}],\"text\":\"normalization,\"},{\"boundingBox\":[{\"x\":635,\"y\":323},{\"x\":766,\"y\":324},{\"x\":765,\"y\":359},{\"x\":634,\"y\":356}],\"text\":\"Calculate\"},{\"boundingBox\":[{\"x\":772,\"y\":324},{\"x\":818,\"y\":325},{\"x\":818,\"y\":359},{\"x\":772,\"y\":359}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":825,\"y\":325},{\"x\":936,\"y\":325},{\"x\":936,\"y\":359},{\"x\":824,\"y\":359}],\"text\":\"average\"},{\"boundingBox\":[{\"x\":942,\"y\":325},{\"x\":1008,\"y\":326},{\"x\":1008,\"y\":358},{\"x\":942,\"y\":359}],\"text\":\"face\"},{\"boundingBox\":[{\"x\":247,\"y\":444},{\"x\":321,\"y\":445},{\"x\":320,\"y\":476},{\"x\":246,\"y\":476}],\"text\":\"SVM\"},{\"boundingBox\":[{\"x\":330,\"y\":445},{\"x\":465,\"y\":445},{\"x\":464,\"y\":478},{\"x\":329,\"y\":476}],\"text\":\"classitier\"},{\"boundingBox\":[{\"x\":628,\"y\":459},{\"x\":700,\"y\":459},{\"x\":700,\"y\":491},{\"x\":628,\"y\":491}],\"text\":\"SIFT\"},{\"boundingBox\":[{\"x\":707,\"y\":459},{\"x\":808,\"y\":460},{\"x\":808,\"y\":492},{\"x\":706,\"y\":491}],\"text\":\"Extract\"},{\"boundingBox\":[{\"x\":815,\"y\":460},{\"x\":885,\"y\":460},{\"x\":884,\"y\":492},{\"x\":814,\"y\":492}],\"text\":\"local\"},{\"boundingBox\":[{\"x\":892,\"y\":460},{\"x\":1012,\"y\":461},{\"x\":1010,\"y\":494},{\"x\":890,\"y\":492}],\"text\":\"features\"},{\"boundingBox\":[{\"x\":236,\"y\":578},{\"x\":261,\"y\":578},{\"x\":260,\"y\":611},{\"x\":234,\"y\":610}],\"text\":\"Is\"},{\"boundingBox\":[{\"x\":268,\"y\":578},{\"x\":339,\"y\":580},{\"x\":338,\"y\":612},{\"x\":266,\"y\":611}],\"text\":\"there\"},{\"boundingBox\":[{\"x\":346,\"y\":580},{\"x\":365,\"y\":580},{\"x\":364,\"y\":612},{\"x\":345,\"y\":612}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":371,\"y\":580},{\"x\":451,\"y\":580},{\"x\":451,\"y\":611},{\"x\":370,\"y\":612}],\"text\":\"face?\"},{\"boundingBox\":[{\"x\":650,\"y\":594},{\"x\":727,\"y\":595},{\"x\":728,\"y\":626},{\"x\":651,\"y\":625}],\"text\":\"SDM\"},{\"boundingBox\":[{\"x\":736,\"y\":595},{\"x\":853,\"y\":595},{\"x\":854,\"y\":627},{\"x\":736,\"y\":626}],\"text\":\"Iterative\"},{\"boundingBox\":[{\"x\":859,\"y\":595},{\"x\":983,\"y\":596},{\"x\":984,\"y\":627},{\"x\":860,\"y\":627}],\"text\":\"Solution\"},{\"boundingBox\":[{\"x\":224,\"y\":713},{\"x\":330,\"y\":715},{\"x\":329,\"y\":745},{\"x\":222,\"y\":744}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":336,\"y\":715},{\"x\":485,\"y\":715},{\"x\":486,\"y\":746},{\"x\":335,\"y\":745}],\"text\":\"extraction\"},{\"boundingBox\":[{\"x\":701,\"y\":713},{\"x\":770,\"y\":714},{\"x\":769,\"y\":746},{\"x\":700,\"y\":744}],\"text\":\"Face\"},{\"boundingBox\":[{\"x\":776,\"y\":714},{\"x\":930,\"y\":716},{\"x\":930,\"y\":747},{\"x\":775,\"y\":747}],\"text\":\"Alignment\"},{\"boundingBox\":[{\"x\":77,\"y\":881},{\"x\":183,\"y\":881},{\"x\":184,\"y\":915},{\"x\":77,\"y\":913}],\"text\":\"Feature\"},{\"boundingBox\":[{\"x\":190,\"y\":881},{\"x\":277,\"y\":880},{\"x\":278,\"y\":914},{\"x\":190,\"y\":915}],\"text\":\"point\"},{\"boundingBox\":[{\"x\":399,\"y\":887},{\"x\":463,\"y\":886},{\"x\":461,\"y\":916},{\"x\":397,\"y\":916}],\"text\":\"Face\"},{\"boundingBox\":[{\"x\":468,\"y\":886},{\"x\":583,\"y\":887},{\"x\":582,\"y\":917},{\"x\":467,\"y\":916}],\"text\":\"rotation\"},{\"boundingBox\":[{\"x\":672,\"y\":881},{\"x\":800,\"y\":880},{\"x\":800,\"y\":916},{\"x\":672,\"y\":914}],\"text\":\"Calculate\"},{\"boundingBox\":[{\"x\":807,\"y\":880},{\"x\":852,\"y\":880},{\"x\":851,\"y\":916},{\"x\":806,\"y\":916}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":858,\"y\":880},{\"x\":1019,\"y\":880},{\"x\":1018,\"y\":917},{\"x\":858,\"y\":916}],\"text\":\"perspective\"},{\"boundingBox\":[{\"x\":1025,\"y\":880},{\"x\":1194,\"y\":881},{\"x\":1193,\"y\":917},{\"x\":1025,\"y\":917}],\"text\":\"relationship\"},{\"boundingBox\":[{\"x\":1255,\"y\":891},{\"x\":1310,\"y\":891},{\"x\":1311,\"y\":922},{\"x\":1256,\"y\":921}],\"text\":\"The\"},{\"boundingBox\":[{\"x\":1316,\"y\":891},{\"x\":1377,\"y\":892},{\"x\":1378,\"y\":922},{\"x\":1317,\"y\":922}],\"text\":\"lens\"},{\"boundingBox\":[{\"x\":112,\"y\":927},{\"x\":240,\"y\":924},{\"x\":240,\"y\":952},{\"x\":112,\"y\":954}],\"text\":\"selection\"},{\"boundingBox\":[{\"x\":446,\"y\":927},{\"x\":533,\"y\":928},{\"x\":532,\"y\":959},{\"x\":446,\"y\":958}],\"text\":\"Angle\"},{\"boundingBox\":[{\"x\":708,\"y\":924},{\"x\":826,\"y\":923},{\"x\":826,\"y\":956},{\"x\":710,\"y\":957}],\"text\":\"between\"},{\"boundingBox\":[{\"x\":832,\"y\":923},{\"x\":875,\"y\":923},{\"x\":875,\"y\":956},{\"x\":833,\"y\":956}],\"text\":\"2D\"},{\"boundingBox\":[{\"x\":881,\"y\":923},{\"x\":934,\"y\":923},{\"x\":935,\"y\":956},{\"x\":882,\"y\":956}],\"text\":\"and\"},{\"boundingBox\":[{\"x\":941,\"y\":923},{\"x\":983,\"y\":923},{\"x\":983,\"y\":957},{\"x\":941,\"y\":956}],\"text\":\"3D\"},{\"boundingBox\":[{\"x\":990,\"y\":923},{\"x\":1156,\"y\":925},{\"x\":1155,\"y\":959},{\"x\":990,\"y\":957}],\"text\":\"coordinates\"},{\"boundingBox\":[{\"x\":1269,\"y\":932},{\"x\":1362,\"y\":932},{\"x\":1361,\"y\":964},{\"x\":1268,\"y\":961}],\"text\":\"library\"},{\"boundingBox\":[{\"x\":39,\"y\":1044},{\"x\":121,\"y\":1046},{\"x\":120,\"y\":1077},{\"x\":38,\"y\":1075}],\"text\":\"Shoot\"},{\"boundingBox\":[{\"x\":127,\"y\":1046},{\"x\":177,\"y\":1046},{\"x\":176,\"y\":1078},{\"x\":126,\"y\":1077}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":276,\"y\":1045},{\"x\":330,\"y\":1046},{\"x\":330,\"y\":1079},{\"x\":276,\"y\":1077}],\"text\":\"The\"},{\"boundingBox\":[{\"x\":336,\"y\":1047},{\"x\":441,\"y\":1047},{\"x\":440,\"y\":1079},{\"x\":336,\"y\":1079}],\"text\":\"glasses\"},{\"boundingBox\":[{\"x\":520,\"y\":1042},{\"x\":554,\"y\":1043},{\"x\":553,\"y\":1074},{\"x\":519,\"y\":1072}],\"text\":\"3d\"},{\"boundingBox\":[{\"x\":560,\"y\":1044},{\"x\":645,\"y\":1044},{\"x\":645,\"y\":1075},{\"x\":559,\"y\":1074}],\"text\":\"effect\"},{\"boundingBox\":[{\"x\":707,\"y\":1044},{\"x\":893,\"y\":1048},{\"x\":891,\"y\":1079},{\"x\":708,\"y\":1075}],\"text\":\"Transparency\"},{\"boundingBox\":[{\"x\":999,\"y\":1048},{\"x\":1163,\"y\":1045},{\"x\":1163,\"y\":1076},{\"x\":999,\"y\":1080}],\"text\":\"perspective\"},{\"boundingBox\":[{\"x\":1265,\"y\":1049},{\"x\":1378,\"y\":1052},{\"x\":1377,\"y\":1082},{\"x\":1265,\"y\":1081}],\"text\":\"picking\"},{\"boundingBox\":[{\"x\":17,\"y\":1089},{\"x\":104,\"y\":1089},{\"x\":104,\"y\":1120},{\"x\":18,\"y\":1120}],\"text\":\"try-on\"},{\"boundingBox\":[{\"x\":110,\"y\":1089},{\"x\":198,\"y\":1087},{\"x\":197,\"y\":1117},{\"x\":110,\"y\":1120}],\"text\":\"effect\"},{\"boundingBox\":[{\"x\":246,\"y\":1088},{\"x\":348,\"y\":1088},{\"x\":347,\"y\":1120},{\"x\":245,\"y\":1120}],\"text\":\"overlap\"},{\"boundingBox\":[{\"x\":355,\"y\":1088},{\"x\":401,\"y\":1088},{\"x\":400,\"y\":1120},{\"x\":354,\"y\":1120}],\"text\":\"the\"},{\"boundingBox\":[{\"x\":407,\"y\":1088},{\"x\":472,\"y\":1088},{\"x\":471,\"y\":1120},{\"x\":406,\"y\":1120}],\"text\":\"face\"},{\"boundingBox\":[{\"x\":534,\"y\":1084},{\"x\":631,\"y\":1086},{\"x\":630,\"y\":1119},{\"x\":534,\"y\":1116}],\"text\":\"design\"},{\"boundingBox\":[{\"x\":730,\"y\":1089},{\"x\":868,\"y\":1089},{\"x\":869,\"y\":1118},{\"x\":730,\"y\":1117}],\"text\":\"treatment\"},{\"boundingBox\":[{\"x\":977,\"y\":1088},{\"x\":1185,\"y\":1085},{\"x\":1185,\"y\":1117},{\"x\":976,\"y\":1117}],\"text\":\"transformation\"},{\"boundingBox\":[{\"x\":1246,\"y\":1089},{\"x\":1293,\"y\":1089},{\"x\":1293,\"y\":1117},{\"x\":1246,\"y\":1116}],\"text\":\"out\"},{\"boundingBox\":[{\"x\":1298,\"y\":1089},{\"x\":1318,\"y\":1088},{\"x\":1318,\"y\":1117},{\"x\":1298,\"y\":1117}],\"text\":\"a\"},{\"boundingBox\":[{\"x\":1323,\"y\":1088},{\"x\":1385,\"y\":1087},{\"x\":1384,\"y\":1116},{\"x\":1323,\"y\":1117}],\"text\":\"lens\"},{\"boundingBox\":[{\"x\":76,\"y\":1208},{\"x\":136,\"y\":1208},{\"x\":136,\"y\":1237},{\"x\":76,\"y\":1237}],\"text\":\"End\"}]}",
        "{\"language\":\"en\",\"text\":\"Published online: 27 November 2018\",\"lines\":[{\"boundingBox\":[{\"x\":0,\"y\":16},{\"x\":1062,\"y\":17},{\"x\":1062,\"y\":69},{\"x\":0,\"y\":69}],\"text\":\"Published online: 27 November 2018\"}],\"words\":[{\"boundingBox\":[{\"x\":1,\"y\":17},{\"x\":281,\"y\":17},{\"x\":280,\"y\":70},{\"x\":0,\"y\":70}],\"text\":\"Published\"},{\"boundingBox\":[{\"x\":291,\"y\":17},{\"x\":500,\"y\":17},{\"x\":499,\"y\":70},{\"x\":291,\"y\":70}],\"text\":\"online:\"},{\"boundingBox\":[{\"x\":510,\"y\":17},{\"x\":585,\"y\":17},{\"x\":585,\"y\":70},{\"x\":510,\"y\":70}],\"text\":\"27\"},{\"boundingBox\":[{\"x\":595,\"y\":17},{\"x\":903,\"y\":18},{\"x\":903,\"y\":70},{\"x\":595,\"y\":70}],\"text\":\"November\"},{\"boundingBox\":[{\"x\":913,\"y\":18},{\"x\":1060,\"y\":18},{\"x\":1060,\"y\":70},{\"x\":913,\"y\":70}],\"text\":\"2018\"}]}",
        "{\"language\":\"en\",\"text\":\"-108 PmBY 19:18 @¥( 100%* 15:48 < # AGLASSES = GLASSES Rayban xfett RB33 ... . - ...... *FEWQrb3025 112 ... ¥ 450 ¥ 560 2 3 5 6 7 8 O i - & @ .? ! ABC space Done V ¥ 1010\",\"lines\":[{\"boundingBox\":[{\"x\":465,\"y\":6},{\"x\":549,\"y\":6},{\"x\":549,\"y\":19},{\"x\":465,\"y\":19}],\"text\":\"-108 PmBY\"},{\"boundingBox\":[{\"x\":1145,\"y\":2},{\"x\":1175,\"y\":2},{\"x\":1175,\"y\":14},{\"x\":1145,\"y\":14}],\"text\":\"19:18\"},{\"boundingBox\":[{\"x\":1218,\"y\":2},{\"x\":1321,\"y\":2},{\"x\":1321,\"y\":17},{\"x\":1218,\"y\":16}],\"text\":\"@¥( 100%*\"},{\"boundingBox\":[{\"x\":1686,\"y\":2},{\"x\":1710,\"y\":2},{\"x\":1711,\"y\":13},{\"x\":1686,\"y\":12}],\"text\":\"15:48\"},{\"boundingBox\":[{\"x\":4,\"y\":23},{\"x\":25,\"y\":22},{\"x\":26,\"y\":42},{\"x\":5,\"y\":43}],\"text\":\"<\"},{\"boundingBox\":[{\"x\":92,\"y\":25},{\"x\":199,\"y\":24},{\"x\":199,\"y\":41},{\"x\":92,\"y\":41}],\"text\":\"# AGLASSES\"},{\"boundingBox\":[{\"x\":471,\"y\":31},{\"x\":492,\"y\":30},{\"x\":493,\"y\":44},{\"x\":472,\"y\":45}],\"text\":\"=\"},{\"boundingBox\":[{\"x\":586,\"y\":28},{\"x\":676,\"y\":28},{\"x\":676,\"y\":46},{\"x\":586,\"y\":46}],\"text\":\"GLASSES\"},{\"boundingBox\":[{\"x\":1588,\"y\":102},{\"x\":1719,\"y\":102},{\"x\":1719,\"y\":116},{\"x\":1588,\"y\":116}],\"text\":\"Rayban xfett RB33 ...\"},{\"boundingBox\":[{\"x\":485,\"y\":192},{\"x\":508,\"y\":191},{\"x\":508,\"y\":208},{\"x\":485,\"y\":209}],\"text\":\".\"},{\"boundingBox\":[{\"x\":1645,\"y\":191},{\"x\":1669,\"y\":192},{\"x\":1668,\"y\":210},{\"x\":1644,\"y\":209}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":124,\"y\":221},{\"x\":170,\"y\":220},{\"x\":170,\"y\":231},{\"x\":124,\"y\":231}],\"text\":\"......\"},{\"boundingBox\":[{\"x\":478,\"y\":252},{\"x\":605,\"y\":253},{\"x\":605,\"y\":266},{\"x\":478,\"y\":266}],\"text\":\"*FEWQrb3025 112 ...\"},{\"boundingBox\":[{\"x\":710,\"y\":240},{\"x\":774,\"y\":238},{\"x\":775,\"y\":260},{\"x\":710,\"y\":262}],\"text\":\"¥ 450\"},{\"boundingBox\":[{\"x\":1735,\"y\":325},{\"x\":1808,\"y\":325},{\"x\":1808,\"y\":341},{\"x\":1735,\"y\":340}],\"text\":\"¥ 560\"},{\"boundingBox\":[{\"x\":37,\"y\":374},{\"x\":52,\"y\":374},{\"x\":51,\"y\":388},{\"x\":37,\"y\":389}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":65,\"y\":375},{\"x\":78,\"y\":375},{\"x\":78,\"y\":387},{\"x\":64,\"y\":388}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":127,\"y\":375},{\"x\":138,\"y\":375},{\"x\":138,\"y\":387},{\"x\":126,\"y\":387}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":156,\"y\":376},{\"x\":167,\"y\":375},{\"x\":167,\"y\":387},{\"x\":156,\"y\":388}],\"text\":\"6\"},{\"boundingBox\":[{\"x\":185,\"y\":375},{\"x\":198,\"y\":375},{\"x\":198,\"y\":387},{\"x\":185,\"y\":387}],\"text\":\"7\"},{\"boundingBox\":[{\"x\":214,\"y\":375},{\"x\":226,\"y\":375},{\"x\":226,\"y\":387},{\"x\":214,\"y\":388}],\"text\":\"8\"},{\"boundingBox\":[{\"x\":272,\"y\":389},{\"x\":273,\"y\":373},{\"x\":285,\"y\":375},{\"x\":283,\"y\":390}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1039,\"y\":387},{\"x\":1057,\"y\":388},{\"x\":1056,\"y\":407},{\"x\":1038,\"y\":406}],\"text\":\"i\"},{\"boundingBox\":[{\"x\":1816,\"y\":387},{\"x\":1840,\"y\":386},{\"x\":1841,\"y\":397},{\"x\":1816,\"y\":398}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":211,\"y\":416},{\"x\":228,\"y\":414},{\"x\":229,\"y\":433},{\"x\":212,\"y\":435}],\"text\":\"&\"},{\"boundingBox\":[{\"x\":240,\"y\":417},{\"x\":256,\"y\":417},{\"x\":256,\"y\":433},{\"x\":240,\"y\":433}],\"text\":\"@\"},{\"boundingBox\":[{\"x\":89,\"y\":451},{\"x\":250,\"y\":449},{\"x\":250,\"y\":483},{\"x\":89,\"y\":484}],\"text\":\".? !\"},{\"boundingBox\":[{\"x\":23,\"y\":501},{\"x\":49,\"y\":501},{\"x\":49,\"y\":513},{\"x\":23,\"y\":513}],\"text\":\"ABC\"},{\"boundingBox\":[{\"x\":126,\"y\":502},{\"x\":164,\"y\":502},{\"x\":164,\"y\":514},{\"x\":126,\"y\":514}],\"text\":\"space\"},{\"boundingBox\":[{\"x\":239,\"y\":501},{\"x\":274,\"y\":500},{\"x\":273,\"y\":513},{\"x\":238,\"y\":514}],\"text\":\"Done\"},{\"boundingBox\":[{\"x\":1550,\"y\":497},{\"x\":1571,\"y\":496},{\"x\":1571,\"y\":510},{\"x\":1550,\"y\":512}],\"text\":\"V\"},{\"boundingBox\":[{\"x\":1713,\"y\":491},{\"x\":1770,\"y\":491},{\"x\":1770,\"y\":505},{\"x\":1713,\"y\":506}],\"text\":\"¥ 1010\"}],\"words\":[{\"boundingBox\":[{\"x\":465,\"y\":8},{\"x\":490,\"y\":7},{\"x\":490,\"y\":19},{\"x\":465,\"y\":19}],\"text\":\"-108\"},{\"boundingBox\":[{\"x\":492,\"y\":7},{\"x\":550,\"y\":6},{\"x\":550,\"y\":20},{\"x\":492,\"y\":19}],\"text\":\"PmBY\"},{\"boundingBox\":[{\"x\":1146,\"y\":2},{\"x\":1175,\"y\":2},{\"x\":1175,\"y\":14},{\"x\":1146,\"y\":14}],\"text\":\"19:18\"},{\"boundingBox\":[{\"x\":1219,\"y\":2},{\"x\":1258,\"y\":2},{\"x\":1258,\"y\":15},{\"x\":1219,\"y\":16}],\"text\":\"@¥(\"},{\"boundingBox\":[{\"x\":1261,\"y\":2},{\"x\":1321,\"y\":3},{\"x\":1321,\"y\":18},{\"x\":1261,\"y\":15}],\"text\":\"100%*\"},{\"boundingBox\":[{\"x\":1686,\"y\":2},{\"x\":1711,\"y\":2},{\"x\":1711,\"y\":13},{\"x\":1686,\"y\":12}],\"text\":\"15:48\"},{\"boundingBox\":[{\"x\":8,\"y\":23},{\"x\":24,\"y\":22},{\"x\":25,\"y\":42},{\"x\":9,\"y\":43}],\"text\":\"<\"},{\"boundingBox\":[{\"x\":93,\"y\":25},{\"x\":108,\"y\":25},{\"x\":108,\"y\":42},{\"x\":93,\"y\":42}],\"text\":\"#\"},{\"boundingBox\":[{\"x\":111,\"y\":25},{\"x\":199,\"y\":25},{\"x\":200,\"y\":41},{\"x\":111,\"y\":42}],\"text\":\"AGLASSES\"},{\"boundingBox\":[{\"x\":471,\"y\":31},{\"x\":486,\"y\":30},{\"x\":487,\"y\":44},{\"x\":472,\"y\":45}],\"text\":\"=\"},{\"boundingBox\":[{\"x\":587,\"y\":29},{\"x\":676,\"y\":29},{\"x\":675,\"y\":46},{\"x\":587,\"y\":47}],\"text\":\"GLASSES\"},{\"boundingBox\":[{\"x\":1589,\"y\":105},{\"x\":1635,\"y\":103},{\"x\":1634,\"y\":117},{\"x\":1588,\"y\":116}],\"text\":\"Rayban\"},{\"boundingBox\":[{\"x\":1637,\"y\":103},{\"x\":1672,\"y\":103},{\"x\":1671,\"y\":117},{\"x\":1637,\"y\":117}],\"text\":\"xfett\"},{\"boundingBox\":[{\"x\":1674,\"y\":103},{\"x\":1702,\"y\":103},{\"x\":1702,\"y\":117},{\"x\":1674,\"y\":117}],\"text\":\"RB33\"},{\"boundingBox\":[{\"x\":1704,\"y\":103},{\"x\":1719,\"y\":104},{\"x\":1719,\"y\":117},{\"x\":1704,\"y\":117}],\"text\":\"...\"},{\"boundingBox\":[{\"x\":485,\"y\":192},{\"x\":502,\"y\":191},{\"x\":503,\"y\":208},{\"x\":486,\"y\":209}],\"text\":\".\"},{\"boundingBox\":[{\"x\":1646,\"y\":191},{\"x\":1664,\"y\":192},{\"x\":1664,\"y\":210},{\"x\":1646,\"y\":209}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":125,\"y\":221},{\"x\":170,\"y\":221},{\"x\":169,\"y\":231},{\"x\":124,\"y\":231}],\"text\":\"......\"},{\"boundingBox\":[{\"x\":479,\"y\":252},{\"x\":564,\"y\":253},{\"x\":564,\"y\":266},{\"x\":479,\"y\":267}],\"text\":\"*FEWQrb3025\"},{\"boundingBox\":[{\"x\":566,\"y\":253},{\"x\":585,\"y\":254},{\"x\":585,\"y\":266},{\"x\":566,\"y\":266}],\"text\":\"112\"},{\"boundingBox\":[{\"x\":588,\"y\":254},{\"x\":605,\"y\":254},{\"x\":605,\"y\":266},{\"x\":588,\"y\":266}],\"text\":\"...\"},{\"boundingBox\":[{\"x\":710,\"y\":241},{\"x\":721,\"y\":240},{\"x\":722,\"y\":262},{\"x\":711,\"y\":263}],\"text\":\"¥\"},{\"boundingBox\":[{\"x\":726,\"y\":240},{\"x\":775,\"y\":239},{\"x\":774,\"y\":261},{\"x\":726,\"y\":262}],\"text\":\"450\"},{\"boundingBox\":[{\"x\":1765,\"y\":325},{\"x\":1774,\"y\":325},{\"x\":1774,\"y\":341},{\"x\":1765,\"y\":341}],\"text\":\"¥\"},{\"boundingBox\":[{\"x\":1777,\"y\":325},{\"x\":1808,\"y\":326},{\"x\":1808,\"y\":341},{\"x\":1777,\"y\":341}],\"text\":\"560\"},{\"boundingBox\":[{\"x\":38,\"y\":374},{\"x\":51,\"y\":374},{\"x\":52,\"y\":388},{\"x\":39,\"y\":389}],\"text\":\"2\"},{\"boundingBox\":[{\"x\":70,\"y\":375},{\"x\":77,\"y\":375},{\"x\":78,\"y\":387},{\"x\":71,\"y\":388}],\"text\":\"3\"},{\"boundingBox\":[{\"x\":126,\"y\":375},{\"x\":138,\"y\":375},{\"x\":138,\"y\":387},{\"x\":126,\"y\":387}],\"text\":\"5\"},{\"boundingBox\":[{\"x\":156,\"y\":376},{\"x\":166,\"y\":375},{\"x\":167,\"y\":386},{\"x\":156,\"y\":387}],\"text\":\"6\"},{\"boundingBox\":[{\"x\":187,\"y\":375},{\"x\":198,\"y\":375},{\"x\":198,\"y\":387},{\"x\":187,\"y\":387}],\"text\":\"7\"},{\"boundingBox\":[{\"x\":214,\"y\":375},{\"x\":225,\"y\":375},{\"x\":225,\"y\":388},{\"x\":214,\"y\":388}],\"text\":\"8\"},{\"boundingBox\":[{\"x\":272,\"y\":388},{\"x\":273,\"y\":376},{\"x\":285,\"y\":377},{\"x\":283,\"y\":389}],\"text\":\"O\"},{\"boundingBox\":[{\"x\":1043,\"y\":387},{\"x\":1057,\"y\":388},{\"x\":1056,\"y\":407},{\"x\":1042,\"y\":406}],\"text\":\"i\"},{\"boundingBox\":[{\"x\":1816,\"y\":386},{\"x\":1827,\"y\":386},{\"x\":1827,\"y\":398},{\"x\":1816,\"y\":398}],\"text\":\"-\"},{\"boundingBox\":[{\"x\":211,\"y\":416},{\"x\":227,\"y\":414},{\"x\":229,\"y\":433},{\"x\":213,\"y\":435}],\"text\":\"&\"},{\"boundingBox\":[{\"x\":242,\"y\":417},{\"x\":256,\"y\":417},{\"x\":256,\"y\":433},{\"x\":242,\"y\":433}],\"text\":\"@\"},{\"boundingBox\":[{\"x\":95,\"y\":455},{\"x\":163,\"y\":452},{\"x\":163,\"y\":481},{\"x\":95,\"y\":481}],\"text\":\".?\"},{\"boundingBox\":[{\"x\":180,\"y\":451},{\"x\":205,\"y\":451},{\"x\":205,\"y\":482},{\"x\":180,\"y\":481}],\"text\":\"!\"},{\"boundingBox\":[{\"x\":23,\"y\":501},{\"x\":49,\"y\":501},{\"x\":49,\"y\":513},{\"x\":23,\"y\":513}],\"text\":\"ABC\"},{\"boundingBox\":[{\"x\":127,\"y\":503},{\"x\":164,\"y\":503},{\"x\":164,\"y\":514},{\"x\":127,\"y\":515}],\"text\":\"space\"},{\"boundingBox\":[{\"x\":239,\"y\":501},{\"x\":273,\"y\":500},{\"x\":273,\"y\":513},{\"x\":240,\"y\":514}],\"text\":\"Done\"},{\"boundingBox\":[{\"x\":1558,\"y\":496},{\"x\":1570,\"y\":496},{\"x\":1571,\"y\":509},{\"x\":1559,\"y\":510}],\"text\":\"V\"},{\"boundingBox\":[{\"x\":1718,\"y\":491},{\"x\":1729,\"y\":491},{\"x\":1729,\"y\":506},{\"x\":1718,\"y\":506}],\"text\":\"¥\"},{\"boundingBox\":[{\"x\":1732,\"y\":491},{\"x\":1770,\"y\":491},{\"x\":1770,\"y\":506},{\"x\":1732,\"y\":506}],\"text\":\"1010\"}]}"
      ]
    }
  ]
}